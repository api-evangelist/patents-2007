---

title: Techniques for use with device drivers in a common software environment
abstract: Described are techniques for facilitating communication between device drivers. A device driver is provided that makes at least one call using an application programming interface. The application programming interface facilitates communication between the device driver module and a target code module. The application programming interface provides support for performing the at least one call from a first execution mode associated with the device driver module and a second execution mode associated with the target code module. The application programming interface provides support for user space to user space communication, user space to kernel space communication, kernel space to kernel space communication, and kernel to user space communication. The first execution mode of the device driver module and said second execution mode of the target code module are determined. A communication path between the modules is established in accordance with said first execution mode and said second execution mode.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07950022&OS=07950022&RS=07950022
owner: EMC Corporation
number: 07950022
owner_city: Hopkinton
owner_country: US
publication_date: 20070629
---
This application generally relates to data storage and more particularly to techniques used for data storage configuration.

Computer systems may include different resources used by one or more host processors. Resources and host processors in a computer system may be interconnected by one or more communication connections. These resources may include for example data storage devices such as those included in the data storage systems manufactured by EMC Corporation. These data storage systems may be coupled to one or more servers or host processors and provide storage services to each host processor. Multiple data storage systems from one or more different vendors may be connected and may provide common data storage for one or more host processors in a computer system.

A host processor may perform a variety of data processing tasks and operations using the data storage system. For example a host processor may perform basic system operations in connection with data requests such as data read and write operations.

Host processor systems may store and retrieve data using a storage device containing a plurality of host interface units disk drives and disk interface units. The host systems access the storage device through a plurality of channels provided therewith. Host systems provide data and access control information through the channels to the storage device and the storage device provides data to the host systems also through the channels. The host systems do not address the disk drives of the storage device directly but rather access what appears to the host systems as a plurality of logical disk units. The logical disk units may or may not correspond to the actual disk drives. Allowing multiple host systems to access the single storage device unit allows the host systems to share data in the device. In order to facilitate sharing of the data on the device additional software on the data storage systems may also be used.

A data storage system environment may consist of a wide variety of different hardware and software. For example a data storage system may use a variety of different operating systems hardware platforms file systems and the like. Problems may arise in connection with development of new code modules as well as for existing code modules intended for execution on data storage systems in order for the code modules to be usable in the different environments.

Thus it may be desirable to utilize a flexible architecture and framework which allows a same code module to be used in the variety of different data storage system environments.

In accordance with one aspect of the invention is a computer implemented method for facilitating communication between device drivers comprising providing a device driver module making at least one call using an application programming interface the application programming interface facilitating communication between the device driver module and a target code module wherein the application programming interface provides support for performing the at least one call from a first execution mode associated with the device driver module and a second execution mode associated with the target code module said application programming interface providing support for user space to user space communication user space to kernel space communication kernel space to kernel space communication and kernel to user space communication determining by code of the application programming interface said first execution mode of the device driver module and said second execution mode of the target code module wherein said first execution mode and said second execution mode are one of a user execution mode or a kernel execution mode and establishing using code of the application programming interface a communication path between said device driver module and said target code module in accordance with said first execution mode and said second execution mode. The device driver module may perform a first call using the application programming interface to register a first interrupt service routine to be invoked upon the occurrence of an interrupt to a device associated with said device driver. The first interrupt service routine may execute in said kernel execution mode. An interrupt may occur causing said first interrupt service routine to be invoked. The first interrupt service routine may perform processing including acknowledging said interrupt saving device state information regarding said interrupt occurring for said device and performing a second call using said application programming interface for scheduling a deferred procedure call for execution. If said device driver module is executing in said user execution mode when said deferred procedure call executes said deferred procedure call may perform processing to signal a wait thread to resume execution and the wait thread may execute in said user execution mode. The wait thread and the device driver module may execute in a same user address space. When the wait thread resumes execution the wait thread may perform processing including invoking a second interrupt service routine included in said device driver module. The second interrupt service routine may performs processing including servicing said interrupt. The second interrupt service routine may perform processing including scheduling a deferred procedure call for execution to perform additional processing to service said interrupt. If the device driver module is executing in the kernel execution mode when the deferred procedure call executes the deferred procedure call may perform processing to invoke a second interrupt service routine included in said device driver module. The second interrupt service routine may perform processing including scheduling a deferred procedure call for execution to perform additional processing to service said interrupt. When the device driver module including said interrupt service routine is executing in user execution mode the second call may perform processing to emulate deferred procedure call processing and wherein when said device driver module including said interrupt service routine is executing in kernel execution mode said second call may perform processing including using an operating system primitive for scheduling a deferred procedure call. When the wait thread resumes execution the wait thread may perform processing including taking a write lock prior to performing said invoking and releasing said write lock after performing said invoking said write lock being used to emulate enabling and disabling interrupt processing wherein if said wait thread is unable to take said write lock interrupt processing is disabled and if said wait thread is able to take said write lock interrupt processing is enabled. The same device driver module may be used when said device driver executes in said user execution mode and when said device driver executes in said kernel execution mode.

In accordance with another aspect of the invention is a data storage system comprising a computer readable medium with executable code stored thereon for facilitating communication between device drivers the computer readable medium comprising executable code for providing a device driver module making at least one call using an application programming interface the application programming interface facilitating communication between the device driver module and a target code module wherein the application programming interface provides support for performing the at least one call from a first execution mode associated with the device driver module and a second execution mode associated with the target code module said application programming interface providing support for user space to user space communication user space to kernel space communication kernel space to kernel space communication and kernel to user space communication determining by code of the application programming interface said first execution mode of the device driver module and said second execution mode of the target code module wherein said first execution mode and said second execution mode are one of a user execution mode or a kernel execution mode and establishing using code of the application programming interface a communication path between said device driver module and said target code module in accordance with said first execution mode and said second execution mode. The data storage system may include a plurality of processors each of said processors executing a different instance of said device driver module. The data storage system may include a plurality of instances of said device driver module each of said instances executing in user execution mode and each of said instances having its own user process address space. Upon failure of a first of a first of said instances a second of said instances may handle processing of requests currently being serviced by said first instance when said first instance failed.

With the growing popularity of all types of data storage devices there is also a growing demand for software and features for data storage devices. However developing software components for the devices is a difficult task because storage devices operate under constraints which at least in some cases are distinct or prioritized differently from those imposed on other types of computing systems.

For example data storage devices require solutions to different sets of problems. A wide variety of data storage hardware solutions are available in the market. The solutions require significant efforts from software developers to provide high performance and reliability and other desired storage features and to integrate them with software solutions that would present to the end customers easy and friendly user interfaces. In addition providers of hardware solutions are challenged to provide reasonable hardware to software interface mechanisms.

In many cases these constraints have resulted in providing largely static and non expandable programming environments for data storage devices. The programming environments for these devices also tend to lack a common or standard interface to handle the integration of software components in a data storage environment. Thus the creation of component oriented software is rendered difficult and becomes a custom solution. Accordingly conventional programming and testing environments for such devices present a substantial obstacle to software developers for such devices. Adding functionality to the operating system of a storage device can be difficult. Adding the same functionality to a storage device having a different operating system may require in general not only a different set of function calls and programming methods but a different programming environment altogether.

Examples of conventional methods providing platform independence include the CORBA architecture and Sun Microsystems Java. A CORBA architecture employs a middle layer called Object Request Broker ORB to facilitate integration of software objects. The middle layer requires memory and a CPU s processing power.

A conventional Java architecture employs a virtual machine which provides platform independence at run time. A virtual machine facilitates different object components to find each other and the object components interact with each other via the virtual machine. Because object components interact and execute via the virtual machine versus execution of native code of the underlying processor the processing speed is noticeably slowed down in a Java architecture. In addition the virtual machine requires a large amount of memory and only executes code in user space. Furthermore a software developer is required to use the Java language and thus needs to expend a large amount of time and effort to become versatile in using a Java system. In addition a large amount of legacy code written in non Java language becomes unavailable in a Java architecture.

It is desirable to have flexible and platform independent programming environments for storage devices especially given the growing demand for storage devices having a variety of different data storage system environments.

As described at least in part below a storage software platform architecture can be provided that converges and leverages existing platform capabilities and technologies with other assets to provide a sustainable advantage.

In at least some implementations the architecture allows developers to focus on the customer experience and quality improved product scalability reliability and availability innovation in response to customer need development of best of breed products and solutions product line breadth and enterprise and data center technologies. In at least some implementations the architecture also facilitates development and or improvement in key areas such as convergence and leverage ease of use channel readiness consistency and flexibility application awareness storage solutions and services success at the lower end of the market and efficiency productivity and focus of development resources.

In at least one aspect the architecture is or includes a scalable common architecture that can be extended across many technical and industry dimensions and that takes into account that performance considerations vary that availability and quality concerns may be high but have different complexities that security is constant but with perimeter versus internal security priorities varying and that many different topologies exist. In at least one implementation the architecture is or includes a unified architecture for integrated management of network attached storage NAS and object and storage block services.

The architecture may include features such as openness application awareness ease of use and management partner enablement scaling globalization enhanced platform architecture and enhanced availability and reliability. Openness may rely on and or leverage proprietary and third party technologies for accessibility and user interface. Application awareness may include automated discovery application provisioning and self management. Ease of use and management may include a unified user experience total lifecycle coverage self management and active communities. Partner enablement may include features that facilitate sales channels and OEM arrangements. Scaling may include a range from small and medium size businesses to enterprise and may include scaling up and scaling out. Globalization may include fully internationalized systems with localized user interface screens and behavior. Enhanced platform architecture may include modular building blocks and well defined interfaces. Enhanced availability and reliability may include fault domains and autonomous management.

At least one implementation of the architecture takes into account that from a high level perspective many different storage platforms have many of the same features such as moving data from one I O chip to memory to another I O chip high availability clustering peer to peer replication and drive management and such platforms also support similar interface protocols transformations and methods. However if such platforms have significantly varying implementations and external interfaces and little commonality development involves significant duplication of functionality and work and it can be difficult to move technology or techniques from platform to platform share or reuse technology or techniques combine technology or techniques from different platforms together or with new applications or otherwise avoid doing the same work multiple times. For example if a new feature or new standard is needed the new feature or standard must be implemented separately for each platform.

A convergence oriented common software environment based on the architecture takes into account different base architectural assumptions different terminology for similar concepts different behaviors or expressions for similar features different high availability different clustering scaling and non destructive upgrade models different wire protocols e.g. replication mainframe and different management interfaces and look and feel interfaces. As a result the environment takes into account different software environments different base operating systems dictating hardware and different hardware dictating base operating systems.

Thus the common software environment enables mechanical commonality as a prelude to enabling architectural commonality with the results that the value of developed technology increases commonality increases it takes less work to maintain the same base of functions or add features flexibility increases the ability to effect rapid change is improved technology and techniques are freed from existing mechanical then architectural constraints the ability to combine existing technology and techniques with new technology and techniques in new ways increases lost opportunity costs are regained resources are freed up to refactor and rationalize rather than rewrite or discard current technology or techniques the underlying basics of technology is preserved enabling virtualization code is strengthened by preserving field experience development testing and support are made more efficient and reliability is improved.

Referring to shown is an example of an embodiment of a computer system that may be used in connection with performing the techniques described herein. The computer system includes one or more data storage systems connected to server or host systems through communication medium . The system also includes a management system connected to one or more data storage systems through communication medium . In this embodiment of the computer system the management system and the N servers or hosts may access the data storage systems for example in performing input output I O operations data requests and other operations. The communication medium may be any one or more of a variety of networks or other type of communication connections as known to those skilled in the art. Each of the communication mediums and may be a network connection bus and or other type of data link such as a hardwire or other connections known in the art. For example the communication medium may be the Internet an intranet network or other wireless or other hardwired connection s by which the host systems may access and communicate with the data storage systems and may also communicate with other components not shown that may be included in the computer system . In one embodiment the communication medium may be a LAN connection and the communication medium may be an iSCSI or fibre channel connection.

Each of the host systems and the data storage systems included in the computer system may be connected to the communication medium by any one of a variety of connections as may be provided and supported in accordance with the type of communication medium . Similarly the management system may be connected to the communication medium by any one of variety of connections in accordance with the type of communication medium . The processors included in the host computer systems and management system may be any one of a variety of proprietary or commercially available single or multi processor system such as an Intel based processor or other type of commercially available processor able to support traffic in accordance with each particular embodiment and application.

It should be noted that the particular examples of the hardware and software that may be included in the data storage systems are described herein in more detail and may vary with each particular embodiment. Each of the host computers the management system and data storage systems may all be located at the same physical site or alternatively may also be located in different physical locations. In connection with communication mediums and a variety of different communication protocols may be used such as SCSI Fibre Channel iSCSI and the like. Some or all of the connections by which the hosts management system and data storage system may be connected to their respective communication medium may pass through other communication devices such as a Connectrix or other switching equipment that may exist such as a phone line a repeater a multiplexer or even a satellite. In one embodiment the hosts may communicate with the data storage systems over an iSCSI or a fibre channel connection and the management system may communicate with the data storage systems over a separate network connection using TCP IP. It should be noted that although illustrates communications between the hosts and data storage systems being over a first connection and communications between the management system and the data storage systems being over a second different connection an embodiment may also use the same connection. The particular type and number of connections may vary in accordance with particulars of each embodiment.

Each of the host computer systems may perform different types of data operations in accordance with different types of tasks. In the embodiment of any one of the host computers may issue a data request to the data storage systems to perform a data operation. For example an application executing on one of the host computers may perform a read or write operation resulting in one or more data requests to the data storage systems .

The management system may be used in connection with management of the data storage systems . The management system may include hardware and or software components. The management system may include one or more computer processors connected to one or more I O devices such as for example a display or other output device and an input device such as for example a keyboard mouse and the like. A data storage system manager may for example view information about a current storage volume configuration on a display device of the management system .

In one embodiment the one or more data storage systems of may be an appliance with hardware and software for hosting the data storage of the one or more applications executing on the hosts . The appliance may include one or more storage processors and one or more devices upon which data is stored. The appliance may include software used in connection with storing the data of the hosts on the appliance and also software used in connection with techniques described in following paragraphs which are part of a common software environment.

In another embodiment the data storage systems may include one or more data storage systems such as one or more of the data storage systems offered by EMC Corporation of Hopkinton Mass. Each of the data storage systems may include one or more data storage devices such as disks. One or more data storage systems may be manufactured by one or more different vendors. Each of the data storage systems included in may be inter connected not shown . Additionally the data storage systems may also be connected to the host systems through any one or more communication connections that may vary with each particular embodiment and device in accordance with the different protocols used in a particular embodiment. The type of communication connection used may vary with certain system parameters and requirements such as those related to bandwidth and throughput required in accordance with a rate of I O requests as may be issued by the host computer systems for example to the data storage systems . It should be noted that each of the data storage systems may operate stand alone or may also be included as part of a storage area network SAN that includes for example other components such as other data storage systems. Each of the data storage systems may include a plurality of disk devices or volumes. The particular data storage systems and examples as described herein for purposes of illustration should not be construed as a limitation. Other types of commercially available data storage systems as well as processors and hardware controlling access to these particular devices may also be included in an embodiment.

In such an embodiment in which element of is implemented using one or more data storage systems each of the data storage systems may include code thereon for performing the techniques as described herein for the common software environment.

Servers or host systems such as provide data and access control information through channels to the storage systems and the storage systems may also provide data to the host systems also through the channels. The host systems may not address the disk drives of the storage systems directly but rather access to data may be provided to one or more host systems from what the host systems view as a plurality of logical devices or logical volumes LVs . The LVs may or may not correspond to the actual disk drives. For example one or more LVs may reside on a single physical disk drive. Data in a single storage system may be accessed by multiple hosts allowing the hosts to share the data residing therein. An LV or LUN logical unit number may be used to refer to the foregoing logically defined devices or volumes.

In following paragraphs reference may be made to a particular embodiment such as for example an embodiment in which element of is an appliance as described above. However it will be appreciated by those skilled in the art that this is for purposes of illustration and should not be construed as a limitation of the techniques herein.

The common software environment may include components described herein executing on each data storage system. Each of the data storage systems may have any one of a variety of different hardware and software platforms. For example a first data storage system may include the common software environment with a first operating system and underlying hardware. A second data storage system may include the common software environment with a different operating system and different underlying hardware.

The common software environment includes a framework which may be implemented using APIs application programming interface and other code modules described herein. The APIs may implement the underlying functionality which varies with the different possible data storage system hardware and software platforms. As such code may be written using the APIs so that the code is insulated from the underlying platform dependencies. The code may be executed on any data storage system utilizing the APIs regardless of the particular hardware and or software platform of the data storage system. Additionally the API may be written so that the code is allowed to execute in user space or kernel space as will be described in more detail herein. As such the API may utilize the underlying primitives of the particular operating system or may also emulate functionality on an operating system lacking a particular feature. A code module using the API can also execute in user mode or kernel mode on a supported operating system. For example a code module may make a first API call on a data storage system having a first operating system. For the first operating system the API may implement the first API call utilizing the underlying primitives of the first operating system. The code module may also be executed on another data storage system having a second different operating system. For the second operating system the first API call may be implemented using the primitives of the second operating system. The second operating system may not have a rich or full set of primitives so the API may emulate the necessary functionality of the primitives missing from the second operating system. The API uses the underlying operating system primitives where available and may otherwise synthesize or emulate the functionality necessary as may vary with the capabilities of each operating system. The code module may also execute in user or kernel mode on the first and second operating systems.

Referring to shown is an example of components that may be executing on a processor node of a data storage system. If a data storage system has multiple processors illustrates components that may be executed by each such processor. In the example shown are user mode or user space and kernel mode or kernel space with different entities executing in each mode. As known in the art code executing in the kernel mode may be characterized as a privileged execution mode with unrestricted access to system memory and hardware devices. Operating system code typically executes in kernel mode. In contrast code executing in user mode may be characterized as a non privileged mode of execution with restricted access to the system memory and hardware devices. In the example elements and may be user space processes or containers each having their own process address space. Thus each user space process may be characterized as a single container or fault domain for fault containment purposes. In other words each user process has its own state and can have an execution fault independent of or isolated from the other user processes. Thus when one of the user processes experiences a fault the other user processes may continue to execute without being affected by the fault. When a first of the executing processes is notified of the failing process by the kernel helper or other notification means the first process may also notify other executing user and or kernel space modules. The first process or other currently executing user space process may perform processing on behalf of the failing process and may perform cleanup associated with the failing process. In one embodiment each user process can save information about its own state in an area of memory external to the process so that another instance of the same user process can perform cleanup resume processing of the failed process and the like. Additionally a currently executing user space process may take steps in response to the failing process in accordance with any outstanding requests or processing being performed by the failing process on behalf of the currently executing process. For example a first process may reissue its request previously made to a failing user process to another user process instance performing the same services or functionality as the failing process. In contrast all code executing in the kernel mode may execute in the context of the same address space so that if a fault occurs during execution of a kernel mode process or thread the operating system may experience a failure. Thus all the code executing in kernel mode may be characterized as a single kernel fault domain or container in contrast to each instance of and executing in user mode . Typically code such as device drivers execute in kernel mode. As will be described in following paragraphs using the common software environment herein a device driver using APIs which implement user and kernel mode variations of necessary operations can execute in both user and kernel mode without modification to the original source code. In other words for a given API call any coding difference in implementing the API call when executing in user or kernel mode different operating system or other data storage system environment particular may be embedded in the code of the API.

In the example each of the user mode processes and may use the same API . Code executing in the kernel space such as software component or module may also utilize the same API . The underlying details of implementing the functionality of the API call are embedded in the API code and not the code associated with and . Using the API an embodiment may make a same set of functionality available to code that executes in both user and kernel space and leaves the implementation details of the API calls to be included in the API code. The API may provide services to kernel space code which are implemented using and may be otherwise only available to code executing in user space. Similarly the API may provide services to user space code which are implemented using and may be otherwise only available to code executing in kernel space.

To facilitate communications between code executing in user mode and code executing in kernel mode user helper module and kernel helper module may be used in combination with the API. In one embodiment module may maintain a list of the currently active containers or processes in user space along with the services performed by each. Similarly module may maintain a list of the software components executing in kernel mode and the services performed by each. Process may make a call using the API for a service or to access a data structure available using code executing in kernel mode. For example a user space process may allocate physical memory and may utilize API for this allocation request. The API may determine that this functionality is performed by code executing in kernel mode and may communicate with the kernel helper module as illustrated by . The kernel helper module then contacts the appropriate code in kernel space such as to perform the requested operation. Any return information from the requested operation such as an output parameter may be returned to process using the same communication path through which the requested operation was initiated. Alternatively the module may be used to facilitate establishing a communication path directly from the calling module and the called target module . Communications may then occur directly between and rather than through the helper module . The kernel helper module may be characterized as a proxy module for processes executing in user space. The API has code included therein which determines whether the requested operation is done by calling code executing in user mode or kernel mode. If the API code determines that the requested operation is to be done in kernel mode the helper module is invoked as just described. Otherwise the code of the API performs a call to the appropriate module executing in user mode such as for example illustrated by

As a variation to the foregoing a user process may have knowledge about the user or kernel space code which performs a particular service is responsible for maintaining a particular data structure and the like. For example the name of the user space or kernel space code e.g. module name to be invoked may be coded in the module of the calling user space process passed as a parameter and the like. As such the invoking user space process such as performs an API call and may pass as a parameter the name of the module to be invoked. The API may determine whether the name of the module to be invoked resides in user or kernel space. If the module being invoked resides in user space the API establishes a communication path from the initiator or calling user process to the code module being invoked. From there the calling user process and the called user process may communicate directly to perform the requested service of the calling user process. If the module being invoked resides in kernel space the API communicates with the kernel helper module to establish a communication path from the initiator or calling process in user space and the called code module in kernel space. From there the calling user process and the called kernel space code module may communicate directly to perform the requested service. In the latter embodiment or variation the initiator has knowledge regarding which module to call and the services performed by the called modules but uses the kernel helper module where appropriate to establish a communication path between the initiator and the called module. This is in contrast to having the initiator request a particular service data structure and the like and utilizing the API and kernel helper module to determine the correct module to invoke.

Referring to shown is an example illustrating how the user helper module facilitates processing of kernel mode requests. In a manner similar to as described in connection with the user helper module may be characterized as a proxy module for code executing in kernel space. As an example to illustrate the API may include a call to load a module. The code to load a module may be included in user space process . Kernel code may use the API to make a call to load a library. Code implementing the API call may make a determination as to whether the requested operation is performed by code executing in kernel mode or user mode. With respect to module if the API determines that the call is performed by invoking code executing in user space the API of contacts the user helper module as illustrated by . In turn the module examines its registry of user processes to determine the appropriate user process to contact. In this example the module determines that user process performs the requested function and issues the request to to perform the requested operation. Any return information such as an output parameter may be communicated to using the same communication path through the user helper module which initiated the request. Alternatively the module may be used to facilitate establishing a communication path directly from the calling module and the called target module . Communications may then occur directly between and rather than through the helper module . The API has code included therein which determines whether the requested operation is done by calling code executing in user mode or kernel mode. With respect to element executing in kernel mode if the API code determines that the requested operation is to be done in user mode the helper module is invoked as just described. Otherwise the code of the API performs a call to the appropriate component executing in kernel mode such as for example illustrated by

As a variation to the foregoing kernel space code may have knowledge about the user or kernel space code which performs a particular service is responsible for maintaining a particular data structure and the like. For example the name of the user space or kernel space code e.g. module name to be invoked may be coded in the module of the calling kernel space code passed as a parameter and the like. As such the invoking kernel space code such as performs an API call and may pass as a parameter the name of the module to be invoked. The API may determine whether the name of the module to be invoked resides in user or kernel space. If the module being invoked resides in kernel space the API establishes a communication path from the initiator or calling kernel space code module to the other kernel code module being invoked. From there the calling kernel code module and the called kernel code module may communicate directly to perform the requested service. If the module being invoked resides in user space the API communicates with the user helper module to establish a communication path from the initiator or calling code module in kernel space to the called code module in user space. From there the calling kernel code module and the called user space code module may communicate directly to perform the requested service. Thus in this latter variation the user helper module helps to establish a communication path between the initiator kernel code module and the called user code module so that the foregoing two modules can communicate directly to perform the requested service of the initiator. In the latter embodiment the initiator has knowledge regarding which module to call and the services performed by the called modules but uses the user helper module where appropriate to establish a communication path between the initiator and the called module. This is in contrast to having the initiator request a particular service data structure and the like and utilizing the API and user helper module to determine the correct module to invoke.

In connection with the foregoing an embodiment may execute multiple instances of the same user space container on the same data storage system processor. If a first instance of the user space container experiences a fault or other failure another instance of the same user space container may be started using previously saved state information from the failing container. A process may save its own context or state information in memory or a file as it executes. As such processing can be resumed from the point of failure by the another instance of the same container using the saved state information. The saved state information may vary with the code executed and may include for example information about a request from another module being serviced by the executing code. Additionally the same instance of the user space container can be restarted and resume processing from the point of failure. This combined with the ability to execute a same code module in user or kernel mode provides a flexible framework affording many different uses and advantages. For example a device driver or other code module typically executed in kernel mode may alternatively be executed in user mode with the ability to have multiple instances for use on failure as described. As another example during development of code that will execute in kernel mode the code modules may be developed and executed in the user mode to facilitate debugging. At a later point once debugging is complete the code may be executed in kernel mode unmodified.

As described above the common software environment may include the API and other code modules to implement the framework providing the user kernel portability as well as portability among different hardware and software platforms e.g. different operating systems data storage systems and underlying hardware and the like . The common software environment may include other code provided as a layer between the API and operating system specific code for example to facilitate communications with devices.

As described above the same API may be used by a code module when the code module is executed in user space kernel space on different data storage systems having different environments such as different operating system and or processor architecture. The code module may make API calls so that the API implements the same set of API calls to facilitate user space to kernel space user space to user space kernel space to kernel space and kernel space to user space communications as appropriate depending on the execution mode of the calling or invoking code and that of the target or called code. The API handles the appropriate communication between the calling and called code modules. As described above if the calling and called code modules are in two different modes e.g. one in user mode and one in kernel mode then the API utilizes the appropriate helper module to facilitate communications between the calling and called code modules. Thus a module coded using the API as described herein may be executed in user mode or kernel mode unmodified. Furthermore the same module may be executed on different data storage systems having different data storage system environments provided the particular data storage system environment is supported by the API. Thus processing dependencies that may vary with user or kernel mode as well as operating system and underlying processor architecture may be handled by the API code so that a module utilizing the API as described herein may be executed in a variety of different data storage system environments as well as user or kernel mode.

What will now be described is an example illustrating how the techniques herein may be used in connection with a device driver so that the device driver can execute in user mode or kernel mode. In some instances it may be desirable to execute a driver is user mode for example if the driver does not need to be shared among multiple processes. Otherwise it may be more desirable to have the driver execute in kernel mode because sharing may be implemented more efficiently using code executing in kernel mode than user mode. Also by executing code in user mode multiple instances of the device driver can be available as described above so that if a first instance of the driver fails a second instance may be used to resume executing from point of failure. The second instance may also be used in connection with processing other requests directed to the device driver. Also there may be multiple storage processors SPs and each SP may execute an instance of a device driver for servicing requests.

Following paragraphs and figures describe use of the techniques herein with the device driver executing in kernel mode and the device driver executing in user mode.

Referring to shown is an example illustrating registering or attaching a device driver in kernel mode. Attaching may be characterized as the process in which registers an ISR interrupt service routine . Once registered the ISR is invoked by the operating system to service any subsequent interrupts for the device.

In connection with the techniques herein at system startup the device driver methods module is registered as the owner of those devices which are going to be used by the API. The module may be characterized as code that is part of the common software environment which is layered on top of the underlying operating system software that communicates with the devices. The module interfaces with the API to communicate with the devices.

Executing in kernel mode the device driver of module may issue an API call as illustrated by Sto attach the device driver to a device. The attach API call results in API code determining whether to communicate with the user mode helper module to perform the requested attach call. In this instance the API code determines that the requested attach operation is performed as illustrated by Sby communicating with the module to perform the attach operation in kernel mode. The module invokes the operating system primitive to attach the ISR so that the ISR is invoked for servicing any faults or interrupts for the device. The attach results in the hardware table of interrupt vectors being updated to indicate that ISR is invoked for servicing any subsequent interrupts for the device.

It should be noted that in accordance with the techniques herein the code of module including the ISR may be implemented using a same API as will be used by the driver when executed in user mode as described below.

Referring now to shown is an example illustrating processing when an interrupt occurs for the attached kernel mode device driver from . Upon the occurrence of an interrupt the ISR code is invoked to service the interrupt. As known in the art interrupts may occur for a variety of reasons. For example if the driver is controlling the device the device may interrupt when a requested operation has been completed resulting in invocation of the ISR for servicing the device. Servicing the interrupt may include for example setting appropriate device registers to acknowledge the interrupt and queuing a deferred procedure call DPC to perform processing to complete the work of the signaled interrupt or queue a DPC to schedule a thread which performs processing to service the interrupt. The DPC call within the ISR code may be an API call DPC which implements the DPC. The API may implement the DPC by making the appropriate calls into code in kernel mode. The DPC may be characterized as a scheduled kernel mode callback which is performed at a later time. The DPC may be performed at a higher priority than other scheduled entities and is executed after any higher priority tasks.

The ISR is coded using API calls to implement the DPC operation so that the same ISR code may be ported for execution in user mode with the API handling the necessary processing to implement the DPC functionality.

Referring now to shown is an example of the techniques herein used in connection with performing an attach operation to register the device driver in user mode rather than kernel mode as illustrated in . The example illustrates the user mode device driver executing an attach API call resulting in invoking API code as illustrated by S. In this instance the API code determines that the requested attach operation is performed as illustrated by Sby first spawning wait thread illustrated by S. The wait thread includes code which executes an API call to block or stop execution of the thread until signaled to resume execution by other code described in more detail in following paragraphs. The block operation may be an API call used for synchronizing execution between thread and other code. In this instance the execution of will be resumed upon the occurrence of an interrupt for the device now being attached to. The block API call may result in a call to the kernel to place the wait thread in a blocked execution state using scheduling primitives in the underlying operating system. As described elsewhere herein but not further illustrated for simplicity the API may utilize the kernel helper module when issuing the kernel mode call to place the wait thread in a blocked state. After spawning the wait thread the API communicates with the kernel helper module as illustrated by Sto register with the operating system a generic ISR in kernel mode as the ISR to be invoked upon the occurrence of an interrupt for the device. The module then issues the attach request to the module which results in registering the kernel mode generic ISR to service any subsequent requests for the device. As will be seen in following figures and paragraphs the generic ISR may perform minimal work in connection with servicing the interrupt. The ISR in the user mode device driver will be invoked to perform processing to service the interrupt as described in .

Referring now to shown is an example illustrating processing upon the occurrence of an interrupt for the user mode device driver attached in . As a first step the generic ISR is invoked. The generic ISR performs minimal processing needed so that the real ISR ISR is able to service the request. The generic ISR performs processing to acknowledge the interrupt. This may include setting particular registers on the device. The generic ISR may also save the device state information regarding the interrupt to be processed later by the real ISR ISR . As will be appreciated by those skilled in the art such device state information may be needed by the ISR in servicing the interrupt. The device state information may include for example data in the device s hardware registers indicating a reason for the interrupt such as an error successful completion of a previous request device time out and the like. The generic ISR then schedules or queues a DPC denoted as DPC A to wake up the WAIT thread . The generic ISR is written using API calls such as a DPC API call. At some point the DPC A executes and signals or wakes up thread . The signaling results in the wait thread being scheduled for execution by the processor. When the wait thread resumes execution it resumes execution following the point at which the wait thread s state was previously blocked. The wait thread performs processing to invoke the real ISR ISR which services the interrupt. The wait thread first uses a WRITELOCK synchronization operation. The WRITELOCK operation may be an API call which results in invoking the appropriate operating system primitives to implement a reader writer lock. A reader writer lock may have processes accessing the lock for read access or write access. Only a single process is allowed to access the lock for write providing exclusive access to the lock by the writer and not allowing any readers or other writers to access the lock. One or more processes may access the lock for read access when the lock is not already accessed for writing. A process cannot access the lock for write while there are any readers accessing the lock. A reader writer lock may be a lock used to emulate enabling and disabling interrupts. When there are no readers accessing the lock interrupts are enabled. When there are one or more readers accessing the lock interrupts are disabled and the ISR is not allowed to execute. Thus the wait thread does not invoke the ISR until the WRITELOCK is available. Once the WRITELOCK is obtained the real ISR code included in ISR is invoked. The real ISR is the code that actually services the interrupt and then queues DPCs to complete the interrupt service processing and or otherwise has the DPC schedule threads to complete the processing. The ISR may include an API call to perform the DPC operation and make the appropriate kernel mode calls using the underlying operating system primitives with the assistance of the kernel mode helper module not illustrated in figure .

The reader writer lock may be logically implemented using a reader counter and a writer counter. A reader is able to access the lock if the writer counter is 0. A writer is able to access the lock if the reader counter and writer counter are both 0. Each operation to take a READLOCK increases the reader counter by 1 if there are no writers and otherwise waits until there are no writers. Each operation to release a READLOCK decreases the reader counter by 1. Each operation to take a WRITELOCK increases the writer counter by 1 if there are no readers and other waits until there are no readers. Each operation to release a WRITELOCK decreases the writer counter by 1. The wait thread may make API calls for the operations to take a WRITELOCK and release a WRITELOCK. The user process may make API calls for the operations to disable interrupt processing and enable interrupt processing . In implementing the disable interrupt processing for code executing in user mode the API may take a READLOCK by using a reader writer lock or other operating system primitive. In implementing the enable interrupt processing for code executing in user mode the API may release a READLOCK by releasing the native reader writer lock or other operating system primitive. The API may perform the processing needed to utilize the underlying operating system primitives and make appropriate calls into kernel space using the kernel mode helper module not illustrated for simplicity . In one embodiment the API may include calls to disable interrupt processing and enable interrupt processing . Each of these may be published APIs for use by code modules. Within the API there may be unpublished routines which implement the READLOCK and WRITELOCK operations. As such a code module may include a first published API call to disable interrupt processing or enable interrupt processing . The published API call may result in other unpublished API calls for taking or releasing a READLOCK as appropriate. Each of these unpublished API calls may utilize the underlying operating system primitives such as a reader writer lock or other primitive that may vary with the underlying operating system. The wait thread in this example may utilize other unpublished APIs for taking and releasing a WRITELOCK. Each of these unpublished API calls may also utilize the underlying operating system primitives such as a reader writer lock or other primitive that may vary with the underlying operating system. The foregoing describes the behavior with respect to the API for user mode code. If code module for example is executed in kernel mode the code module may include the same published API calls to enable interrupt processing and disable interrupt processing . However the forgoing published API calls when made from code executing in the kernel mode may directly invoke the operating system or hardware primitives to directly manage enabling disabling the interrupt state.

In one embodiment herein the ISR in user mode runs at a real time priority level so that it is not pre empted and is scheduled to execute prior to other user space code so that the ISR executes with a priority substantially the same as the ISR of and generic ISR . For the user space code in the ISR may have a higher priority than the emulated DPC call for user mode and the emulated DPC may have a higher priority than other user processes such as . In user mode the API may implement the DPC functionality by scheduling a user mode thread for execution having a priority substantially the same as a DPC included in an operating system for kernel mode execution.

Once the real ISR code of ISR completes control is returned to the wait thread which then issues an API call to run queued DPCs. The API call to run the DPCs in user mode causes execution of those DPCs associated with the same processor on which the wait thread is currently executing. The last step of the wait thread e.g. to run the DPCs causes the processor to execute the DPCs of the processor on which the wait thread is executing prior to other DPCs or other code having the same priority. This last step of the wait thread may be optionally performed in an embodiment.

It should be noted that for the user space emulated DPC operation whenever user space code performs a DPC call this results in the API scheduling a DPC user mode thread for execution on the same CPU as the requesting thread or code module. In other words the emulated DPC operation queues a DPC thread for execution on the same CPU as the thread making the DPC call. The foregoing is made with reference to an embodiment in which there are multiple storage system processors so that the emulated DPC operation in user mode may result in scheduling a DPC user mode thread with a relative priority as indicated above on any one of the data storage system processors. However in one embodiment the DPC thread is scheduled for execution on the same processor as that of the requestor or thread performing the DPC call.

It should be noted that the reader writer lock is used to ensure that the ISR and other code that may share the same data structures as the ISR do not execute at the same time. In other words the ISR needs exclusive access to data structures used in connection with performing I O operations for the device. For example one of the data structures may be a buffer used to store data in connection with performing the I O request for the device. Other threads may also be executing which utilize the same data structures. One or more of these other threads may be able to simultaneously access the data structures. Use of the reader writer lock allows this to occur and also provide exclusivity to the ISR . To further illustrate user process may include code which accesses the same data structures as ISR . Prior to performing processing using these data structures the process may take a READLOCK to disable interrupt processing. If the wait thread tries to take the WRITELOCK while has a READLOCK wait thread will have to wait until issues a release READLOCK indicating that there are no more readers.

It should be noted in the foregoing that each code portion in user mode and kernel mode may utilize APIs to implement the functionality described. The API may perform the necessary processing to implement the requested operation by invoking the helper modules as necessary and utilizing underlying primitives of the operating system.

Using the foregoing techniques with reference to with the device driver in kernel mode the attach API call results in ISR being executed to service the interrupt. Using the foregoing techniques with reference to with the device driver in user mode the attach API call results in ISR being executed to service the interrupt. With one modification the same driver code illustrated may be executed in user mode or kernel mode. In this example the generic ISR performs the required minimal processing to acknowledge the interrupt and allow subsequent processing to proceed. Generally servicing an interrupt can be performed by code which is executed within the ISR executed by a DPC or code executed in a thread. When using the techniques herein with the device driver executing in user mode the code which is typically executed within the ISR may be partitioned into two portions with a first small portion of processing performed within the generic ISR and the remaining portion performed by the REAL ISR ISR . In implementation it may be necessary to have the generic ISR acknowledge the interrupt immediately from kernel mode. In one embodiment any coding difference between the ISR when executing in user mode as in and having the same ISR execute in kernel mode as illustrated by ISR of may be handled for example using compilation conditionals. In other words the code portion of the ISR for acknowledging the interrupt and saving the device state may be conditionally included when the driver is executed in kernel mode.

As an alternative to the conditional compilation and to using the techniques illustrated in when executing the device driver in kernel mode an embodiment may register or attach the generic ISR of . Upon the occurrence of an interrupt the generic ISR may perform processing as described in connection with with the modification that the queued DPC will invoke the real ISR for execution in kernel mode rather than a wait thread as in connection with user mode. illustrate this in more detail in connection with the alternative embodiment for kernel mode device driver execution using the same generic ISR and real ISR code for both user and kernel mode.

Referring to shown is an example of an alternative embodiment that may be used in connection with a device driver executing in kernel mode. In this example the device driver may include the real ISR code as described previously in connection with . As a first step an attach API call may be performed as indicated by Sto attach as illustrated by Sthe generic ISR . The generic ISR may be as described in connection with . With reference to at some point later in time an interrupt occurs. In response to the occurrence of the interrupt the generic ISR is invoked which executes and queues a DPC A for execution. The DPC invokes the real ISR ISR of the device driver module . The ISR may be as described in connection with .

In connection with the embodiment illustrated in the same device driver code including the ISR may be used executed in both kernel and user mode. The generic ISR is used when the device driver executes in user and kernel mode. When executing in user mode additional code modules including the wait thread and DPC to signal the wait thread may be used. When executing in kernel mode a DPC may be used to invoke the ISR rather than signal a wait thread since the wait thread is not used in connection with the kernel mode device driver execution.

Referring now to shown is a flowchart of processing steps performed in connection with the kernel mode device driver. The flowchart summarizes processing described connection with . At step the kernel mode driver invokes the attach API. At step the attach API registers the kernel mode ISR to be invoked upon the occurrence of an interrupt for the device. At step an interrupt occurs and the kernel mode ISR is invoked to service the interrupt.

Referring now to shown is a flowchart of processing steps of an alternative embodiment of processing performed in connection with the kernel mode device driver. The flowchart summarizes processing as described in connection with . At step the kernel space device driver invokes the attach API. The attach API results in registering a step the kernel mode generic ISR to be invoked upon the occurrence of an interrupt. At step an interrupt occurs and the kernel mode generic ISR is invoked which acknowledges the interrupt and saves device state information. At step a DPC is queued to invoke the real ISR in the device driver code. At step the DPC queued in step executes and invokes the real ISR code in the device driver to service the interrupt. The steps of may be performed as an alternative to processing of .

Referring now to shown is a flowchart of processing steps performed in connection with the user mode device driver to register the generic ISR. The flowchart summarized processing described connection with . At step the user mode device driver invokes the attach API. At step the wait thread is spawned and blocked by calling in to the kernel to block execution of the wait thread. At step the generic ISR in the kernel mode is registered for the device to be invoked upon the occurrence of an interrupt for the device.

Referring now to shown is a flowchart of processing steps performed in connection with the user mode device driver upon the occurrence of a device interrupt. The flowchart summarized processing described connection with . At step the generic ISR is invoked which acknowledges the interrupt and saves device state information. At step a DPC is queued for execution which will wake up the user mode wait thread when executed. At step the DPC executes and signals or wakes up the wait thread to resume execution. At step the wait thread resumes execution and takes a WRITELOCK. The wait thread then invokes the real ISR to service the interrupt. At step the interrupt is serviced by the real ISR in user mode and the real ISR may queue any DPCs to perform additional processing to service the interrupt or may queue a DPC to schedule a thread for execution to complete servicing the interrupt. At step control returns to the wait thread which releases the WRITELOCK and may run any DPCs queued for the processor on which the wait thread is currently executing.

Referring now to shown is an example of components that may be included on a data storage system using the techniques herein. The example includes user processes and . The PCI device driver may be implemented as a user mode device driver using techniques herein. The PCI device driver may facilitate communications with the disks. The example also includes an Ethernet device driver executing in kernel mode to facilitate communications with one or more hosts or servers. The example also includes helper modules and as described elsewhere in connection with techniques herein. Each of the processes and and module may utilize the same API as described herein. The device driver may receive a file system request from a host. The hosts may utilize any one of a variety of different files systems such as for example NFS Network File System or CIFS Common Internet File System . The process may receive the file system request from the device driver and form one or more block I O requests. The process may communicate the block I O requests to the process . The user process may take a block I O request and communicate with the appropriate device to service the request. The process may return data obtained from the device such as in connection with a read request to the user process which then further communicates with the device driver to return any data to the host. The example illustrates what may be executing on a single data storage system processor of a multiprocessor data storage system using a dedicated single processor. Alternatively the components of may be scheduled to run on different processors of the data storage system. The particular one of the processors selected at a point in time may vary with the processor selected from the pool of data storage system processors. For example an embodiment may schedule a code module for execution on the next available processor selected from the pool of data storage system processors.

An embodiment may also include multiple instances of and as described elsewhere herein. If process fails such as due to failure of device driver other instances of and and the operating system of the data storage system remain running. Other instances of and are affected only to the extent they are utilizing services of the failed process . For example process may be processing a request for process . However the connection between and can be terminated and reestablished when the failed process is restarted. Upon the failure of process any necessary cleanup may be performed. For example any resources consumed by may be deallocated and then process may be restarted. Failures of may have affects similar to those as described above for with respect to the operating system and other processes.

An embodiment may utilize a data storage system such as the CLARiiON data storage system from EMC Corporation which has multiple processing nodes. Each node may have its own data storage system processor as well as other components. In such an embodiment each node may have executing thereon separate instances of the modules included in the example . As a variation to the foregoing an embodiment may have zero or more instances of each of the modules and executing on each node so that collectively across all the nodes of the data storage system there is at least one instance of module and at least one instance of module . To further illustrate as an example a first node may have executing thereon all modules of the example except for module . One or more instances of module may reside on the first node. A second node may have executing thereon all modules of the example except for module . One or more instances of module may reside on the second node. As another example each of the first and second nodes may have multiple instances of both modules and so that either node experiences a failure or is otherwise offline the remaining node may handle the processing of the failed node.

It should be noted that upon the failure of one of the user processes a new user process may be instantiated dynamically to assume the identity and processing of the failed process.

Referring now to shown is a representation illustrating the relationship of the common software environment CSE components to other components of the data storage system. In the example the CSE includes the API helper modules and other infrastructure code such as module used to interface code of the API to other operating system components. The CSE may isolate any code in user space or kernel space above the CSE from dependencies in the operating system or hardware platform. Furthermore code writing using the API of the CSE may be executed in either user or kernel mode as illustrated herein.

As will be appreciated by those skilled in the art the techniques herein may be used for existing code as well as newly developed code. For existing code the platform specific calls may be determined and replaced with appropriate API calls. The API code may be modified to provided the necessary support for any additional platform. Similarly new code may be developed using the API calls which may utilize the platform specific primitives while isolating the code from these platform dependencies.

An embodiment may implement the techniques herein using code executed by a computer processor. For example an embodiment may implement the techniques herein using code which is executed by a processor of the data storage system. As will be appreciated by those skilled in the art the code may be stored on the data storage system on any one of a computer readable medium having any one of a variety of different forms including volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can accessed by a data storage system processor.

While the invention has been disclosed in connection with preferred embodiments shown and described in detail their modifications and improvements thereon will become readily apparent to those skilled in the art. Accordingly the spirit and scope of the present invention should be limited only by the following claims.

