---

title: Association of terms with images using image similarity
abstract: Methods, systems, and apparatus, including computer program products, for associating terms with images. A plurality of images is received. One or more of the received images are associated with one or more terms. Degrees of similarity between a plurality of pairs of images in the plurality of images is determined. A respective weight with respect to a first image of the plurality of images is assign to one or more of the terms based at least on the degrees of similarity. One or more of the terms are selected based on the respective weights of the terms with respect to the first image, and the selected terms are associated with the respective image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08356035&OS=08356035&RS=08356035
owner: Google Inc.
number: 08356035
owner_city: Mountain View
owner_country: US
publication_date: 20070410
---
Image search engines have become a popular tool on the Internet. These search engines help users find images that match their criteria. Some of these search engines index images by keywords or labels. Such image search engines retrieve images by matching users search queries against these keywords or labels.

The keywords or labels for indexing an image can be drawn from text surrounding the image in a webpage or other text associated with the image e.g. the filename of the image for example. The keywords or labels can provide some indication of the content of an image. For example an image of a horse can be associated with the keyword horse. In some cases the keywords for an image that are extracted from the text may not be informative or useful. For example words such as cool and wow generally are not very informative about the content of an image. Poorly labeled images can degrade the quality of an image search result.

In general one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a plurality of images where one or more of the images are associated with one or more terms determining degrees of similarity between a plurality of pairs of images in the plurality of images assigning to one or more of the terms a respective weight with respect to a first image of the plurality of images based at least on the degrees of similarity and selecting one or more of the terms based on the respective weights of the terms with respect to the first image and associating the selected terms with the respective image. Other embodiments of this aspect include corresponding systems apparatus and computer program products.

In general another aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a plurality of images where one or more terms are associated with one or more of the images determining degrees of similarity between a plurality of pairs of images in the plurality of images generating a weighted graph data structure where the weighted graph data structure has nodes corresponding to the images and an edge connecting a first node and a second node of the weighted graph data structure has a weight based on a degree of similarity between an image corresponding to the first node and an image corresponding to the second node propagating the terms to the nodes of the weighted graph data structure based on the weighted edges and selecting one or more terms for a respective image of the plurality of images based on the propagating and associating the selected terms with the respective image. Other embodiments of this aspect include corresponding systems apparatus and computer program products.

In general another aspect of the subject matter described in this specification can be embodied in methods that include the actions of identifying similarities between features of images in a set of images where the images are associated with one or more terms representing the similarities in a similarity graph and using the similarity graph to identify a set of terms for an image search query that will produce a search result that includes similar images. Other embodiments of this aspect include corresponding systems apparatus and computer program products.

Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Images can be associated with one or more terms based on the associations between the terms and similar images. The most relevant term for an image can be identified. Uninformative or unrelated terms for an image can be identified and disassociated from the image. Respective weights can be assigned to terms with respect to an image. The weights assigned to terms can be used to find the best match between search query terms and images.

The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features aspects and advantages of the subject matter will become apparent from the description the drawings and the claims.

Content providers host content. The hosted content can include text images audio video and so forth. In some implementations the content host hosts one or more images. An image hosted by the content host can be downloaded by a user client or pushed to the user client . The image may be downloaded or pushed with a webpage written in the Hypertext Markup Language HTML or any other suitable language for authoring webpages. In some implementations content host is a web server that hosts webpages and images.

The environment includes one or more user clients . The user client can include a desktop computer laptop computer a media player e.g. an MP3 player a streaming audio player a streaming video player a television a computer a mobile device etc. a mobile phone a browser facility e.g. a web browser application an e mail facility or other device that can access content via network .

The content provider may permit user client to access images. In some implementations a user at a user client can access images at a content host through a web browser application for example.

An image search engine indexes images receives search queries for the images and returns results in response to the queries. In some implementations a crawler associated with the image search engine crawls the content providers for images and indexes the crawled images. In some implementations the image search engine stores copies of the crawled images. In some implementations the image search engine is a part of a search engine for various types of content e.g. web pages images etc. .

The image search engine can receive search queries for images from user clients . An image search query can include one or more terms which can be words phrases numbers and any combination thereof. In some implementations a user at a user client accesses a user interface of the image search engine and enters a search query through for example a browser application at the user client . For an image search query the image search engine finds the indexed images that match the query and returns the search results to a user client for presentation to the user. The search results for an image search can include thumbnails of the images that match the query and hyperlinks to the images or webpages that include the images.

The image search engine can index images based on one or more terms associated with the images. A term can be a word phrase number or any combination thereof. An image matches an image search query if the query matches the terms associated with the image. One or more terms associated with an image can also be referred to as a label. 

In some implementations the terms that are associated with an image are derived from a metadata of the image. For example the metadata of a JPEG image file can include terms that indicate a title a description categories keywords and the like. In some implementations the image metadata is data embedded in an image file in accordance with the Exchangeable Image File Format Exif specification. The image search engine can read the metadata of the image as well as the filename of the image and extract terms from the metadata or the filename.

In some implementations the terms that are associated with an image can be determined by the image search engine . The image search engine can extract one or more terms associated with content related to the image and associate the terms with the image. For example the image search engine can extract terms from text from a webpage in which the image appears anchor text of links to the image text from a webpage to which the image links if the image is an anchor for a hyperlink to the webpage and so forth.

In some implementations the image search engine can determine which terms should be associated with an image based on user testing. For example the image search engine can show a population of users an image and ask the users to specify terms that come to mind when they see the image and optionally order them. The terms entered by the users provide an indication of what users believe the topics concepts or subject matter of the image are. The image search engine can select the most popular of the user specified terms to associate with the image.

The image crawling module crawls content providers for images. The image crawling module accesses content at the content providers images and any other content associated with the images. The crawling module receives copies of the crawled images e.g. by downloading and other content for further processing by the image search engine . In some implementations the image crawling module may be a more general crawling module that crawls for any content not just images hosted by content providers .

The image terms module determines what terms are associated with an image. The image terms module extracts terms from metadata associated with an image e.g. filename Exif metadata etc. . The image terms module can also determine the terms that should be associated with an image from other content associated with the image e.g. a text of a webpage in which the image is associated anchor text of hyperlinks to the image text of a webpage to which the image links etc. .

The image features module identifies features of the images. The image features module can determine for an image one or more features. The image similarity module determines a degree of similarity for pairs of images. The image similarity module compares for any pair of images within a repository or set of images the features of the images in the pair as identified by the image features module and determines a value representing a degree of similarity between the two images in the pair. Further details regarding the identification of features and the determination of the degree of similarity are described below.

The image similarity graph and term propagation module generates a similarity graph of the images and propagates terms associated with images using the graph. A similarity graph as described in this application is a weighted graph where each node corresponds to an image and edges between nodes are weighted by the degree of similarity between the images corresponding to the connected nodes. Terms associated with images can be propagated to other images using a similarity graph further details of which are described below in reference to .

Image indexing module indexes images according to the terms associated with an image. The indexing enables searching of the images by terms.

The image search module processes search queries for images and returns search results in response to the queries. A search query for images can include one or more terms. The image search module receives a query from user client for example finds images e.g. images from image repository that are associated with terms that match the terms of the search query and returns the search results. In some implementations the search results are returned in a webpage that shows thumbnails of the matching images and information related to the images e.g. image file size and file type image dimensions domain in which the image is located etc. 

Information regarding the images crawled by the image crawling module and optionally copies of the crawled images can be stored in an image repository . In some implementations the image repository is a database e.g. a MySQL database of images and image information. The image repository also stores an index of images that have been crawled or received by the image search engine . A copy of an image does not have to be stored at the image repository to be in the index.

Multiple images are received . In some implementations the images can be received as part of a crawl of content providers hosting the images. In some other implementations the images can be received from an upload of images to the system. In some implementations content associated with the images e.g. webpages in which the images are located are received as well. These received images are eventually indexed by the image indexing module .

A received image can be associated with one or more terms. For example the image search engine can read the metadata of the image e.g. filename Exif data etc. for an image and extract any title description category keyword or other terms from the metadata and associate the terms with the image. As another example terms that can be associated with the image can be determined from other content associated with the image e.g. webpage in which the image is located anchor text of hyperlink to the image etc. . Any number of the images can be associated with a particular term and an image can be associated with multiple terms.

In some implementations the terms can be extracted from the metadata or other content associated with the image without any discrimination as to whether a term is useful. In some other implementations an importance or relevance metric can be determined for terms and terms with values for the metric that are above a threshold or satisfy some criterion are selected and associated with the image. In an exemplary implementation the importance or relevance metric is the well known term frequency inverse document frequency TF IDF of the term. For example say that an image is included in a webpage that includes terms X and Y. If term X has a TF IDF with respect to the webpage that is above a predefined threshold and the TF IDF for term Y is not above the threshold then term X and not term Y is associated with the image. Using the TF IDF as an importance or relevance metric can be useful for removing relatively uninformative terms e.g. a the prepositions etc. from consideration.

In some implementations degrees of similarity are determined for pairs of images . That is an image can be paired with one or more of the other images and for each pair a degree of similarity between the image pair can be determined.

One or more global and or local features of Image A and one or more global and or local features of Image B are identified . Examples of image features that may be used include image features based on for example intensity color edges texture wavelet based techniques or other aspects of the image. For example regarding intensity each image may be divided into small patches e.g. rectangles circles etc. and an intensity histogram computed for each patch. Each intensity histogram may be used as a feature for the image. Similarly as an example of a color based feature a color histogram may be computed for each patch or for different patches within each image. A color histogram can be similarly computed to obtain a color based histogram. The color histogram may be calculated using any color space e.g. the RGB red green blue color space YIQ luma and chrominance or another color space.

Histograms can also be used to represent edge and texture information. For example histograms can be computed based on patches of edge information or texture information in an image. For wavelet based techniques a wavelet transform may be computed for each patch and used as an image feature.

The features discussed above represent an exemplary list of possible image features that may be used for determining similarities between images. Other image features may be used. In an exemplary implementation the features are identified using the known scale invariant feature transform SIFT . An examples of the SIFT technique is described in Lowe Distinctive Image Features from Scale Invariant Keypoints Vol. 60 Issue 2 2004 pp. 91 110 which is incorporated by reference herein in its entirety.

In some implementations a global feature is a feature that is identified for the image as a whole. For example a color or intensity histogram of the entire image is a global feature. A local feature is a feature that is identified for a portion of the image. For example a color or intensity histogram of a patch in the image is a local feature.

In some implementations to improve computation efficiency features may be computed only for certain areas within images. For example objects of interest within an image may be determined and image features may only be computed for the objects of interest. For example if the image feature being used is a color histogram a histogram may be computed for each patch in the image that includes an object of interest. Objects of interest within an image can be determined in a number of ways. For example for color objects of interest may be defined as points where there is high variation in color i.e. areas where color changes significantly . Objects of interest can be determined mathematically in a variety of ways and can be based on determining discontinuities or differences from surrounding points. In some implementations the objects of interest are determined based on points of interest or keypoints identified in the image. The SIFT technique is an example of one technique for locating keypoints and objects of interest and features for these keypoints. In other words the computed image features are local features local to the keypoints or objects of interest.

In some implementations a feature identified for a keypoint include a position e.g. X Y coordinates of the image orientation and scale e.g. radius of a patch centered on the keypoint . When these image features are compared for a pair of images as described below the orientations and scales of the features can be compared as a geometrical verification between the two images in the pair.

As an example of the use of keypoints say that Image A is an image of the Eiffel Tower where the Eiffel Tower takes up a large portion of the image and that Image B is an image of a tourist with the Eiffel Tower in the background where the Eiffel Tower takes up a small portion of the image. Using keypoint identification points on the Eiffel Tower in Image A and on the Eiffel Tower in Image B can be identified as keypoints. The comparison of these features may indicate that Image B contains an object Eiffel Tower that is in Image A and vice versa.

Additionally in some implementations the various features described above may be computed using different image scales. For example an image can be examined and features computed in its original scale and then features may be successively examined at smaller scales. Additionally or alternatively features may be selected as features that are scale invariant or invariant to affine transformations. The SIFT techniques for example can be used to extract distinctive invariant objects from images. The extracted objects are invariant to image scale and rotation.

The identified features are compared the features of Image A are compared to the corresponding features of Image B . A degree of similarity or a similarity score between Image A and Image B is determined based on the comparison . For each feature that is to be used a comparison function may be selected. A number of different comparison functions may be used to compare images. The particular comparison function to use may be decided ahead of time or offline by an administrator. The output of the comparison function can be used to determine the similarity. For example the similarity can be a linear combination of the outputs of the comparison functions.

In general a comparison function may operate to generate a value defining a similarity between a particular feature computed for two images. As an example of a possible comparison function consider a simple histogram comparison function which is described in pseudo code in Table I below. As shown in Table I the histogram comparison function returns a value that is the sum of the absolute values of the differences between corresponding bins in the input histograms. Smaller values returned from this function indicate greater similarity between the input histograms.

The histogram comparison function of Table I is exemplary. It can be appreciated that other comparison functions can be used to compare histograms. For example squared differences may be used rather than absolute differences bin correlations may be taken into account instead of absolute differences or percent differences may be used instead of absolute differences. Additionally for image features other than those based on histograms different comparison functions may be used.

The selection of the image features to use and the comparison functions to use may be performed offline or in non realtime operation. For example an administrator may initially design or configure image features module and image similarity module to use one or more image features and one or more comparison functions. After these initial acts image features module and image similarity module may function in a realtime mode to determine pairwise degrees of similarity for a set of input images such as images received from a crawl by image crawling module .

In some implementations the similarity between Image A and Image B is a linear combination of scores from comparisons between corresponding features of Image A and Image B. An example implementation is shown in pseudo code in Table II below.

In the implementation shown in Table II each feature Fof Image A is compared with every feature Fof Image B that is the same feature type. In other words if Fis a color histogram then the comparison is performed with the color histogram features of Image B if Fis an edge histogram then the edge histogram features of Image B are compared etc.

In the operation shown in Table II each image feature is weighted equally. In some implementations different features may be weighted differently. For example color based features may be less important than intensity or edge based features. Accordingly a features similarity score may be multiplied by a weight that reflects the importance of the particular feature.

In some other implementations the degree of similarity between two images is calculated as the number of shared keypoints i.e. keypoints whose feature s match divided by the total number of keypoints.

The operation shown in Table II can be relatively computationally expensive if repeated for many images as it requires Ncomparisons for a set of N images and for each comparison M Mfeature comparisons for Mand Mlocal features in each image. Techniques are known that may potentially accelerate this type of operation. For example one such technique is described in Grauman et al. The Pyramid Match Kernel Discriminative Classification with Sets of Image Features Tenth IEEE International Conference on Computer Vision October 2005 Vol. 2 pp. 1458 1465.

Returning to weights with respect to an image of the image set are assigned to the terms . The weights are assigned based at least on the degrees of similarity as described in reference to block .

In some implementations the assigning of the weights to the terms is performed using a similarity graph data structure. illustrates block in further detail. For convenience is described with respect to an exemplary Image A in a set of exemplary Images A E as described in reference to .

An image similarity graph data structure is generated . The image similarity graph includes a node for each image in a set of images e.g. the images received by the image search engine . In some implementations the image similarity graph is weighted an edge in the graph is associated with a weight. An edge connecting two nodes in the similarity graph is weighted based on the degree of similarity between the images corresponding to the connected nodes. In some implementations the edge weight is the degree of similarity or similarity score as determined in block . In some other implementations the edge weight is a function of the degree of similarity. In some implementations the graph includes injector nodes which act as the term nodes the terms initially associated with the images are initially associated with the injector nodes in the graph rather than the image nodes. The graph can be generated by the system to include image nodes and injector nodes. In some other implementations if an image is associated with one or more terms the corresponding node is associated with the terms. Additionally a term associated with a node can have a weight with respect to the node.

An example similarity graph is shown in . The similarity graph includes nodes A B C D and E corresponding to images A B C D and E respectively. The graph also includes injector nodes and . The graph includes edges between some pairs of the nodes. For example the edge connecting A and B has a weight of 0.1 the degree of similarity between Image A and Image B is 0.1. As another example the edge connecting B and D has a weight of 0.9 the degree of similarity between Image D and Image B is 0.9.

Terms are propagated through the similarity graph . A term associated with one node is propagated to another node in the similarity graph. In some implementations as terms propagate through the similarity graph a term weight is adjusted by the weight of the edge through which the term is propagated. For example in the graph an edge of weight 0.5 connects node C and node A. Node C is associated with the terms france and arc de triomphe with some weight for each e.g. 1 . The terms france and arc de triomphe are propagated from node C to node A through the edge connecting C to A and the weights of the term propagated from node C to node A are adjusted by the weight of 0.5. In other words a term s weight at node C is multiplied by the weight of the edge from C to A to get a weight for the term at A. Thus for example the term arc de triomphe has a weight of 1 at node C the term is propagated to node A and assigned a weight of 1 0.5 0.5. Further details on term propagation are described below.

A term is assigned a weight with respect to Image A based on weights of edges connected to the node corresponding to the initial image . As described above as a term is propagated through the graph its weight is multiplied by the weight of an edge it is propagated through. With respect to Image A the weight of a term propagated to node A is multiplied by the weight of an edge connected to the node A. When the propagation process is complete one or more terms are associated with node A with each term having a weight that is calculated based in part on the weights of one or more of the edges connected to the initial image node e.g. based on the sum or product of the edge weights which can be normalized . In other words the weights of the terms are affected by the degrees of similarity between Image A and other images. Further details regarding the term weights are described below in reference to .

Returning to one or more terms are selected for an image based on the term weights with respect to the image and the selected terms are associated with the image . After the propagation process an image can be associated with one or more terms. A subset of these terms can be selected and associated with the image. In some implementations the top X highest weighted terms with respect to the image is selected where X is any predefined integer of 1 or larger. In some other implementations any term with a weight with respect to the image that is higher than a predefined threshold is selected and associated with the image. It should be appreciated that the image can be associated with a term with which it was not originally associated before the propagation process.

As described above the edges in graph have respective weights. In graph the edge connecting A and B has a weight of 0.1 and the edge connecting A and C has a weight of 0.5. The edge connecting B and E has a weight of 0.3. The edge connecting B and D has a weight of 0.9. The edge connecting C and E has a weight of 0.1.

In some implementations the edges between image nodes are undirected i.e. bi directional . In some other implementations the edges between the image nodes are directed i.e. unidirectional . Images can be compared based on how much of one image is in the other and an edge can be directed based on the comparison. For example an edge between Image A and Image B can be directed from Image A to Image B if an object that takes up a large portion of Image A is also in Image B and takes up a small portion of Image B. In graph the directed edges have an arrow end indicating the direction while the edges without arrow ends are undirected.

In some implementations the graph includes one or more injector nodes . An injector node is connected to an image node by a directed edge of weight 1 directed from the injector node to the image node. An injector node can inject term weights into an image node. In some implementations the terms at an injector node connected to an image are the terms initially associated with the image and which are derived from the image metadata or from other content associated with the image as described in reference to block . For example injector node injects weights for terms paris france and eiffel tower into node A the image corresponding to node A is associated with the terms paris france and eiffel tower. Injector node injects weights for terms paris and uncle leo into node E. Injector node injects weights for terms paris and arc de triomphe into node D. Injector node injects weights for terms france and arc de triomphe into node C. Node B is not connected to an injector node image B is not initially associated with a term.

In some other implementations the graph does not include injector nodes the terms associated with an image are directly associated with the corresponding image node.

In some implementations a term weight at an injector node can be 1 for each of the terms at the injector node. In some other implementations some terms at an injector node may be more heavily weighted than other terms at the injector node to reflect the importance of certain terms. For example at injector node the term paris may have weight 1 term france may have weight 1.5 and term eiffel tower may have weight 2.

Optionally in some implementations the weights at an injector nodes are normalized so that the sum of the term weights is equal to 1. For example at injector node assuming a weight of 1 for each of the terms paris france and eiffel tower the normalized weights will be 1 1 1 1 or 0.333 after rounding for each of the terms. In other words weight normalization can be performed by dividing a term weight at a node by the sum of the term weights at the node. In some other implementations other functions for normalization can be used.

In some implementations the injector node is a source of constant weight to an image node for the terms with which the corresponding image is initially associated. Each injector node is connected to an image node by a directed edge of weight 1. The directed edge goes from the injector node to the image node but not the other way around.

In some implementations terms are propagated through the graph and weights assigned to the terms in accordance with an algorithm described in pseudocode below in Table III.

The algorithm in Table III starts with the initialization of the iteration counter t to 0. The initialization continues with the setting of the term weights at all of the image nodes to 0.

The algorithm then proceeds to an iterative loop. The iterative loop can iterate indefinitely for a specified number of times or until each term weight at all of the nodes change by less than a predefined difference threshold. In some implementations the loop iterates for 1000 times as shown in Table III.

The term weights at the injector nodes remain the same throughout the iterative loop. This is indicated by the initialization at each iteration t of the term weight at t 1 to be equal to the term weight at t for terms at the injection nodes. Further the term weights at the injector nodes are not affected by term weights at the image nodes because the edge between an injector node and an image node is unidirectional a term can propagate from an injector node to an image node but not the other way around. Optionally the term weights at the injector nodes are normalized before the beginning of the iteration loop.

At each iteration t the weight n for each term at each image node for the next iteration of propagation is initialized to 0.

Then the terms at a node are propagated to other nodes and weights are assigned. At a node n the weight of a term for use in the next iteration t 1 is the sum of the products of wand mfor all nodes m connected to node n where wis the weight of the edge connecting m to n and mis the weight for term l at t. Further it should be appreciated that node n for this operation cannot be an injector node because there are no nodes connected to an injector node due to the directed edge from the injector node to an image node but no edge in the other direction . At the end of an iteration the term weights at the image nodes are normalized.

To illustrate the algorithm shown in Table III an example run of the algorithm using graph will now be described. At the start of the algorithm before the iterative loop begins the term weights at the image nodes are initialized and optionally the term weights at the injector nodes are normalized. The term weights at each node after the initialization and normalization are as follows 

At the first iteration t 0 the term weights for the injector nodes are carried over for use in the next iteration t 1 . Then the terms are propagated to the image nodes through the edges in the graph which of course does not include edges that were elided and weights assigned to the terms with respect to each of the image nodes. The term weights at the image nodes are then normalized. At the end of iteration t 0 the term weights are the following 

In iteration t 0 because the term weights at the image nodes are 0 before the propagation the term weights come entirely from the injector nodes.

These term weights are used in iteration t 1 to calculate the term weights for iteration t 2. At the beginning of iteration t 1 the term weights for the injector nodes are carried over for use in iteration t 2. For an image node the term weights are calculated. For example for node A which is connected to nodes B and C the term weight for term france to be used in t 2 is as follows 

The weight for term france for the next iteration is initialized to 0. Then weights for term france from nodes connected to node A is added to the weight. Note that the weights from the nodes connected to node A are adjusted by the weights of the respective edges to node A. For example the weight from node C is adjusted by multiplying it by 0.5 which is the weight of the edge connecting node A to node C i.e. the degree of similarity between the image corresponding to node A and the image corresponding to node C .

Further it should be appreciated that the weight for term france at node A at the end of iteration t 0 is not used to calculate the weight for term france node A during iteration t 1.

Thus the weight for term france at node A for use in iteration t 2 before normalization is 0.583. At the end of the iteration t 1 this weight and other term weights in the graph are normalized. At the end of iteration t 1 the normalized term weights are the following 

These weights are used to calculate the term weights during iteration t 2 for use in iteration t 3. At the end of iteration t 2 the normalized term weights to be used in iteration t 3 are as follows 

Depending on the particular implementation the iteration loop can iterate indefinitely until the specified amount of iterations has run or until each respective term weight changes between iterations by less than a predefined difference threshold. In some implementations the predefined difference threshold e is 0.00000001. In other words the loop can stop when the absolute value of the difference between the term weights for a term at a node for iterations t 1 and t is less than or equal to for all nodes and all terms in the graph.

The algorithm in Table III propagates terms to the image nodes of graph and assigns weights. Even for a node that was not associated with any term before the algorithm the algorithm can propagate terms to that node. After the loop has stopped for a node each term has a weight. After the iterations have run the image search engine can select for a node a subset of the terms to associate with the node. In some implementations the search engine can select the top Y highest weighted terms with respect to the node where Y is an integer 1 or greater. For example for node A if the algorithm stopped at the end of iteration t 2 and Y is 2 then the terms paris and france would be selected and associated with the image corresponding to node A because these two terms are the 2 highest weighted with respect to node A. In some other implementations all terms whose weights with respect to the node meet or exceed a predefined minimum weight threshold are selected and associated with the node. In some implementations the terms associated with a node and thus associated with the image corresponding to the node can be used as keywords for the image in an image search engine or as targeting keywords for advertisements.

In some implementations for an image that was associated with one or more terms a priori e.g. terms from the image metadata terms determined from a webpage in which the image appears etc. before the term propagation algorithm the image search engine can use the a priori terms or the terms selected as a result of the term propagation algorithm. It should be appreciated that the set of a priori terms and the set of terms selected from the term propagation algorithm are not necessarily the same.

In some implementations a similarity graph can be generated for a subset of the images in the image search engine and the term propagation algorithm can be performed for the graph of the subset of images. For example the subset of images can be the images from a particular domain or website images that have a particular characteristic images from a website that are associated with a particular term and so forth.

The algorithm in Table III above was described in reference to a similarity graph that includes injector nodes. However the algorithm can be adapted for use with similarity graphs that do not include injector nodes where the terms initially associated with images are associated with the image nodes rather than an injector node.

In some implementations the similarity graph can include dummy nodes. A dummy node behaves like an injector node the dummy node is associated with a dummy term which is associated with a weight and the dummy node injects into the graph the weight of the dummy term. The dummy term and its weight is propagated through the graph in the same manner as the other terms and their weights are propagated. The additional weight can be used to dilute by increasing the total pre normalization weight at an image node term weights that are propagated from distant nodes and thus reduce the influence on the term weights at an image node of term weights from distant nodes. When terms are selected for a node as described in reference to block for example the dummy terms and their weights are ignored. For example the dummy terms and their respective weights at an image node are not considered when selecting terms by taking the top Y highest weighted terms. The graph can include a dummy node for each image node or only for the image nodes that do not have a corresponding injector node e.g. node B in graph .

In some implementations before the iterative loop begins the weights of a term across all injector nodes can be normalized. For example the weights of the term paris at nodes and can be normalized so that these weights sum to 1.

In some other implementations other algorithms for selecting terms for a node using a similarity graph can be used. An example is an algorithm based on random walks. For convenience the algorithm will be described below in reference to similarity graph . The algorithm begins by generating the similarity graph but reversing the direction of any directed edge. Thus for example the edge connecting node A to node is directed from node A to node rather than the other way around. A bi directional edge is left as is. Then the algorithm proceeds by performing random walks from an image node of interest i.e. a node for which one or more terms are desired. At each node starting from the node of interest the edge leading out to the next node is randomly selected. If the edges are weighted the out edge can be selected using the optionally normalized edge weights as probabilities. If a walk reaches an image node the walk is continued until the walk reaches an injector node. When a walk reaches an injector node a count is incremented for each term that is associated with the injector node and the algorithm proceeds to perform a new walk starting from the node of interest. A random walk is performed multiple times e.g. 1000 times 100000 times etc. for each node of interest. The term weights for a node of interest is derived from the proportions of the counts for the terms maintained for the random walks from the node of interest.

The memory is a computer readable medium such as volatile or non volatile memory that stores information within the system . The memory could store data structures representing image repository for example. The storage device is capable of providing persistent storage for the system . The storage device may be a floppy disk device a hard disk device an optical disk device or a tape device or other suitable persistent storage means. The input output device provides input output operations for the system . In one implementation the input output device includes a keyboard and or pointing device. In another implementation the input output device includes a display unit for displaying graphical user interfaces.

The disclosed and other embodiments and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. The disclosed and other embodiments can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter effecting a machine readable propagated signal or a combination of one or more them. The term data processing apparatus encompasses all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as a program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user the disclosed embodiments can be implemented on a computer having a display device e.g. a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

The disclosed embodiments can be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of what is disclosed here or any combination of one or more such back end middleware or front end components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN e.g. the Internet.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

While this specification contains many specifics these should not be construed as limitations on the scope of what being claims or of what may be claimed but rather as descriptions of features specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.

Similarly while operations are depicted in the drawings in a particular order this should not be understand as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

Thus particular embodiments have been described. Other embodiments are within the scope of the following claims.

