---

title: Efficient access of flash databases
abstract: Techniques for efficient access to flash databases are described. In one implementation, a method includes performing an operation on a flash database, supplementing at least one portion of a node translation table corresponding to at least one node involved in the operation, and semantically compressing at least one portion of the node translation table. The semantic compression includes discarding at least one log entry that is rendered obsolete by at least one subsequent log entry, and incrementing a version number of the log entries corresponding to the at least one portion of the node translation table. In further embodiments, discarding at least one log entry includes discarding at least one log entry that is at least one of opposed by or overruled by at least one subsequent log entry.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08037112&OS=08037112&RS=08037112
owner: Microsoft Corporation
number: 08037112
owner_city: Redmond
owner_country: US
publication_date: 20070423
---
This patent application is related to co pending commonly owned U.S. patent application Ser. No. 11 739 018 entitled Self Tuning Index for Flash Based Databases filed concurrently herewith on Apr. 23 2007 which application is incorporated herein by reference.

Current databases are typically designed for the storage and interface characteristics of hard disk devices. An alternative form of storage device however is a flash device. While flash based storage devices were not common in previous databases recent technology improvements in flash device storage capacities have made the use of flash devices in databases more feasible. The increasing use of mobile and low power devices further motivates the use of flash devices in such mobile devices because hard disks may be prohibitively heavy or energy intensive.

A key feature of flash storage devices is that their characteristics vary depending on the interface used to access the flash device. A flash device may be accessed through many different interface types such as in a USB Universal Serial Bus stick in a compact flash card a secure digital SD card a mini SD card a micro SD card or in a printed circuit board of a mobile or embedded device. While speed of data transfer across the interface may not be a bottleneck differences exist in the nature of low level flash management implemented behind the interface by the flash device manufacturer. The performance of the flash based database and the underlying indexing algorithm depends on the characteristics of the flash device as observed through each specific interface.

There are many applications where it is desirable to store data within a sensor network rather than transmit the data to a central database. Example applications include remote deployments where an economical communication infrastructure is not available mobile sensor nodes with sporadic and short lived connections and sensor networks of mobile devices which have significant local processing power.

In many cases where data storage is part of a sensor network flash based storage devices are commonly used rather than hard disks due to their more favorable characteristics including shock resistance node size weight and energy considerations. Additionally flash is also common in many mobile devices such as personal data assistants PDAs cell phones music players and personal exercise monitors. These devices may greatly benefit from the favorable characteristics of flash memory.

Existing database products however are typically designed for hard disks and may therefore suffer from drawbacks when using flash devices. For example such database products are typically not optimized for flash device characteristics and unlike many traditional database applications may be used in various applications for which the workload is highly write intensive. Indexing schemes have been proposed to address these concerns however existing indexing schemes are not optimized for many available flash devices or for many realistic workloads. In this way existing indexing schemes may not be suitable in many practical systems especially when the systems are designed to be highly flexible and capable of handling multiple types of workloads.

Techniques for efficient access to flash databases are described. For a flash database having a data tree structure an indexing scheme may include storing log entries involving the nodes of the data tree in a node translation table. Techniques described herein may improve the efficiency of accessing and storing the node translation table as well as recovering the node translation table after a system crash.

In one embodiment an operation is performed on a flash database and a portion of a node translation table corresponding to at least one node involved in the operation is semantically compressed. The semantic compression includes discarding at least one log entry that is rendered obsolete by a subsequent log entry and incrementing a version number of the log entries corresponding to the portion of the node translation table. In further embodiments the log entry is discarded because it is opposed by or overruled by at least one subsequent log entry. In other embodiments a portion of the node translation table may be checkpointed. The checkpointing may include replacing one or more sector addresses with one or more corresponding physical addresses and storing the physical addresses.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Techniques for efficient access to flash databases are disclosed herein. For a flash database having a data tree structure an indexing scheme may include storing log entries involving the nodes of the data tree in a node translation table. Embodiments of techniques in accordance with the teachings of the present disclosure may provide significant advantages. For example processes in accordance with the present disclosure may advantageously reduce the size of the node translation table in the flash database memory and may enable the flash memory to be accessed faster and more energy efficiently. Such processes may also improve the ability of the flash memory to recover from crashes. Techniques in accordance with the teachings of the present disclosure may therefore improve the overall performance of flash memories and therefore the performance of the devices sensor networks and other components and systems that use flash memories.

As further shown in at least one of the mobile devices e.g. a PDA includes a system memory having a flash memory and a non flash memory . The flash memory stores a data tree having a plurality of nodes . A node translation table resides in the non flash memory e.g. in volatile RAM and stores log entries associated with nodes of a data tree stored in the flash memory .

Techniques for efficient access to flash databases in accordance with the teachings of the present disclosure may be implemented in the system memory of the device . More specifically the system memory may be configured to provide improved compactness of the node translation table so that less space is required in memory. Such techniques may also significantly improve the efficiency of the memory operations associated with the node translation table and the flash memory thereby improving the energy consumption and overall robustness of the devices as described more fully below.

A flash database or FlashDB is stored in the non flash memory . The FlashDB is a database compatible with and in some embodiments optimized for flash devices. More specifically in some embodiments the FlashDB may be of a self tuning database such that after it is initially configured using the underlying storage device it may automatically adapt its storage structure in a way that optimizes energy consumption and latency for the workload it experiences as described for example in our co pending commonly owned U.S. patent application Ser. No. 11 739 018 entitled Self Tuning Index for Flash Based Databases previously incorporated by reference. Thus different flash databases running on different flash devices or having different workloads e.g. with different read write ratio or different data correlation may choose different organizations of data on the underlying physical device.

In this embodiment the FlashDB includes a database management system that implements one or more database functions e.g. index manager query planner query compiler etc. and a storage manager that implements efficient storage related functionalities e.g. data buffering and garbage collection . One or more database applications communicate with a flash translation layer FTL of a flash device e.g. the flash memory via the database management system and the storage manager . One or more non database applications communicate with the FTL of the flash device through a file system . The database management system may operate in cooperation with a known data structure such as a B tree data structure.

Flash devices are primarily of two types NOR and NAND. While NOR devices typically have faster and simpler access procedures their storage capacities are typically lower and hence NOR devices are typically preferred for program storage rather than data storage. NAND flash offers significantly higher storage capacity e.g. 32 GB in a single chip and is typically more suitable for storing large amounts of data.

In flash devices read and write operations typically happen at page granularity for some devices up to page granularity . Pages are organized into blocks typically of 32 or 64 pages. A page can only be written after erasing the entire block to which the page belongs. Page write cost is typically higher then read. The block erase requirement makes writes even more expensive since if a block is to be erased then pages containing useful data within that block must be moved to another block before erase. In addition a block may wear out after repeated writes e.g. 10 000 to 100 000 writes and so it is desirable to spread out the write load evenly over the flash device.

The FTL provides a disk like interface to the flash device that includes the capability to read and write a page directly without worrying about the erase before write constraint. The FTL also provides wear leveling by distributing writes uniformly across the media. Typically the FTL is implemented using a micro controller within the flash storage device and is hidden behind the interface such as compact flash USB SD that is used to access the flash memory. However the FTL may be implemented on the processor and memory of the embedded device in some cases.

As further shown in the storage manager includes a logical storage that provides functionalities of an FTL for devices without an FTL e.g. flash chip . Components of the FlashDB that reside over the logical storage may access sectors of the logical storage through application programming interfaces APIs . For example in some embodiments sectors of the logical storage are accessed through two APIs ReadSector and WriteSector with a granularity of a sector of the same size as a physical flash page. Also available addresses for writing may be obtained using another API e.g. Alloc and unused sectors freed using another API e.g. Free .

The logical storage may hide flash specific complexities using an out of place update. In brief it is known that a block is the smallest unit of erase operation in a NAND flash memory while reads and writes are handled by pages. Because existing data on flash memory may not be over written updated unless it is erased first i.e. in place updating it is typically more economical to not overwrite data on update but rather to write data to free space and the older versions of data are then invalidated or considered as dirty . This practice is known as out of place updating. Using out of place updating when the API WriteSector addr data is called the logical storage finds the next unused physical page p writes data to it and maintains a mapping from a logical address addr to a physical page p. The page previously mapped by addr is marked dirty. 

The mapping from the logical address addr to a physical page p required for out of place updating may be maintained using a structure table labeled node translation table NTT in in the non persistent random access memory. Techniques for efficient access to flash database indices in accordance with the teachings of the present disclosure may use various known indexing schemes. In further embodiments techniques disclosed herein may also use a novel self tuning indexing scheme that can adapt itself to the dynamic behavior of multiple device and workload parameters that affect performance as described in our co pending commonly owned U.S. patent application Ser. No. 11 739 018 entitled Self Tuning Index for Flash Based Databases previously incorporated by reference.

With continued reference to the storage manager also includes a garbage collector that cleans dirty pages produced by the logical storage . Since a page cannot be erased independently the garbage collector first chooses a flash block containing dirty pages. Then valid pages of the block are copied to another block and finally the block is erased.

The storage manager may be configured with a partition not shown of the physical storage space. Other non database applications bypassing the storage manager using for example the file system operate outside this partition. The storage manager partition can be grown or shrunk dynamically. Growing the partition does not affect existing data. Subsequent API operations e.g. Alloc and WriteSector take this additional physical space into account. Shrinking the partition however typically requires remapping used sectors and copying their data to pages within the new partition.

As further shown in the storage manager also includes a node translation table NTT a log buffer and a log garbage collection LGC component . The functionalities of these components will be described below in association with an index structure used by the FlashDB .

The data stored on the flash is organized using an index tree. More specifically shows a representative index tree to store data. In this embodiment the data tree includes a plurality of index nodes configured in a B tree data structure. In general the B tree data structure is a popular indexing data structure known for efficiently supporting queries and operations and used in various incarnations in different database systems. When an operation is performed on an index node of the index tree each node update operation e.g. adding or deleting keys is written as a separate log entry. Thus to read a node all its log entries which may be spread over multiple sectors need to be read and parsed.

Referring once again to the log buffer of the storage manager is used only by the index nodes currently in Log mode. When an index node in Log mode is modified the corresponding entries are temporarily held in the log buffer . The log buffer may be flushed to flash when the log buffer has approximately one sector worth of data advantageously helping to avoid relatively expensive small writes.

As previously noted the mapping from the logical address addr to a physical page p required for out of place updating is maintained using a table denoted NTT . The node translation table NTT of the storage manager maintains the mapping between the index nodes to their physical representations. For example shows a portion of the NTT for the data tree of . In this embodiment the NTT portion includes a list corresponding to each of the index nodes of the data tree . Each sub list such as within corresponds to a single node and includes one or more addresses .

The list for the node contains the addresses of the entries relevant to the node . More specifically for some nodes such as the index node the NTT records the addresses of a single sector e.g. address 5 where the index node is written on flash and for other nodes the NTT maintains a plurality of addresses of all the sectors or pages that contained at least one valid log entry for that node. For example for the index node the corresponding list contains three addresses indicating that the node has at least one log entry in sector address sector address and sector address . In brief the NTT portion maintains a list corresponding to each index node of the index tree .

A sector or page containing log entries of one index node may also contain log entries for other index nodes. For example shows an exemplary portion of a flash storage suitable for implementing techniques in accordance with the teachings of the present disclosure. The flash storage portion includes three sectors e.g sectors and and each sector includes one or more log entries corresponding to various index nodes of the data tree . In the representative embodiment shown in the fourth sector includes log entries for two index nodes index node and index node and the sixth sector also contains log entries for to index nodes index node and index node . The fifth sector includes log entries for a single index node . Comparison of the log entries shown in with the addresses shown in e.g. sector contains log entries for index nodes and reveals the correlation between the NTT portion and the flash storage portion .

Generally program modules executed on the device may include routines programs objects components data structures etc. for performing particular tasks or implementing particular abstract data types. These program modules and the like may be executed as a native code or may be downloaded and executed such as in a virtual machine or other just in time compilation execution environments. Typically the functionality of the program modules may be combined or distributed as desired in various implementations.

An implementation of these modules and techniques may be stored on or transmitted across some form of computer readable media. Computer readable media can be any available media that can be accessed by a computer. By way of example and not limitation computer readable media may comprise computer storage media that includes volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium including paper punch cards and the like which can be used to store the desired information and which can be accessed by a computer.

Having described an exemplary environment and an exemplary device in which techniques in accordance with the present disclosure may be implemented exemplary processes for efficiently accessing flash databases will be described in the following section.

As is generally known database operations such as key search addition and deletion on a data tree of the type shown in translate to create read and update of the index nodes . Given the node translation table NTT shown in we perform these node level operations as described below.

To create a node with an identifier x an entry is created with the identifier x in the NTT . To read or update the node x we read the node from or update to the sectors given by the corresponding list for the node x. More specifically in a log structured index to update a node x a log entry is constructed for the update operation and placed into the log buffer . Later when the log buffer has approximately one sector worth of data all the log entries in the log buffer are written to an available sector. In some embodiments the available sector is provided by an API e.g. Alloc of the logical storage . The address of the sector is then added to the linked list corresponding to the node x into NTT . To read the node x the log buffer is read and then all the sectors in the linked list are read to collect log entries for the node x and parse the logs to construct the logical node.

In building an index an index node of the data tree can get updated many times resulting in a large number of log entries potentially spread over a large number of sectors on flash. This has two disadvantages. First it makes each of the lists of the node translation table NTT long and increases the memory footprint of the NTT. Second it becomes more expensive resource intensive to read the node since a large number of sectors are read.

To overcome these concerns a conventional log compaction may be used. In addition a process disclosed herein called semantic compression may also be used. For example is a flow diagram of an exemplary process for accessing a flash database in accordance with the teachings of the present disclosure. The process is illustrated as a collection of blocks in a logical flow graph which represents a sequence of operations that can be implemented in hardware software or a combination thereof. In the context of software the blocks represent computer instructions that when executed by one or more processors perform the recited operations. For discussion purposes the process is described with reference to the exemplary components described above with reference to .

As shown in one or more node level operations e.g. create read update etc. are performed at and the corresponding lists of the NTT are supplemented at . At a determination is made whether to perform log compaction. Typically the determination may be made based on a log compaction threshold on available space on the storage device for example when the storage device is 90 full. If it is determined that log compaction is desired at the process performs log compaction at .

In log compaction at all the log entries for a particular node are read and then written back to a small number of new sectors. This is helpful since log entries for the node may share sectors with log entries of other nodes and hence provides the opportunity to be clustered into a fewer number of sectors. An upper bound on the number of sectors required for a node cannot be predetermined since the number of log entries for a node can grow indefinitely over time. As described below semantic compression is intended to address this concern.

As shown in if it is determined at that log compaction is unnecessary or after log compaction is performed at the process determines whether semantic compression is desired at . The determination whether to perform semantic compression may be made based on a semantic compression threshold on the number of log entries for the node for example when the node has more than 32 log entries it can be semantically compressed.

If semantic compression is desired then semantic compression is performed at block . In semantic compression log entries having opposite semantics are discarded during compaction. For example if a data item k is added to the node x and then deleted from it later e.g. after a node split operation after the node x becomes full the node x will have log entries ADD KEY k and DELETE KEY k. Since the subsequent log entry opposes the earlier log entry these two log entries cancel each other and may therefore be discarded. Similarly multiple UPDATE POINTER log entries for the node x may be replaced by overruled by a subsequent log entry. Other examples of log entries having opposite semantics may be conceived and semantic compression is not limited to the particular examples of opposite semantics recited above.

For semantic compression the sequence number of the log entries must be considered such that the logs are applied in proper order. It may be shown that if a node can contain at most n data items it will have at most n 1 log entries bounding the size of the linked list in node transition table NTT to be n 1 entries per sector. Semantic compression is greatly facilitated by log entries having a version number which is incremented after each semantic compression. After compression the NTT is updated with the current sector address list. During subsequent reads log entries of older versions are ignored.

Semantic compression introduces stale log entries having older version numbers and the log garbage collection LGC component may be used to reclaim the space. Therefore the process determines whether log garbage collection using the LGC component is desired at .

Note that the LGC component is different from the garbage collector . The garbage collector reclaims spaces from dirty pages while the LGC component reclaims spaces from dirty log entries. The LGC component may be activated for example when the flash is low in available space e.g. when the storage manager fails to allocate a new sector . In some embodiments the LGC component may reside within the storage manager as shown in . In alternate embodiments however the LGC component may reside in any other suitable portion of the database architecture .

If log garbage collection is desired the LGC component performs log garbage collection at . The LGC component may begin by scanning the whole flash. For each sector the LGC component first looks at the sector s header information to determine if the sector contains log entries. Such sectors are termed Log sectors. For each Log sector the LGC component counts the fraction of stale log entries in that sector. If the fraction is above a threshold value the sector is selected for garbage collection. The LGC component and then writes the fresh log entries to the log buffer removes the sector address from the NTT and returns the sector to the storage manager . The log buffer eventually flushes the log entries to flash and adds the new addresses to the NTT .

As further shown in if it is determined that semantic compression is not desired at or after semantic compression is performed at or after LGC is performed at the process determines whether additional node level or database operations are to be performed at . If so the process returns to performing node level operations at and the above described activities may be repeated indefinitely. Alternate if no additional operations are desired at the process continues or terminates at .

Occasionally a crash occurs such that the information in the in memory node translation table is lost. Typically the log entries in the flash database contain enough information such that even if the application crashes and loses its in memory NTT the NTT can be reconstructed. Thus existing methods of recovering from such a crash typically involve scanning the entire flash to reconstruct the in memory NTT . This process of recovering the in memory NTT is expensive resource intensive .

Techniques in accordance with the present disclosure provide improved processes for recovering from crashes and reconstructing NTT using checkpointing and rollback. Checkpointing allows a device to capture the state of an index while rollback allows the device to go back to a previously checkpointed state. These techniques may advantageously help the device deal with software bugs hardware glitches energy depletion and other possible faults in the environment .

For example is a flow diagram of another exemplary process for accessing a flash database in accordance with the teachings of the present disclosure. Again the process is illustrated as a collection of blocks in a logical flow graph which represents a sequence of operations that can be implemented in hardware software or a combination thereof.

At one or more node level operations are performed and the corresponding lists of the NTT are supplemented at . At a determination is made whether to perform a checkpoint storage. Typically the determination may be made based on a checkpoint threshold. For example the checkpointing process may be periodic so that the system periodically e.g. once every hour checkpoints its state. If it is determined that checkpoint storage is desired at the process performs the checkpoint storage at .

Checkpointing at requires making both in memory states and in flash data persistent. Simply storing the node translation table may not be sufficient due to logical storage and garbage collection functions in the storage manager . One consideration is that the NTT keeps track of logical addresses of sectors and the logical storage may change the mapping between logical to physical addresses over time. So if a later rollback operation loads a previously checkpointed node translation table physical pages currently mapped by sector addresses in NTT may not be the same ones mapped during that checkpoint time. To address this during checkpointing at the sector addresses in NTT are replaced with their physical addresses and the physical addresses are stored.

As further shown in a determination is made at whether to perform garbage collection. If garbage collection is desired then it is performed at . Another consideration important to checkpointing is that garbage collection may copy the content of the page p to a new location p and erase p. If p is part of a checkpointed version however a future rollback operation will fail to find the data for p which is now in p . To address this during garbage collection the checkpointed NTT is updated with p . Note that it is not necessary to update the whole NTT only the page containing p needs updating. Moreover garbage collection is an infrequent operation so the amortized cost is small. Since updating in flash NTT is expensive blocks with no checkpointed data are preferred over the ones having checkpointed data for garbage collection.

The process determines whether additional operations are to be performed at . If so the process proceeds to determine whether a rollback is needed at . If so a rollback is performed at . Rollback requires loading the NTT into memory creating new logical addresses in logical storage at an active physical addresses and in flash NTT and placing the logical addresses in the restored NTT in memory. The process then returns to performing node level operations at and the above described activities may be repeated indefinitely. Alternate if no additional operations are desired at the process continues or terminates at .

Techniques in accordance with the teachings of the present disclosure may provide significant advantages. For example processes in accordance with the present disclosure may reduce the size of the node translation table in the flash database memory and may enable the flash memory to be accessed faster and more energy efficiently. Such processes may also improve the ability of the flash memory to recover from crashes. Techniques in accordance with the teachings of the present disclosure may therefore improve the overall performance of flash memories and of the devices that use flash memories.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claims.

