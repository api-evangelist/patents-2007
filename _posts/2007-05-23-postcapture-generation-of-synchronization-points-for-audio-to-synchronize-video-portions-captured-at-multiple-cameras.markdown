---

title: Post-capture generation of synchronization points for audio to synchronize video portions captured at multiple cameras
abstract: Embodiments of the invention relate generally to computing devices and systems, software, computer programs, applications, and user interfaces, and more particularly, to synchronizing portions of video as a function of the post-capture generation of synchronization points for audio, the portions of video being captured at multiple cameras.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08111326&OS=08111326&RS=08111326
owner: Adobe Systems Incorporated
number: 08111326
owner_city: San Jose
owner_country: US
publication_date: 20070523
---
Embodiments of the invention relate generally to computing devices and systems software computer programs applications and user interfaces and more particularly to synchronizing portions of video as a function of the post capture generation of synchronization points for audio the portions of video being captured at multiple cameras.

When editing audio and video captured by multiple cameras traditional media editing applications typically operate on the premise that audio portions captured at different cameras angles are coextensive with the captured video and align at a common point in time. But this is often not the case. In practice the spatial arrangement of the multiple cameras as well as the environment contribute to deviations in audio relative to some point in time. These deviations which can be as small as a fraction of a second can lead to two or more captured audio portions being out of synchronization as perceived for example by a human listener.

One common technique for synchronizing the video captured at capture devices and is to implement time codes associated with each video or otherwise use some sort of global synchronization signal to synchronize both the video and audio portions. In particular a user is usually required to manually adjust the different videos to bring their time codes into agreement. A time code normally describes the relative progression of a video images in terms of an hour minute second and frame e.g. HH MM SS FR . But a drawback to using time codes to synchronize audio requires the user to synchronize different video portions to a particular frame before synchronizing the audio portions. The effort to synchronize the audio is further exacerbated due to the number of samples of audio sound that is captured relative to the number of video frames. Typically for each frame of video e.g. 30 frames per second there are 1 600 samples of audio e.g. 48 000 samples per second . As such audio portions for capture devices and are typically synchronized based on the video portions and their time codes which can contribute to undesired sound delays and echoing effects. Another common technique for synchronizing the audio and the video captured at capture devices and is to use a clapper to generate a distinctive sound during the capture of the audio and video. A clapper creates an audible sound as a reference sound to synchronize audio during the capture of the audio. The clapper sound is used for editing purposes and is discarded during editing. Consider that a clapper not shown generates a sound noise for capture by capture devices and . Thus clapper noise can be used to synchronize the audio. A drawback to using clapper noise to synchronize audio is that the distance from noise and capture devices and can cause delays that hinder synchronization of the audio relating to scene .

It would be desirable to provide improved computing devices and systems software computer programs applications and user interfaces that minimize one or more of the drawbacks associated with conventional techniques for synchronizing either audio or video or both.

Like reference numerals refer to corresponding parts throughout the several views of the drawings. Note that most of the reference numerals include one or two left most digits that generally identify the figure that first introduces that reference number.

In view of the foregoing audio synchronization point generator can implement post capture synchronization points to automatically identify synchronization points for at least two portions of audio according to at least one embodiment of the invention. This can reduce the manual identification of equivalent portions of audio and or video for synchronization purposes. Further audio synchronization point generator can generate post capture synchronization points as a function of subject audio which can include sounds generated by the subject and or scene for which audio and or video was captured. Subject audio is sought to be captured as content rather than for production e.g. editing purposes. As such the generation of post capture synchronization points for audio to synchronize video reduces the necessity to rely on external synchronization information to synchronize audio. Examples of external synchronization information include time codes and clapper sounds as well as other synchronization signals and artificially inserted sounds i.e. non subject sounds that provide for synchronization points either prior to or during the capture of audio and video or both. In addition the implementation of audio synchronization point generator can conserve resources and computational overhead by reducing the need to implement hardware to create the external synchronization information.

In operation reference audio selector can be configured to analyze content files to identify audio content within each content file for synchronization purposes. For example reference audio selector can be configured to extract audio portions from content files stored in a repository not shown while preserving associations to the corresponding video portions to synchronize both the audio and video. Reference audio selector can also be configured to designate an audio portion from a content file as reference audio . In a specific embodiment an audio portion can be designated as reference audio as a function of an amount of data associated with the content file or its audio portion. Specifically reference audio selector can determine which content file has the largest amount of data for either the content or the audio or both and then designate the audio portion from that content file as reference audio . In at least one specific embodiment reference audio can have the longest duration relative to other audio portions from other content files. In one embodiment reference audio selector can also select one of the other audio portions as specimen audio .

Vicinity determinator can be configured to generate a vicinity range within which band is compared to specimen audio . Vicinity determinator can be configured to size vicinity range to any duration. For example vicinity determinator can size vicinity range to 40 of RL i.e. 12 minutes if RL is 30 minutes . Aligner can be configured to align vicinity range with an alignment point that is coincident or substantially coincident with alignment point for band . In one embodiment a time reference such as a time code can constitute alignment points and . While aligner can be configured to position vicinity range in any relation to alignment point aligner has centered vicinity range in this example such that a first half and a second half each includes 20 RL. In some embodiments vicinity range can extend up to 100 of reference audio .

Sound attribute analyzer and candidate synchronization point detector can be respectively configured to analyze an attribute of sound associated with reference audio and determine whether the attribute of sound for subsets of specimen audio is substantially equivalent. In at least one embodiment sound attribute analyzer can be configured to characterize a portion of reference audio such as band in terms of at least one audio attribute to form a characterized portion not shown of reference audio . In one embodiment sound attribute analyzer can be configured to form the characterized portion of reference audio to identify a pattern and to search specimen audio to find other matching patterns. Candidate synchronization point detector can be configured to determine that specimen audio includes the characterized portion of reference audio and to generate at least a candidate synchronization point. In one embodiment candidate synchronization point detector can be configured to detect a matching pattern in specimen audio to establish a candidate synchronization point. In at least one embodiment band selector generator is configured to select another band should candidate synchronization point detector fail to detect a matching pattern and is further configured to continue selecting other bands until either at least one candidate synchronization point is detected or none is. In the latter case audio synchronization point generator can so indicate a state of no match to a user via for example a user interface not shown . Then reference audio selector can select another specimen audio not shown for performing similar analysis.

In one embodiment the attribute of sound can be the amplitude for an audio waveform that can expressed in percentages decibels and the like relative to time. As such sound attribute analyzer can be configured to analyze the audio waveform amplitude in band of reference audio to identify a pattern of waveform amplitudes as shown in band of . As such sound attribute analyzer can compare waveform amplitudes of band with waveform amplitudes of specimen audio . To determine whether a candidate synchronization point exists candidate synchronization point detector determines whether the waveform amplitudes for band match the waveform amplitudes for one or more subsets of specimen audio . In the example shown candidate synchronization point detector is configured to detect that the waveform amplitudes for band are equivalent to the waveform amplitudes for subset . Subsequently candidate synchronization point detector can generate a candidate synchronization point for band and subset . In at least one embodiment the candidate synchronization point for band and subset can be located at or near band and subset so as to provide for the alignment of band and subset relative to each other. In a specific embodiment a candidate synchronization point can be implemented as a post capture synchronization point. In various embodiments the attribute of sound can represent any characteristic of audio with which to compare and match portions of reference audio and specimen audio . For example the attribute of sound can also be the amplitude for an audio waveform relative to frequency. As such sound attribute analyzer can be configured to analyze the spectral frequencies and audio waveform amplitude in band to identify a pattern for a frequency spectrum which can be compared against subsets of specimen audio including subset .

Candidate synchronization point detector can be configured to provide a tolerance among values for the audio attribute to reduce false negatives i.e. improper indications of mismatches between reference audio and specimen audio due to differences in tone background noise volume and the like that manifest in different audio portions that are captured by different cameras at different angles and spatial locations according to at least one embodiment. In one embodiment candidate synchronization point detector can be configured to establish a deviation from a pattern for band to form a deviated pattern not shown within which a subset of specimen audio is deemed to match the pattern. As such candidate synchronization point detector can be configured to generate a candidate synchronization point if a portion of specimen audio such as vicinity range includes the deviated pattern such as in subset . In one instance if the amplitudes for the audio waveform for both band and a particular subset of specimen audio deviate less than an amount defined as a tolerance such as by 5 then band and that particular subset of specimen audio can be deemed as being equivalent or substantially equivalent .

As used herein the term synchronization point refers generally at least in one embodiment to a point at which portions of two or more audio waveforms such as those captured by multiple capture devices at different angles and or positions are in synchronization. As an example consider that matching shapes for portions of two or more audio waveforms relative to a point in time can constitute a synchronization point. As such a synchronization point can indicate that a part of two or more audio waveforms are in synchronicity. In one embodiment a synchronization point can represent matching portions of multiple audio waveforms. In another embodiment a synchronization point can also refer to point in time relative to the matching portions. As used herein the term post capture refers generally at least in one embodiment to post production activity that occurs after capturing video and audio and includes the process of editing content. As used herein the term subject audio refers generally at least in one embodiment to the audio generated by something such as a person an object or a scene that is captured by multiple capture devices for purposes of producing either a video with audio or an audio recording along such as a movie or music. An example of a subject audio is the sounds produced by actors such as their voices.

As used herein the term post capture synchronization point refers generally at least in one embodiment to a synchronization point that can be generated as part of post production activity based on the subject audio. As such the audio used to generate synchronization points includes sounds that are intended to remain within the final product such as a movie or music in accordance with at least one embodiment. In various embodiments post capture synchronization points may or may not result from certifying candidate synchronization points. As used herein the term candidate synchronization point refers generally at least in one embodiment to a synchronization point that has yet to be certified and thus is not confirmed as being a post capture synchronization point with sufficient certainty. As used herein the term confirmatory candidate synchronization points refers generally at least in one embodiment to additional candidate synchronization points that are examined to certify the candidate synchronization point.

As used herein the term audio refers generally at least in one embodiment to one or more sounds that are audible e.g. perceived by humans and can be of or relate to the transmission storage reproduction or reception of sound. For example audio can be in the form of an audio waveform an audio file an audio signal an audio clip an audio track and the like. As used herein the term video refers generally at least in one embodiment to one or more images that are visible e.g. perceived by humans and can be of or relate to the transmission storage reproduction or reception of images. For example video can be in the form of a video waveform a video file a video signal a video clip a video track and the like. As used herein the term external synchronization information refers generally at least in one embodiment to one or more indicia e.g. time codes clapper sounds and the like as well as other synchronization signals and artificially inserted sounds i.e. non subject sounds that provide for synchronization points either prior to or during the capture of audio and video or both. As used herein the terms audio attribute and attribute of sound refer generally at least in one embodiment to a characteristic property quality or state that an audio or a portion thereof has that can be quantified to determine whether two or more portions of audio are either equivalent or are not equivalent. Examples of audio attributes include the shapes or patterns of audio waveforms e.g. in terms of amplitude and time as well as the shapes of frequency spectra e.g. in terms of amplitude and frequency . As used herein the term content refers generally at least in one embodiment to information and or material presented within a display an interface or the like in relation to for example an audio and or visual presentation of sounds and or imagery. Examples of content include text such as an electronic document e.g. a document in Portable Document Format PDF as well as audio images audio video media such as Flash presentations text and the like. As used herein the term panel at least in one embodiment can refer to displays palettes tabs windows screens portions of an interface and the like. As such a content file or media file can include a digital data file which is composed of images sound and words for one camera angle.

Sound attribute analyzer is configured to compare band which is band to various subsets such as subsets and . In one embodiment sound attribute analyzer compares the first audio sample of band and subsequent samples therein to the first audio sample of specimen audio portion which is the 0sample i.e. the sample at S and subsequent samples within subset . In some embodiments reference audio and specimen audio portions and include 48 000 audio samples per second. As such 18 seconds of audio in band as well as subsets and include 1 600 samples. So sound attribute analyzer can compare 864 000 samples or fewer in some cases between band and a subset of a specimen audio.

Continuing with the above example candidate synchronization point detector can determine whether each of the audio samples match e.g. within an amount of tolerance . If so then candidate synchronization point detector generates a candidate synchronization point. If not then candidate synchronization point detector generates an indication that subset is not equal to band . Moving along to the next analysis sound attribute analyzer compares the first audio sample of band and subsequent samples therein to the second audio sample of specimen audio which is the 1sample i.e. the sample at S and subsequent samples within subset . Here subset does not match with band and thus candidate synchronization point detector generates an indication to indicate the mismatch. Sound attribute analyzer and candidate synchronization point detector continue to cooperate as described above until the naudio sample of specimen audio is reached. Here the nsample i.e. the sample at Sn and subsequent samples within subset are matched to corresponding audio samples in band . Upon determining a match candidate synchronization point detector generates an indication that the subset is equal or is substantially equal to band and generates a candidate synchronization point. In various embodiments sound attribute analyzer can compare bands and subsets in any order or quantity. In some embodiments sound attribute analyzer can compare fewer than the quantity of samples in bands or any number of samples by skipping audio sample comparisons to for example reduce the number of comparisons.

Point of reference synchronizer is configured to select corresponding subsets of specimen audio that are separated by similar amounts of time units from each other. For example point of reference synchronizer can designate additional band as a point of reference from which to index other additional bands to . As such additional bands and are separated by t time units additional bands and are separated by t time units and so on. Similarly point of reference synchronizer can designate subset as a point of reference from which to index other subsets to . As such additional subsets and are separated by t time units additional subsets and are separated by t time units and so on. In one embodiment synchronization point certifier subdivides reference audio into regions such as 5 regions and randomly selects a band in each region. Each randomly selected band can be indexed in time with respect to a point of reference for reference audio such as the first randomly selected band. Further synchronization point certifier can identify subsets in specimen audio that correspond to the randomly selected bands.

In one embodiment synchronization point certifier includes matchers to match additional bands and to corresponding subsets and note that matchers for additional bands and are omitted . For example matcher is configured to determine whether additional band matches corresponding subset . Matcher performs similar matching operations. Synchronization point certifier can be configured to provide a threshold for a number of positive matches by matchers and as well as others not shown to certify that the candidate synchronization point is a post capture synchronization point. In a specific embodiment synchronization point certifier implements 5 matchers similar to matchers and and provides a threshold of 3 positive matches. As such synchronization point certifier can operate to certify that a candidate synchronization point is a post capture synchronization point if 3 or more matchers determine matches between additional bands and subsets of specimen audio . But if synchronization point certifier detects 2 or fewer matches then the candidate synchronization point is discarded as rejected in favor of for example determining another candidate synchronization point.

In this example multi camera panel portion includes content captured at different capture devices each of which is at a different angle and or position such as content and . Here content and include a reference video Ref Video a first specimen video Spec Vid and a second specimen video Spec Vid respectively which in turn are associated with a reference audio track a first specimen audio track Spec Audio Track and a second specimen audio track Spec Audio Track . Audio based synchronization panel is configured to present a reference audio track a first specimen audio track Spec Audio Track and a second specimen audio track Spec Audio Track each of which is synchronized at or near a post capture synchronization point according to one embodiments. Presentation of post capture synchronization point can be optional. An editor then can observe the videos and and audio tracks and to determine whether they have been properly synchronized using an audio synchronization point generator not shown . For example the editor can observe e.g. view and or listen to the audible sounds among band subset and subset to determine whether they are sufficiently synchronized. If so the editor can accept a post capture synchronization point by for example selecting an accept button A to generate an accept command. Otherwise the selection of rejection button R causes the generation of a reject command to for example initialize the audio synchronization point generator to again try to find the proper post capture synchronization point.

Interactive panel portion can be configured to accept input from the editor to modify the operation of the audio synchronization point generator to reexamine the candidate synchronization point. For example interactive panel portion can include band size input field to modify the size of one or more bands a vicinity range input field to modify the size of a vicinity range in which perform comparisons a number of comparisons per band input field No. Comp Band to change the number of audio samples per band e.g. 80 of audio samples in a band that are used to compare against specimen audio subsets a tolerance range input field to modify the number of deviant waveform values that can result in a match and a number of confirmatory candidate synchronization points per certification of a post capture synchronization point No. Confirm. SP such as changing a threshold from 3 matches out of 5 confirmatory candidate synchronization points to for example 4 out of 5 to certify.

In some examples logic module can be configured to control panel generator to form a multi camera panels that are configured to present audio tracks that are automatically synchronized based on subject audio. Rendering engine can be configured to operate as a layout engine for web pages for example to manipulate both content e.g. as expressed in or including HTML XML image files etc. and formatting information e.g. as expressed in or including CSS XSL etc. for rendering the data or information as one or more panels on interface . Interface module can exchange panel presentation data including content data image data audio data as well as other data between application and another application e.g. a host client web services based distributed i.e. enterprise application programming interface API operating system program procedure or others that can use data and information generated from panel generator to render presented panels on a display screen. In other examples the above described techniques and elements can be varied in design implementation and function and are not limited to the descriptions provided. In one embodiment logic module can include an audio synchronization point generator that is configured to include structure and or functionality similar to one or more previously described audio synchronization point generators.

In some examples logic module and panel generator can be implemented as part of application which can be implemented separately from other functional components or modules such as interface module display module rendering module and repository . Data bus can be implemented to communicate data over a given port between application and interface module display module rendering module and repository . In some instances application can be implemented as a standalone application or as a component i.e. module of another application. Data or information e.g. content file data including either video data or audio data or both data for modifying the operation of the audio synchronization point generator data for describing candidate and post capture synchronization points data for describing one or more audio attributes and the like associated with a panel can be stored in repository which can be implemented using a database data store data warehouse or any other type of data repository or structure. In other examples more fewer or different modules can be used to implement the described techniques for panel presentation and are not limited to those provided.

According to some examples computer system performs specific operations in which processor executes one or more sequences of one or more instructions stored in system memory . Such instructions can be read into system memory from another computer readable medium such as static storage device or disk drive . In some examples hard wired circuitry can be used in place of or in combination with software instructions for implementation. In the example shown system memory includes modules of executable instructions for implementing an operation system O S an application and an audio synchronization point generation module .

The term computer readable medium refers at least in one embodiment to any medium that participates in providing instructions to processor for execution. Such a medium can take many forms including but not limited to non volatile media volatile media and transmission media. Non volatile media includes for example optical or magnetic disks such as disk drive . Volatile media includes dynamic memory such as system memory . Transmission media includes coaxial cables copper wire and fiber optics including wires that comprise bus . Transmission media can also take the form of acoustic or light waves such as those generated during radio wave and infrared data communications.

Common forms of computer readable media includes for example floppy disk flexible disk hard disk magnetic tape any other magnetic medium CD ROM any other optical medium punch cards paper tape any other physical medium with patterns of holes RAM PROM EPROM FLASH EPROM any other memory chip or cartridge carrier wave or any other medium from which a computer can read.

In some examples execution of the sequences of instructions can be performed by a single computer system . According to some examples two or more computer systems coupled by communication link e.g. LAN PSTN or wireless network can perform the sequence of instructions in coordination with one another. Computer system can transmit and receive messages data and instructions including program code i.e. application code through communication link and communication interface . Received program code can be executed by processor as it is received and or stored in disk drive or other non volatile storage for later execution. In one embodiment system is implemented as a hand held device such as a mobile phone . But in other embodiments system can be implemented as a personal computer i.e. a desk top computer or any other computing device.

In some examples one or more panels for synchronizing video can be presented on interface which can be an interface for an application such as a video and audio editing application or as a web browsing program Internet content portal client or desktop application for any purpose. Panels can be used to provide additional or supplemental information that can be contextually relevant to another panel presented in interface . Computer notebook computer notebook or laptop smart phone personal digital assistant PDA server and administrator computer can provide content data for rendering content as well as other data which can be implemented to generate for example post capture synchronization points and or synchronized audio tracks in interface . In some cases an operating system installed on computer can communicate i.e. via an application programming interface API content data and or other related data to another application installed on computer to render i.e. interpreting data and information to draw or display the content in an interface one or more panels presented in interface . In some examples different types of panels can be rendered in interface . In one embodiment interface can include any number and or any type of display environments such as CRT and LCD displays. Note that the above described system and elements can be varied and are not limited to the descriptions or examples provided.

In at least some of the embodiments of the invention the structures and or functions of any of the above described interfaces and panels can be implemented in software hardware firmware circuitry or a combination thereof. Note that the structures and constituent elements shown in as well as their functionality can be aggregated with one or more other structures or elements. Alternatively the elements and their functionality can be subdivided into constituent sub elements if any. As software the above described described techniques can be implemented using various types of programming or formatting languages frameworks syntax applications protocols objects or techniques including C Objective C C C Flex Fireworks Java Javascript AJAX COBOL Fortran ADA XML HTML DHTML XHTML HTTP XMPP and others. These can be varied and are not limited to the examples or descriptions provided.

The foregoing description for purposes of explanation used specific nomenclature to provide a thorough understanding of the invention. However it will be apparent to one skilled in the art that specific details are not required in order to practice the invention. In fact this description should not be read to limit any feature or aspect of the present invention to any embodiment rather features and aspects of one embodiment can readily be interchanged with other embodiments.

Thus the foregoing descriptions of specific embodiments of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed many alternatives modifications equivalents and variations are possible in view of the above teachings. For the purpose of clarity technical material that is known in the technical fields related to the embodiments has not been described in detail to avoid unnecessarily obscuring the description. Thus the various embodiments can be modified within the scope and equivalents of the appended claims. Further the embodiments were chosen and described in order to best explain the principles of the invention and its practical applications they thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated. Notably not every benefit described herein need be realized by each embodiment of the present invention rather any specific embodiment can provide one or more of the advantages discussed above. In the claims elements and or operations do not imply any particular order of operation unless explicitly stated in the claims. It is intended that the following claims and their equivalents define the scope of the invention.

