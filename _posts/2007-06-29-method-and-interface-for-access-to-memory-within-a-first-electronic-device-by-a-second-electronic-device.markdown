---

title: Method and interface for access to memory within a first electronic device by a second electronic device
abstract: In one embodiment of the present invention, a two-register interface is provided by a first electronic device to allow access to memory within the electronic device by external electronic devices. The two-register interface is mapped from the memory of an accessing, second electronic device. READ and WRITE accesses are transmitted from the accessing, second electronic device to the two-register interface through a communications medium. A first register of the two-register interface directs access to a particular memory location, and the second register of the two-register interface provides a portal for both READ and WRITE access to the particular memory location.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08281084&OS=08281084&RS=08281084
owner: Emlilex Design & Manufacturing Corp.
number: 08281084
owner_city: Costa Mesa
owner_country: US
publication_date: 20070629
---
This application is a continuation in part of application Ser. No. 11 655 778 filed Jan. 19 2007 now abandoned which is a continuation in part of application Ser. No. 10 702 137 filed Nov. 4 2003 now U.S. Pat. No. 7 634 614 which is a continuation in part of application Ser. No. 10 602 529 filed Jun. 23 2003 now U.S. Pat. No. 7 353 321 which is a continuation in part of application Ser. No. 10 341 835 filed Jan. 13 2003 now abandoned.

The present invention is related inter device communications and interfaces to electronic devices including single integrated circuit implementations of I O controllers and other electronic devices incorporated within mass storage device systems and in particular to an efficient and reliable memory access interface.

The fibre channel FC is an architecture and protocol for a data communication network that interconnects a number of different combinations of computers and peripheral devices. The FC supports a variety of upper level protocols including the small computer systems interface SCSI protocol. A computer or peripheral device is linked to the network through an FC port and copper wires or optical fibers. An FC port includes a transceiver and an interface controller and the computer peripheral device in which the FC port is contained is called a host. The FC port exchanges data with the host via a local data bus such as a peripheral computer interface PCI bus. The interface controller conducts lower level protocol exchanges between the fibre channel and the computer or peripheral device in which the FC port resides.

A popular paradigm for accessing remote data in computer networks is the client server architecture. According to this architecture a client computer sends a request to read or write data to a server computer. The server computer processes the request by checking that the client server has authorization and permission to read or write the data by mapping the requested read or write operation to a particular mass storage device and by serving as an intermediary in the transfer of data from the client computer to the mass storage device in case of a write operation or from the mass storage device to the client in case of a read operation.

In common currently available and previously available communication network architectures the server computer communicates with the client computer through a local area network LAN and the server computer communicates with a number of mass storage devices over a local bus such as a SCSI bus. In such systems the server is required to store and forward the data transferred as a result of the read or write operation because the server represents a bridge between two dissimilar communications media. With the advent of the FC client computers server computers and mass storage devices may all be symmetrically interconnected by a single communications medium. The traditional client server architecture is commonly ported to the FC using the same type of client server protocols as are used in the LAN and SCSI networks discussed above.

SCSI bus compatible mass storage devices including high capacity disk drives are widely available and widely used particularly in mid sized and large sized computer systems and many FC based systems employ FC compatible disk drives each including one or more FC ports and logic needed for the disk drives to function as FC responders. In smaller systems including personal computers PCs a different family of disk drives referred to as Integrated Drive Electronics IDE or Advanced Technology Attachment ATA disk drives is widely employed. A serial ATA disk SATA generally interconnects with a system via an Industry Standard Architecture ISA bus.

The present invention is related to FC SCSI and IDE ATA technologies. Each will be discussed in turn in three separate subsections below. Those familiar with any or all of these technologies may wish to skip ahead to the final subsection of this section describing FC based disk arrays and to the Summary of the Invention section that immediately follows that subsection.

The Fibre Channel FC is defined by and described in a number of ANSI Standards documents including the standards documents listed below in Table 1 

The following description of the FC is meant to introduce and summarize certain of the information contained in these documents in order to facilitate discussion of the present invention. If a more detailed discussion of any of the topics introduced in the following description is desired the above mentioned documents may be consulted.

The FC is an architecture and protocol for data communications between FC nodes generally computers workstations peripheral devices and arrays or collections of peripheral devices such as disk arrays interconnected by one or more communications media. Communications media include shielded twisted pair connections coaxial cable and optical fibers. An FC node is connected to a communications medium via at least one FC port and FC link. An FC port is an FC host adapter or FC controller that shares a register and memory interface with the processing components of the FC node and that implements in hardware and firmware the lower levels of the FC protocol. The FC node generally exchanges data and control information with the FC port using shared data structures in shared memory and using control registers in the FC port. The FC port includes serial transmitter and receiver components coupled to a communications medium via a link that comprises electrical wires or optical strands.

In the following discussion FC is used as an adjective to refer to the general Fibre Channel architecture and protocol and is used as a noun to refer to an instance of a Fibre Channel communications medium. Thus an FC architecture and protocol port may receive an FC architecture and protocol sequence from the FC communications medium .

The FC architecture and protocol support three different types of interconnection topologies shown in . shows the simplest of the three interconnected topologies called the point to point topology. In the point to point topology shown in a first node is directly connected to a second node by directly coupling the transmitter of the FC port of the first node to the receiver of the FC port of the second node and by directly connecting the transmitter of the FC port of the second node to the receiver of the FC port of the first node . The ports and used in the point to point topology are called N Ports.

In the FC arbitrated loop topology nodes contend for or arbitrate for control of the arbitrated loop. In general the node with the lowest port address obtains control in the case that more than one node is contending for control. A fairness algorithm may be implemented by nodes to ensure that all nodes eventually receive control within a reasonable amount of time. When a node has acquired control of the loop the node can open a channel to any other node within the arbitrated loop. In a half duplex channel one node transmits and the other node receives data. In a full duplex channel data may be transmitted by a first node and received by a second node at the same time that data is transmitted by the second node and received by the first node. For example if in the arbitrated loop of node opens a full duplex channel with node then data transmitted through that channel from node to node passes through NL Port of node and data transmitted by node to node passes through NL Port of node .

The FC is a serial communications medium. Data is transferred one bit at a time at extremely high transfer rates. illustrates a very simple hierarchy by which data is organized in time for transfer through an FC network. At the lowest conceptual level the data can be considered to be a stream of data bits . The smallest unit of data or grouping of data bits supported by an FC network is a 10 bit character that is decoded by FC port as an 8 bit character. FC primitives are composed of 10 bit characters or bytes. Certain FC primitives are employed to carry control information exchanged between FC ports. The next level of data organization a fundamental level with regard to the FC protocol is a frame. Seven frames are shown in . A frame may be composed of between 36 and 2 148 bytes including delimiters headers and between 0 and 2048 bytes of data. The first FC frame for example corresponds to the data bits of the stream of data bits encompassed by the horizontal bracket . The FC protocol specifies a next higher organizational level called the sequence. A first sequence and a portion of a second sequence are displayed in . The first sequence is composed of frames one through four . The second sequence is composed of frames five through seven and additional frames that are not shown. The FC protocol specifies a third organizational level called the exchange. A portion of an exchange is shown in . This exchange is composed of at least the first sequence and the second sequence shown in . This exchange can alternatively be viewed as being composed of frames one through seven and any additional frames contained in the second sequence and in any additional sequences that compose the exchange .

The FC is a full duplex data transmission medium. Frames and sequences can be simultaneously passed in both directions between an originator or initiator and a responder or target. An exchange comprises all sequences and frames within the sequences exchanged between an originator and a responder during a single I O transaction such as a read I O transaction or a write I O transaction. The FC protocol is designed to transfer data according to any number of higher level data exchange protocols including the Internet protocol IP the Small Computer Systems Interface SCSI protocol the High Performance Parallel Interface HIPPI and the Intelligent Peripheral Interface IPI . The SCSI bus architecture will be discussed in the following subsection and much of the subsequent discussion in this and remaining subsections will focus on the SCSI protocol embedded within the FC protocol. The standard adaptation of SCSI protocol to fibre channel is subsequently referred to in this document as FCP. Thus the FC can support a master slave type communications paradigm that is characteristic of the SCSI bus and other peripheral interconnection buses as well as the relatively open and unstructured communication protocols such as those used to implement the Internet. The SCSI bus architecture concepts of an initiator and target are carried forward in the FCP designed as noted above to encapsulate SCSI commands and data exchanges for transport through the FC.

The next high level section called the data payload contains the actual data packaged within the FC frame. The data payload contains data and encapsulating protocol information that is being transferred according to a higher level protocol such as IP and SCSI. shows four basic types of data payload layouts used for data transfer according to the SCSI protocol. The first of these formats called the FCP CMND is used to send a SCSI command from an initiator to a target. The FCP LUN field comprises an 8 byte address that may in certain implementations specify a particular SCSI bus adapter a target device associated with that SCSI bus adapter and a logical unit number LUN corresponding to a logical device associated with the specified target SCSI device that together represent the target for the FCP CMND. In other implementations the FCP LUN field contains an index or reference number that can be used by the target FC host adapter to determine the SCSI bus adapter a target device associated with that SCSI bus adapter and a LUN corresponding to a logical device associated with the specified target SCSI device. An actual SCSI command such as a SCSI read or write I O command is contained within the 16 byte field FCP CDB .

The second type of data payload format shown in is called the FCP XFER RDY layout. This data payload format is used to transfer a SCSI proceed command from the target to the initiator when the target is prepared to begin receiving or sending data. The third type of data payload format shown in is the FCP DATA format. The FCP DATA format is used for transferring the actual data that is being read from or written to a SCSI data storage device as a result of execution of a SCSI I O transaction. The final data payload format shown in is called the FCP RSP layout used to transfer a SCSI status byte as well as other FCP status information from the target back to the initiator upon completion of the I O transaction.

A computer bus is a set of electrical signal lines through which computer commands and data are transmitted between processing storage and input output I O components of a computer system. The SCSI I O bus is the most widespread and popular computer bus for interconnecting mass storage devices such as hard disks and CD ROM drives with the memory and processing components of computer systems. The SCSI bus architecture is defined in three major standards SCSI 1 SCSI 2 and SCSI 3. The SCSI 1 and SCSI 2 standards are published in the American National Standards Institute ANSI standards documents X3.131 1986 and X3.131 1994 respectively. The SCSI 3 standard is currently being developed by an ANSI committee. An overview of the SCSI bus architecture is provided by The SCSI Bus and IDE Interface Freidhelm Schmidt Addison Wesley Publishing Company ISBN 0 201 17514 2 1997 Schmidt .

Two important types of commands called I O commands direct the SCSI device to read data from a logical device and write data to a logical device. An I O transaction is the exchange of data between two components of the computer system generally initiated by a processing component such as the CPU that is implemented in part by a read I O command or by a write I O command. Thus I O transactions include read I O transactions and write I O transactions.

The SCSI bus is a parallel bus that can simultaneously transport a number of data bits. The number of data bits that can be simultaneously transported by the SCSI bus is referred to as the width of the bus. Different types of SCSI buses have widths of 8 16 and 32 bits. The 16 and 32 bit SCSI buses are referred to as wide SCSI buses.

As with all computer buses and processors the SCSI bus is controlled by a clock that determines the speed of operations and data transfer on the bus. SCSI buses vary in clock speed. The combination of the width of a SCSI bus and the clock rate at which the SCSI bus operates determines the number of bytes that can be transported through the SCSI bus per second or bandwidth of the SCSI bus. Different types of SCSI buses have bandwidths ranging from less than 2 megabytes Mbytes per second up to 40 Mbytes per second with increases to 80 Mbytes per second and possibly 160 Mbytes per second planned for the future. The increasing bandwidths may be accompanied by increasing limitations in the physical length of the SCSI bus.

In general a SCSI bus adapter such as SCSI bus adapters and initiates I O operations by sending commands to target devices. The target devices and receive the I O commands from the SCSI bus. The target devices and then implement the commands by interfacing with one or more logical devices that they control to either read data from the logical devices and return the data through the SCSI bus to the initiator or to write data received through the SCSI bus from the initiator to the logical devices. Finally the target devices and respond to the initiator through the SCSI bus with status messages that indicate the success or failure of implementation of the commands.

The sending of an I O command from an initiator SCSI bus adapter to a target SCSI device illustrated in initiates a read or write I O operation by the target SCSI device. Referring to the SCSI bus adapter initiates the I O operation as part of an I O transaction. Generally the SCSI bus adapter receives a read or write command via the PCI bus system controller and CPU bus from the CPU directing the SCSI bus adapter to perform either a read operation or a write operation. In a read operation the CPU directs the SCSI bus adapter to read data from a mass storage device and transfer that data via the SCSI bus PCI bus system controller and memory bus to a location within the system memory . In a write operation the CPU directs the system controller to transfer data from the system memory via the memory bus system controller and PCI bus to the SCSI bus adapter and directs the SCSI bus adapter to send the data via the SCSI bus to a mass storage device on which the data is written.

When the target senses that the target has been selected by the initiator the target assumes control of the SCSI bus in order to complete the command phase of the I O operation. The target then controls the SCSI signal lines in order to enter the MESSAGE OUT state . In a first event that occurs in the MESSAGE OUT state the target receives from the initiator an IDENTIFY message . The IDENTIFY message contains a LUN field that identifies the LUN to which the command message that will follow is addressed. The IDENTIFY message also contains a flag that is generally set to indicate to the target that the target is authorized to disconnect from the SCSI bus during the target s implementation of the I O command that will follow. The target then receives a QUEUE TAG message that indicates to the target how the I O command that will follow should be queued as well as providing the target with a queue tag . The queue tag is a byte that identifies the I O command. A SCSI bus adapter can therefore concurrently manage 256 different I O commands per LUN. The combination of the SCSI ID of the initiator SCSI bus adapter the SCSI ID of the target SCSI device the target LUN and the queue tag together comprise an I T L Q nexus reference number that uniquely identifies the I O operation corresponding to the I O command that will follow within the SCSI bus. Next the target device controls the SCSI bus signal lines in order to enter the COMMAND state . In the COMMAND state the target solicits and receives from the initiator the I O command . The I O command includes an opcode that identifies the particular command to be executed in this case a read command or a write command a logical block number that identifies the logical block of the logical device that will be the beginning point of the read or write operation specified by the command and a data length that specifies the number of blocks that will be read or written during execution of the command.

When the target has received and processed the I O command the target device controls the SCSI bus signal lines in order to enter the MESSAGE IN state in which the target device generally sends a disconnect message back to the initiator device. The target disconnects from the SCSI bus because in general the target will begin to interact with the logical device in order to prepare the logical device for the read or write operation specified by the command. The target may need to prepare buffers for receiving data and in the case of disk drives or CD ROM drives the target device may direct the logical device to seek to the appropriate block specified as the starting point for the read or write command. By disconnecting the target device frees up the SCSI bus for transportation of additional messages commands or data between the SCSI bus adapter and the target devices. In this way a large number of different I O operations can be concurrently multiplexed over the SCSI bus. Finally the target device drops the BSY signal line in order to return the SCSI bus to the BUS FREE state .

The target device then prepares the logical device for the read or write operation. When the logical device is ready for reading or writing data the data phase for the I O operation ensues. illustrates the data phase of a SCSI I O operation. The SCSI bus is initially in the BUS FREE state . The target device now ready to either return data in response to a read I O command or accept data in response to a write I O command controls the SCSI bus signal lines in order to enter the ARBITRATION state . Assuming that the target device is successful in arbitrating for control of the SCSI bus the target device controls the SCSI bus signal lines in order to enter the RESELECTION state . The RESELECTION state is similar to the SELECTION state described in the above discussion of except that it is the target device that is making the selection of a SCSI bus adapter with which to communicate in the RESELECTION state rather than the SCSI bus adapter selecting a target device in the SELECTION state.

Once the target device has selected the SCSI bus adapter the target device manipulates the SCSI bus signal lines in order to cause the SCSI bus to enter the MESSAGE IN state . In the MESSAGE IN state the target device sends both an IDENTIFY message and a QUEUE TAG message to the SCSI bus adapter. These messages are identical to the IDENTITY and QUEUE TAG messages sent by the initiator to the target device during transmission of the I O command from the initiator to the target illustrated in . The initiator may use the I T L Q nexus reference number a combination of the SCSI IDs of the initiator and target device the target LUN and the queue tag contained in the QUEUE TAG message to identify the I O transaction for which data will be subsequently sent from the target to the initiator in the case of a read operation or to which data will be subsequently transmitted by the initiator in the case of a write operation. The I T L Q nexus reference number is thus an I O operation handle that can be used by the SCSI bus adapter as an index into a table of outstanding I O commands in order to locate the appropriate buffer for receiving data from the target device in case of a read or for transmitting data to the target device in case of a write.

After sending the IDENTIFY and QUEUE TAG messages the target device controls the SCSI signal lines in order to transition to a DATA state . In the case of a read I O operation the SCSI bus will transition to the DATA IN state. In the case of a write I O operation the SCSI bus will transition to a DATA OUT state. During the time that the SCSI bus is in the DATA state the target device will transmit during each SCSI bus clock cycle a data unit having a size in bits equal to the width of the particular SCSI bus on which the data is being transmitted. In general there is a SCSI bus signal line handshake involving the signal lines ACK and REQ as part of the transfer of each unit of data. In the case of a read I O command for example the target device places the next data unit on the SCSI bus and asserts the REQ signal line. The initiator senses assertion of the REQ signal line retrieves the transmitted data from the SCSI bus and asserts the ACK signal line to acknowledge receipt of the data. This type of data transfer is called asynchronous transfer. The SCSI bus protocol also allows for the target device to transfer a certain number of data units prior to receiving the first acknowledgment from the initiator. In this transfer mode called synchronous transfer the latency between the sending of the first data unit and receipt of acknowledgment for that transmission is avoided. During data transmission the target device can interrupt the data transmission by sending a SAVE POINTERS message followed by a DISCONNECT message to the initiator and then controlling the SCSI bus signal lines to enter the BUS FREE state. This allows the target device to pause in order to interact with the logical devices which the target device controls before receiving or transmitting further data. After disconnecting from the SCSI bus the target device may then later again arbitrate for control of the SCSI bus and send additional IDENTIFY and QUEUE TAG messages to the initiator so that the initiator can resume data reception or transfer at the point that the initiator was interrupted. An example of disconnect and reconnect are shown in interrupting the DATA state . Finally when all the data for the I O operation has been transmitted the target device controls the SCSI signal lines in order to enter the MESSAGE IN state in which the target device sends a DISCONNECT message to the initiator optionally preceded by a SAVE POINTERS message. After sending the DISCONNECT message the target device drops the BSY signal line so the SCSI bus transitions to the BUS FREE state .

Following the transmission of the data for the I O operation as illustrated in the target device returns a status to the initiator during the status phase of the I O operation. illustrates the status phase of the I O operation. As in the SCSI bus transitions from the BUS FREE state to the ARBITRATION state RESELECTION state and MESSAGE IN state as in . Following transmission of an IDENTIFY message and QUEUE TAG message by the target to the initiator during the MESSAGE IN state the target device controls the SCSI bus signal lines in order to enter the STATUS state . In the STATUS state the target device sends a single status byte to the initiator to indicate whether or not the I O command was successfully completed. In the status byte corresponding to a successful completion indicated by a status code of 0 is shown being sent from the target device to the initiator. Following transmission of the status byte the target device then controls the SCSI bus signal lines in order to enter the MESSAGE IN state in which the target device sends a COMMAND COMPLETE message to the initiator. At this point the I O operation has been completed. The target device then drops the BSY signal line so that the SCSI bus returns to the BUS FREE state . The SCSI bus adapter can now finish its portion of the I O command free up any internal resources that were allocated in order to execute the command and return a completion message or status back to the CPU via the PCI bus.

In many computer systems there may be additional internal computer buses such as a PCI bus between the target FC host adapter and the target SCSI bus adapter. In other words the FC host adapter and SCSI adapter may not be packaged together in a single target component. In the interest of simplicity that additional interconnection is not shown in .

IDE ATA drives were developed in order to integrate a disk logic controller and a hard disk together as a single module. IDE ATA drives were specifically designed for easy integration via an ISA bus into PC systems. Originally IDE ATA drives were designed with parallel 16 bit interconnections to permit the exchange of two bytes of data between the IDE ATA drives and the system at discrete intervals of time controlled by a system or bus clock. Unfortunately the parallel bus interconnection is reaching a performance limit with current data rates of 100 to 133 MB sec. and the 40 or 80 pin ribbon cable connection is no longer compatible with the cramped high density packaging of internal components within modern computer systems. For these reasons a SATA SATA standard has been developed and SATA disk drives are currently being produced in which the 80 pin ribbon cable connection is replaced with a four conductor serial cable. The initial data rate for SATA disks is 150 MB sec expected to soon increase to 300 MB sec and then to 600 MB sec. Standard 8B 10B encoding is used for serializing the data for transfer between the ATA serial disk drive and a peripheral component interconnect PCI based controller. Ultimately south bridge controllers that integrate various I O controllers that provide interfaces to peripheral devices and buses and that transfer data to and from a second bridge that links one or more CPUs and memory may be designed to fully incorporate SATA technology to offer direct interconnection of SATA devices.

The ATA interface in particular the ATA 5 and ATA 6 standard interfaces support a variety of commands that allow an external processor or logic controller to direct the logic controller within the ATA disk drive to carry out basic data transfer commands seeking cache management and other management and diagnostics related tasks. Table 2 below relates a protocol number such as protocol 1 with a general type of ATA command. The types of commands include programmed input output PIO non data commands and direct memory access DMA commands.

In mid sized and large computer systems data storage requirements generally far exceed the capacities of embedded mass storage devices including embedded disk drives. In such systems it has become common to employ high end large capacity devices such as redundant arrays of inexpensive disks RAID that include internal processors that are linked to mid sized and high end computer systems through local area networks fibre optic networks and other high bandwidth communications media. To facilitate design and manufacture of disk arrays disk manufacturers provide disk drives that include FC ports in order to directly interconnect disk drives within a disk array to a disk array controller. Generally the FC arbitrated loop topology is employed within disk arrays to interconnect individual FC disk drives to the disk array controller.

However at each FC node within the FC arbitrated loop such as an FC disk drive there is a significant node delay as data is processed and transferred through the FC ports of the node. Node delays are illustrated in with short arrows labeled with subscripted lower case letters t. The node delays are cumulative within an FC arbitrated loop leading to significant accumulated node delays proportional to the number of FC nodes within the FC arbitrated loop.

A second problem with the disk array implementation illustrated in is that the FC arbitrated loop represents a potential single point of failure. Generally FC disks may be augmented with port bypass circuits to isolate nonfunctional FC disks from the arbitrated loop but there are a number of different modes of failure that cannot be prevented by port bypass circuits alone.

A third problem arises when an FC port that links a node to the arbitrated loop fails. In such cases complex and unreliable techniques must be employed to try to identify and isolate the failed FC port. In general a failed FC port disrupts the loop topology and the disk array controller must sequentially attempt to activate port bypass circuits to bypass each node in order to isolate the failed node. However this technique may fail to identify the failed node under various failure modes. Thus node failure is a serious problem with arbitrated loop topologies.

A need for a variety of lower cost solutions for implementing FC based disk arrays and other FC based mass storage device has been recognized. In designing and developing new hardware components for mass storage devices designers manufacturers and users of disk arrays and disk array components continue to strive for more efficient implementations including efficient interfaces for accessing memory within various device components.

In one embodiment of the present invention a two register interface is provided by a first electronic device to allow access to memory within the electronic device by external electronic devices. The two register interface is mapped from the memory of an accessing second electronic device. READ and WRITE accesses are transmitted from the accessing second electronic device to the two register interface through a communications medium. A first register of the two register interface directs access to a particular memory location and the second register of the two register interface provides a portal for both READ and WRITE access to the particular memory location.

Embodiments of the present invention provide efficient queue management in a variety of complex electronic devices including I O controllers used in RAID controllers. A variety of complex electronic devices are first described below including a storage shelf router a PCIe SAS I O controller and a storage bridge and the efficient queue management techniques of the present inventions are discussed with reference to a queue shared between a RAID controller and an I O controller.

The FC arbitrated loop employed in the implementation shown in contains only two nodes the disk array controller and the storage shelf router. Assuming that each storage shelf router can interconnect eight disk drives with the FC arbitrated loop a single FC arbitrated loop can be used to interconnect storage shelf routers to a disk array controller or 126 storage shelf routers if an address normally reserved for the FC fabric is used by a storage shelf router thereby interconnecting 8 000 or more individual disk drives with the disk array controller via a single FC arbitrated loop. As noted above when high availability is not needed 16 000 or more individual disk drives may be interconnected with the disk array controller via a single FC arbitrated loop. By contrast as illustrated in when individual FC compatible disk drives each function as a separate FC node only 125 disk drives may be interconnected with the disk array controller via a single FC arbitrated loop or 126 disk drives if an address normally reserved for the FC fabric is used for a disk drive.

The disk drives are connected to the storage shelf router via any of a number of currently available internal interconnection technologies. In one embodiment SATA compatible interconnects are used to interconnect SATA disk drives with the storage shelf router. A storage shelf router includes logic that translates each FCP command received from the disk array controller into one or more equivalent ATA interface commands that the storage shelf router then transmits to an appropriate SATA disk drive. The storage shelf router shown in is interconnected with the disk array controller via a single FC arbitrated loop but as discussed below a storage shelf router is more commonly interconnected with the disk array controller through two FC arbitrated loops or other FC fabric topologies.

As shown in there is at least two fold redundancy in each of the intercommunications pathways within the disk array containing the disk array controller and the storage shelf . Moreover there is a two fold redundancy in storage shelf routers. If any single link or one storage shelf router fails the remaining links and remaining storage shelf router can assume the workload previously assumed by the failed link or failed storage shelf router to maintain full connectivity between the disk array controller and each of the sixteen SATA disk drives within the storage shelf . The disk array controller may additionally implement any of a number of different high availability data storage schemes such as the various levels of RAID storage technologies to enable recovery and full operation despite the failure of one or more of the SATA disk drives. The RAID technologies may for example separately and fully redundantly restore two or more complete copies of stored data on two or more disk drives. The servers intercommunicate with the disk array comprising the disk array controller and one or more storage shelves such as storage shelf through a communications medium such as an FC fabric with built in redundancy and failover. The disk array controller presents a logical unit LUN and logical block address LBA interface that allows the server computers and to store and retrieve files and other data objects from the disk array without regard for the actual location of the data within and among the disk drives in the storage shelf and without regard to redundant copying of data and other functionalities and features provided by the disk array controller . The disk array controller in turn interfaces to the storage shelf through an interface provided by the storage shelf routers and . The disk array controller transmits FC exchanges to and receives FC exchanges from what appear to be discrete FC compatible disk drives via the FCP protocol. However transparently to the disk array controller the disk shelf routers and translate FC commands into ATA commands in order to exchange commands and data with the SATA disk drives.

By contrast the storage shelf implementation shown in is highly available. In this storage shelf two storage shelf routers and are linked via point to point serial links to each of the 16 SATA disk drives . During normal operation storage shelf router interconnects half of the SATA disk drives to the disk array controller while storage shelf router interconnects the other half of the SATA disk drives to the disk array controller. The internal point to point serial links employed during normal operation are shown in bold in such as serial link and are referred to as primary links. Those internal serial links not used during normal operation such as interior serial link are referred to as secondary links. If a primary link fails during operation then the failed primary link and all other primary links connected to a storage shelf router may be failed over from the storage shelf router to which the failed primary link is connected to the other storage shelf router to enable the failed primary link to be repaired or replaced including replacing the storage shelf router to which the failed primary link is connected. As discussed above each of the two storage shelf routers serves as the FC node for one of two FC arbitrated loops that interconnect the storage shelf with a disk array controller. Should one FC arbitrated loop fail data transfer that would normally pass through the failed FC arbitrated loop is failed over to the remaining operable FC arbitrated loop. Similarly should a storage shelf router fail the other storage shelf router can assume the full operational control of the storage shelf. In alternative embodiments a primary path failure may be individually failed over without failing over the entire storage shelf router. In certain embodiments and situations a primary path failover may be carried within the storage shelf router while in other embodiments and situations the primary path failover may involve failing the primary path over to a second storage shelf router.

As discussed above two components facilitate construction of a high availability storage shelf employing SATA disks or other inexpensive disk drives and that can be interconnected with an FC arbitrated loop or other high bandwidth communications medium using only a single slot or node on the FC arbitrated loop. One component is the storage shelf router and the other component is the path controller card that provides redundant interconnection of an ATA drive to two storage shelf routers. illustrate two implementations of a path control card suitable for interconnecting an ATA disk drive with two storage shelf routers. The implementation shown in provides a parallel connector to a parallel ATA disk drive and the implementation shown in provides a serial connection to a SATA disk drive. Because as discussed above SATA disk drives provide higher data transfer rates the implementation shown in is preferred and the implementation that is discussed below.

The path controller card provides an SCA 2 connector for external connection of a primary serial link and a management link to a first storage shelf router and secondary serial link and second management link to a second storage shelf router. The primary link and secondary link are multiplexed by a 2 1 multiplexer that is interconnected via a serial link to a SATA disk drive . The management links and are input to a microcontroller that runs management services routines such as routines that monitor the temperature of the disk drive environment control operation of a fan within the disk drive carrier and activate various light emitting diode LED signal lights visible from the exterior of the disk drive enclosure. In essence under normal operation ATA commands and data are received by the path controller card via the primary link and are transferred via the 2 1 multiplexer to the serial link input to the SATA disk drive . If a failover occurs within the storage shelf that deactivates the default storage shelf router connected via the primary link to the path controller card a second storage shelf router assumes transfer of ATA commands and data via the secondary link which are in turn passed through the 2 1 multiplexer to the serial link directly input to the SATA disk drive .

The path controller card provides redundant interconnection to two separate storage shelf routers and is thus needed in order to provide the two fold redundancy needed in a high availability storage shelf. The storage shelf router provides interconnection between different types of communications medium and translation of commands and data packets between the different types of communications media. In addition the storage shelf router includes fail over logic for automatic detection of internal component failures and execution of appropriate fail over plans to restore full interconnection of disk drives with the disk array controller using redundant links and non failed components.

The routing layer comprises a number of routing tables stored in a memory discussed below and routing logic that determines where to forward incoming FC frames from both FC ports. The FCP layer comprises various queues for temporary storage of FC frames and intermediate level protocol messages control logic for processing various types of incoming and outgoing FC frames and an interface to the CPU complex to allow firmware routines executing on the CPU complex to process FCP CMND frames in order to set up FC exchange contexts in memory to facilitate the exchange of FC frames that together compose an FCP exchange.

The global shared memory switch is an extremely high speed time multiplexed data exchange facility for passing data between FCP layer queues and the SATA ports . The global shared memory switch GSMS employs a virtual queue mechanism to allow allocation of a virtual queue to facilitate the transfer of data between the FCP layer and a particular SATA port. The GSMS is essentially a very high bandwidth high speed bidirectional multiplexer that facilitates highly parallel data flow between the FCP layer and the 16 SATA ports and is at the same time a bridge like device that includes synchronization mechanisms to facilitate traversal of the synchronization boundary between the FCP layer and the SATA ports.

The CPU complex runs various firmware routines that process FCP commands in order to initialize and maintain context information for FC exchanges and translate FCP commands into ATA equivalent commands and that also monitor operation of the SATA disk drives and internal components of the storage shelf router and carry out sophisticated fail over strategies when problems are detected. In order to carry out the fail over strategies the CPU complex is interconnected with the other logical components of the storage shelf router. The external flash memory stores configuration parameters and firmware routines. Note that the storage shelf router is interconnected to external components via the two FC ports and the 16 SATA ports serial management links an IC BUS and a link to a console .

As discussed above storage shelf router based storage shelf implementations provide greater flexibility in many ways than do current FC node per disk drive implementations. The storage shelf router can provide any of many different logical interfaces to the disk array controller to which it is connected. illustrate a number of different logical interfaces provided by a high availability storage shelf incorporating one or more storage shelf routers that in part represent one embodiment of the present invention. shows the interface provided by current FC compatible disk drive implementations of storage shelves as described above with reference to . uses an abstract illustration convention used throughout . In each disk drive is logically represented as a series of data blocks numbered 0 through 19. Of course an actual disk drive contains hundreds of thousands to millions of logical blocks but the 20 logical blocks shown for each disk in are sufficient to illustrate various different types of interfaces. In each separate disk drive is a discrete node on an FC arbitrated loop and therefore each disk drive is associated with a separate FC node address represented in as AL PA AL PA AL PA and AL PA respectively. Note however that unlike in current FC arbitrated loop disk array implementations such as those discussed with reference to there is no cumulative node delay associated with the nodes because each node is interconnected with the complementary SATA port of the storage shelf router via a point to point connection as shown in . Thus a disk array controller may access a particular logical block within a particular disk drive via an FC address associated with the disk drives. A given disk drive may in certain cases provide a logical unit LUN interface in which the logical block address space is partitioned into separate logical block address spaces each associated with a different LUN. However for the purposes of the current discussion that level of complexity need not be addressed.

A second possible interface provided by a storage shelf is shown in . In this case all four disk drives are associated with a single FC arbitrated loop node address AL PA. Each disk drive is considered to be a different logical unit with disk drive considered logical unit zero disk drive considered logical unit one disk drive considered logical unit two and disk drive considered logical unit three. Thus a disk array controller can access a logical block within any of the four disk drives in the storage shelf via a single FC node address a logical unit number and a logical block address within the logical unit.

An alternative interface to the four disk drives within the hypothetical storage shelf is shown in . In this case all four disk drives are considered to be included within a single logical unit. Each logical block within the four disk drives is assigned a unique logical block address. Thus logical blocks 0 19 in disk drive continue to be associated with logical block addresses 0 19 while logical blocks 0 19 in disk drive are now associated with logical block address 20 39. This interface is referred to below as a pure logical block address interface as opposed to the pure LUN based interface shown in .

A final interface is shown in . In this case as in the previous two interfaces and each pair of disk drives associated with a single FC node address are considered to constitute a single LUN with a single logical block address space. However at this interface the logical block addresses alternate between the two disk drives. For example in the case of the pair of disk drives and logical block address 0 is associated with the first logical block and the first disk drive and logical block address 1 is associated with the first block in the second disk drive .

The routing layer is responsible for determining from FC frame headers whether the FC frames are directed to the storage router or to remote storage routers or other entities interconnected with the storage router by the FC arbitrated loops or other FC fabrics. Those frames directed to remote entities are directed by the routing layer to output FIFOs within the FC port layer for transmission via the FC arbitrated loops or other FC fabrics to the remote entities. Frames directed to the storage router are directed by the routing layer to the FCP layer where state machines control their disposition within the storage shelf router.

FCP DATA frames associated with currently active FC exchanges for which contexts have been established by the storage shelf router are processed in a highly stream lined and efficient manner. The data from these frames is directed by the FCP layer to virtual queues within the GSMS from which the data is transferred to an input buffer within the SATA port layer . From the SATA port layer the data is transmitted in ATA packets via one of many SATA links to one of the number of SATA disk drives interconnected with the storage shelf router.

FCP CMND frames are processed by the FCP layer in a different fashion. These frames are transferred by the FCP layer to a memory shared between the FCP layer and the CPUs within the storage shelf router. The CPUs access the frames in order to process the commands contained within them. For example when an incoming WRITE command is received a storage shelf router CPU under control of firmware routines needs to determine to which SATA drive the command is directed and establish a context stored in shared memory for the WRITE operation. The CPU needs to prepare the SATA drive to receive the data and direct transmission of an FCP XFER RDY frame back to the initiator generally the disk array controller. The context prepared by the CPU and stored in shared memory allows the FCP layer to process subsequent incoming FCP DATA messages without CPU intervention streamlining execution of the WRITE operation.

The various logical layers within the storage shelf router function generally symmetrically in the reverse direction. Responses to ATA commands are received by the SATA port layer from SATA disk drives via the SATA links. The SATA port layer then generates the appropriate signals and messages to enable a CPU under firmware control or the FCP layer to carry out the appropriate actions and responses. When data is transferred from an SATA disk to a remote entity in response to a READ command a CPU generates an appropriate queue entry that is stored in shared memory for processing by the FCP layer. State machines within the FCP layer obtain from shared memory an FC frame header template arrange for data transfer from an output buffer in the SATA port layer via a virtual queue prepare an FC frame header and coordinate transfer of the FC frame header and data received from the SATA port layer to output FIFOs and of the FC port layer for transmission to the requesting remote entity generally a disk array controller.

The DISM requests a GSMS channel from an FCP data mover logic module FDM which in turn accesses a virtual queue VQ within the GSMS receiving parameters characterizing the VQ from the context logic via the FISM. The FDM then writes the data contained within the frame to the VQ from which it is pulled by the SATA port that shares access to the VQ with the FDM for transmission to an SATA disk drive. Once the data is written to the VQ the FDM signals the context manager that the data has been transferred and the context manager in turn requests that a completion queue manager CQM queues a completion message CMSG to a completion queue within a shared memory . The CQM in turn requests that a CPU data mover CPUDM write the CMSG into shared memory.

S fabric management frames identified as such by a two bit reserved subfield within the DF CTL field of an FC frame header that is used within the S fabric and that is referred to as the S bits are directed between storage shelf routers via either X ports or Y ports and the point to point internal FC links. Each storage shelf router is assigned a router number that is unique within the storage shelf and that in management frames forms part of the FC frame header D ID field. The storage shelf routers are numbered in strictly increasing order with respect to one of the X and Y fabrics and strictly decreasing order with respect to the other of the X and Y fabrics. For example in storage shelf routers and may be assigned router numbers 1 2 3 and 4 respectively and thus may be strictly increasing or ascending with respect to the X fabric and strictly decreasing or descending with respect to the Y fabric. This ordering is assumed in the detailed flow control diagrams discussed below.

As shown in two reserved bits within the DF CTL field of the FC frame header are employed as a sort of direction indication or compass for frames stored and transmitted within a storage shelf or in other words within the S fabric. Table 4 below shows the encoding of this directional indicator 

Finally illustrates FC frame header fields involved with the routing of an received FCP TRANSFER RDY and FCP RESPONSE frames. IN the case of FCP TRANSFER RDY and FCP RESPONSE frames the routing logic immediately recognizes the frame as directed to a remote entity typically a disk array controller by another storage shelf router. Thus the routing logic needs only to inspect the R CTL field of the FC frame header to determine that the frame must be transmitted back to the X fabric or the Y fabric.

The IRT table includes a row for each disk drive connected to the storage shelf router or in other words for each local disk drive. The row includes the AL PA assigned to the disk drive contained in the low order byte of the D ID field of a frame directed to the disk drive the LUN number for the disk drive the range of logical block addresses contained within the disk drive a CPU field indicating which of the two CPUs manages I O directed the disk drive and a valid bit indicating whether or not the row represents a valid entry in the table. The valid bit is convenient when less than the maximum possible number of disk drives is connected to the storage shelf router.

The ERT X and ERT Y tables and contain the lower byte of valid D IDs that address disk drives not local to the storage shelf router but local to the storage shelf. These tables can be used to short circuit needless internal FC frame forwarding as discussed below.

The X fabric and Y fabric ITT tables and include the full S ID corresponding to remote FC originators currently logged in with the storage shelf router and able to initiate FC exchanges with the storage shelf router and with disk drives interconnected to the storage shelf router. The login pair tables and are essentially sparse matrices with bit values turned on in cells corresponding to remote originator and local disk drive pairs that are currently logged in for FCP exchanges. The login tables and thus provide indications of valid logins representing an ongoing interconnection between a remote entity such as a disk array controller and a local disk drive interconnected to the storage shelf router.

Next the routing logic that constitutes the routing layer of a storage shelf router is described with reference to a series of detailed flow control diagrams. provides a simplified routing topology and routing destination nomenclature used in the flow control diagrams. are a hierarchical series of flow control diagrams describing the routing layer logic.

As shown on the routing layer is concerned with forwarding incoming FC frames from the FC ports and either directly back to an FC port to the FCP layer for processing by FCP logic and firmware executing on a CPU or relatively directly to the GSMS layer in the case of data frames for which contexts have been established. The routing layer receives incoming FC frames from input FIFOs and within the FC ports designated From FP0 From FP1 respectively. The routing layer may direct an FC frame back to an FC port by writing the FC frame to one of the output FIFOs and designated To FP0 and To FP1 respectively. The routing layer may forward an FCP DATA frame relatively directly to the GSMS layer via a virtual queue a process referred to as To GSMS and may forward an FC frame to the FCP layer for processing referred to as To FCP. The designations From FP0 From FP1 To FP0 To FP1 To GSMS and To FCP are employed in the flow control diagrams as shorthand notation for the processes of reading from and writing to FIFOs data transfer through the GSMS virtual queue mechanism and state machine mediated transfer through a shared memory interface to CPUs.

In step the routing layer logic RLL reads the next incoming FC frame from one of the input FIFOs of the FC ports designated From FP0 and From FP1 respectively. In step the routing layer logic determines whether or not the FC frame is a class 3 FC frame. Only class 3 FC frames are supported by the described embodiment of the storage shelf router. If the FC frame is not a class 3 FC frame then the FC frame is directed to the FCP layer To FCP for error processing in step . Note that in this and subsequent flow control diagrams a lower case e associated with a flow arrow indicates that the flow represented by the flow arrow occurs in order to handle an error condition. If the FC frame is a class 3 FC frame as determined in step the RLL next determines in step whether the FC port from which the FC frame was received is an S fabric endpoint or in other words an X fabric or Y fabric node. A storage shelf router can determine whether or not specific ports are endpoints with respect to the S fabric or are in other words X fabric or Y fabric nodes from configurable settings. The FC frame header contains the port address of the source port as discussed above.

If the source port of the FC frame is an S fabric endpoint indicating that the FC frame has been received from an entity external to the local S fabric then the RLL determines in step whether any of the S bits are set within the DF CTL field of FC frame header. If so then an error has occurred and the FC frame is directed to the FCP layer To FCP for error processing in step . If not then appropriate S bits are set to indicate whether the FC frame belongs to the X fabric or X space or to the Y fabric or Y space in step . Note that one of the two FC ports corresponds to the X fabric and other of the two FC ports corresponds to the Y fabric regardless of the position of the storage shelf router within the set of interconnected storage shelf routers within a storage shelf. As noted above the association between FC ports and the X and T fabrics is configurable. Next the RLL determines in step whether the S bits are set to indicate that the frame is an S fabric frame. If so then the sublogic Management Destination is invoked in step to determine the destination for the frame after which the sublogic Route To Destination is called in step to actually route the FC frame to the destination determined in step . If the FC frame is not an S fabric management frame as determined in step then in step the RLL determines whether or not the RLL is currently operating in transparent mode described above as a mode in which each disk drive has its own FC node address. If the storage shelf router is operating in transparent mode then the sublogic Transparent Destination is called in step in order to determination the destination for the frame and then the sublogic Route To Destination is called in step to actually route the frame to its destination. Otherwise the sublogic Destination is called in step to determination the destination for the frame after which it is routed to its destination via a call to the sublogic Route To Destination in step .

If the D ID field does not match the contents of the appropriate FAR registers as determined in step then in step the RLL determines whether or not the frame is an X fabric frame. If so then in step the RLL determines whether or not the frame is directed to another storage shelf router within the storage shelf. If not then the variable destination is set to To FP0 to return the frame to the external X fabric for forwarding to another storage shelf in step . If the ERT X table contains an entry indicating that the destination of the frame is a disk drive attached to another storage shelf router within the storage shelf as determined in step then in step the RLL determines whether or not the current storage shelf router represents the Y fabric endpoint. If so then the frame was not correctly processed and cannot be sent into the Y fabric and therefore the variable destination is set to the value To FCP in step so that the frame can be directed to the FCP layer for error handling. Otherwise the variable destination is set to To FP1 in step to forward the frame on to subsequent storage shelf routers within the storage shelf via the S fabric. If the received frame is not an X fabric frame as determined in step then in step the RLL determines whether or not the received frame is a Y fabric frame. If so then the frame is processed symmetrically and equivalently to processing for X fabric frames beginning in step . Otherwise the variable destination is set to To FCP in step to direct the frame to the FCP layer for error handling.

As discussed above a storage shelf router that represents one embodiment of the present invention receives FCP CMND frames directed by the disk array control to the storage shelf router as if the FCP CMND frames were directed to FC disk drives and translates the SCSI commands within the FCP CMND frames into one or more ATA commands that can then be transmitted to an SATA disk drive to carry out the SCSI command. Table 5 below indicates the correspondence between SCSI commands received by the storage shelf router and the ATA commands used to carry out the SCSI commands 

In various embodiments a storage shelf router or a number of storage shelf routers within a storage shelf may provide virtual disk formatting in order to allow disk array controllers and other external processing entities to interface to an expected disk formatting convention for disks within the storage shelf despite the fact that a different unexpected disk formatting convention is actually employed by storage shelf disk drives. Virtual disk formatting allows the use of more economical disk drives such as ATA disk drives without requiring disk array controllers to be re implemented in order to interface with ATA and SATA disk formatting conventions. In addition a storage shelf router or a number of storage shelf routers together can apply different disk formatting conventions within the storage shelf in order to incorporate additional information within disk sectors such as additional error detection and error correction information without exposing external computing entities such as disk array controllers to non standard and unexpected disk formatting conventions.

The storage shelf router that in various embodiments is the subject of the present invention allows economical ATA disk drives to be employed within storage shelves of a fiber channel based disk array. However certain currently available FC based controllers may be implemented to interface exclusively with disk drives supporting 520 byte sectors. Although the manufacturer of an ATA or SATA based storage shelf may elect to require currently non ATA compatible disk array controllers to be enhanced in order to interface to 512 byte sector containing ATA or SATA disk drives a more feasible approach is to implement storage shelf routers to support virtual disk formatting. Virtual disk formatting provides to external entities such as disk array controllers the illusion of a storage shelf containing disk drives formatted to the FC disk drive 520 byte sector formatting convention with the storage shelf router or storage shelf routers within the storage shelf handling the mapping of 520 byte sector based disk access commands to the 512 byte sector formatting employed by the ATA disk drives within the storage shelf.

Summarizing the storage shelf router implemented WRITE access in a virtual formatting environment illustrated in the storage shelf router generally needs to first read the boundary sectors from the actual disk drive map received data into the boundary sectors in memory and then WRITE the boundary sectors and all non boundary sectors to the disk drive. Therefore in general a 520 byte sector based virtual write operation of n sectors is implemented by the storage shelf router using two actual disk sector reads and 2 n 1 actual disk sector writes WRITE I O virtual 520 sectors 2 reads 2 writes 1 writes with a correspondingly decreased write efficiency of 

The illustration of the implementation of virtual disk formatting in and A B is a high level conceptual illustration. Internally the storage shelf router employs the various data transmission pathways discussed in previous subsections in order to receive data from incoming FC DATA packets route the data through the storage shelf router to an SATA port for transmission to a particular SATA disk drive receive data from the SATA disk drive at a particular SATA port route the data back through the storage shelf router and transmit the data and status information in FC DATA and FC STATUS packets transmitted back to the external processing entity. While several discrete memory buffers are shown in and D the actual processing of data by the storage shelf router may be accomplished with minimum data storage using the virtual queue mechanisms and other data transport mechanisms described in previous subsections. The memory buffers shown in and B are intended to illustrate data processing by the storage shelf router at a conceptual level rather than at the previously discussed detailed level of data manipulation and transmission carried out within a storage shelf router.

To summarize the read operation illustrated in the storage shelf router needs to read n plus 1 disk sectors in order to carry out a virtual READ of n virtual sectors with a correspondingly decreased read efficiency as expressed in the following equations READ I O virtual 520 sectors 1 reads reads with a correspondingly decreased read efficiency of 

When the READ operation of the low boundary sector completes as detected in step the storage shelf router writes the initial portion of the received data associated with the WRITE command to the low boundary sector in step and initiates a WRITE of the low boundary sector to the disk drive in step . Similarly when the storage shelf router detects completion of the read of the high boundary sector in step the storage shelf router writes the final portion of the received data into a memory buffer including the data read from the high boundary sector step and initiates a WRITE of the high boundary sector to the disk drive in step . In a one embodiment of the present invention the disk sectors are written to disk in order from lowest sector to highest sector. For non boundary sectors the storage shelf router writes each non boundary sector in step to the disk drive as part of the for loop including steps and . When the storage shelf router detects an event associated with the virtual WRITE operation the storage shelf router step determines whether all initiated WRITE operations have completed. If so then the WRITE operation has successfully completed in step . Otherwise the storage shelf router determines whether the WRITE operation of the virtual sectors has timed out in step . If so then error condition obtains in step . Otherwise the storage shelf router continues to wait in step for completion of all WRITE operations.

The mapping of 520 byte FC disk drive sectors to 512 byte ATA disk drive sectors in one embodiment of the virtual formatting method and system of the present invention can be efficiently computed. illustrates the calculated values needed to carry out the virtual formatting method and system representing one embodiment of the present invention. In the top most horizontal band of sectors represents virtually mapped 520 byte sectors and the bottom horizontal band represents physical 512 byte ATA sectors. illustrates mapping virtual sectors through to physical sectors through . For the example shown in assume that virtual sectors are to be mapped to corresponding physical sectors. The logical block address LBA of the first virtual sector fc lba therefore has the value 400 and the number of virtual blocks to be mapped fc block count is therefore 10. The calculated value fc lba last is 410 the LBA of the first virtual sector following the virtual sector range to be mapped. The logical block address of the first physical sector including data for the virtual sectors to be mapped ata lba is computed as ata lba fc lba fc lba 6 using familiar C language syntax and operators. In the example the computed value for ata lba is 406. This calculation can be understood as adding to the LBA of the first virtual sector a number of physical sectors computed as the total number of virtual sectors preceding the first virtual sector divided by 64 since each continuous set of 64 virtual sectors exactly maps into a corresponding contiguous set of 65 physical sectors or in other words 64 520 65 512 33280 The offset from the beginning of the first physical sector to the byte within the first physical sector corresponding to the first byte of the first virtual sector ata lba offset is computed as follows ata lba offset fc lba 63 6 In the example the calculated value for ata ending lba is 416. The above computation is equivalent to that for the first physical sector ata lba. The offset within the last physical boundary block corresponding to the first byte not within the virtual sectors ata ending lba offset is computed as ata ending lba offset fc lba last 63 

Note that the control flow diagrams in represent fairly high conceptual illustration of storage shelf operations associated with virtual WRITE and virtual READ commands. In particular the details of data flow and disk operations detailed in above sections are not repeated in the interest of brevity and clarity.

The virtual disk formatting described with reference to allows as discussed above a storage shelf router to provide an illusion to external computing entities such as disk array controllers that the storage shelf managed by the storage shelf router contains 520 byte sector FC disk drives while in fact the storage shelf actually contains 512 byte sector ATA or SATA disk drives. Similarly virtual disk formatting can be used by the storage shelf router to provide an interface to any type of disk formatting expected or desired by external entities despite the local disk formatting employed within the storage shelf. If for example a new extremely economical 1024 byte sector disk drive becomes available the virtual disk formatting technique allows a storage shelf router to map virtual 520 byte sector based access operations or 512 byte sector based access operations to the new 1024 byte sector based disk drives. In addition multiple layers of virtual disk formatting may be employed by the storage shelf router in order to provide or enhance error detection and error correction capabilities of disk drives that rely on added information stored within each sector of the disk drive.

The contents of the LRC field allows the storage shelf router to detect various types of errors that arise in ATA disk drives despite the hardware level ECC information and disk drive controller techniques employed to detect various data corruption errors. For example a READ request specifying a particular sector within a disk drive may occasionally result in returning data by the disk drive controller associated with a different sector. The LBA within the LRC field allows the storage shelf router to detect such errors. In addition the disk drive may suffer various levels of data corruption. The hardware supplied ECC mechanisms may detect one bit or two bit parity errors but the CRC values stored in the CRC field can detect depending on the technique employed to compute the CRC value all one bit two bit and three bit errors as well runs of errors of certain length ranges. In other words the CRC value provides enhanced error detection capabilities. By employing the two tiered virtual disk formatting technique illustrated in the storage shelf router is able to detect a broad range of error conditions that would be otherwise undetectable by the storage shelf router and to do so in a manner transparent to external processing entities such as disk array controllers. As mentioned above the only non transparent characteristic observable by the external processing entity is a smaller number of sectors accessible for a particular disk drive.

The storage shelf router may carry out either full LRC checks or deferred LRC checks during WRITE operations. illustrates the deferred LRC check. As shown in and as discussed earlier when a single second level virtual 512 byte sector is written by the storage shelf router to a disk drive the storage shelf router must first read the two boundary sectors associated with the second level virtual sector into memory . The boundary sectors generally each includes an LRC field and . The second LRC field occurs within the first level 520 byte virtual sector corresponding to the second level virtual sector . In deferred LRC mode the storage shelf router inserts the data and LBA value into a buffer carries out the CRC computation and inserts the computed CRC into the CRC field and then writes the resulting first level virtual sector into the memory buffer . The contents of the memory buffer then are returned to the disk drive via two WRITE operations and . Note that the contents of the LRC field associated with the first level virtual sector are assumed to be valid. However the two WRITE operations also write data and an LRC field corresponding to neighboring first level virtual sectors back to the disk drive. Rather than checking that this data and additional LRC field is valid the storage shelf router simply defers checking of neighboring first level virtual sectors until the neighboring first level virtual levels are subsequently read.

The storage shelf router may employ various additional techniques to detect problems and correct problems transparent to external processing entities. For example should the storage shelf router fail to successfully read the lower boundary sector in the storage shelf router may nonetheless write the portion of the lower boundary sector received in the second level virtual sector to the lower boundary sector on the disk and return a recovered error status to the disk array controller. Subsequently when the preceding virtual sector is accessed the disk array controller trigger data recover from a mirror copy of the sectors involved in order to retrieve that portion of the original lower boundary sector that was not read during the previous write operation and write the data to the disk drive correcting the error. Thus an LRC failure can be circumvented by the storage shelf router.

As discussed in previous subsections a storage shelf router facilitates the development of high availability storage shelves that include less expensive SATA disk drives that can be interconnected via FC communications media to currently available RAID controllers in disk arrays. However additional approaches to incorporating less expensive SATA disk drives in FC based disk arrays are possible. illustrates an alternative approach to incorporating SATA disk drives within FC based disk arrays that employ FC SAS RAID controllers. In a disk array is interconnected with servers through two FC communications media and . The disk array shown in includes two FC SAS RAID controllers and . Each FC SAS RAID controller interfaces to an FC communications medium e.g. FC SAS RAID controller interfacing to FC link and to a Serial Attached SCSI SAS communications medium that interconnects each FC SAS RAID controller to a number of SAS and or SATA disk drives . The disk array can provide an FC based interface to host computers identical to that provided by currently available disk arrays that employ internal FC loops and FC disk drives and may use significant portions of existing RAID controller software developed for FC based disk arrays.

The SAS communications medium and PCIe communications medium are both new serial communications media recently developed to replace older parallel communications media. Serial communications media provide direct interconnection between an initiator and target device. Both SAS and PCIe hierarchical architectures provide for switches that can directly interconnect an initiator or higher level switch with any of multiple lower level switches and or target devices. SAS and PCIe communications medium are analogous to telephone switching based communications system in which various combinations of exchanges and switching components provide direct interconnection between two telephones. Serial communications medium can be designed to achieve much higher data transfer bandwidths and lower data transfer latencies than parallel communications media in which a bus like medium is shared through arbitration by a number of devices.

An SAS port may include multiple Phys a Phy being one side of a physical link as shown in . illustrates a number of different SAS ports with different widths. A 1 SAS port includes a single Phy. A 2 SAS port includes two Phys. Similarly a 4 SAS port includes four Phys and an 8 SAS port includes eight Phys. When a first 8 SAS port is interconnected with a second 8 SAS port the eight physical links allow for eight times the bandwidth obtained by interconnecting two 1 SAS ports. When two SAS ports of different widths are interconnected the bandwidth obtained determined via an initial negotiation is the bandwidth obtainable through the lowest width port of the two SAS ports. For example interconnection of an 8 SAS port to a 4 SAS port allows provides four times the bandwidth provided by interconnecting two 1 SAS ports. Thus an SAS communications medium or link is generally designated as 1 2 4 or 8 as determined by the lowest width of the two ports interconnected by the link.

PCIe is similarly structured. PCIe links may also be classified as 1 2 4 and 8 depending on the smallest width of the two PCIe ports interconnected by the link. PCI Phys also employ differential signal pairs and use 8b10b encoding. A currently available PCIe differential signal pair provides for transmission of 2.5 Gbps in one direction with much higher transmission rates projected for future PCI versions and as with SAS each PCIe port contains at least 1 Phy comprising a receiver and transceiver each connected to a differential signal pair.

As is shown in a disk array using the FC SAS RAID controller shown in generally employs at least two RAID controllers in order to allow for independent dual porting of each disk drive within the disk array to achieve fault tolerance and high availability. illustrates the SAS based connections of disk drives to PCIe SAS I O controllers in a dual controller disk array. In the example configuration shown in a first PCIe SAS I O controller is interconnected via an 8 SAS link to a first fan out expander . The second PCIe SAS I O controller is connected via an 8 SAS link to a second SAS fan out expander . Each of the fan out expanders and can in turn be connected to up to 128 edge expanders such as edge expanders . Each of the edge expanders can in turn be interconnected via 1 SAS links to a maximum of 128 target devices in the present example SATA disk drives such as SATA disk drive . Thus each SATA disk drive may be connected through a first port such as port of SATA disk drive to the first PCIe SAS I O controller and through a second port such as SAS port of SATA disk to the second PCIe SAS I O controller. Although SATA disk drives are not manufactured as dual ported device each SATA disk drive may be enhanced by a two SAS port connector module to allow for interconnection of the SATA disk drive to two different SAS domains via two SAS ports. A huge number of different SAS topologies can be implemented using different configurations of switches.

As discussed above with reference to the PCIe SAS I O controller in interfaces a multi processor RAID controller in via an 8 PCIe link to one two or four SAS ports depending on the configuration of the PCIe SAS I O controller. illustrates the interfacing of the multi processor RAID controller to two SAS ports in a two SAS port PCIe SAS I O controller configuration. As shown above the horizontal dashed line in a dual core RAID controller CPU in the displayed embodiment of the present invention can support up to four different concurrently executing device drivers . The PCIe SAS I O controller correspondingly provides four PCIe functions that each provides a functional interface to one of the concurrently executing device drivers executing on the multi processor RAID controller. The PCIe SAS I O controller essentially acts as a type of switch that allows each PCIe function and the device driver that interfaces to the PCIe function to send commands to and receive responses from any SAS or SATA disk connected to either of the two SAS ports .

The PCIe layer manages all PCIe traffic inbound from the PCIe link and outbound to the PCIe link. The PCIe layer implements four PCIe functions for up to four RAID controller device drivers as discussed with reference to . Each PCIe function provides a set of queues and registers discussed below that together comprise the RAID controller I O controller interface.

The global shared memory switch is a time division multiplexing non blocking switch that routes data from the PCIe layer to the SAS layer and from the SAS layer to the PCIe layer as discussed more generally with reference to . The global shared memory switch temporarily buffers data exchanged between the PCIe layer and the SAS layer.

The context manager includes an I O context cache table ICCT and a device attribute table DAT . These data structures discussed below allow for tracking translating and managing I O commands. The ICCT is a cache of I O cache table ICT entries moved from the ICT in RAID controller memory to the PCIe SAS I O controller. The DAT is initialized by the RAID controller to contain device attribute information needed for proper translation and execution of I O commands.

The SAS layer implements one or more SAS ports as discussed above with reference to as well as the SAS link port and transport layers that together with the SAS physical layer embodied in the SAS ports implements the SAS protocol. Each SAS port individually interfaces to the global shared memory switch in order to achieve high bandwidth transfer of information between the PCIe layer and SAS layer. The CPU subsystem includes a processor and various tightly coupled memories and runs PCIe SAS I O controller firmware that processes SMP management commands and provides a flexible interface to the RAID controller processor for handling SSP and STP errors.

The six circular queues include 1 the I O request queue IRQ into which the RAID controller enters I O requests for processing by the PCIe SAS I O controller 2 the asynchronous request queue ARQ which provides a flexible communication channel for asynchronous commands transferred between a device driver and firmware executing within the PCIe SAS I O controller including SMP commands and other management commands 3 the completion queue CQ used by the PCIe SAS I O controller to notify a device driver of completion of a task or request previously queued by the device driver to the IRQ or ARQ 4 the transfer ready queue XQ used by the PCIe SAS I O controller for managing FC XFER RDY frames 5 the small buffer queue SBQ used to provide the PCIe SAS I O controller with small RAID controller memory buffers and 6 the large buffer queue LBQ used to provide the PCIe SAS I O controller with large memory buffers within the RAID controller.

Two different strategies for incorporating low cost SATA disk drives into disk arrays have been discussed in previous sections. A first approach involves a high available storage shelf controlled by one or more storage shelf routers. A second approach involves FC SAS RAID controllers that interface to host computers via FC media and interface to SAS and SATA disk drives via SAS communications media. The first approach involves no modification to FC based disk array controller software while the second approach involves modification of FC based disk array controller software to interface via the PCIe link to the PCIe SAS I O controller.

In this subsection a third technique for employing SATA disk drives in FC disk drive based disk arrays is described. illustrates use of SATA disk drives within an FC disk drive based disk array by using a bridge interface card. In a disk array or storage shelf includes either two RAID controllers or two enclosure I O cards and respectively. The RAID controllers or enclosure I O cards receive commands and data via two FC links and and route commands and data to and receive data from disk drives such as disk drive via two internal FC loops and . The disk drives may be dual ported FC disk drives which directly connect through a mid plane to the internal FC loops or may be SATA disk drives such as SATA disk drive that interfaces through a bridge interface card to the internal FC loops and . By using a bridge interface card a SATA disk drive can be adapted to the internal FC loops of a standard FC based disk array.

The present invention is related to a memory access interface that allows an external device to efficiently access memory within a single integrated circuit implementation of a complex electronic device. The present invention is discussed with reference to an implementation of the present invention incorporated in the above discussed PCIe SAS I O controller. However the method of memory access and memory access interface provided by the present invention can be applied to any number of different single integrated circuit implemented electronic devices in addition to I O controllers.

The PCIe layer uses memory for the PCIe interface and includes state machine logic and circuitry responsible for receiving data access requests and responding to data access requests via the PCIe communications medium. The SAS layer also uses memory for the SAS interface and the GSMS is a memory based non blocking switch that interconnects PCIe ports with SAS ports. In memory refers to RAM memory high speed registers and all other types of data storage components used within the I O controller. Portions of the I O controller memory may be shared by various components and other portions may be used exclusively by a single component. The memory may be a single memory component or may instead represent many discrete registers memory devices and other data storing components spread throughout the I O controller integrated circuit.

As discussed above in a previous subsection the I O controller interfaces to the RAID controller via a memory interface including a number of memory registers memory locations and circular queues See e.g. . The portions of this memory interface stored within the I O controller reside in the PCIe interface memory . For each PCIe function the PCIe allocates during PCIe initialization a region of PCI configuration space . During I O controller configuration by the RAID controller regions in host memory are allocated for mapping to memory registers memory locations and circular queues within the PCIe interface memory of the I O controller. The PCIe SAS I O controller features a 32 bit memory address space. The RAID controller accesses the I O controller PCIe memory interface registers memory locations and circular queues by accessing the mapped memory registers memory locations and circular queues in the RAID controller memory address space and these accesses are redirected to PCIe data transfers. The memory regions of structures in PCI configuration space include 64 bit references to the RAID controller memory regions or structures within a RAID controller 64 bit address space.

While the PCIe interface memory supports the various circular queues registers and memory locations discussed above with reference to that allow for high throughput data transfer between the PCIe communications medium and the SAS communications medium the non PCIe interface I O controller memory includes much additional stored information that may need to be accessed at certain times by the RAID controller. For example under certain error conditions the RAID controller may need to recover as much state information as possible from the I O controller in order to recover interrupted operations and to diagnose the failure.

A number of techniques may be employed to provide an interface allowing the RAID controller to access the non PCIe interface I O controller memory . First as shown in an application programming interface API may be implemented to allow the RAID controller to access non PCIe interface I O controller memory through the API. The API includes API software or firmware executed by the I O controller CPU and various additional queues memory locations and registers within the PCIe interface memory needed by the CPU to exchange information with the RAID controller via the API. These new API related data structures memory locations and registers would need to be mapped from host memory by additional mappings . The CPU would be interconnected with the PCIe interface memory and I O controller memory via a memory bus or other such data transfer channel within the I O controller.

Development of an I O controller memory API as shown in has a number of drawbacks. First accessing memory locations through an API involves significant computational overhead and therefore significantly slower memory access. Furthermore should the I O controller CPU fail then the API also necessarily fails preventing access by the RAID controller to the non PCIe interface I O controller memory. However CPU failure is an example of the types of error events that may require diagnosis and state information recovery by the RAID controller.

A second approach to providing access to the non PCIe interface I O controller memory by the RAID controller is shown in . In this technique the non PCIe interface I O controller memory is directly mapped through the PCIe interface to host memory by adding additional mappings in host memory referenced by additional fields in the data structures allocated within PCI configuration space. In this technique the memory locations registers and additional data structures within non PCIe interface I O controller memory are mapped to RAID controller memory through the PCIe interface just as the circular queues memory locations and registers discussed above with reference to and that together comprise the memory interface for high throughput data transfer between the PCIe communications medium and the SAS communications medium are mapped to RAID controller memory.

This memory mapping technique also has disadvantages. The primary disadvantage is that a significant additional amount of memory needs to be allocated both within host memory and within PCI configuration space in order to provide direct mapping from host memory to the non PCIe interface I O controller memory. PCI configuration space is limited and better used for other purposes. Furthermore changes to the structure of non PCIe interface I O controller memory or additions to the memory locations registers and data structures stored within non PCIe interface I O controller memory may require extensive changes to the memory regions and structures allocated within RAID controller memory and PCI configuration space that provide the mappings from RAID controller memory to non PCIe interface I O controller memory.

For the above described reasons neither direct mapping of host memory to non PCIe interface I O controller memory or development of an API for external device access to non PCIe interface I O controller memory are desirable. Instead according to one embodiment of the present invention a third method for providing external access to non PCIe interface I O controller memory is provided as shown in . According to one embodiment the present invention a two register interface is created for each PCIe function . The two register interfaces are mapped from host memory by the addition of a mapping in each RAID controller memory region or structure associated with a PCIe function. A first multiplexing switch multiplexes the four two register sets to a second multiplexing switch that interconnects the first multiplexing switch with I O controller memory . The I O controller CPU is interconnected with I O controller memory via the second multiplexing switch . The second multiplexing switch is implemented in hardware as a state machine includes a shared address decoder used both by the CPU and by the two register interfaces and provides fail safe arbitration with priority provided to the two register interfaces. The RAID controller can access any location in non PCIe interface I O controller memory via one or more of the two register interfaces .

Although the present invention has been described in terms of particular embodiments it is not intended that the invention be limited to these embodiments. Modifications within the spirit of the invention will be apparent to those skilled in the art. For example the config  register of the two register interface may in alternative embodiments establish an association between a memory location in I O controller memory and the config  register with the config  used for accessing the memory location. A large number of different state machine implementations can be used to implement the first and second multiplexing switches. A number of two register interfaces equal to the number of PCIe functions or the number of communications channels in alternative communications media can be implemented to allow access to I O controller memory through each PCIe function of communications medium channel. While in the described embodiment all of the I O controller memory is accessible to the external device through the memory access interface in alternative embodiments only portions of the memory may be exposed with access controlled by multiplexing switch configuration or other techniques. While in the described embodiment of the present invention RAID controller memory is mapped to the two register interfaces the two register interfaces may be accessed by any of various alternative methods including dedicated signal lines or via alternative methods available in alternative communications media. In the described embodiment the two registers of each two register interface are 32 bit registers but in alternative embodiments the registers may be smaller or larger. Although inn the description above a distinction is drawn between PCIe interface memory and non PCIe interface I O controller memory the two register interface may be used to access both PCIe interface memory and non PCIe interface I O controller memory in many embodiments of the present invention.

The foregoing description for purposes of explanation used specific nomenclature to provide a thorough understanding of the invention. However it will be apparent to one skilled in the art that the specific details are not required in order to practice the invention. The foregoing descriptions of specific embodiments of the present invention are presented for purpose of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments are shown and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalents 

