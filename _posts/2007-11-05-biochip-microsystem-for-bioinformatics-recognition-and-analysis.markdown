---

title: Biochip microsystem for bioinformatics recognition and analysis
abstract: A system with applications in pattern recognition, or classification, of DNA assay samples. Because DNA reference and sample material in wells of an assay may be caused to fluoresce depending upon dye added to the material, the resulting light may be imaged onto an embodiment comprising an array of photodetectors and an adaptive neural network, with applications to DNA analysis. Other embodiments are described and claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07910873&OS=07910873&RS=07910873
owner: California Institute of Technology
number: 07910873
owner_city: Pasadena
owner_country: US
publication_date: 20071105
---
This application claims the benefit of U.S. Provisional Application No. 60 856 512 filed 3 Nov. 2007.

The invention described herein was made in the performance of work under a NASA contract and is subject to the provisions of Public Law 96 517 35 USC 202 in which the Contractor has elected to retain title.

Genetic material may be analyzed by placing DNA Deoxyribonucleic acid material in an array of wells dots . Polymerase chain reaction PCR amplification is often used in genetic analysis where the PCR amplification augments the amount of DNA material placed in a well. Fluorescent dyes such as CY3 or CY5 may be added to the DNA material so that it fluoresces when excited by monochromatic light. Because with PCR amplification there may be different growth rates of DNA material from well to well sample and reference channels may be set up whereby in each well there is reference DNA and sample DNA. Fluorescent dye of one type may be used for the sample DNA and fluorescent dye of another type may be used for the reference DNA.

This method also reduces the sources of variability and noise due to various aspects of an individual spot that affect both specimens DNA sample and reference similarly. In order to accurately calculate the density of the sample DNA material in a particular well after PCR amplification the integral of the total fluorescence intensity presumably representing the density of the DNA material inside the well from the topological profile of the well is usually computed. The logarithmic value of the ratio of the two intensities of the fluorescent dye labeled specimens one value for the sample specimen the other value for the reference specimen measured from the same well is calculated based on the assay s fluorescence image. The ratio of the two intensities would provide the normalized population of the gene material in the well disregarding the initial population density.

In most of the available commercial solutions the assay s fluorescence image is usually scanned by a color scanner with high resolution and then transferred to a computer for image analysis. The profile analysis software usually computes the normalized intensity of each well sequentially. The intensity of the fluorescence is usually relatively low. Using higher excitation light intensity or increasing detection time may lead to brighter fluorescence patterns. However lower power consumption and faster detection may be preferable. Furthermore some fixed pattern noises in the input pattern may exist e.g. fixed pattern noises created by scattered lights or non uniformity of the detector array response . These noises may introduce errors in the measurement of the density of the DNA materials

The development of low cost portable instruments for rapidly analyzing genetic assays in noisy environments and with relatively low intensity of fluorescence would be of utility in medical services.

In the description that follows the scope of the term some embodiments is not to be so limited as to mean more than one embodiment but rather the scope may include one embodiment more than one embodiment or perhaps all embodiments.

For example for some embodiments CY3 and CY5 dyes may be used where the DNA material to which CY3 has been added is excited by monochromatic light having a center frequency wavelength of 535 nm and a bandwidth of 10 nm and the DNA material to which CY5 has been added is excited by monochromatic light at 625 nm with a bandwidth of 10 nm. For such embodiments using CY3 and CY5 dyes when fluorescing the CY3 dye gives off green light having a peak value at 570 nm and the CY5 dye gives off red light having a peak value at 670 nm. The excitation of wells with CY3 and CY5 may be performed concurrently. Assay array may be front side illuminated or backside illuminated if transparent for example.

The term monochromatic is a term of art where of course in theory no excitation source is purely monochromatic. In the examples given the bandwidth is less than about 1 50 of the center frequency. For some embodiments the excitation need not be monochromatic in this sense.

Lens system images the light from the array of wells onto sensor array . Lens system may comprise more than one lens element. Sensor array comprises an array of pixels but for simplicity of illustration only one pixel labeled is shown in . Pixel comprises two optical filters and where optical filter has a pass band to allow the fluorescence from the DNA sample material to pass through but to substantially reject light outside the frequency range of this fluorescence. Similarly optical filter has a pass band to allow the fluorescence from the DNA reference material to pass through but to substantially reject light outside the frequency range of this fluorescence.

For example for embodiments using CY3 and CY5 dyes as discussed above one of the optical filters say is a thin film micro optical filter having a passband centered at about 580 nm with a bandwidth of 40 nm and the other optical filter say is a thin film micro optical filter having a passband centered at about 765 nm with a bandwidth of 40 nm. In this way a sensor below optical filter is responsive to the DNA material having the CY3 dye and a sensor below optical filter is responsive to the DNA material having the CY5 dye.

An exploded view of the sensors and circuit for pixel of the embodiment of is provided as circuit which may be referred to as a differential logarithm circuit. In the embodiment of photodetector is below optical filter and photodetector is below optical filter . In the particular embodiment of photodetector is responsive to the light imaged from the sample DNA material and photodetector is responsive to the light imaged from the reference DNA material. In the particular embodiment of photodetectors and are NPN photodetectors but other embodiments may use other types of photodetectors.

Photodetector is connected in series with two transistors nMOSFETs n Metal Oxide Field Effect Transistor and . The drain source currents for nMOSFETs and are substantially equal to the current sourced by photodetector . The current sourced by photodetector is proportional to the amplitude of the incident light. Transistors and are each diode connected. When operating in their sub threshold regions their gate to source voltages are substantially proportional to the logarithm of the current sourced by photodetector which in turn is proportional to the logarithm of the amplitude of the light incident on photodetector . Similar remarks apply to photodetector and transistors and but where photodetector is responsive to incident light from the reference material.

Differential transistor pair and resistors and and tail current transistor form a differential amplifier where the input signals are the gate voltages of transistors and and the output voltage is taken at the drain of one of the transistors in the differential transistor pair. For the particular embodiment of the drain on transistor is taken as an output port labeled as V. Denoting the amplitude of the sample incident light by Aand the amplitude of the reference incident light by the A the output voltage may be written as

Other embodiments may employ circuits different from the particular circuit illustrated in . For example a circuit complementary to circuit may be realized where pMOSFETs are used instead of nMOSFETs in circuit and PNP photodetectors are used instead of photodetectors and . Furthermore some embodiments may employ other types of transistors such as bipolar transistors. As another example whereas the embodiment for circuit illustrates two diode connected transistors in series with each photodetector other embodiments may use a different number of diode connected transistors. Other embodiments may use other types of differential amplifiers in place of the differential amplifier represented by transistor pair and resistors and and tail current transistor . For example some embodiments may employ active devices in place of resistors and where such active devices have a relatively wide range linear impedance response or other embodiments may employ a different configuration of transistors to provide the tail current provided by transistor .

Accordingly pixel may be represented by the functional blocks indicated in . Referring to optical filter has a first passband to pass to optical detector light that has been filtered by the first passband. The combination of optical detector and logarithmic circuit provides a voltage to differential amplifier indicative of the logarithm of the light intensity provided to optical detector . Similar remarks apply to optical filter optical detector and logarithmic circuit but where optical filter has a second passband different tuned to a different frequency spectrum from that of the first passband. The output of differential amplifier is indicative of the logarithm of the ratio of the light intensities provided to optical detectors and

Given an array of voltage signals each voltage signal indicative of the logarithm of the ratio of the sample light amplitude to the reference light amplitude for a particular pair of wells from the assay array embodiments may use an adaptive neural network to classify the voltage signals. Classification may be viewed as pattern recognition. Some embodiments may employ an adaptive neural network structure such as that illustrated in where the layer of neurons is a layer of input neurons and the layer of neurons is a layer of output neurons. Input neurons pass on their input to the next layer of neurons neurons where neurons perform processing on their input.

Shown in is an enlarged view of a neuron indicating a summation function using weights w i 1 2 . . . M and a transfer function A. The integer M denotes the number of inputs to neuron which for the embodiment of is the number of input neurons . Denoting the inputs to neuron as x i 1 2 . . . M the summation function provides an intermediate term h where h wx. The weights depend upon the particular output neuron performing the summation but to avoid multiple subscripts this dependency is not explicitly indicated. For some embodiments a bias term b may be used where where the bias term depends upon the particular output neuron.

The intermediate term h is fed as input to the transfer function A to provide an output y where in general y A h . For some embodiments A h may take the following form 

The particular transfer function described above may be termed a sigmoid logarithmic transfer function or piece wise sigmoid logarithmic transfer function. This is to be distinguished from the relatively common sigmoid transfer function where A h 1 exp h for all h.

For some embodiments employing digital processing to perform the adaptive neural network function the input voltage signals to input neurons are quantized by one or more analog to digital converters so that input neurons and output neurons operate in the digital domain. However other embodiments may be mixed signal systems where some processing functions are performed in the analog domain and some processing functions are performed in the digital domain. Some embodiments may be realized in which almost all functions or all functions are performed in the analog domain. For example the weighted summation performed by a neuron as well as the transfer function may be performed in the analog domain using analog multiplier circuits and analog summation circuits.

For some embodiments the adaptive neural network may comprise more than one processing layer so that there are hidden layers. For example illustrated in is an adaptive neural network with two input neurons layer a hidden layer with three neurons layer and an output layer with two neurons layer where neuron is a neuron in hidden layer with transfer function A and neuron is an output neuron in layer with transfer function A. The adaptive neural network illustrated in is a feedforward network because there are no feedback paths from a neuron in one layer to another neuron in a preceding layer. Some embodiments for example may include feedback paths from layer to layer to implement a recursive adaptive neural network.

In the inputs are represented by xand x. These are provided as inputs to the hidden layer such as for example neuron . For neuron it is understood that the summation operation operates on two inputs i.e. the inputs xand x . The output of any one neuron in hidden layer is provided as an input to all neurons in output layer . The output of neuron is denoted as x where the index i 1 2 3 corresponds to neurons and respectively. This output is provided as one of three inputs to each neuron in output layer such as neuron . It is understood that the summation sign in neuron operates on three inputs i.e. x x and x . For some embodiments the transfer function A may be that as described in the previously displayed expressions for A. The transfer function need not be the same for each layer but for ease of discussion the same symbol A is used for neurons in the hidden layer and in the output layer.

During training of an adaptive neural network a training set of input data is provided and the weights and perhaps biases are updated based upon some desired output and criterion of goodness. For example suppose for an adaptive neural network there is a set of input variables x x . . . x and a set of output variables y y . . . y. It is convenient to define an input vector variable right arrow over x x x . . . x and an output vector variable right arrow over y y y . . . y . It is also convenient to define particular realizations of these vectors where we define input vectors right arrow over x i x i x i . . . x i and output vectors right arrow over y i y i y i . . . y i with the index i denoting a particular realization. For example right arrow over x i i 1 2 . . . T may represent T input training data vectors and right arrow over y i i 1 2 . . . T may represent the resulting T output vectors given by an adaptive neural network for some given set of weights and also perhaps for some given set of biases . For ease of notation the dependency of the output vectors on the set of weights and perhaps biases is not shown.

During training for each right arrow over x i there is a corresponding desired response vector right arrow over d i . For example suppose the pattern recognition function performed by an adaptive neural network is to map the input into one of two classes. That is there are two patterns to recognize. For a particular example in which there are two output neurons so that the dimension of right arrow over y is N 2 the desired response may be taken as right arrow over d i 10 if right arrow over x i belongs to one of the two classes and right arrow over d i 0 1 if right arrow over x i belongs to the other one of the two classes. A criterion of goodness may be to find the set of weights and perhaps biases that minimize the sum of errors e i over the training set right arrow over x i i 1 2 . . . T where e i right arrow over y i right arrow over d i .

For arbitrarily dimensioned vectors and desired responses the above described minimization is a well known mathematical problem and various mathematical techniques for finding the set of weights and biases that satisfy the criterion of goodness are well known. For example the method of steepest descents may be used which may be used in conjunction with the error back propagation neural network learning algorithm well known in the art of adaptive neural networks.

For an adaptive neural network with a sigmoid logarithmic transfer function as discussed previously some embodiments may utilize the back propagation algorithm for training as follows. First train the adaptive neural network using a sigmoid transfer function until some criterion of goodness is satisfied. For example some set of training input vectors right arrow over x i i 1 2 . . . T desired responses right arrow over d i i 1 2 . . . T and threshold A is selected where the initial set of weights are chosen randomly. The back propagation algorithm is run until e i 

Once the adaptive neural network has been trained it may then be operated with static weights pattern recognition mode to perform pattern recognition. Post processing may be applied to the output vector from the output neurons. For some embodiments a winner take all module may be applied whereby the output neuron with the largest output is chosen as the winner. Some embodiments may perform a multiple winner take all module whereby the next highest neuron after the winner is selected and so on for other neurons. The final outcome result of the adaptive neural network may be represented by some L bit number where L log N denoting the selected neuron where the bracket denotes the smallest integer larger than or equal to log N .

Some embodiments may perform signal processing algorithms other than those described previously. For example note that the sum h wxmay be viewed as an inner product of a weight vector right arrow over w w w . . . w with the input vector right arrow over x . The weight vector right arrow over w may also be referred to as a codevector. For some embodiments a processing neuron finds the square of the Euclidian distance denoted as d between an input vector right arrow over x and a codevector right arrow over w that is d right arrow over x right arrow over w right arrow over x right arrow over w . The function d may be termed a distortion. It is passed on as input to the transfer function. That is the output of the neuron employing a distortion measure is A d . For such embodiments in which the distortion is calculated the winning neuron will have a minimum output. Alternatively the output may be taken as 1 A d so that the winning neuron will have a maximum output.

The weights in the weight vector right arrow over w w w . . . w are sometimes also referred to as synapse weights. The processing of right arrow over x right arrow over w or d right arrow over x right arrow over w right arrow over x right arrow over w may be considered as part of a synapse cell where the neuron cell involves applying the transfer function to the result of the synapse cell. However there is no conceptual difference whether or not the synapse cell function is considered part of a neuron cell or is separated out from the neuron cell although there may be implementation differences in realizing the processing in hardware.

Some embodiments may perform processing other than the inner product right arrow over x . right arrow over w or the distortion d right arrow over x right arrow over w right arrow over x right arrow over w so that more generally some embodiments may pass on to a neuron some value right arrow over x right arrow over w where is some function mapping two vectors into a number.

Furthermore some embodiments may employ a learning function other than a conventional back propagation algorithm commonly used in adaptive neural networks. For example some embodiments may perform the following processing operations during the learning mode of an adaptive neural network.

Index the weight vectors codevectors as right arrow over w where the index i refers to the neuron or synapse cell if that terminology is being used . Furthermore it is useful to add another index to right arrow over w to designate a particular learning iteration where right arrow over w t refers to the codevector for neuron i at iteration t.

Associate with each neuron i a winning frequency f. It is also convenient to index faccording to the iteration index so that f t refers to the winning frequency for neuron i at iteration index t. Following the convention that t 0 for the first iteration initialize right arrow over w 0 by choosing them from a set of random or pseudorandom numbers. Set f 1 for each i.

Compute the distortion d t we have also indexed d according to the neuron index and the iteration index where d t d right arrow over w t right arrow over x t for each neuron note that we have also indexed the input vector right arrow over x to refer to neuron i and the iteration index. Select the neuron with the smallest distortion and set its output denoted as O t as follows a value of 1 is considered high O t 1 if d t 

The above operations are performed for the set of training vectors. Use of the upper threshold frequency may avoid codevector under utilization during the training process for an inadequately chosen initial codebook of codevectors. The selection of the upper threshold frequency is heuristic and depends on source data statistics and the training sequence. Empirically an adequate fmay be chosen to be two to three times larger than the average winning frequency. The initial codebooks may be created from a pseudorandom number generation function.

A feedforward adaptive neural network is amenable to a parallel processing architecture because all of the neurons in any one layer may process data concurrently. If the functionality of providing the inner product distortion function or other types of functions involving the weight vector and input vector to a neuron is to be separated out from the neuron and realized by separate circuits e.g. the synapses as discussed previously then the previous sentence should be modified to indicate that the synapses for a layer also may process data concurrently. Furthermore for multiple layers in a feedforward adaptive neural network there may be further concurrency in the sense that one or more layers of neurons and synapses may be processing in a pipelined fashion. As a result an adaptive neural network is suitable for a VLSI Very Large Scale Integration circuit that takes advantage of concurrent parallel processing.

The functional blocks of VLSI circuit according to an embodiment are illustrated in . The analog voltage signals from the differential logarithmic amplifiers e.g. the circuit illustrated in are provided at input port . These analog signals are provided to sample and hold . Host Processor provides several functions to VLSI circuit . For example host processor provides weight vectors to sample and hold . These weight vectors may be the weight vectors obtained after the adaptive neural network has been trained in which case they are provided to synapse weight matrix unit or they may be the weight vectors that are used for training during a learning mode in which case they are provided to synapse weight matrix . During learning a training set is provided to input port . The term matrix is used in the description for functional units and because the weight vectors may be considered rows or columns of a matrix.

Note that for the embodiment of VLSI circuit performs much of the learning algorithm so that the parallel processing available from functional units and may be utilized in which case host process performs some non parallel learning functions and data flow control. For some embodiments all or some of the functions provided by host processor may be integrated on VLSI circuit .

Control lines allow host processor to select the sources of input vectors to VLSI circuit and whether weight matrix pattern recognition mode or adaptive or learning mode are used to store the weight vectors. For example for some embodiments if for control lines IV mnemonic for input vector is set to a logic 0 a LOW digital signal value then the input vector is from the host processor whereas if IV is set to a logic 1 a HIGH digital signal value then the input vector is from sensor array that is the analog output of the differential logarithmic amplifier in circuit . If for control lines WV mnemonic for weight vector is set to a logic 1 then weight matrix is selected to store weight vectors loaded from host processor . These weight vectors are those that are obtained after the learning algorithm has been performed so that the adaptive neural network is operating in its pattern recognition mode or they may be the desired response vectors d i used in a supervisory learning algorithm when the neural network is operating in its adaptive or supervisory learning mode.

The particular weight vector in functional units and for a neuron is addressed by address decoders and where the particular address is provided by host processor by way of vector address bus lines . Input neurons provides the analog input vector to either synapse weight matrix or depending upon whether the adaptive neural network is in a learning mode or a pattern recognition mode. The latter may be termed an encoding mode in the sense that an input vector is encoded into a recognizable class. Functional units or perform the synapse function where for the previously described embodiments may involve forming the inner product of the weight vectors with input vectors or calculating the distortions. These results are passed to functional unit .

Functional unit performs the neuron functions discussed previously that is functional unit applies the transfer function to the synapse result. The particular transfer function is selectable. For example the transfer function may be a sigmoid function or a sigmoid logarithm as discussed previously. The outputs of these neurons are provided to functional unit which performs a winner take all function or perhaps selects one among the top several neurons. This result may be encoded into a binary number provided at output port .

Sensor array circuit and VLSI circuit may be integrated on a single die system on chip for some embodiments whereas for other embodiments these components may reside on two or more die or comprise a multi chip module for example.

Although the subject matter has been described in language specific to structural features and methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims. Accordingly various modifications may be made to the described embodiments without departing from the scope of the invention as claimed below.

It is to be understood in these letters patent that the meaning of A is connected to B where A or B may be for example a node or device terminal is that A and B are connected to each other so that the voltage potentials of A and B are substantially equal to each other. For example A and B may be connected together by an interconnect transmission line . In integrated circuit technology the interconnect may be exceedingly short comparable to the device dimension itself. For example the gates of two transistors may be connected together by polysilicon or metal interconnect where the length of the polysilicon or metal interconnect is comparable to the gate lengths. As another example A and B may be connected to each other by a switch such as a transmission gate so that their respective voltage potentials are substantially equal to each other when the switch is ON.

It is also to be understood in these letters patent that the meaning of A is coupled to B is that either A and B are connected to each other as described above or that although A and B may not be connected to each other as described above there is nevertheless a device or circuit that is connected to both A and B. This device or circuit may include active or passive circuit elements where the passive circuit elements may be distributed or lumped parameter in nature. For example A may be connected to a circuit element that in turn is connected to B.

It is also to be understood in these letters patent that a current source may mean either a current source or a current sink. Similar remarks apply to similar phrases such as to source current .

It is also to be understood in these letters patent that various circuit components and blocks such as current mirrors amplifiers etc. may include switches so as to be switched in or out of a larger circuit and yet such circuit components and blocks may still be considered connected to the larger circuit.

Throughout the description of the embodiments various mathematical relationships are used to describe relationships among one or more quantities. For example a mathematical relationship or mathematical transformation may express a relationship by which a quantity is derived from one or more other quantities by way of various mathematical operations such as addition subtraction multiplication division etc. Or a mathematical relationship may indicate that a quantity is larger smaller or equal to another quantity. These relationships and transformations are in practice not satisfied exactly and should therefore be interpreted as designed for relationships and transformations. One of ordinary skill in the art may design various working embodiments to satisfy various mathematical relationships or transformations but these relationships or transformations can only be met within the tolerances of the technology available to the practitioner.

Accordingly in the following claims it is to be understood that claimed mathematical relationships or transformations can in practice only be met within the tolerances or precision of the technology available to the practitioner and that the scope of the claimed subject matter includes those embodiments that substantially satisfy the mathematical relationships or transformations so claimed.

