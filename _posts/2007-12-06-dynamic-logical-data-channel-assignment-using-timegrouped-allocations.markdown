---

title: Dynamic logical data channel assignment using time-grouped allocations
abstract: A method, system and program are provided for dynamically allocating DMA channel identifiers to multiple DMA transfer requests that are grouped in time by virtualizing DMA transfer requests into an available DMA channel identifier using a channel bitmap listing of available DMA channels to select and set an allocated DMA channel identifier. Once the input values associated with the DMA transfer requests are mapped to the selected DMA channel identifier, the DMA transfers are performed using the selected DMA channel identifier, which is then deallocated in the channel bitmap upon completion of the DMA transfers. When there is a request to wait for completion of the data transfers, the same input values are used with the mapping to wait on the appropriate logical channel. With this method, all available logical channels can be utilized with reduced instances of false-sharing.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07865631&OS=07865631&RS=07865631
owner: International Business Machines Corporation
number: 07865631
owner_city: Armonk
owner_country: US
publication_date: 20071206
---
The present invention is directed in general to the field of data processing systems. In one aspect the present invention relates to direct memory access management within data processing systems.

Data processing systems typically include one or more central processing units CPU one or more levels of caches one or more external memory devices and input output I O mechanisms all interconnected via an interconnection of buses and bridges. In addition to these hardware components data processing systems also include one or more software or firmware components such as an Operating System OS and one or more programs which interact with the OS. To facilitate memory access operations direct memory access DMA operations are used to provide data transfer operations e.g. from external memory to a local cache memory without requiring direct processor oversight and control. Typically a hardware DMA controller is provided for a processor which allows multiple DMA requests to be made on different logical channels. These channels can be used independently to initiate multiple simultaneous asynchronous DMA data transfers. A typical software programming model would consist of initiating and waiting for DMA data requests on specific logical channels that are based on some sort of static mapping of the data to a channel. This results in operations on unrelated data that can potentially map to the same logical channel resulting in false dependencies which may in turn result in performance degradation.

Accordingly there is a need for a system and method for controlling multiple DMA transfer requests to efficiently and quickly allocate DMA channels. There is also a need for a method and system for dynamically allocating logical channels for DMA transfers so as to reduce false dependencies. In addition there is a need for a system and method to rapidly and efficiently reclaim deallocated DMA channels. Further limitations and disadvantages of conventional DMA allocation solutions will become apparent to one of skill in the art after reviewing the remainder of the present application with reference to the drawings and detailed description which follow.

A dynamic allocation system and methodology are provided for allocating logical channels for DMA transfers by reusing a channel ID for allocations made within a certain period of time. With this approach the logical channels for DMA transfers can be rapidly allocated deallocated and reclaimed under software control thereby reducing false dependencies for unrelated data transfers and improving performance. When an asynchronous DMA transfer is initiated an available logical channel ID is automatically assigned and recorded based on an input value e.g. an effective address and is re used for any additional DMA requests received within a predetermined time frame. Since these channel DMA requests should complete around the same time this input value can then be used to later wait on the outstanding requests using the same input value that was used to initiate it. With this method all available logical channels can be utilized without any false sharing and multiple DMA transfer requests can be efficiently allocated over a limited number of channel IDs. In a selected embodiment any DMA transfer requests occurring within a given time increment are dynamically assigned a logical channel ID under control of software by identifying an available logical channel ID from a channel bitmap listing of channel IDs. Once an available channel ID is identified the bitmap position corresponding to the identified channel ID is set and a channel map is updated to associate the DMA transfer requests with the identified channel ID.

In accordance with various embodiments DMA channels may by dynamically allocated or assigned under software control using the methodologies and or apparatuses described herein which may be implemented in a data processing system with computer program code comprising computer executable instructions. In whatever form implemented a plurality of DMA transfer requests are received in a predetermined time increment during operation of a data processing system. In response a first DMA channel identifier is allocated for use in performing the plurality of DMA transfer requests by selecting a first DMA channel identifier that is available from a plurality of candidate DMA channel identifiers. The allocation may be performed by sequentially incrementing a DMA channel identifier counter over a plurality of time increments and allocating a current value of the DMA channel identifier counter in a first time increment for use in performing at least the first and second DMA transfers that are requested within the first time increment. Once the DMA channel identifier counter is incremented over the plurality of time increments the DMA channel identifier counter may be reset. As a result the plurality of DMA transfer requests may be grouped by time by allocating the first DMA channel identifier for use in performing the first DMA transfer and a second DMA transfer that is received within a predetermined time period of receiving the request for the first DMA transfer. To assist with waiting on the DMA transfer the DMA transfer requests are mapped to the first DMA channel identifier at which point the first DMA channel identifier is used to perform the DMA transfer requests. The mapping may be implemented by maintaining a table for associating each allocated DMA channel identifier with the memory addresses associated with the DMA transfer requests such as application defined tags or effective addresses in external memory that are associated with the DMA transfer requests. Once the requested DMA transfers are completed the first DMA channel identifier is deallocated. In selected embodiments the allocation of the first DMA channel identifier is implemented by searching a channel bitmap to select and set an available DMA channel identifier. This may be done by identifying a first bit position corresponding to the first DMA channel identifier in the channel bitmap in which a predetermined value e.g. a zero is stored and then storing a second predetermined value e.g. a one in the first bit position thereby allocating the first DMA channel identifier to the DMA transfer requests. In these embodiments the first DMA channel identifier may be deallocated upon completion of the requested DMA transfers by storing the first predetermined value e.g. a zero in the first bit position upon completion of the requested DMA transfers.

A method system and program are disclosed for dynamically allocating logical channel identifiers to multiple DMA transfer requests that are grouped in time. In selected embodiments the logical channel identifiers are allocated under software control using one or more multi bit words as channel bitmaps with each bit representing the availability of a DMA channel identifier where the bit being set represents that the channel is in use and the bit being re set represents that the channel is available for allocation. When a plurality of DMA transfers are initiated within a predetermined time increment a first DMA channel identifier is selected and assigned to the first DMA transfer request and any subsequent DMA transfer request s within the predetermined time increment are assigned the same channel identifier. In selected embodiments an available DMA channel identifier is selected by finding a first bit position that is not set and then setting it. The DMA channel identifier designated by this bit position is then used for the plurality of data transfers. A mapping is then created between the input values associated with the DMA transfer requests and the DMA channel identifier. When there is a request to wait for completion of any of the data transfers that have been allocated to a particular logical channel the same input value is used with the mapping to wait on the logical channel. The input value used to create the mapping could be for example an effective address or some application generated identifier. For example where the input value associated with a DMA request is a tag ID the channel bitmap could be used to virtualize each of the tag IDs for the time grouped DMA requests into a different dynamically assigned unused tag ID. In other embodiments tag IDs are dynamically assigned based on effect addresses to DMA operations initiated by a software managed cache.

Various illustrative embodiments of the present invention will now be described in detail with reference to the accompanying figures. It will be understood that the flowchart illustrations and or block diagrams described herein can be implemented in whole or in part by dedicated hardware circuits firmware and or computer program instructions which are provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus implement the functions acts specified in the flowchart and or block diagram block or blocks. In addition while various details are set forth in the following description it will be appreciated that the present invention may be practiced without these specific details and that numerous implementation specific decisions may be made to the invention described herein to achieve the device designer s specific goals such as compliance with technology or design related constraints which will vary from one implementation to another. While such a development effort might be complex and time consuming it would nevertheless be a routine undertaking for those of ordinary skill in the art having the benefit of this disclosure. For example selected aspects are shown in block diagram form rather than in detail in order to avoid limiting or obscuring the present invention. In addition some portions of the detailed descriptions provided herein are presented in terms of algorithms or operations on data within a computer memory. Such descriptions and representations are used by those skilled in the art to describe and convey the substance of their work to others skilled in the art. Various illustrative embodiments of the present invention will now be described in detail below with reference to the figures.

Referring to a diagram depicts a computer architecture of a data processing system in which selected embodiments of the present invention may be implemented. The depicted data processing system contains one or more processing units e.g. . . . an external or main system memory and a system or interconnect bus that couples various system components including the processing unit s and the system memory. In an example configuration the data processing system may be implemented as a high performance system with multiple independent vector processors such as IBM s Cell Broadband Engine CBE processor. As depicted the CBE processor employs nine cores integrated on a single chip multiprocessor including eight attached processing units APUs and a power core which acts as a controller to set up and maintain an environment for the APUs.

The power core which is sometimes referred to as the power processor element PPE may be implemented with a traditional processor core such as a 64 bit PowerPC architecture core for handling general purpose computing chores using either 32 bit or 64 bit operating systems and applications. As depicted the power core includes one or more processor units such as an ALU and or FPU first level L1 cache a second level L2 cache and other logic and control circuitry for supporting simultaneous multithreading SMT . In an example embodiment the power core contains a 64 bit dual thread PowerPC Architecture RISC core and supports a PowerPC virtual memory subsystem. In addition 32K L1 instruction and data caches and a 512K L2 unified instruction and data cache may be provided on the power core which is intended primarily for control processing running operating systems managing system resources and managing APU threads.

Each of the APU cores which are sometimes referred to as the synergistic processor elements SPE may be implemented as independent high performance processor elements each having a block of very high speed memory running their own individual application programs and threads. In selected embodiments each APU e.g. contains a processor unit e.g. a RISC core a software controlled local store for instructions and data and a 128 bit 128 entry unified register file not shown . Each APU manages DMA transfers to and from the local store using a DMA controller module in combination with channel allocation instructions stored in the local store . In addition each APU has full access to coherent shared memory and performs DMA transfers to and from the memory using the effective addresses of the memory mapped I O space. This is depicted in with reference to an example first APU which includes a first processor unit which has full access to shared coherent memory via the interconnect bus .

In operation of an example embodiment each APU s processor unit e.g. can fetch instructions only from its own local store memory e.g. and load and store instructions executed by the APU can only access the local store using local store addresses not main memory effective addresses . To this end each APU core e.g. controls DMA transfers and communicates with the remainder of the data processing system by means of channels that are implemented in and managed by the APU memory flow controller MFC e.g. . Channels are unidirectional interfaces for sending and receiving variable size e.g. up to 32 bit messages with other system components and for sending commands such as direct memory access transfer commands to the APU S associated memory flow controller MFC . Each APU has its own set of channels. The MFC serves as the APU S interface by means of the element interconnect bus EIB to the external main memory and to other processor elements and system devices. The primary role of the MFC is to interface between the local store memory and external main memory by means of a DMA controller that moves instructions and data between a local store address and an effective address in main memory .

Each APU accesses its corresponding MFC state information using a channel interface not shown . The power core and other devices in the system including other APU cores can also access the MFC state through memory mapped I O MMIO registers and queues in the MFC which are visible to software in the main storage address space. With this configuration data transfers between an APU s local store e.g. and main memory are performed using the APU s DMA controller in the MFC associated with the local store . Each DMA command may be tagged with a tag ID that allows software to check or wait on the completion of the DMA command. Channel allocation software running on the associated APU uses a channel bitmap and mapping structure to dynamically assign logical DMA channel identifiers as described more fully hereinbelow.

With multi core embodiments of the data processing system processing power is increased by adding more APU cores. Even so the full processing power of such multiprocessor embodiments is not readily realized because memory latency as measured in processor cycles has gone up dramatically due to improvements in processor designs. To improve processing speed each APU core e.g. is provided with a small e.g. 256K high performance local memory while slower speed access to a larger e.g. multi gigabyte dynamic random access memory DRAM is also provided. In selected embodiments each APU e.g. is optimized for running computationally intensive SIMD applications and depends on the power core to run the operating system and in many cases the top level thread control for an application. In turn the power core uses the APUs to provide the bulk of the application performance.

System memory may be implemented with computer storage media in the form of non volatile memory and or volatile memory in the form of a collection of dynamic random access memory DRAM modules that store data and instructions that are immediately accessible to and or presently operated on by the processing unit s etc. System memory may also have an associated memory controller not shown for controlling access to and from system memory .

The depicted interconnect bus may be implemented as a memory coherent element interconnect bus EIB which connects the APUs together and the L2 cache on the power core using a plurality of multi byte data rings. It will be appreciated that any desired bus architecture can be used. Though not shown a communication adapter may be used to provide access to communication link s a user interface adapter s connected to various user input output devices such as a keyboard mouse touch screen stylus microphone storage printer display etc. .

To an application programmer the data processing system looks like a 9 way coherent multiprocessor in which the power core is designed to efficiently handle and switch between control intensive tasks while the APU cores etc. are designed to efficiently handle computationally intensive tasks albeit at a slower switching rate. Based in part on the different specializations of the cores the power and APU cores may use different memory access techniques. In support of the power core s control functionality the power core may access the main memory by using the effective address space to execute load and store instructions to move data between main memory and a private register file e.g. in the processor unit of the power core the contents of which may be cached e.g. in L1 cache or L2 cache . In contrast the APU cores e.g. may access the main memory with direct memory access DMA commands that move data and instructions between the main memory and a local memory store or local storage e.g. . Because an APU core s instruction fetches and load and store instructions access only its local store rather than shared main storage and the local store has no associated cache. The result is a 3 level organization of storage register file local store main memory with asynchronous DMA transfers between local store and main memory which explicitly parallelizes computation with the transfers of data and instructions that feed computation and store the results of computation in main memory.

Even with a local store cache memory e.g. at each APU core e.g. the performance of an APU core is still limited by memory latency. For example when a sequential program on a conventional architecture performs a load instruction that misses in the caches program execution comes to a halt for several hundred cycles. Compared to this penalty the few cycles it takes to set up a DMA transfer for an APU core are a much better trade off especially considering the fact that each of the DMA controllers on the eight APU cores can have up to DMA transfers in flight simultaneously. While hardware DMA controllers can be used to allow multiple DMA transfer requests on different logical channels such special purpose hardware has a number of drawbacks including slowing down processing increasing power consumption and reducing the number of APU cores that could be place on a single die. With a software DMA control scheme memory latency can be hidden by an APU core whenever a programmer initiates a DMA transfer by having the APU core perform work by executing other code and then having the programmer wait on the transfer to complete. In this way memory latency is hidden by doing work instead of waiting.

Thus instead of using a conventional memory management schemes the local store memory block e.g. local store in each APU may be implemented as a small e.g. 256K software managed cache which stores data that is pulled directly from the main memory under control of a DMA controller e.g. which assigns DMA channels as a resource that can be scheduled in software. By providing each APU e.g. with a DMA controller e.g. the programmer is able to issue requests for memory out of main memory by using the DMA controller to pull the memory directly into the local store or cache using explicit DMA commands. With a fixed number channels that can have concurrent DMA requests pending the programmer uses explicit DMA commands to request that a memory transfer begin and can then still do work on something else while that memory is being brought into the local store cache . To this end the programmer issues a command to wait on the channel to ensure that the transfer is complete.

Since each APU core e.g. has its own DMA controller e.g. each APU core will manage its own channel assignment and have its own channel allocator. The allocator may be provided as a library or application programming interface API that the programmer would use to issue DMA requests. The library would maintain its own internal data structures such as the channel bitmap which are stored in the local store memory . In addition the allocator would maintain a mapping between effective addresses for the time grouped DMA transfer requests and the corresponding allocated channel identifiers. As a result the programmer only needs to pass the effective address and a destination address to the library or API when submitting a DMA transfer request and is not required to supervise or manage the specifics of the DMA operation. Effectively the DMA channel assignment responsibility is transferred from the programmer to the library or API in each APU core.

In an example implementation each APU core performs DMA transfers by supporting a list such as a scatter gather list of DMA transfers that is constructed and stored in the APU core s local store memory. In accordance with selected embodiments of the present invention the list is constructed by using a channel bitmap to select an available logical channel which is then mapped to a plurality of DMA transfer requests occurring within a predetermined time increment using a channel map data structure. With such a list the DMA transfers can be set up and controlled by a first APU core that is sourcing or receiving the data or by the power core or by another APU core e.g. . The list also allows the APU core s DMA controller to process the list asynchronously while the APU core operates on previously transferred data.

In an example embodiment program instructions or code for dynamically assigning DMA logical channels may execute on an APU core with or without a power core counterpart or may be part of a larger program that could contain multiple APU programs or a power core program. To the extent that each APU core uses its own DMA controller to manage the DMA channel assignment and allocation this DMA controller may be implemented with software or other code that is tangibly embodied in a computer readable medium e.g. local storage device or any other fixed or removable data storage devices. Further the DMA controller e.g. may comprise instructions which when read and executed by the processor unit cause the APU core to perform the steps necessary to implement and or use the present invention. As such the terms article of manufacture program storage device and computer program product as may be used herein are intended to encompass a computer program accessible and or operable from any computer readable device or media.

Each of the eight APU cores may be implemented as a single instruction multiple data SIMD vector processor which may be generically referenced as a vector processor which employs an independent instruction stream. In this configuration each vector processor includes instructions capable of processing multiple data items in parallel. For example such a vector processing architecture may multiply corresponding 32 bit floating point quantities in a 128 bit word in parallel. As depicted in each of the vector processors is operatively coupled to its own a high speed local store memory or cache memory e.g. which may be generically referenced as a local store cache memory. In turn the local store cache memory of each vector processor is operatively coupled to a larger main memory . However it is important to note that embodiments of the invention are not limited to any specific number of vector processors. In addition embodiments of the invention are not limited to any particular cache level and the local store cache memory may reside in the vector processor or be a separate component.

In the vector processor embodiment described herein a memory access occurs whenever data is read or written to any memory device. In the embodiments where each vector processor is capable of directly accessing only its own local store memory e.g. 256K RAM the vector processors use direct memory access DMA techniques to copy between main memory and their respective local store memory. Management of the data between the local store cache memory e.g. and the main memory is facilitated by using any desired cache management technique such as a cache metadata array having virtual tags and physical addresses of the main memory and local store cache memory e.g. respectively.

When a vector processor initiates a DMA transfer the DMA controller assigns each transfer to a logical channel which is identified by a logical channel ID. In IBM s Cell processor the logical channels are represented by tags and in this sense the terms tag and channel may be used interchangeably. By using a channel map as described herein each tag may be mapped to a channel ID. If there were no limit on the number of DMA transfer channels then concurrent DMA transfer requests would never need to share a logical channel. However in situations where there a finite number of channels unrelated DMA requests occasionally may be forced to share a channel. In situations where a DMA channel is shared a program may have to wait for data to transfer that it does not care about because another request was made on the same DMA channel.

Instances of shared DMA channels may be reduced by intelligently allocating channels at the time the DMA requests are made by allocating a single channel ID to multiple DMA transfers initiated within a predetermined time period or increment. In an example embodiment a bitfield method may be used to allocate channels by maintaining a channel bitmap where each bit position in the channel bitmap represents a logical channel and the value of the bit indicates whether the logical channel is available e.g. the bit is set to a zero value or allocated e.g. the bit is set to a one value . An example of such a channel bitmap is depicted in which illustrates an example channel bitmap that is used to dynamically allocate logical channels to DMA transfer requests. The channel bitmap may be formed as one or more n bit words e.g. a 32 bit word where each bit represents the availability of a DMA channel identifier. Thus a first logical channel is represented by the first bit position a second channel ID is represented by the second bit position a third channel ID is represented by the third bit position a fourth channel ID is represented by the fourth bit position and so on until the nth channel ID is represented by the last bit position . As shown in if the bit for a given channel ID is set e.g. a one is stored for the first channel ID this represents that the channel is in use. Alternatively if the bit for a given channel ID is reset e.g. a zero is stored for the fourth channel ID this represents that the channel is available for allocation. The bitfield method uses the channel bitmap to dynamically allocate DMA channels whenever one or more DMA transfers are initiated by finding the first bit position in the channel bitmap that is not set and then setting it thereby selecting an available DMA channel identifier for the DMA transfer s . The DMA channel identifier designated by this bit position is then used for the DMA transfer s and a mapping is then created between the input value associated with the requested DMA transfer s and the DMA channel identifier.

An example of such a mapping is depicted in which illustrates an example channel map which is used to associate allocated logical channels with one or more corresponding DMA transfer requests. In the depicted channel map a simple table is maintained in which each row e.g. row associates a DMA transfer with its allocated channel bitmap position in the channel bitmap e.g. . As depicted the first row associates an input value for a first DMA transfer e.g. the main memory effective address a for the first DMA transfer with the channel bitmap position in the channel bitmap corresponding to the logical channel identifier e.g. channel ID that was allocated for the first DMA transfer. For example if the channel bitmap is a 32 bit word and the channel ID corresponding to bitmap position was allocated to the first DMA transfer then the channel bitmap position for channel ID stored in the map row would be 00000. In embodiments where multiple concurrent DMA transfers e.g. DMA transfer occurring within a predetermined time period or increment are allocated to a single channel ID the channel map associates the concurrent DMA transfers with their allocated channel bitmap position in the channel bitmap e.g. . For example if the channel bitmap was used to allocate channel ID to a second DMA transfer and a third DMA transfer that occur within a certain period of time then the second and third rows of the map each associate the input values for the second and third DMA transfers with the channel bitmap position in the channel bitmap corresponding to channel ID . This is shown in where the second row in the map associates an input value for the second DMA transfer e.g. the main memory effective address b for the second DMA transfer with the channel bitmap position in the channel bitmap corresponding to channel ID e.g. bitmap position 00001 . In similar fashion the third row in the map associates an input value for the third DMA transfer e.g. the main memory effective address c for the third DMA transfer with the channel bitmap position in the channel bitmap corresponding to channel ID e.g. bitmap position 00001 .

For a new DMA transfer request e.g. a fourth DMA transfer associated with main memory effective address d the channel bitmap is searched to find the first free or unallocated logical channel identifier. In the example of the first available channel identifier would be the third channel ID since its value is reset or zero. Upon setting the value of the third bit position in the channel bitmap to a one not shown the map would be updated so that the fourth row associates an input value for the new DMA transfer e.g. the main memory effective address d for the fourth DMA transfer with the channel bitmap position in the channel bitmap corresponding to channel ID e.g. bitmap position 00010 . And if additional DMA transfer requests are received and allocated within a certain period of time from when the fourth DMA transfer is received the map would be updated so that an additional row associates an input value for the additional DMA transfer with the channel bitmap position in the channel bitmap corresponding to channel ID e.g. bitmap position 00010 .

To further illustrate selected embodiments of the present invention depicts a logical flowchart of the steps used to allocate and deallocate DMA channels to individual DMA requests. At step the process starts such as when an APU core is running a program that requires data from memory. When an initial memory access is required a first DMA channel transfer is initiated affirmative outcome to decision by requesting a DMA channel identifier for the transfer. A DMA channel identifier may be selected by using the channel bitmap to select and set an available e.g. non allocated channel identifier step . This may be done by selecting a channel identifier corresponding to a bit position in the channel bitmap in which a zero is stored and then setting the corresponding bit to one. 

If there are no more additional memory accesses required within a designated time frame negative outcome to decision then the selected channel ID is mapped to the effective address for the requested DMA channel transfer step the DMA transfer is performed using the selected channel ID step and the corresponding bit in the channel bitmap is reset upon completion of the DMA transfer step at which point the process is repeated for the next DMA transfer. However if there are one or more concurrent memory accesses required within the designated time frame affirmative outcome to decision then the selected channel ID is retrieved for use with the concurrent DMA channel request s step . As will be appreciated the selected channel ID is retrieved for each of the concurrent DMA channel requests occurring during the designated time frame in effect making step an iterative process which is repeated until there are no more DMA channel requests detected during the designated time frame.

For each DMA channel request occurring within the predetermined time frame the selected channel ID is mapped to the input values associated with the plurality of DMA transfer requests step . The input value used to create the mapping could be an effective address or some application generated identifier. For example where a DMA transfer request contains both a local store address and a main memory effective address the channel ID may be mapped to the main memory effective address such as by using a channel map shown in . Alternatively the input value associated with a DMA request may be the tag ID in which case the channel bitmap could be used to virtualize the tag ID into a different dynamically assigned unused tag ID. In other embodiments tag IDs are dynamically assigned based on effect addresses to DMA operations initiated by a software managed cache.

Whatever allocation technique is used the allocated channel ID is used to perform the plurality of DMA transfers step . In this way the mapping scheme of the present invention may be used given the same input value when there is a request to wait for completion of the data transfer to wait on the appropriate logical channel. Once the plurality of DMA transfers are completed the channel ID is no longer needed and may be deallocated by resetting the bit in the channel bitmap that corresponds to the channel ID step . The deallocated channel ID is then available for use by a subsequent DMA transfer s as indicated by the loopback path to the DMA channel request detection step .

In general terms the concurrent DMA transfer requests may be identified on the basis of detecting DMA channel requests that occur within a predetermined time period or increment from an initial DMA channel request. In this way when multiple concurrent memory accesses are required they may be allocated as a group to any available channel ID or tag that may be identified by using a bitmap to track which IDs or tags have outstanding requests. For example an allocation code sequence or routine may be used which defines the channel ID or tag for any DMA transfer requests occurring within a time increment and which is periodically increments the channel ID or tag to define a new channel ID or tag for any DMA transfer requests occurring within the next time increment and so on. The allocation code routine may sequentially step through a plurality of channel IDs or tags until they run out at which time the channel ID or tag is reset.

In an example allocation code routine where channel IDs or tags are being dynamically allocated to DMA transfer requests the following routine tagid tick may be called periodically e.g. by a decrementer 

To illustrate the operation of how the tagid tick routine increments the current channel id reference is made to which depicts a state diagram of the tagid tick routine. After starting state the tagid value is cleared or set to a zero value in an initial state state . The routine then proceeds to a wait state state where the routine waits for the decrementer to issue an increment command. When an increment command is issued the routine increments the tagid state and proceeds to the detection state to determine if the tagid has exceeded the maximum tagid value state . If the maximum tagid value is exceeded affirmative outcome to detection state the routine resets the tagid value as indicated by the feedback to the initial state . If not negative outcome to detection state the routine returns to wait for the decrementer as indicated by the feedback to the wait state . With the tagid tick routine the tagid is incremented with each clock tick and is reset to 0 if all of the tags have been used or incremented. In this way each DMA request uses the same tagid until the timer ticks again. As a result all DMA requests that occur between timer ticks use the same tagid.

As described hereinabove a single DMA channel identifier may be used for allocations made within a certain period of time. With this approach DMA requests would be grouped by time to ensure that when the program waits that channels DMA requests should complete around the same time. An example implementation of a time grouped allocation is shown in which depicts a logical flowchart of the steps used to allocate and deallocate DMA channels to individual DMA requests occurring within a predetermined time frame. The process starts at step when for example an APU core is running a program that requires data from memory. When a memory access is required a first DMA transfer request is received at a DMA controller step and a first channel ID is allocated to the first DMA request step . If one or more additional DMA requests are received at a DMA controller within a predetermined time period affirmative outcome to detection step the first channel ID is re used for any such additional DMA request s step as indicated by the loopback path from step to detection step . Upon expiration of the predetermined time period negative outcome to detection block any pending DMA transfer is performed using the allocated first channel ID step . Once the DMA transfer is completed the channel ID is no longer needed and may be deallocated step . The deallocated channel ID is then available for use by a subsequent DMA transfer as indicated by the loopback path to the DMA request detection step .

By grouping DMA requests so that they are allocated to a single channel ID more DMA requests can be allocated than would be permitted by the finite size of any channel bitmap or by any one to one mapping scheme. Channel bitmap allocation techniques also allow logical channels to be dynamically allocated for DMA transfers so as to reduce false dependencies and also enable deallocated DMA channels to be rapidly and efficiently reclaimed.

As will be appreciated by one skilled in the art the present invention may be embodied in whole or in part as a method system or computer program product. Accordingly the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore the present invention may take the form of a computer program product on a computer usable storage medium having computer usable program code embodied in the medium. For example the functions of dynamic DMA channel allocation module may be implemented in software that is stored in the local store or in a separate memory storage unit and may be executed by a DMA controller regardless of its location in the APU core.

The foregoing description has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. It is intended that the scope of the invention be limited not by this detailed description but rather by the claims appended hereto. The above specification and example implementations provide a complete description of the manufacture and use of the composition of the invention. Since many embodiments of the invention can be made without departing from the spirit and scope of the invention the invention resides in the claims hereinafter appended.

