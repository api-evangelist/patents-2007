---

title: Grid-enabled, service-oriented architecture for enabling high-speed computing applications
abstract: Disclosed herein are systems and methods for a distributed computing system having a service-oriented architecture. The system is configured to receive workloads from client applications and to execute workloads on service hosts. The distributed computing system dynamically assigns the workloads to the applications running on the service hosts, with the workloads being assigned according to the service needs and the availability of service hosts and other resources on the system. The presently disclosed systems and methods provide for high-throughput communications through an asynchronous binary or a synchronous binary communications protocol. Further disclosed embodiments include flexible failover and upgrade techniques, isolation between execution users of the system, virtualization through mobility and the ability to grow and shrink assigned resources, and for a software development kit adapted for the present architecture.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08156179&OS=08156179&RS=08156179
owner: Platform Computing Corporation
number: 08156179
owner_city: Markham, Ontario
owner_country: CA
publication_date: 20070426
---
Disclosed embodiments herein relate generally to grid enabled systems for high speed computing applications and more specifically to such architectures in the context of service oriented architectures. Such architectures provide for virtualization of applications such that applications can be dynamically and flexibly assigned to various resources available on the distributed computing network.

Service oriented architectures are architectures that make computing services available as independent services that can be accessed without knowledge of their underlying platform implementations. Prior art service oriented architectures provide the resources to the applications through network based protocols which are text web based protocols such as are commonly used for Internet based communications. Common network protocols of this type include HTTP HTTPS SOAP WSDL and UDDI. These various protocols may perform one or more functions for the network communications protocols but the common root of these protocols is that they are a part of rely on or otherwise use web text based communications. Further the prior art network communications protocols used are typically implemented through synchronous SOAP HTTP communications signaling pathways.

Another prior art approach includes a service broker which is provided within a network to distribute application workloads from network clients to be run as service instances on the network resources. As with the other prior art approach this approach has suffered from the limitations of using text based web based communications protocols for communicating between the client the broker and the service instances.

This prior art approach also is limited in that it uses a pull based access protocol to determine the statuses of the available service instances whereby the service instances will notify the broker that they have additional capacity for processing jobs from the client. The difficulty with the pull based approach is that it can introduce latency in the roundtrip job processing time. Specifically if there are multiple service instances running the service instances are set to poll the broker for incoming jobs so that the service instances know when there is work to be done.

Disclosed embodiments provide a distributed computing environment and middleware for operating that environment. Specifically the described embodiments provide better throughput faster application execution and improved flexibility compared to prior art approaches whereby the described embodiment systems allow for improved ability to scale and thus to handle extremely compute intensive applications. Some of the very demanding applications to which the disclosed architectures can be applied include complex investment models extremely large database searching and complex engineering computations.

The disclosed embodiments further provide improved mobility such that the virtualized applications can run on any number of hosts rather than being tied to a fixed host for execution. Also provided are embodiments demonstrating improved flexibility in configuring and upgrading the middleware as well as improved monitoring and failover of network components and resources.

Shown in is a prior art figure illustrating the architecture of a Service Oriented Architecture SOA over web service. Shown in the figure is a system which includes a client and remote hosts . The client would have one or more services to be executed by remote hosts connected to the client through a web text based communications protocol such as for example SOAP HTTP or another Internet oriented and text based network communication protocol over communications pathways . The servers provide service instances that will operate on the servers .

A disadvantage of the prior art approach illustrated in relates to the throughput limitations that are imposed by the coding and decoding of the web text based communications that are used in the network communications protocols that this prior art architecture uses. Specifically in order to transmit data from client applications operating on the clients native binary information will have to be encoded into text based information for transmission over the text based network communications protocol through synchronous SOAP HTTP communications signaling pathways . Further at the service hosts the text based information will be decoded back into binary format for further processing. Thus as the applications to be run on the prior art system transition to high performance contexts the text based coding and decoding of program data being exchanged between the clients and the service instances will introduce major throughput delays.

Because of the encoding decoding delays associated with the web text based communications protocol this prior art approach is not generally scalable to multiple parallel high performance hosts. Such encoding decoding delays specifically cause bandwidth and speed concerns and those delays will be cumulative if not exponential depending on the particular application being run as the number of parallel service instances are increased. This prior art approach further does not allow for dynamic provisioning to generate increasing and decreasing assignments of resources to application instances running on the prior art computing network .

Illustrated in is a prior art distributed services approach in which a broker is provided within a network in order to distribute applications from clients to be run in various service instances . The broker receives jobs from one or more clients and distributes those jobs to the service instances . The broker also receives the output of the jobs from the various service instances to provide those job outputs back to the clients who are requesting the work to be done.

As with the other prior art approach shown in this approach suffers from the limitations of using text based web based communications protocols for communicating between the client the broker and the service instances . This approach also is typically implemented with a pull based access protocol from the service instances whereby the service instances will notify the broker through an HTTP protocol that the service instances have additional capacity for processing jobs from the client .

A difficulty with the pull based nature of this web text based communications approach is that it can introduce latency in the roundtrip job processing time. Specifically if there are multiple service instances the service instances must be set to check for incoming jobs and the input queue of the broker so that the service instances or the servers running those service instances know when there is work that needs to be done. If that interval is set for too short of a time the various service instances will overwhelm the broker with their polling. If the interval is set for too long then the service instances may wait some period of time before pinging the service broker and accordingly if the interval is set for too long there could then be work waiting for a relatively long period of time due to the length of the interval between pings from the service instances 

Illustrated in is a present embodiment architecture for a high performance grid enabled service oriented architecture computing system . The illustrated system is a network based computing system upon which client applications can be run in a distributed fashion. In this approach client hosts are provided having client applications that will generate workload units or tasks to be processed on resources within the system . The client applications interface with the rest of the system through a client application programming interfaces APIs .

Within a given system there would generally be multiple client hosts and client applications submitting workloads for execution on the operating on within a given system . These client applications interface through the client APIs with the system through a network resource manager . The resource manager provides a look up service for the client applications whereby the client applications are able to find an available session director. The client application alternatively may access a DNS or other look up service to find network elements with which to initially interface. The resource manager provides overall supervision for the resources within the system including provisioning resource failover load balancing and remote execution services for applications that are operating in a distributed fashion on the network or system .

As described in this embodiment a service session manager is provided to distribute the application of workloads that it has received from the client host through the interface . Unlike the communication protocols described with respect to the prior art systems above the protocol used in the present embodiment is a binary asynchronous communication protocol which provides for higher throughput lower latency communications with the improved efficiency due in part to eliminating the necessity to encode and decode program data at both of the client side and host side into text data for communication on a web text based communication protocol as would have been used in a prior art system. Another approach that will achieve improved throughput and lower latency in the communications is a synchronous binary approach. This approach still preserves the advantage of avoiding coding encoding of the binary application data that the prior art approaches would have used to transmit over the web text based communication protocols used therein.

The binary communications protocol applied to the present system is specifically applied to the data flowing through the service session managers as well as to other data and control communications flowing through the high performance distributed computing system . In particular the illustrated embodiment of provides for synchronous or asynchronous binary communications through the interface between the client hosts and the service session managers as well as through the interface between the service session manager and the service instance managers 

Synchronous or asynchronous binary communications would also be used for communications with the session director and the resource manager . Generally the mode of communication between all of the illustrated grid enabled network computing components of the present application Client API SSM SD SIM resource conductor not shown see and resource manager is that of synchronous or asynchronous binary communications protocols. Not all of these components need to be separate processes in the grid enabled networks however. For example in certain embodiments the resource conductor can be provided as a dll library that is linked into a common execution module with the service session manager and during execution these modules may operate together to provide their functionality. In that context the combined entities can be considered as a service manager having separate resource conductor and service session manager components or modules.

As throughout this application the number of system elements shown is for illustration purposes. The choice of illustrating three of such service instance managers and three interfaces is merely for illustration purposes and any number of service hosts service instances and connections to those service hosts and instances would be used in a given distributed system . In the present embodiments the number of these elements that are operational at a given time are dynamically adjusted in the presently described embodiments to provide for a highly scalable architecture to provide high performance computing in a grid enabled service oriented architecture.

Also provided in communication with the multiple service session managers is a session director which provides overall supervision for the service session managers . Each of the service session managers is responsible for operating an application within the distributed computing system . In this way the service session manager distributes the application workloads according to the resources e.g. service hosts made available to it through the resource manager . The session director receives assignment requests from the client hosts over the interface . Through the interface the session director receives notice that the client hosts have new applications to run over the network .

The session director accordingly provides service session managers to manage the client workloads provided by the client hosts . Communications to and from the resource manager and the other elements are provided through high speed communication interfaces . To provide scalable high bandwidth communications among e.g. the session director the service session managers the clients the service instance managers and the resource manager the communications interfaces and used between these network elements and other high bandwidth network elements in the system are synchronous or asynchronous binary interfaces. Although certain interfaces are shown in these are not intended to be an exhaustive list of communications interfaces to be used in the system and other such interfaces may be implemented as synchronous or asynchronous binary interfaces or as one or more other types of interfaces.

Still referring to after the establishment of the service session managers under the direction of the session director the resource manager provides service resources or hosts as computing resources on the distributed computing system . The establishment of this virtual application execution infrastructure enables the distributed computation of the client applications on the network . The present application is highly scalable to provide a high level of performance with respect to the assigned applications. In this case three hosts are shown in the figure for illustration purposes although an effective implementation of the present embodiments might use hundreds or thousands of separate hosts . The service instances are specific application kernels that are loaded and operating on the hosts .

Each of these separate instances may be operating on different hosts or they may be operating on shared host . In other words one or more of the service instances may operate on a single host although as described in certain embodiments herein it may be advantageous to provide logical and or physical segmentation between the different applications for application isolation purposes.

Within the hosts are the service instance managers which provide for the execution of the workloads on service instances operating on the hosts . Each service instance comprises in the present example a service application component which contains the core operating software for the application being run and a service API component which contains the specific interface for the high performance middleware network . The service applications are made available to the network through service APIs . The service APIs specifically provide interface capabilities for the service applications whereby they allow for standardized communications with and interfacing to the network computing system .

Still referring to the service instance managers provide for task workload management and dispatch for the services assigned from the service session manager . Again the communications provided between the hosts and the service session managers are through asynchronous binary communications protocols as contrasted to web text based communications protocols that are known in this context in the prior art. Although in the service session managers are shown located separately from the hosts it is possible for the service session managers to run in some instances on the same host as at least some of the service instance managers . This would not be the usual circumstance since the systems described here are grid enabled or distributed computing systems but some of the described elements in the described embodiments will be co located and therefore the described embodiments should not be limited in their application only to computing networks that literally follow the exemplary described architectures herein.

Another advantage provided through this architecture is a demand event driven approach whereby the service session manager is aware of which hosts service instance managers and service instances are available for executing the application workloads that have been sent from the client hosts . Because the service session manager is knowledgeable about the resources available and their statuses the service session manager does not have to wait for HTTP inquiries from various application hosts that are available for executing the various applications. Put differently the service session manager is able to provide workload units to the various service instance managers immediately upon those workload units becoming assignable by the service session manager rather than waiting on a remote HTTP request from one of the available hosts .

Still referring to the service session manager in this approach will manage the workloads for certain applications which come from multiple application submitting clients . The service session manager receives from the resource manager information detailing the available resources that can be assigned for executing these submitted applications. The resource conductor is informed by the service session manager of the SSM s needs for additional resources. This is communicated either through statistics or summary information from the SSM detailing the resource needs or by workloads routed through the resource conductor .

In embodiments involving the resource conductor the resource conductor is responsible for prioritizing the workload requests from the multiple clients according to service level agreements in a manner such as described in the commonly owned patent application Method and system for utilizing a resource conductor to optimize resource management in a distributed computing environment U.S. application Ser. No. 11 694 658 filed on Mar. 30 2007 which is incorporated by reference herein in its entirety. In this way the work can be assigned efficiently so as to effectively assign resource capacity according to policies set according to the needs of the one or more clients hosts and their owners.

Shown in is a broad level architectural scheme for use for example with the system of . Specifically illustrated in is a resource conductor which is used as was described in accordance with the above cited Method and system for utilizing a resource conductor application No. 11 694 658 . In a system such as that shown in the resource conductor would be interposed between the service session manager and the resource manager .

Each service session manager can handle multiple clients and each of the clients might have multiple sessions. In a single queue embodiment the workloads from all of these clients and sessions could be combined in a single queue for outgoing workload assignment and the workload results could be combined into a single output queue for returning to the multiple clients and multiple client sessions. Alternatively there might be a unique workload queue and output queue assigned to each individual session in which case there could be hundreds or thousands of separately defined workload queues or output queues .

The flow of the workloads through the service session manager are generally as indicated by the arrows shown in . The incoming workloads are received via the client interface by the SSM and placed in the workload queue . The workloads generally flow from the workload queue to assigned service instance managers through the service interface but the bidirectional arrow here indicates that workloads may be re queued in the workload queue even after they have been sent out from the SSM . The arrows in the path through the output queue are shown as unidirectional arrows flowing from the service interface to the client interface . Multiple queuing designs are possible in the context of the present disclosure however and accordingly neither of the above described approaches should be taken as limiting upon the claims set forth at the end of this specification.

Still referring to the embodiments in this figure shows that the resource conductor assigns resources to the service session manager for handling of the workloads in the workload queue based on the resources made available through the resource manager and generally as requested by the service session manager . The resource conductor is specifically able to balance the needs of the application as requested by the service session manager with the resources made available by the resource manager . Once the resource conductor in coordination with the resource manager has made this assignment of resources to the service session manager the service session manager is then operable to directly manage the service instance managers operating on the assigned resources as indicated in the connections as shown on .

With further reference to the resource conductor provides an interface between the service session manager and the resource manager . The resource conductor makes a top level assignment of resources e.g. services hosts available for the service session manager s use. Effectively the use of the resource conductor with the service session manager provides two level scheduling which further improves the performance of the network . The improved performance comes from the fact that with the two levels of scheduling it becomes unnecessary to have a single resource manager that provides for shifting allocations and resources happening at a lower level of detail.

Once the service session manager has a certain level of resources assigned to it it is able to make dynamic assignments of workloads to the assigned resources according to the system needs at the time. Specifically the service session manager can consider a number of possible policy and or resource availability and efficiency issues. For example the service session manager can consider one or more of the following issues 

Referring now to illustrated in these figures is a series of block diagrams illustrating the steps that are taken when a service instance is initiated through a workload assignment through the service session manager . As shown in the process begins when the client application does a look up to the session director which will look to see whether a service session manager is available for operation with the client API . If the service session manager is available the session director returns a URL or other address for the service session manager such that the client API may now directly access the service session manager . If a service session manager is not then available and running however the session director can start the service session manager and provide an operating instance of the service session manager .

The data communications and control communications among the session director the service session manager and the client API are synchronous or asynchronous binary communications in the disclosed embodiments in order to increase throughput and reduce communications latency. This approach will be used in some or all of the below referenced interfaces as needed in order to similarly increase throughput and reduce latency.

Once a service session manager is in communication with a client API as described with respect to then shows the service session manager communicating with the resource manager to make an allocation request through the resource manager . This resource request is done through the resource conductor as was described with respect to . The resource manager has knowledge of the available resources for the service session manager s use and the resource manager therefore is able to make a return allocation of the resources. Specifically as was previously described the resource manager provides addresses at which the service session manager can communicate directly with various resources in the network .

With further reference to as shown to the left of the resource manager an exemplary resource sharing plan is illustrated in this figure. The top level of resources 100 CPUs in this example was assigned according to the resource conductor s balancing the needs of the client hosts with the available resources under the control of the resource manager . As shown in the present example of the resource sharing plan the 100 CPUs may have been divided by the resource manager into groups of 50 25 and 25 according to various applications that are being run.

Procedurally a service session manager may request new allocation levels either at an increased or decreased level and the resource manager can make new allocations to the requesting service session manager and depending on need to one or more additional service session managers according to service level agreements that are used by the resource manager to make such assignment decisions. In addition to the allocation of resources according to various service level agreements as described above the resource manager may also be responsible for implementing a lending borrowing plan as generally illustrated in box . Through the lending borrowing plan the various applications operating on the system can develop a plan for sharing resources or lending or borrowing resources as between each other according to shifting demands of the various applications. The resource manager would be responsible for implementing that lending borrowing plan .

Referring now to once resources such as the service hosts have been committed to it by the resource manager the service session manager can then set the resources to executing the applications according to the execute command shown in . This execute command contains the allocation and containers for the resources for that application where the allocation basically provide the identity of resources that were returned by the resource manager and the container is the command to be executed. In the present embodiment for example the container points to the service instance manager executable. The resource manager will then pass on the execution command to the responsible resources through a Process Execution Manager PEM agent which is responsible for the remote execution service on e.g. the service hosts as an agent of the resource manager . Once the assignment of resources has been made through the process execution manager agent the service instance manager can be placed in direct communication with the service session manager such that the intermediate elements can now be removed from the communication chain.

There may however be instances where resources are to be lent out borrowed back reclaimed or otherwise requested for additional or fewer resources to be applied to or on behalf of the current service session manager or from another service session manager . The action of adding or subtracting resources may be initiated by the service session manager notifying the resource manager through the resource conductor . The lending out of resources borrowing of resources returning of borrowed resources or reclaiming of lent out resources however are actions that are initiated in the present embodiment by the resource manager . Through the policies described in service level agreements the resource manager can accordingly make a new allocation of the resources in response to the shifting resource allocation requests from the service session managers . If resources are to be assigned to a new service session manager the service session manager establishes the direct communication with resources through the resource manager as was previously described.

The embodiment described in would operate in those instances in which a persistent store is desired. Such persistent store back up is not necessary for all applications however. Particularly for fast executing jobs it might be easier for the client application itself to monitor for workload execution results to be sent back to the client application . But in other instances the storage back up may be advantageous for example in very compute intensive applications where the result may be delayed. In those cases the client application may want to be able to provide its workload request and then forget about the process eschewing any real time monitoring of the process.

Thus as illustrated in the persistent store can be provided for periodic follow up by the client application . The reference numbers in this section use parenthetical references for the actions whereas the system elements are indicated using normal reference numbers. As shown in the client sends a workload input to the service session manager . The service session manager upon receiving the workload input initiates the workload input store process substantially at the same time as it is sending the input to the service instance manager not shown see within the service host . Now if the service host provides an output from its execution of the assigned task before the initiated input store process is completed then the service session manager can initiate an abort input store process . The reason for aborting the input store process is that there will be no need to continue to store the workload input as a back up if the workload output is already available.

As an option of the client application with the return of the output the service session manager substantially at the same time can initiate an output store . If the client application then acknowledges receipt of the output back to the service session manager service session manager initiates abort the output store because if the client application has received the output then there is no longer a need for a persistent storage of that output from a back up standpoint. Using this method it is possible to provide for back up or recoverability while because of the simultaneous passing on of the workloads the back up and recoverability are provided without sacrificing performance. Further efficiencies are gained by the ability to abort the persistent storage process if workloads and results are expeditiously provided and acknowledged by the network computing elements.

Using this persistent storage method for example if the service session manager fails before the input had been acknowledged the client would have to resend the work. But if the acknowledgment had been sent to the client the client would not have had to re send the workload. In that case had the service session manager failure happened or had another system failure have happened the service session manager would be able to recover the workload from the persistent store and continue working.

Referring now to illustrated in this figure is an architecture showing multiple client application instances multiple service session manager instances and multiple service instance manager instances operating in the grid enabled high performance distributed computing system . Generally speaking this architecture elaborates upon the general architectures previously described but further illustrates the parallelism that exists in the general context. This described architecture provides for isolation between different applications such that multiple applications will not share the same resources simultaneously in an unmanaged way between each other.

More specifically the architecture described here provides the capability for applications to be operating on multiple resources in a grid oriented architecture but also provides for the flexibility of dynamic allocation of the resources through the resource manager s lending borrowing process which is described above in this application and in the commonly owned patent application U.S. application Ser. No. 11 694 658 as previously cited and incorporated by reference herein in its entirety.

The present illustrates a case with two different client applications . Together there are four instances of those two client applications with two instances of Client Application A and two instances of Client Application B . Each instance of the applications represents a different user of the respective application such that the four instances in total represent two users of Client Application A and two users of Client Application B. These client applications interface with the networked computing system through service session manager . One service session manager is the service session manager for Application A SSM App A whereas another one is for Application B SSM App B .

The relationship of these grid network components is generally described above in of the present application and is further described below. As described these service session managers operate under the control of the session director . Also as described with respect to the service session manager may further interface with the persistence storage depending on whether the client application or other network components or elements request that a persistence storage process be implemented for them. Because there are two separate service session managers and two separate persistence stores this architecture provides for isolation between the data as between those network elements. Further described are the various service instances as implemented by the service instance managers . Shown in this figure is an isolated group of service instance network components or elements Res App A and another isolated group of service instance network components or elements Res App B for the Applications A and B respectively.

Through the resource manager and using the techniques for resource management described above if resources become available or are needed to be lent or borrowed between the various client applications under management of the service session managers the resource management techniques implemented by the resource manager can be implemented using the resource reassignment techniques previously described. As illustrated here however when such resource reallocations or initial allocations are not being made the resource manager remains in the background and the service session manager is able to communicate directly with the service instance managers see also . This architecture provides for isolation flexibility mobility and many other advantages as will be described below.

Through these isolation techniques the distributed computing system can provide a first group of network elements and resources for a first client or a first execution user and a second group of network elements and resources for a second client or a second execution user. The first group of network elements and resources are logically separated from each other although according to differing system design concerns it is not necessary that each and every network element and resource be logically separated. This logical separation relates to the workload processing network elements such as the service session managers service instance managers service instances and persistent store or logical divisions of these elements. The resource manager remains responsible for the overall resource assignments and is operable to communicate on an as needed basis with the above network elements and specifically at least with the service session managers .

By the implementation described above the isolation techniques provide for both logical isolation and security but also provide a flexible means of allocation of resources in a service oriented architecture context. Various means of isolation as enabled and implemented herein include providing the resources and network elements 1 for a virtual machine VM executing on the distributed computing system wherein the VM would only have access to those elements defined as being within the VM 2 for permissions based resource access by applications and or execution users to whom those resources and network elements have been assigned and 3 for a system clean up scrub and re initiation process whereby if a set of resources and or network elements are assigned to a new application or execution user those resources and or network elements are sufficiently cleaned such that sensitive data is deleted moved provided to the user and made unavailable for any new application or execution user that is given access to the same physical resources.

As mentioned the architecture described here provides a number of advantages over known prior art systems. Specifically included in these advantages are the advantages of application mobility and application isolation. It is further possible due to these advantages to implement an improved and transparent middleware upgrade process wherein the client applications and their workloads continue to operate without affect by the upgrade process.

Regarding the improved upgrade process the embodiments described herein provide an approach for upgrading the middleware for the network elements including when those network elements are currently executing handling applications operating on workloads within the distributed computing system . These advantages are enabled because of the mobility and flexibility disclosed herein.

Specifically to perform an upgrade in accordance with the present embodiments the new middleware software version can be installed in a new directory of an operating server for the middleware. The operating server can be hosting any of the network elements for the system so that the specific network element can receive its updated software this way. For example there may be middleware updates for the SSM SIM client API service API resource session director resource conductor persistent store or other network elements. Dependent on software design approaches these elements can be upgraded as a group or separately.

Assuming for purposes of discussion that the upgrades occur separately for each element then for the element being upgraded the new middleware can be installed and placed in its execution mode on the host server for that network element. As mentioned the various network elements can in some cases be located on the same host server and in some cases on different host servers. Due to the flexibility of the architecture and the portability of resources new workloads can either be transitioned to the upgraded middleware instances by operating those new workloads in parallel or by immediately switching over the execution of the workloads to the new middleware installation. In other words the flexibility and mobility of the embodiments described herein provide for the simultaneous installation and possibly running of most versions of the middleware within the same grid network . Essentially these parallel installations can operate as a separate instances on the network applications and in one implementation can run parallel different versions of the middleware and in another implementation can provide for a near instantaneous switch from one version of the middleware operating as one instance to another version of the middleware operating as another instance .

Still referring to further advantages provided with the described approach are set forth below. One advantage is the separate administrative control of the different applications operating on the distributed computing system . Because the different client applications use resources that are provided through different service session managers and managed by different service instance managers with persistent storage back up being provided through different persistent stores administrative control of the present application allows for the separate administration of the applications running on the system .

Another advantage that of security from providing data and execution isolation between different applications running on the system . The service session manager the persistence store and other network elements being used all can be logically separated through the separate administration thereof to provide isolation. Such isolation is important to ensure that different applications are not accessing the same network elements or the same resources simultaneously as such simultaneous accesses runs the risk of different users employing different applications that are accessing the same data and possibly receiving information to which they would not be entitled to because they are from a different organization or have a different security levels within the same organization.

As a further example of application and data isolation illustrated in as the Application A resource group are multiple service instance managers . Along with the service instances these can be configured as specific to an execution user within the system such that the users themselves have defined resources available to them that would not be assigned to other users. Specifically for this example the fact that a service instance is running as a specific execution user can provide the final security because execution user resources would be defined and delimited as belonging to the execution user alone. Although the resources may be virtualized the system middleware would be defined to clean up after itself when reassigning new physical resources to different virtual application execution tasks.

As another example the isolation and segmentation of the network elements according to their operating applications further provides for segmentation of configuration files for the separate applications. It further provides for different workloads scheduling policies and recovery strategies according to the different applications that are specific to the separate applications. The isolation between workloads can provide for the different administration of the workloads scheduling policies and recovery strategies between applications. Although the isolation described above provides for the above mentioned application specific workloads scheduling policies and recovering strategies at the same time the flexibility of the system allows for the lending borrowing and reclaiming of resources according to dynamic application capacity needs.

Further the segmentation of the client applications provides for configuration policies persistence policies and other types of policies depending on the specific needs of the various client applications. For example certain client applications may operate in a very short timeframe in which case persistence is not necessarily needed to store the temporary results in the work submitted whereas other applications may require a great deal of time to execute and may be advantageous for those applications to have a persistence storage such that the client application can assign the task and forget about it rather than checking up on it immediately after that. Maintaining security of the history of the application is another advantage of the service isolation provided in the disclosed embodiments. Thus the present embodiments provide for security of history files configuration files and other types of application data files.

Illustrated in is an architecture for providing service packages to the service instance managers when the service instance managers are called on to established an operational service instance in a new location in the network at the request of the service session manager . In this context such as for instance when the service session manager has moved a service instance from one service host to another or has requested a new service instance be established the responsible service instance manager will request from a repository service a service package for the installation of that new service. The repository service includes service packages that were established at one point in time by the administrator of the particular application.

Within the architecture shown in an administrator defines the service packages to contain for example a configuration file an application executable file and extraction instructions for opening up the service package and operating it or installing it on a host server as the service instance . The service package may of course include additional instructions or parameters or data such as it would be needed to correctly install the service instance on a server so this list is not meant to be exclusive. The service instance manager would receive service packages from the repository service which in turn would retrieve the service packages from a service package database .

Upon receipt of the service package the service instance manager would then provide the service package for installation upon the server . The service package will then upon extraction and installation operate as a service instance under the management of the service instance manager . It should also be noted that the service instance manager may already have a service package in its local memory or operating on a local server in which case the service instance manager would not need to request the service package from the repository service and would be able to just generate and install the service package on the server or provide an interface to an already operating service instance on the server .

Illustrated in is an exemplary software development kit approach in which the working client applications can be developed for operation on the grid enabled service oriented architecture middleware without the necessity of having the client applications under development actually operate on an operating computing grid. Thus as shown in the software development kit includes the core middleware which is provided with interfaces to the client applications on both the client application side and on the resource host side whereby the application can be developed to ensure operation on both sides of the middleware.

The software development kit described in addresses an important aspect of the middleware which is the resource manager s see interfacing with the grid itself. In the described development kit a simulated resource manager is provided to simulate the operation of the grid itself under conditions when the resource conductor which is a part of the actual core operating middleware software provided in this development kit seeks to communicate with the resource manager to determine things like available resources and the like. The client application under development can thereby be inserted and plugged into the middleware to determine whether it properly operates with the middleware software and similarly the service application under development can be inserted at to test its functionality.

With further reference to the exemplary software development kit of the software development kit provides APIs that are layered. Layered in this context refers to providing a core API along with various software language specific APIs layered on top of the core API such that the core API can remain the same and yet the software development kit is operable any one of the native languages normally used in the applications environment. This flexibility is provided at both of the applications side and the service instance side and this provides an ease and consistency of interface to the users.

In a specific embodiment when a client API has established a direct connection to the service session manager the service session manager can notify the client API if there are run time problems such as described above. This notification can be used to throttle incoming workloads from the client application whereby the system has the opportunity to manage recovery of the run time problem and avoid outright failure.

Embodiments of the middleware described herein can also determine if a script has stopped running or monitor for other runtime problems. As another example if resources trigger alerts or if various managers in the system otherwise detect resource operational issues or capacity issues in the system the middleware can throttle connections such as limiting the bandwidth for receiving incoming workloads or refusing to open the queue in the service session manager for additional workloads until such time as the resource operational issues or capacity issues can be worked out. For example if there is a memory capacity issue the memory can be flushed in an orderly fashion so the capacity issue can be worked out without having a system failure. As the problem clears the channels can be opened up for additional incoming workloads. As another possible capacity management approach as the service session manager is running out of capacity it can notify the session director which can start a new service session manager to handle the additional capacity need.

The ability to throttle incoming workloads combines powerfully with the persistent storage techniques described herein such that in addition to being able to work through capacity issues the persistent store can be used to reduce or eliminate the risk of loss of workload data or output data associated submitted workloads. The persistent store can also be used during the handling of capacity issues or other runtime issues as a temporary store to hold submitted workloads from the client applications and completed outputs submitted from the service instance managers which in turn would have received them from the service instances not shown see that they were managing.

Described herein are embodiments for monitoring within the service session manager or other middleware elements the usage of resources memory space script execution or other resource or run time issues. Further described are mechanisms for gracefully reconfiguring resources or middleware components such as by throttling inputs from clients applications while resources and middleware components can be brought back online. Another element for facilitating the graceful management of capacity is the swap space which can be used to store various application data packages or runtime environments until the resources or computing grid elements can be brought back online. The swap space can be used for such temporary or even redundant storage of operating data and environment to reinstitute operational elements in the distributed computing environment and thereby to avoid failure of processes operating on the network.

With further reference to and with reference also to the present embodiments provide for multiple layers of monitoring of run time execution processes and failover mechanisms. As described above the current embodiments provide for flexible and dynamic allocation and reallocation of resources along multiple layers. Using the mechanisms described should an executing service instance fail while executing on its service host that service instance can be readily switched under control of the service session manager to another service host . At one level the workload can be simply re queued in work queue for new assignment to the next available resource. In other words the workload can be placed in the work queue for dispatch to the next available service instance which might be made available because it had completed a previous workload dispatch or might be made available in accordance with an assignment of additional resources as described below.

At a higher level of recovery management additional resources can be made available to the service session manager when one of its assigned service instance managers going offline. With the loss of the assigned service instance manager the service session manager might determine that it has a shortcoming in resources available for execution of the client application and would accordingly communicate its need for additional resources to the resource conductor which is then operable to determine if resources exist that will fulfill the request for additional resources.

As was the case with the initial provisioning or allocation of resources the resource conductor will seek to make an allocation of resources in accordance with the system policies as set forth in one or more service level agreements. Accordingly the techniques described in the commonly owned patent application Method and system for utilizing a resource conductor to optimize resource management in a distributed computing environment U.S. application Ser. No. 11 694 658 may be used in this context as well as for the initial resource provisioning. If the resource conductor determines that additional resources are to be assigned according to the service level agreements the resource conductor will request those resources from the resource manager .

At a still higher level of recovery management because of the multiple layers of management that exist in the presently described architectures recovery management is available also for the instance that the host running the service session manager goes down. In that circumstance the session director would do substantially the same thing that the service session manager did in the context above when the service instance manager went down except at one level higher in the architecture. More specifically the session director would detect that service session manager is gone and the session director can then go to the resource manager to seek to have a new service session manager assigned. Meanwhile the client API which in many cases may have a direct generally continual connection to the service session manager can detect a failure of the service session manager without going back to the end user through the client application and get new URL of new service session manager that will be serving the application.

The persistent store can be used in conjunction with the above referenced failover techniques to further facilitate the new assignment or reassignment of run time modules e.g. and resources e.g. without needing to return to the client application for further instruction. Specifically if the execution of a workload has been lost the disclosed embodiments provide for accessing the submitted workload in the persistent store and resubmitting that workload such as by re queuing it in the workload queue of the service session manager or queuing the workload in a newly assigned service session manager in any case without returning to the client application to ask again for that workload.

Above a detailed description is provided for the failover mechanism for service instance managers specifically the failover mechanism is managed by the service session managers in conjunction with the resource conductors and the resource managers . At a higher level the above description further sets forth the failover mechanism for service session managers with this higher level mechanism being managed by the client APIs in conjunction with the resource conductors and the resource manager . Further described above is the use of the persistent store in enabling such failover management in an efficient and expedited manner.

In addition to the two higher levels of failover management described above the service instance managers can provide lower level failover management with respect to the service instances comprising the service applications and service APIs . And in addition to those levels of monitoring the resource manager is operable to monitor the operation of the various session directors to ensure their continued operation.

To summarize the levels of monitoring and failover in a tabular format from lowest level to highest level generally speaking the monitoring and failover responsibilities are as follows 

The monitoring is generally and specifically as described above. Monitoring techniques can be further described with respect to certain techniques described in commonly owned U.S. patent application Ser. No. 11 694 658 entitled System for generic service management in a distributed and dynamic resource environment providing constant service access to users which is incorporated herein by reference. This application describes techniques that can operate from a service controller to monitor for application failures but some of the monitoring techniques described therein could be employed advantageously by the disclosed network elements herein for monitoring the continued operation of the other network elements to which they are connected. Other useful monitoring techniques are described in commonly owned U.S. patent application Ser. No. 10 871 350 entitled Autonomic monitoring in a grid environment which is incorporated by reference herein.

With respect to the specific failover techniques employed those follow the techniques previously described above for assignment instantiation and provisioning of service components and resources. Not only do the described techniques provide for efficient and invisible to the user failure monitoring and failover but they also provide for isolation between resources and service components such that the failure of one resource or service component only affects the limited applications and service instances with which they are associated.

In addition to the monitoring and failover handling of the resources and service components described above the present system also provides particularly with the persistent store approaches the ability to recover from failure of the client applications . If a client application and or client host goes down depending on the options set for the client application the system is able to continue to process the workloads sent from the client and place the workload outputs in the persistent store until the client host client application comes back online. Specifically in certain embodiments the service session manager is able to detect that the client has gone offline and coordinate the storage of workload outputs in the persistent store and once the service session manager detects that the client application has come back online the service session manager is able to retrieve the application s workload outputs from the persistent store and queue them into the workload output queue of the service session manager for delivery back to the client application .

Using the above described techniques a grid enabled architecture can be implemented having no single point of failure in the system . There is monitoring available at every level and failovers are provided in an efficient and expeditious manner substantially without the fact of system resource or service component failures even being perceptible to system end users. The isolation of applications between resources and service components further protects these failures from the perception of end users because if the any single item in the system fails that item only affects the application workloads that are actually being executed on it and with the flexibly and efficient failovers particularly when coupled with the persistent store mechanism the fact of resource or service component failure can be substantially isolated and unknown to application end users.

In particular the level of independence that the various service components the session directors the service session managers the service instance managers the resource managers the resource conductors the persistent store all provide various levels of operational independence from each other. Thus the failure of any one of these service components only affects what the processes that the service component is handling. Further with the eminently flexibly failover for these service components the service execution of the system can be preserved almost no matter the failures that may occur in the system during operation. So called ripple effects failures which are often seen in prior art systems are avoided by the layered approach in which the workload management is decoupled from the resource management and in which the peer service components operate independently from each other and in many cases separate from the supervisory service components that originally provisioned them.

Within the previously described embodiments there are many different features that can be implemented. The service instance manager for example can be configured to take different actions according to conditions of the service instance operating on the service host depending upon policy desires of the administrator or the users of the specific applications. As an example should the service instance fail for whatever reason such as becoming hung up on the server failing to return a response to an inquiry from the service instance manager or otherwise becoming non responsive the service instance manager could either attempt to restart or reinitiate the service instance . Under certain defined instances for example the service instance manager could add the hung service instance to a blacklist such that the particular service instance not be used anymore.

Other possible policy implementations might include limits on how long a service instance could operate on the service host without being restarted such that after a certain period of time the service instance manager could be made to restart the service instance gracefully. Another possible limitation would be if the service process image size exceeds memory limits such as perhaps 1.5 GB. The above are examples of various types of parameters and policies that can be placed upon the operation of the service instance on a host and any number of different parameters could be defined and any number of different disposition instructions could be defined for the operation of the service instance depending on systems design needs.

While various embodiments in accordance with the principles disclosed herein have been described above it should be understood that they have been presented by way of example only and are not limiting. Thus the breadth and scope of the invention s should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the claims and their equivalents issuing from this disclosure. Furthermore the above advantages and features are provided in described embodiments but shall not limit the application of such issued claims to processes and structures accomplishing any or all of the above advantages.

For example in the instance that any elements of the systems described in the present application are located on common hosts then the communications between those elements could then be through internal host protocols. In other words in those instances the host would manage the various software modules operating thereon using normal work management protocols within the host. Connections between network elements and the operation of certain network elements can be implemented on and monitored by software daemons which are processes that are normally responsible in computer systems for handling things like responding to network requests or monitoring hardware activity. The labels applied to the various software processes are not dispositive however to whether the claims cover those process elements.

While specific local area network or wide area network communication protocols may be described herein the coverage of the present application and any patents issuing therefrom may extend to other networks operating using other communications protocols. Systems using protocols presently known in the industry and later developed protocols may be covered according to the claims at the end of this document and equivalent elements to the elements claimed.

In the context of this application resources may encompass any types of resources that are necessary for running such applications including hardware such as servers clients mainframe computers networks network storage databases memory central processing unit CPU time scientific instruments and other computing devices as well as software software licenses available network services and other non hardware resources. There are also many different phrases that refer generally to computing grid systems and those alternative phrases are also envisioned as being encompassed within the scope of the claims. Such alternative phrases include distributed computing environments or distributed computing networks or systems. Generally speaking such distributed computing environments include hardware and software infrastructure configured to form a virtual organization comprised of multiple resources which resources may be in geographically disperse locations.

Policies and service level agreements described herein can apply to different consuming entities including different users user groups organizations departments business units queues projects and applications. Services and applications are described in this application using those alternative terms. A service application is a program that traditionally has run on a single host in isolation. Examples of such a service may CAD CAM services financial analysis services and database analysis services. By use of the system disclosed herein such individual services may be run effectively and efficiently on multiple hosts in a distributed computing environment. In the context of a distributed computing environment such services are considered to be transportable in that they may be run on multiple hosts and or migrated from one host to another.

Various terms used herein have special meanings within the present technical field. Whether a particular term should be construed as such a term of art depends on the context in which that term is used. Know or known refers to a computer state of a certain fact or condition being stored in the referenced process whether that fact or condition was received from another process separate from the referenced process or determined by computations within the referenced process. Offline and online refer to a referenced process being active or not active in communications to other processes. This state of activity or inactivity can be by system or process failure or by system or process intention whereby the referenced process was inactivated as a part of the process Connected to in communication with or other similar terms should generally be construed broadly to include situations both where such connections or communications are direct between two referenced elements or through one or more intermediaries between the referenced elements. Network system and architecture within the present application generally refer to distributed computing systems that embody one or more inventive aspects of the present disclosure. These and other terms are to be construed in light of the context in which they are used in the present disclosure and as those terms would be understood by one of ordinary skill in the art would understand those terms used in the disclosed contexts. In the context of the present disclosure the terms sending transmitting interfacing and communicating can all comprise similar or different types of electronic communication including optical communication depending on the context in which those terms are used. The above definitions are not meant to be exclusive of other meanings that might be imparted to those terms based on the contexts herein.

Words of comparison measurement and timing should be understood to not be absolute but to be of the appropriate relative dimension measurement and timing to accomplish the implicitly or expressly stated desired result. Thus these words of comparison measurement and timing such as equal to less than during and the like should be understood to mean substantially equal to substantially less than and substantially during where substantially meaning such comparisons measurements and timings as are practicable to accomplish the implicitly or expressly stated desired results.

The software applications described in the present embodiments may be provided in through computer based electronic transmissions or on CDs DVDs or other physical media. The run time modules and specific operating instances in particular may be transmitted through electronic communications and stored for execution on various client service and middleware host servers in the systems described. Service instance can communicate through the described network based communications protocols or through shared memory space such as the case when software modules are linked together and compiled to form commonly operating software modules. Described software development kits can also be provided as operating code modules and or source or object code on physical media or transmitted through electronic means.

Additionally the section headings herein are provided for consistency with the suggestions under 37 CFR 1.77 or otherwise to provide organizational cues. These headings shall not limit or characterize the invention s set out in any claims that may issue from this disclosure. Specifically and by way of example although the headings refer to a Technical Field such claims should not be limited by the language chosen under this heading to describe the so called technical field. Further a description of a technology in the Background is not to be construed as an admission that technology is prior art to any invention s in this disclosure. Neither is the Brief Summary to be considered as a characterization of the invention s set forth in issued claims. Furthermore any reference in this disclosure to invention in the singular should not be used to argue that there is only a single point of novelty in this disclosure. Multiple inventions may be set forth according to the limitations of the multiple claims issuing from this disclosure and such claims accordingly define the invention s and their equivalents that are protected thereby. In all instances the scope of such claims shall be considered on their own merits in light of this disclosure but should not be constrained by the headings set forth herein.

