---

title: Simulation using resource models
abstract: Operational resource modeling is usable to analyze application and computer system performance over a wide range of hypothetical scenarios. Operational resource modeling involves creating and training one or more resource models, and/or simulating hypothetical scenarios using resource models.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07996204&OS=07996204&RS=07996204
owner: Microsoft Corporation
number: 07996204
owner_city: Redmond
owner_country: US
publication_date: 20070423
---
This application is related to concurrently filed U.S. patent application Ser. No. 11 739 061 and U.S. patent application Ser. No. 11 739 063 both of which are incorporated herein by reference.

Capacity planning involves determining what hardware and software configurations are best suited to meet the needs of an application in a computing system in various hypothetical scenarios. A transaction modeling technique can be used to model performance of the application and the computing system which enables development of capacity planning solutions for arbitrarily complex software applications. These solutions facilitate the analysis of application and computer system performance over a wide range of hypothetical scenarios. However development of a customized performance model for an application requires significant investments in terms of skilled labor and capital expenditure. Furthermore the customized performance model for the application cannot be easily adapted to provide a model of a different application.

Alternatively trending of historical data and forecasting techniques can provide a capacity planning solution for arbitrary applications without requiring performance model customization. However this technique severely limits the range of hypothetical scenarios supported and thereby limits applicability of the solution.

Moreover currently available methods for developing performance models generally require a user to manually configure relationships between components of the system in order to take into account the effect of the various dependencies in the system.

This summary is provided to introduce simplified concepts of operational resource modeling which is further described below in the Detailed Description. This summary is not intended to identify essential features of the claimed subject matter nor is it intended for use in determining the scope of the claimed subject matter. Generally operational resource modeling involves creating and training resource models for components in a system and using the resource models to simulate hypothetical scenarios.

Resource models may be created by selecting a resource model template that matches a component of the system for which the resource model is being created. The selected template can be then customized to fit an instance of a specific application on the system. The resource model template may in turn be created by generating a template manifest and modeling logic. In that case the template manifest declares performance metrics usable to parameterize utilization of resources in the system while the modeling logic includes rules that specify resource utilizations for each component of the system in response to a given operation.

Once created resource models may be trained in for example an operational scenario such as a production environment. Training resource models may include deriving resource costs incurred by components of the system in response to a given operation and correlating the resource costs to the given operation to generate a resource map. The resource costs may be derived from historical data obtained by interrogating an existing deployment of the system. The training may also include determining workload transformations between components of the system and generating a workload map based on the determined workload transformations.

Resource models may be used to simulate hypothetical scenarios for a computing system. Simulating hypothetical scenarios may include generating a workload of operations for a component of the system and outputting resource utilization and or operation latencies for the component. The workload of operations may be generated based on either a usage profile assigned to the component a workload map or both.

This disclosure relates to operational resource modeling usable by for example information technology IT personnel to analyze application and computer system performance over a wide range of hypothetical scenarios. Historically computer system performance analysis has been based on performance models of applications executing on a computer system. The performance models were usually generated using either transaction modeling techniques which involve large development costs or trending techniques which are applicable in a limited range of operational scenarios.

The described operational resource modeling techniques bridge the gap between transaction modeling and trending techniques by providing resource model templates for creating resource models to analyze hardware and or software deployments in a system. The resource models can be trained using historical data and can then be used for simulation of various hypothetical scenarios such as hardware scaling and or workload scaling for effective capacity planning. Thus a wider range of hypothetical scenarios can be covered as compared to trending techniques and at the same time the development expense associated with transaction models can be mitigated. This type of historical data is often already gathered in many organizations for trending or other purposes.

Moreover the resource model templates can be customized to model hardware and software components of a system by automatically interrogating an existing deployment in the system. The relationships between the various components can be automatically configured in the resource model based on correlations determined from the interrogation. In addition the deployment can be monitored in real time to generate alerts whenever changes in the deployment render the resource model potentially invalid. Additionally or alternatively discrepancies between model predictions and performance measurements may indicate misconfiguration of the deployment and or failure of a component e.g. a disk drive that is retrying on a failing sector and an alert can be generated in response.

An operations manager monitors deployment of the managed hardware and software entities and maintains a database of information such as hardware and software configurations in the deployment events or alerts generated by applications executing in the deployment etc. The operations manager may be implemented as software hardware e.g. a computer system or a combination of software and hardware. In an exemplary implementation the operations manager may comprise Microsoft System Center Operations Manager SCOM available from Microsoft Corporation headquartered in Redmond Wash.

A capacity manager is communicatively coupled to the operations manager and can interrogate the operations manager to obtain all or part of the information regarding the hardware and software configurations workload data correlations between hardware and software resources in the deployment etc. The capacity manager uses the obtained information to create and train resource models using resource model templates. The resource models can be used to simulate and analyze performance of hardware and or software applications in the deployment in various hypothetical scenarios. The capacity manager may be implemented as software hardware e.g. a computer system or a combination of software and hardware. In an exemplary implementation the capacity manager may comprise System Center Capacity Planner SCCP also available from Microsoft Corporation headquartered in Redmond Wash.

While resource modeling is described in the context of analyzing existing hardware and software deployments the concepts described herein are broadly applicable to modeling past present and or planned hardware and or software deployments. Also resource modeling may be used to analyze a whole deployment or individual parts thereof.

In particular this disclosure describes exemplary techniques for creating and training resource models and for simulating hypothetical scenarios using resource models. is a flowchart illustrating the overall resource modeling process including creation and training of a resource model and simulation of a hypothetical scenario. Details of each of these processes are described in detail below.

The managed entities include one or more devices such as device that can be monitored by the operations manager either using an agent or in an agent less manner. The operations manager uses a data collector to collect deployment and execution data from the managed entities . The collected data is stored in a database and includes performance and configuration data . In some implementations database may include multiple data collection and or archiving components. In one implementation the database includes an operations database and a data warehouse. The operations database contains detailed short term data that may be post processed and forwarded to the data warehouse for archival.

The capacity manager interrogates the operations manager for performance and configuration information and uses this information to create resource model instances and configure device models . For this the capacity manager uses resource model templates which provide a framework that is readily usable to create customized resource models for applications in the system. The resource model templates include operating system templates and platform templates built upon the operating system templates.

The resource model templates identify operations that can be performed by an application executing on a particular platform or operating system and characteristics that can be modeled in such a system. By way of example a few possible characteristics that may be modeled include hardware configurations resources utilized by the application and functional dependencies between various components in the system to name just a few.

For example consider a system in which an e commerce application is deployed on an IIS web server platform operating in conjunction with a SQL database server. For such a system the capacity manager determines from an interrogation of the deployment that a combination of resource model templates for e commerce applications IIS web servers and SQL database servers can be used to model execution of the e commerce application on the system.

The resource model template for e commerce applications declares application layer operations such as authenticate browse and purchase and functional relationships with the IIS web server template. The resource model template for IIS web servers declares platform level operations such as logon get and post and functional relationships with e commerce application template and the SQL database server template. Similarly the resource model template for SQL database servers declares platform later operations such as transactions and batches and functional relationships with the IIS web server template.

The capacity manager then uses a combination of the three templates based on the interrogation of the system deployment to generate customized resource model instances for the system. The capacity manager also configures device models based on the interrogation of the system deployment and the resource model templates . The device models include models for determining the degree of utilization of hardware resources such as a processor a disk and a network interface card NIC by the components in the system.

The capacity manager may additionally or alternatively receive manual inputs from a model user and or model author through the user interface to customize the resource model instances and configure the device models for the system deployment. The model user may use a model editor to manipulate construction of the hypothetical scenario. The model author may use a model designer to add remove or modify one or more operations or functional dependencies to customize the resource model instances . After customization the resource model instances are instantiated in the hypothetical scenario and are ready for model training.

Once the resource model instances are created and instantiated a training engine in the capacity manager trains the resource model instances using historical performance data from the database in the operations manager . For this the resource model instances identify data required for training that is to be obtained from the operations manager . The training engine obtains the identified data and constructs resource and workload maps that represent resource costs and workload transformations for a given operation respectively. The training engine trains the resource model instances based on the resource and workload maps . While the resource maps are shown as a separate block in in various implementations the resource maps may be contained within one or more other components of the capacity manager . In one implementation the resource model templates may contain parameterizations of the resource maps representing resource costs for a given operation and the resource model instances may contain the parameter values of the resource maps after model training is completed.

The capacity manager then uses a simulation engine to simulate operations on the resource model instances in one or more hypothetical scenarios. Simulation results are presented to a user in the form of model predictions through the user interface . The user can choose to further modify the resource model instances based on the model predictions using the model editor and can perform additional simulations to obtain an effective capacity planning solution.

A resource model template also referred to as model template is an independent unit that provides a framework to create a resource model for an application executing in a system. A variety of different template types are possible. For example the mode template can be a template representing an operating system such as Windows DOS UNIX etc. and or a template representing a platform technology such as ASP.NET IIS SQL server etc. built on one or more of the operating system templates.

In one implementation the model template includes various parameters and rules that can be used to create a resource model a component template structure and enumeration of elements of a trained model such as resource costs workload scalars etc. In addition the model template can also include a description of qualitative dependencies between input and output parameters and a description of a method for training the model template .

The various parameters in the model template include input parameters workload characterization parameters configuration parameters output parameters and resource consumption parameters and the like. The resource consumption parameters specify types of resources such as processor storage network etc. and types of loads such as transacted aggregate or background etc.

The rules in the model template include template instance discovery rules to determine a presence of an application component that can be modeled with the model template discovery rules for automatic discovery of the input parameters from the system rules for training of all trained elements and rules for reducing the scope of modeling as a reaction to insufficient or incorrect input data.

The component template structure of the model template includes multiple components such as template manifests modeling logic and customization fields which are used by the capacity manager to simulate applications built upon various platforms. The component template structure of the model template can be stored in the form of a data structure in the capacity manager .

In this implementation the manifests stored in the manifest field include an operating system template manifest also referred to as OS template manifest a first template manifest and a second template manifest . The OS template manifest corresponds to an operating system template. In this case the first template manifest and the second template manifest together correspond to a platform template. While the manifest field in this implementation includes platform and operating system manifests in other implementations any number of one or more manifests may be included. The template manifests may be generated manually by an author of the template or automatically. In one example the creation of a template manifest is partially automated by interrogating device models to obtain the required parameterization of the resource map. Further the agents which monitor platforms and or applications are interrogated to parameterize their workloads. This may involve identification of instrumentation e.g. perfmon counters which characterize the rates of operations processed by the platforms or applications. This process may be automated by for example the developer of the platform or application marking the instrumentation in the platform or application itself or in the agents monitoring the platform or application in a manner which can be recognized by the template during model creation. For example when the application is deployed a model compatibility file may be created which enumerates the workload rate perfmon counters. Then when the template is instantiated this model compatibility file may be read by the capacity manager and used to populate the manifest with these counters.

Each template manifest stored in the manifest field declares performance metrics required to parameterize either utilization of resources or workloads between components of the system. For example the OS template manifest declares performance metrics that are required to parameterize utilization of hardware resources. Such performance metrics include without limitation performance counters sampling requirements etc.

The performance counters measure hardware related parameters such as processor utilization disk IO and network IO. Processor utilization can be measured as a percentage of processor or CPU utilization. Disk IO can be measured as the number of disk read write bytes processed per second as the number of disk reads write operations processed per second and or as the sequentiality of IO for a particular storage device. Network utilization can be measured as number of bytes sent or received on a particular network interface card NIC per second. Performance counters also measure workload related parameters such as the rate of operations size of operations cache hit ratio etc. The sampling requirements include specifications such as required sample size duration value range and the like.

One illustrative platform template includes the first template manifest and the second template manifest and can correspond to a specific platform such as a SQL Server. The first template manifest declares instrumented performance metrics which are required to parameterize resource utilization and a workload between components of the system and is built on the second template manifest . The second template manifest may be similar to an OS template manifest and may declare performance metrics usable to parameterize resource utilization and a workload between components of the system. The template data structure corresponds to one illustrative template in which one template manifest is built on another template manifest. However templates may be entirely composable. In other examples templates may have one or more dependencies on parameters of modeling logic of the same or different templates or may have no dependencies on other templates at all.

The performance metrics declared in the first template manifest and the second template manifest include performance counters which measure throughput of platform layer operations such as SQL Server transactions per second and SQL Server batch requests per second for example. Also performance counters of processes which measure resource utilization due to processing of application and platform workload can be identified in the manifest .

The modeling logic field in the template data structure includes modeling logic that uses one or more rules for applying heuristics to i transform workload parameterized by the manifest prior to scheduling workload in the device models ii account for utilization overheads iii modify operation latencies or iv otherwise affect performance metrics related to the outcome of simulating resource utilization. For example the modeling logic in a SQL Server template can define a workload heuristic that transforms logical disk IO into physical disk TO using knowledge of the disk caching behavior of the SQL Server.

The customization field can be used to declare methods for customization of the model template for use with a specific application executing on a particular platform. For example the model template can be customized by specifying how application level commands are transformed into platform level commands that are compatible with the model template . In another scenario the model template can be customized by specifying component workflow dependencies between two or more components of the system. In yet another scenario the model template can be customized by specifying scaling limitations of the specific application. As another example the model template can be customized by specifying a correlation usable to map a measure of a number of client entities to an instrument load modifier declared in a manifest of the model template .

In operation as explained above the capacity manager can employ the model template to create a resource model such as a performance or capacity model of a server process or a group of processes deployed in a system. For example an Internet Information Server IIS template can be used to create resource models of particular Web servers built on IIS by programmatically identifying the w3wp processes associated with the web site such as for example msdn.microsoft.com.

A model template can be created at block by generating one or more template manifests corresponding to an operating system or a platform built on an operating system or both and generating modeling logic that uses rules to specify resource utilizations for one or more components of a system.

At block the model template can be customized for an application executing in the system by specifying various parameters that can affect the performance of the application in the system such as workflow dependencies between the various components in the system scaling limitations resource and workload correlations etc.

The customization of the model template enables the model template to better fit an instance of the application in a hypothetical scenario such as a deployment of the application in a lab or production environment. It will be understood that it is possible to create a functional model of the application from platform templates without customization although modeling accuracy and functionality may be reduced.

In one implementation when computers in a hypothetical scenario are modeled as a connected system component dependencies can be specified between the connected components to customize a model template for the system. For example when a hypothetical scenario includes a single web server and a single database server the web server can depend on the database server but the converse may not be true. The web server can be modeled using a model template for IIS and the database server can be modeled using a template for SQL Server. To model this hypothetical scenario as a connected system the IIS model template can specify a workflow dependency on the SQL template. Additionally or alternatively the workflow dependency may be specified in an application level template built on top of the platform template s . Specification of work flow dependencies may be performed manually by a user entering the dependency and or automatically by interrogation of an existing deployment. This interrogation can include but is not limited to correlation of tokens injected into the signatures of operations. Alternatively the capacity manager may suggest a workflow dependency and the user may be given the option to accept the suggested dependency or to reject the dependency.

In another implementation custom performance counters which measure workload generated by application level operations can be specified for an application executing on a platform. For example in the case of an IIS template customized for a web service in an e commerce application the custom performance counters may measure a workload generated by browsing a product catalog and a workload generated by purchasing a product from the catalog for example.

In cases where an application is implemented directly on top of an operating system the operating system template itself may be customized by specifying custom performance counters which quantify workload. For example a model of a Microsoft Operations Manager MOM Management Server can customize the operating system template by specifying counters which measure rates of alerts events and performance data processed by the Management Server.

In another scenario known scaling limitations of an application which can be difficult to determine automatically from the application configuration or by inspection of historical data can be specified by a model author to customize a model template for the application. For example if a SQL Server application is known to scale poorly beyond a determined number of processors e.g. eight processors then the SQL Server template may limit scheduling of SQL Server compute actions to eight processors during simulation in cases where more than eight processors are available.

In yet another scenario a heuristic which maps a measure of the number of client entities to an instrumented load modifier may be specified. In such cases the instrumented load modifier can be declared in the template manifest. For example a client entity may be a user of an e commerce application and an instrumented load modifier may be a number of concurrent connections established with the web service. A counter corresponding to the number of concurrent connections can be declared in the manifest of the IIS template. The heuristic may specify that the number of concurrent users using the e commerce application is a scalar multiple of the number of concurrent connections. The advantage of this heuristic is to provide the model user with a more familiar metric i.e. the number of client entities in order to adjust the workload in hypothetical scenarios.

In an exemplary implementation a model template is customized at block using one or more of the above mentioned customization techniques.

At block an instance of the customized resource model template is bound to an operational scenario. The model instance generally comprises a copy of the model template which is adapted through model training in the operational scenario to become a functioning model of the application in hypothetical scenarios. When instantiated the model template is ready for training as a standalone model or a component model in combination with other resource models as a part of a larger model of a distributed application.

While the creation of a resource model is described as including creation of a resource model template it will be understood that the creation of a resource model does not necessarily include the creation of a resource model template. The resource model can also be created using a resource model template selected from a list of already available resource model templates that matches one or more components of a system for which the resource model is created. In that case a capacity planning program such as the Capacity Manager component in Microsoft System Center Operations Manager SCOM may include one or more resource model templates corresponding to one or more operating systems platforms and or software programs.

The historical data may contain defects that can be removed by conditioning the historical data through combination of filtering workload injection or a combination of the two. Possible defects include data collected at periods when one or more system components were unavailable when system exceptions occurred such as computer rebooting network blackout or database restoring or the like. The training engine can filter the historical data to exclude data collected during such periods as indicated by alerts events or performance counters in the operations manager .

Other possible defects include cases where a required range and variation of model parameter values is not observed in the historical data since in such cases the applicability of the trained resource model for capacity planning in hypothetical scenarios might be limited. In such cases synthetic workload data may be injected along with the historical data to achieve a desired range of operating conditions. In that case the synthetic workload may be injected by for example a workload injector or other module of the operations manager .

At block the resource costs and workload transformations determined from the historical data are represented as a resource map and a workload map respectively. The resource map specifies resource costs for each type of operation while the workload map specifies workload transformations describing workflow dependencies between components in the hypothetical scenario.

In one implementation the training engine in the capacity manager determines correlations from the historical data to generate the resource and workload maps . It is to be noted that the training engine can obtain these maps under composite workload conditions in a production environment and is not restricted to obtaining information under monolithic workload conditions from a controlled lab environment in serial fashion. The training engine uses the resource and workload maps to train the resource model instances .

At block the resource model instances under training are audited to maintain overall model quality. The overall model quality depends on accuracy of the resource and workload maps used by the training engine . The accuracy of the resource map is measured by the error between utilizations predicted using the resource map and actual utilizations observed in the operational scenario for the same workload. The accuracy of the workload map is measured by the error between operations rates predicted using the workload map and actual operations rates observed in the operational scenario.

If the error is determined to be within a predetermined tolerance limit the training engine terminates the training. Else the training engine either collects additional historical data from the operations manager to increase the sample size or injects synthetic workload data into the hypothetical scenario for further training.

Further the training engine may also audit the resource and workload maps during their use by the capacity manager based on a trigger. The trigger can initiate the training if more than a pre defined time elapses since the last model training or if a configuration change occurs in the operational scenario.

At block the training engine can associate constraints with the resource models undergoing training to constrain model inputs accepted during usage of the resource model for capacity planning by the capacity manager . The constraints limit the model inputs to within a range of performance and configuration data obtained during the training period. Predictions can then be provided with confidence metrics which decrease as the distance increases between the model inputs and the data obtained during the training period.

Moreover in some implementations to ensure validity of a trained resource model the training engine may monitor the operational scenario in real time and provides alerts if configuration changes that would render the trained resource model invalid are detected. In such cases model training can be manually re initiated or the capacity manager can automatically re initiate training of the resource model. If model re training fails or predictions significantly deviate from measured observations then the training engine can generate another alert to indicate that an abnormality in the operational scenario may be present e.g. failure of a system component .

The model training engine retrieves the historical data from a database that stores configuration performance and or workload data from managed entities such as a managed entity . The data can additionally or alternatively be retrieved from the actual system in real time.

The model training engine uses the retrieved data for determining correlations to create resource and workload maps using vectors of sample data. The vectors of sample data are generated by combining utilization data with workload data to describe the system during the same period of time. The resource and workload maps are used to train the model template instance .

The model template execution module also performs various computations to compute training elements and to determine a status of training completion. The computations include instructing the training engine to perform computationally intensive common operations such as data aggregation matrix inversion etc. The training engine provides these services to the model template execution module so that the different model templates do not need to re implement the same procedures.

The training engine determines a state of the model training by analyzing state properties of the model template instance that are recomputed according to modeling logic declared in the model template instance . The training state can be computed for all trained elements individually and can indicate problems in the historical data used for training. In case a problem is detected the training engine can request more historical data from the database or can terminate the training.

Once the training is complete for all elements in the model template instance the model template instance is instantiated in the form of a component model containing fully parameterized resource and workload maps and can be used for performance analysis and capacity planning by an IT administrator or other automated tools. In addition the training engine monitors the data in the database in real time by periodically submitting the vectors of sample data to the model template instance to make the model template instance adjust to changing operation conditions.

In one implementation the IIS template for the web server declares logon get and post operations while the SQL template for the database server declares transaction and batch request operations. The training engine derives data corresponding to workload and resources for the web server and data corresponding to workload and resources for the database server from the database .

The training engine then determines correlations C between workload and resources to create a resource map for the IIS template of the web server . Similarly the training engine also determines correlations C between workload and resources to create a resource map for the SQL template of the database server . Further the training engine also determines correlations C between workload and workload to create a workload map for the system.

Resource maps specify unit resource costs of each operation type declared in a template. The resource maps are generated or modeled based on correlations determined by the training engine from historical data. Modeling of resource maps is based on an assumption regarding the manner in which resource utilizations respond to operation rates and may include an effect of background utilization of resources being mapped. The resource utilizations in this example include processor busy time disk IO and network IO while the corresponding resources include a processor a disk subsystem and a network.

For example if resource utilization is assumed to respond linearly to operation rates then given n operation types declared in a model template the total utilization u t of resource k at time tcan be determined using equation 1 

The correlations can be determined by approximating unit resource costs cand background utilization bfor the historical data. The historical data includes m data samples of operation rates and resource utilizations from an operational scenario. The correlations can therefore be computed using equation 2 

Though the correlations have been determined assuming a linear relationship between resource utilization and operation rates in the example above it will be understood that in different scenarios the relationship can be expressed nonlinearly such as a polynomial function or many other nonlinear relationships. Moreover the nonlinear relationship may be pre defined in the model templates or may be specified by a model author or may be determined by the training engine as the function that best fits the historical data. Once the manner in which resource utilizations respond to operation rates is determined equation 2 can be modified accordingly to compute the correlations.

Further to audit a resource map the training engine determines whether the error between utilizations predicted using the resource map and actual utilizations observed in the operational scenario for the same workload is less than a maximum tolerated error i.e. whether equation 3 is satisfied 

Workload maps specify workload transformations useful to understand demands on components of the hypothetical scenario due to workflow dependencies between components in the hypothetical scenario. The workload maps are generated or modeled based on correlations determined by the training engine from historical data. The workload transformations are applied to operations between parent and child nodes in a workflow dependency chain.

Modeling of workload maps may be based on an assumption regarding the manner in which rates of operations in a child node respond to a rate of operations in the parent node. For example if the rates of operations in the child node are assumed to respond linearly to a rate of operations in the parent node then given n operation types declared in a model template of the parent node the rate tilde over r t of operation type k at time tcan be determined using equation 4 

The correlations can be determined by approximating weights wfor the historical data. The historical data includes m data samples of operation rates from an operational scenario. The correlations can therefore be computed using equation 5 

Further to audit a workload map the training engine determines whether the error between operations rates predicted using the workload map and actual operations rates observed in the operational scenario is less than a maximum tolerated error s i.e. whether equation 6 is satisfied 

For example in a system modeled by explained above the IIS template modeling the web server uses the SQL template modeling the database server to fulfill its requests. Therefore the IIS template modeling the web server can be a parent node in a workflow dependency chain and the SQL template modeling the database server can be a child node. Hence a workload map for the system expresses the rate of SQL operations generated as a function of the rate of IIS operations for each operation declared in the SQL template. The function may be linear or a more complex function depending on the relationships between components. If the function is linear then the SQL workload map may be expressed as a linear combination of the IIS operation rates.

Workload maps can also be generalized to express a composition of resource models as workflow dependency chains of arbitrary depth. For example in a system in which an e commerce application is executed over a web server platform such as an IIS server a workflow dependency exists between application layer operations in a resource model of the e commerce application and IIS operations in a resource model of the IIS server.

Further in a system that includes a SQL database server along with an IIS web server and an e commerce application a workflow dependency chain can be expressed as follows 

The model user can use the simulation process to analyze the impact of changes such as hardware scaling and workload scaling on the performance of an application in hypothetical scenarios. Hardware scaling refers to adding removing and or modifying simulated hardware components of the system while application configuration and workload conditions remain fixed to simulate how changes in hardware components affect utilization of hardware components and latencies of operations. Workload scaling refers to increasing or decreasing the rate or size or other parameters of simulated operations while the system configuration remains fixed to simulate how workload changes in the system affect hardware utilization and latencies of operations.

Workload scaling can be performed in a model of an autonomous system i.e. a system in which components are not connected by independently changing rates of individual operations associated with each resource model. For example the rates of IIS operations may be changed independently of the rates of SQL operations if IIS and SQL are deployed on unconnected computing devices.

However in a model of a connected system workflow dependencies exist between components in the system which establish the components as parent and child nodes in a workflow dependency chain. In such a case rates of individual operations associated with the resource model of the parent node can be changed independently. Any changes to the workload in the parent node will affect the workload and performance of the component containing the child node.

The simulation process begins at block with generation of workloads. The simulation engine generates workloads for each component of the system in accordance with a usage profile input by the model user for each resource model instance and or the workload maps generated during training.

In a connected system which includes parent and child nodes the workload map expresses the rate of operations generated by the resource model in the child node as a function of the rate of operations generated by the resource model in the parent node. The simulation engine may generate a workload in the parent node in accordance with the usage profile input by the model user for that resource model instance for example. Further the simulation engine may generate a workload in the child node by applying the workload map to operation rates in the parent node.

Workload generated by creates actions at block to be executed by the simulated component in the system. These actions are scheduled onto device models . The actions may include execution of computations on a processor read write actions on a disk communication via a NIC or the like. Moreover in this implementation the communications operations in the simulation can be parameterized by both send and receive costs for the same component. For example when data is communicated over a network interface the data sent and received by the communication action can be scheduled onto the send and receive channels of the NIC device model of the same component. This facilitates simulation of an individual component of the system without knowledge of other components of the system.

At block device models evaluate the workload scheduled in to determine resource utilizations such as for example percentage CPU utilization disk IO and network IO in the simulated hypothetical scenario.

At block the simulation engine computes latencies of operations. A latency of an operation can be estimated by summing latency contributions from each component in the system involved in processing actions of the operation. The latency thus computed generally represents an upper bound of latencies encountered during execution of the operations since it ignores the possibility of parallel execution of some or all of the operations.

In a model of a connected system the latency of an operation in a child node may be calculated independently of the latency of an operation in a parent node. However the latency of operations in the parent node may depend on the latencies of dependent operations in the child node. Therefore an upper bound for the latency of an operation in the parent node may be estimated using various different methods.

In one method exclusive latency of an operation may be estimated independently of the latencies of operations of the child node. In another method inclusive latency may be estimated as a sum of the exclusive latency and the latency contributions from the dependent child operations. In such a case the latency contributions from the dependent child operations may be weighted according to the workload map. The weights may be used to represent the number of times such latency contributions from the dependent child operations can occur for every parent operation.

In the illustrated implementation the logon operation causes a computation latency of 1 ms and a communication latency of 4 ms on the parent node . Thus an exclusive latency of the logon operation is 1 ms 4 ms 5 ms. Further the logon operation results in SQL transactions with the child node . The SQL transactions cause a computation latency of 2 ms a disk latency of 10 ms and a communication latency of 3 ms. Thus an inclusive latency of the logon operation is 5 ms weight 2 ms 10 ms 3 ms . The weight can be determined from a workload map of the system as described earlier. For example if the workload map specifies a value of 2 indicating that the child node operations will be performed twice for every parent node operation for the weight the inclusive latency can be calculated to be equal to 35 ms.

Exemplary methods of creating and training resource models and simulating hypothetical scenarios using resource models are described above. Some or all of these methods may but need not be implemented at least partially by an architecture such as that shown in . Also it should be understood that certain acts in the methods need not be performed in the order described may be rearranged modified and or may be omitted entirely depending on the circumstances.

Any of the acts described above with respect to any method may be implemented by a processor or other computing device based on instructions stored on one or more computer readable media. Computer readable media can be any available media that can be accessed locally or remotely by the resource modeling application. By way of example and not limitation computer readable media may comprise volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer readable media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the resource modeling application. Combinations of the any of the above should also be included within the scope of computer readable media.

Although the invention has been described in language specific to structural features and or methodological acts it is to be understood that the invention is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the invention.

