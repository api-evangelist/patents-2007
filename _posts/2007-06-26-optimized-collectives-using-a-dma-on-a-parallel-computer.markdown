---

title: Optimized collectives using a DMA on a parallel computer
abstract: Optimizing collective operations using direct memory access controller on a parallel computer, in one aspect, may comprise establishing a byte counter associated with a direct memory access controller for each submessage in a message. The byte counter includes at least a base address of memory and a byte count associated with a submessage. A byte counter associated with a submessage is monitored to determine whether at least a block of data of the submessage has been received. The block of data has a predetermined size, for example, a number of bytes. The block is processed when the block has been fully received, for example, when the byte count indicates all bytes of the block have been received. The monitoring and processing may continue for all blocks in all submessages in the message.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07886084&OS=07886084&RS=07886084
owner: International Business Machines Corporation
number: 07886084
owner_city: Armonk
owner_country: US
publication_date: 20070626
---
This invention was made with Government support under Contract. No. B554331 awarded by Department of Energy. The Government has certain rights in this invention.

The present invention is related to the following commonly owned co pending United States Patent Applications filed on even date herewith the entire contents and disclosure of each of which is expressly incorporated by reference herein as if fully set forth herein. U.S. patent application Ser. No. 11 768 777 for A SHARED PERFORMANCE MONITOR IN A MULTIPROCESSOR SYSTEM U.S. patent application Ser. No. 11 768 781 for DMA SHARED BYTE COUNTERS IN A PARALLEL COMPUTER U.S. patent application Ser. No. 11 768 784 for MULTIPLE NODE REMOTE MESSAGING U.S. patent application Ser. No. 11 768 697 for A METHOD AND APPARATUS OF PREFETCHING STREAMS OF VARYING PREFETCH DEPTH U.S. patent application Ser. No. 11 768 532 for PROGRAMMABLE PARTITIONING FOR HIGH PERFORMANCE COHERENCE DOMAINS IN A MULTIPROCESSOR SYSTEM U.S. patent application Ser. No. 11 768 857 for METHOD AND APPARATUS FOR SINGLE STEPPING COHERENCE EVENTS IN A MULTIPROCESSOR SYSTEM UNDER SOFTWARE CONTROL U.S. patent application Ser. No. 11 768 547 for INSERTION OF COHERENCE EVENTS INTO A MULTIPROCESSOR COHERENCE PROTOCOL U.S. patent application Ser. No. 11 768 791 for METHOD AND APPARATUS TO DEBUG AN INTEGRATED CIRCUIT CHIP VIA SYNCHRONOUS CLOCK STOP AND SCAN U.S. patent application Ser. No. 11 768 795 for DMA ENGINE FOR REPEATING COMMUNICATION PATTERNS U.S. patent application Ser. No. 11 768 799 for METHOD AND APPARATUS FOR A CHOOSE TWO MULTI QUEUE ARBITER U.S. patent application Ser. No. 11 768 800 for METHOD AND APPARATUS FOR EFFICIENTLY TRACKING QUEUE ENTRIES RELATIVE TO A TIMESTAMP U.S. patent application Ser. No. 11 768 572 for BAD DATA PACKET CAPTURE DEVICE U.S. patent application Ser. No. 11 768 593 for EXTENDED WRITE COMBINING USING A WRITE CONTINUATION HINT FLAG U.S. patent application Ser. No. 11 768 805 for A SYSTEM AND METHOD FOR PROGRAMMABLE BANK SELECTION FOR BANKED MEMORY SUBSYSTEMS U.S. patent application Ser. No. 11 768 905 for AN ULTRASCALABLE PETAFLOP PARALLEL SUPERCOMPUTER U.S. patent application Ser. No. 11 768 810 for SDRAM DDR DATA EYE MONITOR METHOD AND APPARATUS U.S. patent application Ser. No. 11 768 812 for A CONFIGURABLE MEMORY SYSTEM AND METHOD FOR PROVIDING ATOMIC COUNTING OPERATIONS IN A MEMORY DEVICE U.S. patent application Ser. No. 11 768 559 for ERROR CORRECTING CODE WITH CHIP KILL CAPABILITY AND POWER SAVING ENHANCEMENT U.S. patent application Ser. No. 11 768 552 for STATIC POWER REDUCTION FOR MIDPOINT TERMINATED BUSSES U.S. patent application Ser. No. 11 768 527 for COMBINED GROUP ECC PROTECTION AND SUBGROUP PARITY PROTECTION U.S. patent application Ser. No. 11 768 669 for A MECHANISM TO SUPPORT GENERIC COLLECTIVE COMMUNICATION ACROSS A VARIETY OF PROGRAMMING MODELS U.S. patent application Ser. No. 11 768 813 for MESSAGE PASSING WITH A LIMITED NUMBER OF DMA BYTE COUNTERS U.S. patent application Ser. No. 11 768 619 for ASYNCRONOUS BROADCAST FOR ORDERED DELIVERY BETWEEN COMPUTE NODES IN A PARALLEL COMPUTING SYSTEM WHERE PACKET HEADER SPACE IS LIMITED U.S. patent application Ser. No. 11 768 682 for HARDWARE PACKET PACING USING A DMA IN A PARALLEL COMPUTER and U.S. patent application Ser. No. 11 768 752 for POWER THROTTLING OF COLLECTIONS OF COMPUTING ELEMENTS .

The present disclosure generally relates to supercomputer systems and architectures and particularly to optimizing the performance of collective communication operations using a DMA on a parallel computer.

Collective communication operations involve several processes at a time if not all. Collective communication operations such as MPI Message Passing Interface broadcast which broadcasts data to all the processes in the communicator and MPI allreduce which performs reduction operations are important communication patterns that can often limit the performance and scalability of applications. Thus it is desirable to get the best possible performance from such operations.

BlueGene L systems massively parallel computers break up a long broadcast into several shorter broadcasts. The message is broken up into disjoint submessages called colors and the submessages are sent in such a way that different colors use different link on the 3D dimension torus. In this way a single broadcast in 1 dimension of a torus could theoretically achieve 2 links worth of bandwidth with 2 colors a 2 dimensional broadcast could achieve 4 links worth of bandwidth and a 3 dimensional broadcast could achieve 6 links worth of bandwidth. On those systems however there is no DMA engine and instead processors are responsible for injecting and receiving each packet. Accordingly what is desirable is a method and system that can utilize features of a DMA engine and network so as to achieve high throughput large message collectives. It is also desirable to have a method and system that utilizes those features to realize low latency small message collectives.

Method and system for optimizing collective operations using direct memory access controller on a parallel computer are provide. A method for optimizing collective operations using direct memory access controller on a parallel computer in one aspect may comprise establishing a byte counter associated with direct memory access controller for each submessage in a message. A byte counter includes at least a base address of memory and a byte count associated with a submessage. The method may also comprise monitoring the byte counter associated with a submessage to determine whether at least a block of data of the submessage has been received. The block of data has a predetermined size. The method may further include processing the block when said block has been fully received and continuing the monitoring and processing step until all blocks in all submessages in the message have been processed.

A system for optimizing collective operations using direct memory access controller on a parallel computer in one aspect may comprise one or more processors in a node and memory in the node. The memory includes at least an injection fifo and a receive buffer. A direct memory access controller in the node includes at least a byte counter for each submessage of a message. A byte counter includes at least a base address in memory for storing associated submessage and a counter value. The direct memory access controller is operable to update the counter value as a result of receiving one or more bytes of the associated submessage into the node. One or more processors are operable to monitor the counter value and when a predetermined number of bytes of the submessage is received the one or more processors are further operable to process a block of data comprising the received predetermined number of bytes of the submessage.

Further features as well as the structure and operation of various embodiments are described in detail below with reference to the accompanying drawings. In the drawings like reference numbers indicate identical or functionally similar elements

A method and system are disclosed that provide high throughput large message and low latency small message collective operations on a parallel machine with a DMA Direct Memory Access engine. In computer architecture designs of massively parallel computers or supercomputers such as BlueGene P jointly developed by International Business Machines Corporation and other institutions there is a DMA engine that is integrated onto the same chip as the processors cache memory memory controller and network logic. Briefly DMA engine allows accesses to system memory independently of a central processing unit.

DMA in one embodiment has multiple byte counters that count the number of bytes injected into the network and received from the network. Each counter includes a base address addressing a location in memory. Each counter also includes a byte count of a packet or message injected or received. A packet contains a destination a counter identifier id and an offset. The offset in packet specifies the memory position relative to the base address contained in the counter identified by the counter id where the data of the packet is stored. A DMA unit may have for example 256 reception and 256 injection counters. This number may vary and depend on a design choice thus a DMA unit may have any number of counters. A message descriptor is placed into an injection fifo first in first out . A processor in a node creates message descriptors for example based on the parameters or attributes of an application call such as an MPI call. This message descriptor specifies or contains information associated with the injection counter id the number of bytes the offset of the send buffer from the injection counter base address a destination a reception counter id and an offset of the receive buffer from the reception counter base address. The destination specification or attribute may include a broadcast bit. In BlueGene L and P systems packets can be broadcast down a single dimension of a three dimensional torus. Several one dimensional broadcasts may be performed to broadcast in multiple dimensions.

The present disclosure describes a method and system in one embodiment in shared memory mode in which the processors on the same node comprise one or more threads in the same application process and have a common memory address space. Thus each processor can access all of the memory in this common address space. One of the processors is assigned to each color in an arbitrary manner. Thus a single processor could handle all colors or different processors each could handle a different color. For simplicity of explanation the description herein assumes that a single processor is handling all colors but one skilled in the art will appreciate variations in which different processors handle different colors. Thus the method and system of the present disclosure is not limited to a single processor handling all colors.

In one embodiment of the present disclosure a DMA engine and a plurality of counters for instance one reception counter per color and injection counter per color may achieve theoretical peaks. In the method and system of the present disclosure in one embodiment a core or a processor monitors the byte counters that track the number of received bytes. If a node is required to send the data corresponding to a particular color to another node or set of nodes when a sufficient number of bytes is received for that color a processor on the node injects a message descriptor into a DMA injection fifo thereby initiating transfer of the bytes out of that node. The message descriptor in one embodiment includes the injection counter identifier id that identifies the counter having the base address of a memory location the offset from that base address where the data to send is stored and the size of the sending data. Messages in different injection fifos can result in data flowing out of different links from the node. In this way all colors can be both received and sent at the same time at every node in the network. For example a node may receive and send on links at the same time.

Broadcast messages may be classified as long or short for example based on performance measurements associated with communicating those messages. For a long broadcast software pre arranges injection and reception counter ids for each color and an offset from the reception counters. In one embodiment counter ids and offsets for each color are common among the node although they do not need to be in all cases. For instance if there is no hardware broadcast they need not be common. Counter ids are put into the packets as part of data that describes those packets for instance in a header or other portion of the packet. Assuming the byte counters decrement upon reception all nodes program the byte counters to a suitably large number for instance one that is at least bigger than the message length. The source of the broadcast injects a message descriptor into an injection fifo. The DMA engine takes that descriptor for example from the injection fifo puts the information from the descriptor into each packet and injects individual packets of the message into the network.

In one embodiment it is assumed that these packets use non overtaking deterministic routing and are always injected into the same fifos in the nodes in the network. Thus packets arrive in the same order in which they are sent. A packet contains a reception counter id and an offset from the base reception address. In a 2D or 3D broadcast some or all nodes along the line of the broadcast re send the message to other nodes along a line in a different dimension of the torus. If these nodes wait until the entire message is received suboptimal performance is achieved for example in a 2D broadcast it takes twice as long as the optimized pipelined method of the present disclosure in one embodiment. Rather there is a message length of M bytes which is broken up into submessages colors of length M i bytes where M M 1 . . . M C where C is the number of colors. Color i has a block size of B i bytes. The re sending nodes poll the respective reception counter until at least B i bytes have been received. Suppose the actual number of bytes received is U i B i . Because of in order delivery the first U i bytes have been received. Then the re sending node injects a descriptor into a fifo used by that color specifying that U i bytes be sent down the next dimension. In one embodiment the re sending node uses the same reception counter ids and specifies the initial offset. The re sending node then waits until the next B i bytes have been received injects the next descriptor with an updated offset of U i and so on until all the bytes have been received and re injected. In this way if M i is the same for all colors and B i is the same for all colors M i B i submessages are sent but links in all dimensions are kept busy at the same time. Thus for long messages and B i M i relatively small the performance for a single color is approximately doubled compared to the non pipelined method and for C 1 color is approximately C times faster than an optimized single color method.

Reduction operations are used to compute a result involving data distributed over a group of processes. Examples include sum product minimum maximum and or user defined global operations. If the operation is a reduction then after seeing that at least B i bytes have been received the nodes perform an arithmetic operation e.g. adding integers in the incoming byte stream to a local contribution and then resending if necessary the result of the operation to other nodes. Thus when the counter indicates that the next B i bytes have been received a processor performs the reduction operation on those B i incoming bytes with the corresponding B i bytes from its local contribution storing the result in the corresponding location or slot in a receive buffer that is a location in memory. When the processor completes this reduction if there is a next node in the reduction tree the processor prepares a descriptor associated with B i reduced bytes and injects the descriptor into an injection fifo for sending the B i reduced bytes to that next node.

The broadcast is complete when each node has received all M bytes. If a node is a re sender of color i then the broadcast for color i is complete when the node receives and sends M i bytes. If a node is only a receiver of color i then the broadcast is complete for color i when the node receives M i bytes. The overall broadcast for the entire message M is complete when the broadcast for all colors are complete.

If the operation is an allreduce the operation can be pipelined as a reduction to a single node with a broadcast from that node back to the other nodes. On a 3D torus this could be a 3 color reduce and a 3 color broadcast that is 3 links worth of bandwidth can be obtained.

For short reductions latency is important. Typical MPI message passing interface implementations send point to point messages in a tree structure to perform the reduction. Messages are usually sent into a memory fifo on receiving nodes and software is responsible for polling the fifo and handling packets. The latency for polling and packet handling can be excessive. The method and system of the present disclosure in one embodiment optimizes short reduction on a set of nodes as explained below. Every node in the reduction agrees on a counter id and reception buffer. The counter identified by the counter id includes an address of a reception buffer that is used for this short message. If there are n nodes in the line and the message length is B bytes then node number m sends its bytes to a portion of the buffer allocated for it. For example node m sends to offset m 1 B from the base address stored in the counter. If the set of nodes is in a line e.g. along a row or column of 2D network of nodes a single descriptor broadcasting the data to every other node in the line can be used. If so these can be arranged so that half the data flows in the positive and half flows in the negative direction. If not n descriptors of point to point messages are injected. Each node receives n 1 B bytes or n B bytes if a node sends to itself . Nodes poll on the counter until all the bytes have been received at which time it performs the reduction operation on the bytes in the receive buffer. For instance DMA logic may poll on its own reception counter until all the bytes have been received. For reductions involving larger numbers of nodes for example on a plane multiple steps of this procedure can be used. That is on a plane each node participates in a row reduction followed by a column reduction. Measurements results using this approach show that a short reduction of a single double precision number along a row of length 8 nodes can be done in approximately one third the time it takes MPI to send and receive a single 0 byte message.

A node shown in includes multiple processors or cores . . . a memory and a DMA . The memory may be DRAM SDRAM or any other memory. The DMA includes a processor interface DMA logic a memory interface and a network interface Injection Counters Injection Fifo metadata Reception Counters Reception Fifo metadata and status and control registers . The Injection Fifo metadata describes where in memory the Injection Fifos are located and the current head and tail of the Fifos . The Reception Fifo metadata describes where in memory the Reception Fifos are located and the current head and tail of the Fifos . Thus DMA has pointers to the fifos in memory for example by means. of a reception fifo metadata and injection fifo metadata . Injection fifos in memory store descriptor data associated with message packets for injection to the network and reception fifos in memory store packets of received data from the network. Memory interface is used to read and write data to the memory from the DMA . For example DMA logic may update injection fifos and reception fifos via the memory interface . One or more processors on the node communicate with DMA via a processor interface . The control registers are used to properly configure the DMA . The status registers reflect the current status such as error on hit condition of the DMA .

In one embodiment message passing on the collective network comprising a plurality of nodes for example may be performed through the use of a packet structure similar to that of the torus network. There may be additional support for a small number of class broadcast operations on the collective network. In one embodiment the collective network is a token based network which may be utilized also for system interrupts. Messages may be non blocking across VCs virtual channels . Interrupts may be carried in the same data stream and may be interleaved into the data at any time including during a packet transmission through the use of an interrupt out of band bit. When configured as a virtual tree messages may be injected into the collective network at any node in the system and they climb up the tree until arriving at either the tree vertex or at a final destination depending on the type of transfer.

For simplicity assumes that the intermediate node re broadcasts all colors. More generally a node may only need to re broadcast some colors. Each node knows which colors to re broadcast and to what destinations for example from having agreed previously by some convention or and an application programming interface telling the node etc. For colors that the node does not re broadcast the node monitors the byte counters for those colors until all bytes have been received.

Step illustrates an initialization step. Nodes agree on the message size M counter ids the number of colors C the message size for each color M i the block size B i the number of bytes received R i 0 the number of bytes sent S i 0 and the number of unsent bytes U i R i S i . The root node in the network sends out multiple color messages to certain nodes in the broadcast. These destination nodes may be different for each color. On the re broadcasting nodes at step a node determines whether it has sent all the bytes for all its colors if so the node is done with processing this operation. At step for colors still in progress the node polls one or more counters identified by the counter ids agreed on during the initialization process and determines the number of bytes received R i and the number of unsent bytes U i . For instance the number of bytes received may be determined from reading a reception counter associated with the color being processed and the number of bytes sent may be determined from reading an injection counter associated with the color being processed. A processor in a node for example reads the one or more counters that the DMA engine updates when the DMA engine actually receives and or sends the bytes.

If the collective is a reduction the reduction operation is performed at step prior to sending the next U i bytes. Sending the next U i bytes involves performing the reduction operation on the newly received data in the receive buffer. In one embodiment the result of this reduction is stored in the location or slot of the receive buffer corresponding to the newly received data.

For a reduction a processor sends a message to each node taking part in the reduction operation. This message may be a single message if there is a broadcast capability. A descriptor of the message specifies the counter id the number of bytes and the offset corresponding to its slot in the buffer. Sending of a message is initiated for example by a processor that injects a descriptor into an injection fifo. A processor also places its own data for instance intermediate data or local contribution used in computations in its own slot in the receive buffer . After sending the message the processor polls its reception byte counter for this reduction until the expected number of bytes in the reduction have been received. The expected number of bytes is known from the semantics of the reduction software call. After receiving all bytes each processor performs the reduction for example a sum minimum maximum etc. Each node obtains or computes the same result for the reduction because the contents of receive buffer is identical on all nodes. Using standard software techniques if there are many nodes in the reduction multiple phases of the above can be used to obtain the result of the overall reduction. For example to perform a reduction in a 2 D dimension plane each node can set up buffers and counters for two reductions a row and a column reduction. Nodes first participate in a reduction along its first dimension row and then feed the result of that reduction into a reduction along the second dimension column .

In another embodiment the method and system may operate in Virtual Node Mode in which the memory is partitioned so that each processor can read and write only a portion of memory. If a processor can read but not write the memory of the other processors the processor handling a color sets a memory address to point to the start of its receive buffer and another address for the number of bytes received for that color. These addresses are known to all processors on the node for example via appropriate software calls.

For a broadcast as bytes are received the processor handling the color updates its bytes received memory location as blocks are processed. The other nodes can read this location and copy bytes from the handler s receive buffer to its own receive buffer as blocks are received.

For a reduction the processor handling the color performs the reduction operation on the received data by combining data received from the network and the local contributions of all the processors. When this is complete for each block the other processors can copy the result of the reduction for that block into its own memory.

If a processor cannot normally read the memory of the other processor a system call can be made that gives other processors read access to a limited portion of the memory. This system call can be made prior to the broadcast or reduction to cover the relevant areas of memory then the above described technique with respect to a broadcast can be used.

The embodiments described above are illustrative examples and it should not be construed that the present invention is limited to these particular embodiments. For example while some of the memory structure were shown and described in terms of fifo any other queuing or structuring mechanism may be used. Thus various changes and modifications may be effected by one skilled in the art without departing from the spirit or scope of the invention as defined in the appended claims.

