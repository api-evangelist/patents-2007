---

title: Application and database context correlation for database application developers
abstract: Infrastructure for capturing and correlating application context and database context for tuning, profiling and debugging tasks. The application context can include events such as data access events, and the database context can include events such as database server events. The events can be obtained from server tracing, data access layer tracing, and/or application tracing and written into respective log files. A data access event can indicate that an application consumed a row from a result set returned from a DBMS query. A post-processing step can correlate the application and database contexts by tokenizing strings and computing intersections between the tokenized strings. A tool inside a development environment may also suggest a query hint for the database or a data access API for the application based on the correlated context.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08307343&OS=08307343&RS=08307343
owner: Microsoft Corporation
number: 08307343
owner_city: Redmond
owner_country: US
publication_date: 20071019
---
Tools for profiling and tuning application code remain disconnected from the profiling and tuning tools for relational database management systems RDBMSs . This presents a challenge for developers of database applications to profile tune and debug applications for example identifying application code that causes deadlocks in the server. RDBMSs serve as the backend for many real world data intensive applications. These applications can use programming interfaces such as ODBC open database connectivity JDBC Java database connectivity ADO.Net and OLE DB object linking and embedding database for example to interact with the database server.

When building these applications application developers can use a variety of tools for understanding and fixing problems in the application. For example development environments can provide profiling tools that allow developers to understand performance characteristics in the application code such as frequently invoked functions time spent in a function etc.

For database application developers this support is not sufficient since APIs are used to invoke DBMS functionality. Thus an important part of the application execution happens inside the DBMS. DBMSs provide profiling and tuning tools that give developers information about which SQL structured query language statements were executed against the server the duration of each statement reads and writes performed by the statement blocking activity etc.

However the information obtained from the development environment profiler and the DBMS profiler remain as two islands of information that have little or no understanding of each other. This makes it difficult for database application developers to identify and fix problems with applications. This is illustrated in one example of detecting functions in the application code that caused a deadlock in the DBMS.

Consider an application that has two threads each executing a task on behalf of a different user. Each thread invokes certain functions that invoke SQL statements that read from and write to a particular table T in the database. Consider a scenario where an intermittent bug in one of the threads causes SQL statements issued by the application to deadlock with one another on the server. The database server will detect the deadlock and terminate one of the statements and unblock the other. This is manifested in the application as one thread receiving an error from the server and the other thread running to completion as normal. Thus while it is possible for the developer to know that there was a deadlock by examining the DBMS profiler output or the server error message it is difficult for the developer to know for example which function in each thread issued the respective statements that caused the server deadlock. Having the ability to identify the application code that is responsible for the problem in the database server can save considerable debugging effort for the developer.

The following presents a simplified summary in order to provide a basic understanding of some novel embodiments described herein. This summary is not an extensive overview and it is not intended to identify key critical elements or to delineate the scope thereof. Its sole purpose is to present some concepts in a simplified form as a prelude to the more detailed description that is presented later.

Disclosed is an infrastructure that simultaneously captures and correlates both the application context as well as the database context thereby enabling a rich class of tuning profiling and debugging tasks. The infrastructure extends the DBMS and application profiling infrastructure. This integration makes it easy for a developer to invoke and interact with a tool from inside the development environment.

Three sources of information are employed when an application is executed server tracing data access layer tracing and application tracing. The server tracing is can be a built in profiling capability of the DBMS. Types of trace events that are exposed by the server for profiling include for example SQLStatementCompleted an event that is generated whenever a SQL statement completes and a Deadlock event which is generated whenever the server identifies a deadlock and terminates a victim request . The data access layer tracing for APIs to connect and interact with the DBMS. This tracing contains detailed information about how the application uses the APIs. For example an event is generated each time the application opens a connection executes a statement consumes a row from the result set etc. For application tracing binary instrumentation techniques are employed to inject code into the application. When the application is run the injected code emits certain events for example an event when a thread enters or leaves a function or when the application enters or leaves a loop.

The events obtained from each of these sources are written into a log file. The log file receives log traces from different processes on a machine in a uniform manner to the same trace session. An event log is generated on each machine that involves either an application process or the DBMS server process. A post processing step is performed over the event log s to correlate the application and database contexts. The output of post processing is a single view where both the application and database profile of each statement issued by the application are exposed. This makes it possible to perform the application level debugging and tuning for SQL application developers.

To the accomplishment of the foregoing and related ends certain illustrative aspects are described herein in connection with the following description and the annexed drawings. These aspects are indicative however of but a few of the various ways in which the principles disclosed herein can be employed and is intended to include all such aspects and equivalents. Other advantages and novel features will become apparent from the following detailed description when considered in conjunction with the drawings.

The disclosed architecture captures and correlates application context and database context for tuning profiling and debugging tasks. The infrastructure extends the database management system DBMS and application profiling infrastructure making it easy for a developer to invoke and interact with a tool from inside the application development environment. Three sources of information are employed when an application is executed server event tracing data access layer event tracing and application event tracing. The events obtained from each of these sources are written into a log file. An event log is generated on each machine that involves either an application process or the DBMS server process and the log file receives log traces from different processes on a machine to the same trace session. A post processing step over the event log s correlates the application and database contexts. The output is a single view where both the application and database profile of each statement issued by the application are exposed.

In one embodiment a tool using this infrastructure enables developers to seamlessly profile tune and debug ADO.Net applications over Microsoft SQL Server by taking advantage of information across the application and database contexts.

Reference is now made to the drawings wherein like reference numerals are used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding thereof. It may be evident however that the novel embodiments can be practiced without these specific details. In other instances well known structures and devices are shown in block diagram form in order to facilitate a description thereof.

The system and more detailed and alternative systems described infra closes the gap that currently exists in profiling technologies for database application developers. The context of an application e.g. threads functions loops number of rows from a structured query language SQL query actually consumed by the application etc. and the context of the database server when executing a statement e.g. duration of execution duration for which the statement was blocked number of rows returned etc. can now be correlated with each other.

Moreover the developer user can view the application and database functions in a single view during the development phase and interact in the form of queries for example thereby enabling a class of development debugging and tuning tasks that are today difficult to achieve for application developers.

A first source is server events . This can be a built in profiling capability of the DBMS server . There are several types of server trace events that are exposed by the server for profiling. For example SQLStatementCompleted is an event that is generated whenever a SQL statement completes. The SQLStatementCompleted event contains attributes such as the SQL text duration of statement reads writes rows returned by the statement etc. Another example is a Deadlock event which is generated whenever the server identifies a deadlock and terminates a victim request. The Deadlock event contains attributes such as the text of the two deadlocking requests which request was chosen as a victim etc.

A second information source is data access events . This can be a built in profiling capability of a data access layer e.g. the ADO.Net . Since the application uses APIs application program interfaces to connect and interact with the DBMS server these events contain detailed information about how the application uses the APIs. For example an event is generated each time the application opens a connection executes a statement consumes a row from the result set etc.

A third information source is application events via application tracing. In support thereof binary instrumentation techniques are employed to inject code into the application . Since binary instrumentation is a post compilation step access to the application source code is not required. When the application is run the injected code emits certain events. For example the application can be made emit an event whenever a thread enters or leaves a function. The attributes of such an event can include the identifier of the function timestamp of entry timestamp of exit etc. Another example of an event is whenever the application enters or leaves a loop.

The system includes a presentation component for presenting a single or consolidated view of the profile of the application and DBMS server for each statement issued by the application .

In operation the analyzer component analyzes the event information in the log file received from the sources server events data access events and application events performs matching as part of correlating the events information thereby facilitating the presentation of the database and application profile in the single view via the application development environment.

The context component analyzer component and presentation component as well as the application can be part of a client development tool for developing the application .

The events obtained from each of the sources server events data access events and application events are written into the log file during execution of the application . The event writing tool is an efficient and scalable logging infrastructure that allows different processes on a machine to generate and log traces in a uniform manner to the same trace session. An event log can be generated on each machine e.g. client server that involves either an application process or the DBMS server process.

A post processing step is performed over the event log s to correlate the application and database contexts. The output of the post processing step is a single view where both the application and database profile of each statement issued by the application are exposed. For example a row in this view can contain information such as ProcessID ThreadID Function Identifier SQL statement Rows returned Rows consumed Columns returned Columns consumed SQL duration Blocked duration Reads Writes Error code etc. This makes it possible to perform the application level debugging and tuning that SQL application developers want to do.

In one implementation the disclosed framework is for use in development environments where the overhead introduced due to application instrumentation is not a significant issue. In another embodiment the disclosed framework is engineered for use in production systems where overhead processing due to application instrumentation is controlled. A brief description is provided herein for adapting the framework to production scenarios as well.

As described above the tool takes as input the binaries of the application which are then instrumented. When the instrumented binaries are executed the log of events is generated from the application as well as the database server server events and data access events . These events are then analyzed by a log analyzer component which produces as output a schematized view of the application trace. This view allows flexible analysis over the trace and allows tasks.

A binary instrumentation toolkit and toolkit API is used to examine the binaries statically and place instrumentation calls at selected places. In one example calls are added each time the application enters or leaves a function. When the instrumented binary is executed the instrumented code does a callback into a function. This function generates a trace event of the form EventType Hostname ProcessID ThreadID TimeStamp Enter functionName and writes the event to a profile session. Events from this session are then logged.

An event tracing infrastructure can be used which incurs a low overhead event logging mechanism. An event represents any activity of interest and is customizable by the event provider. Every event logged to a profiling session contains common attributes such as the name of the event provider type of event ID of the thread that issued the event timestamp and duration for example. In addition there is an attribute that allows provider defined data to be logged. For instance as described above events logged by the instrumented application describe the name of the function that was entered or left .

In one embodiment the fact that SQL Server and the data access layer e.g. ADO.NET ODBC and OLEDB is instrumented to emit events on demand is leveraged. The SQL Server event provider can emit event types such as login audits stored procedure execution completed batch execution completed deadlock etc. Additionally the server event provider emits custom data that has various interesting attributes like duration rows returned by server number of reads and writes etc. The data access layer e.g. ADO.Net provides events corresponding to every data read API opening and closing of connections type of data access API used. When the instrumented application is run a trace control manager interface can be used enable the emitting of events by the three providers application s instrumented binary data access layer and database server layer.

The tool also ensures that all the events are logged to a unique profiling session. Thus the timestamps of events across processes on the same machine are generated using a single mechanism which make correlating these events easier. The single log corresponding to this session can subsequently be analyzed offline after the application has finished running.

As an example for the Function foo shown below which uses data access APIs e.g. ADO.Net to connect to the database server the trace log shown in Table 1 below is generated.

Note that only a few relevant attributes are shown. Further note the three types of events in Table 1 application data access and server and that the common information available across processes is timestamps. The provider data column contains the actual payload of the event e.g. in the form of SQL statement strings etc. .

The analyzer component takes as input the trace log file . The analyzer correlates the events from the three providers and produces as output a schematized view of the application trace that contains both application and server context information. Following is an example of a schematized view represented in XML .

Note that the output shows the thread ID function name the SQL statement issued by the function database server context for the statement. This output allows flexible analysis over the trace and allows tuning and debugging tasks. Thus the log analyzer component correlates the application and database contexts from the event log .

The log analyzer component correlates the three types of events application events e.g. function enter leave data access events e.g. ADO.Net and database server events e.g. SQL Server . Correlating an application event and a data access event can occur since both of these events are emitted from the same process. Given the ThreadID and Timestamp it is possible to correlate exactly the data access events that are executed within a particular invocation of a function.

However matching a data access event D with the corresponding database server event s can be challenging. A unique identifier is used for an event that is unique across the application and server processes. However such a unique identifier is unavailable in conventional systems for data access providers. One useful attribute available from each event in the database server side is the client process ID. Thus the data access event D is matched with the server event S only if the D.ProcessID S.ClientProcessID. However since a single process may issue multiple SQL statements concurrently e.g. on different threads additional techniques for narrowing down the possible matches are desired.

Two additional pieces of information are used to assist matching. First is the timestamp. For example a data access event such as SqlCommand.Execute can precede the corresponding server event which is emitted once the SQL command has executed. This can significantly narrow down the possible server events that can match. Second the provider data contains the actual string of the command being executed. An issue with relying on matching strings is that exact string matching is not robust for this task. This is because the string by the data access layer may get modified in the server. Therefore approximate matching is relied on rather than exact string matches.

There are many techniques that have been developed for approximately matching strings. One version tokenizes the strings based on delimiters and computes the intersection of tokens between the two strings. This technique is significantly more reliable than exact string matching.

Following are scenarios that share a common thread and which can be achieved using both correlated application and database context information. Consider a query Q10 from a TPC H transition processing performance council decision support benchmark that returns for each customer the revenue lost due to items returned by the customer. The query is a join between four tables e.g. customer order lineitem and nation and returns the customers in descending order of the lost revenue. The application code that consumes the results of the query may be written for example as 

For example in SQL server this can be achieved by using an OPTION FAST k query hint. The information about what value of k number of rows consumed by the application is appropriate can be obtained from the application context. Once this information is available it is possible to perform interesting analysis that can suggest to the developer if providing the hint will actually benefit the performance of the query.

A common performance problem on a server arises when applications do not parameterize the SQL. For example consider a function in the application that when invoked with a parameter p e.g. a value 23 executes a SQL statement with the value of that parameter Select . . . FROM R S . . . WHERE . . . AccountId 23 . In another invocation the parameter value could be different and therefore in each invocation a different SQL text is submitted to the server. Thus the server is required to treat each statement as requiring a potentially unique execution plan. Note that conventional DBMSs have autoparameterization capabilities however this typically applies only to simple queries such as single table selection queries . Thus in the above example the application can cause unnecessary compilations and inefficient usage of the DBMS procedure cache.

Since the execution plans that are optimal for each of these instances is likely to be the same it is far more efficient for the application to parameterize its SQL Select . . . FROM R S . . . WHERE . . . AccountId p and pass in the parameter value via the data access APIs in the application. This tells the database server to treat different instances of that query as a single statement with a shared plan. This can dramatically reduce the compilation time as well as resources consumed in the procedure cache.

With respect to opportunities for bulk operations consider a loop in the application code shown below inside which the application is inserting data into a table 

As written above the code is inefficient since each time through the loop an INSERT statement is executed. A more efficient way to achieve the same result is to use the bulk insert APIs of the data access layer which take advantage of batching. Note that the ingredient for identifying this problem is having the application context that a particular SQL statement is executed repeatedly inside a loop and the database context know that each instance is in fact an INSERT statement on a table T. It is then possible to put these pieces of information together to suggest a mapping to the bulk insert APIs.

With respect to suggesting appropriate use of data access APIs many database application developers may be unaware of certain best practices for using data access APIs such as ADO.Net. For example when executing a stored procedure the best practice is to use the command Type.StoredProcedure passing the parameters using the AddParameters API. This results in a remote procedure call RPC event and is therefore efficient. However a developer who is unaware of this may pass in a string such as exec my sp 10 . In this case the database server gets a language event which needs to be parsed to find the stored procedure to be invoked arguments to be passed etc. If the above code is executed many times the performance improvement by using an RPC event compared to language event can be significant. In this example beneficial information from the application context is knowing that a language event was issued by the application and knowing from the database context that what was issued was in fact a stored procedure.

A second example of API usage arises when a query returns a scalar value one row and one column as the result. For such cases ADO.Net provides an ExecuteScalar API that is much more efficient that the more general ExecuteReader API. In this example the application context includes the fact that ExecuteReader API was used and the database context is the fact that the query returns exactly one row and one column in its result set.

With respect to identifying sequences for index tuning consider a sequence of SQL statements such as the one given below that is issued by an application 

A characteristic of the above sequence is that table R is transient. It is possible that if the application were to issue a CREATE INDEX statement on table R after the INSERT statement but before the SELECT statement there may be multiple such statements the SELECT statement s could be sufficiently speeded up to offset the cost of creating the index. A tool can be utilized that given a sequence of SQL statements is able to recommend if indexes should be created or dropped in the sequence to reduce overall cost of the sequence.

The developer who is the user of such a tool still should be able to find such sequences in the application since applying the recommendations of the tool includes changes to the application code. The user should understand the function s involved in producing the sequence of statements. Since a sequence can span function boundaries extracting a sequence of SQL statements uses knowledge of the call graph structure of the application which is available in the application side context.

Once instrumented the developer can click through the wizard which launches the application after enabling tracing for all the three event providers SQL Server tracing data access tracing and instrumented events from the application. This allows events containing both application context and database context to be logged into the event log file. As previously indicated the post processing step is performed by the log analyzer that correlates application and server events using a set of matching techniques. The above collection and matching enables bridging the database context and application context to provide significant value to database developers.

Once the post processing step is complete the tool invokes a module corresponding to the summary drill down box of the output of which is a summary detail view. illustrates an exemplary user interface of a summary detail view. Developers can get a summary and detail view involving various counters from the application the data access layer and the server navigate the call graph hierarchy and invoke specific verticals. The functional overview and usage of the tool is described below.

The summary view gives the function name aggregate time spend in a function how many times the function was invoked and aggregate time spend executing the SQL statement issued by the particular function in the database server. Conventionally the Function Exclusive Time and Number of Invocations counters can be obtained from profiling the application using application side profiling tools however the SQL Duration is an example of a value added feature since it merges database context into the application context.

Consider the function ReadStuff in which issues a SQL call. From the Summary view the developer can determine that the function was called twice bolded by white characters on black background and the aggregate time Exclusive Time spent inside this function across all instances was 5019 ms. Out of the total time spent in the function most of the time was spent executing SQL 5006 ms .

The Detail view gives more information at a function instance level. The tool allows drill down to display attributes of all the statements that were issued under the particular instance of the function or statements that were issued under the call tree of the particular instance of the function. The attributes of the SQL statement that are displayed include counters like Duration Reads Writes also data access counters such as reads issued by the application and the data access API type corresponding to the SQL that was issued.

The tool also allows analysis tools to be built on top of the profiled and correlated trace data such as suggesting FAST k query hint finding patterns of SQL statement invocations detecting deadlocks finding connection object leaks etc.

Up to this point the focus has been for scenarios during application development and debugging where instrumentation overheads are typically not an issue. However to use the tool framework for a production setting overheads can be managed. Following are a few techniques for controlling these overheads.

First static analysis of a binary can help determine if a function can possibly perform a data access call and then only instrument such functions. Note that with this optimization since functions without data access calls are not instrumented the call graph of function calls made by the application could be lost.

Second the developer may be aware of which binaries or even which functions are of interest for profiling. The disclosed binary instrumentation process of can take a list of such binaries functions as input and only instrument the specified functions.

Third it can be configured to only request the data access and server providers to emit only necessary events. For example the events in the data access layer e.g. ADO.Net that are emitted each time an application consumes a row can be disabled or turned off . In this case instrumentation overhead is traded off for some application context information e.g. number of rows consumed by application .

As previously described supra the log analyzer component produces a schematized view of the application trace that contains both application and server context information. The view generated can subsequently be queried using the query component of for providing browse summarize and drill down functionality. For example the tool allows developers to explore function call hierarchies and allows drill down on SQL statements that were issued under a function call tree.

Following is a description of two vertical debugging and tuning tools for performing more advanced analysis over the profiled data of . The first example shows how to detect functions in the application that caused a deadlock in the DBMS server. The second example describes how to recommend to the application developer the suggested use of an OPTION FAST k query hint for an expensive query whose entire result set may not be consumed by the application.

The SQL server trace produces a Deadlock event which contains the wait for graph that describes a deadlock. The graph contains the statements being executed that resulted in the deadlock as well as timestamp and client process ID s information. The log analyzer component extracts this information and stores it in the schematized application trace under the root node of the tree as an event of type deadlock .

For each such deadlock event the deadlock analysis vertical finds the statements issued by the application that correspond to the statements in the deadlock event. Note that once the statement is found all associated application context such as function and thread can be obtained. This can then be highlighted to the developer so they can see exactly which functions in the application issued the statements that lead to the deadlock.

In the above description an example of a query is provided that returns many rows of which only a few are consumed by the application. It is possible to implement a significant speed up if the application developer can rewrite the query to pass in an OPTION FAST k query hint to the database server so that the query optimizer can choose a plan that is more optimal when k rows are needed as opposed to all rows needed . Thus the developer can point to a query and invoke a Fast k analysis tool which returns as output an analysis of how the cost of the query varies with k. This cost information can be used by the developer to decide if it is appropriate to rewrite the query to use the hint.

The Fast k analysis tool explores how the cost of the query varies with k in OPTION FAST k hint . The na ve approach of costing the query for each value of k is not scalable. However in a well behaved query optimizer the cost of the query plan cannot decrease as k is increased. For a large class of queries such as single block SPJ Select Project Join queries with grouping aggregation this assumption typically holds true.

The disclosed approach includes performing a binary search over the range of values of k between kand k where kis the number of rows consumed by the application and kis the total number of rows returned by the query. Note that both these pieces of information are available from the output of the log analyzer component. If the plan and hence the cost of the query remains the same for two different values of k e.g. kand k then it is known that the plan and cost remains the same for all values of k between kand kas well. Thus the binary search strategy allows the pruning out of a large part of search space quickly. By observing such output the developer can determine whether or not providing the OPTION FAST k query hint is appropriate for the application.

Following is a series of flow charts representative of exemplary methodologies for performing novel aspects of the disclosed architecture. While for purposes of simplicity of explanation the one or more methodologies shown herein for example in the form of a flow chart or flow diagram are shown and described as a series of acts it is to be understood and appreciated that the methodologies are not limited by the order of acts as some acts may in accordance therewith occur in a different order and or concurrently with other acts from that shown and described herein. For example those skilled in the art will understand and appreciate that a methodology could alternatively be represented as a series of interrelated states or events such as in a state diagram. Moreover not all acts illustrated in a methodology may be required for a novel implementation.

While certain ways of displaying information to users are shown and described with respect to certain figures as screenshots those skilled in the relevant art will recognize that various other alternatives can be employed. The terms screen screenshot webpage document and page are generally used interchangeably herein. The pages or screens are stored and or transmitted as display descriptions as graphical user interfaces or by other methods of depicting information on a screen whether personal computer PDA mobile telephone or other suitable device for example where the layout and information or content to be displayed on the page is stored in memory database or another storage facility.

As used in this application the terms component and system are intended to refer to a computer related entity either hardware a combination of hardware and software software or software in execution. For example a component can be but is not limited to being a process running on a processor a processor a hard disk drive multiple storage drives of optical and or magnetic storage medium an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a server and the server can be a component. One or more components can reside within a process and or thread of execution and a component can be localized on one computer and or distributed between two or more computers.

Referring now to there is illustrated a block diagram of a computing system operable to execute the disclosed profiling architecture. In order to provide additional context for various aspects thereof and the following discussion are intended to provide a brief general description of a suitable computing system in which the various aspects can be implemented. While the description above is in the general context of computer executable instructions that may run on one or more computers those skilled in the art will recognize that a novel embodiment also can be implemented in combination with other program modules and or as a combination of hardware and software.

Generally program modules include routines programs components data structures etc. that perform particular tasks or implement particular abstract data types. Moreover those skilled in the art will appreciate that the inventive methods can be practiced with other computer system configurations including single processor or multiprocessor computer systems minicomputers mainframe computers as well as personal computers hand held computing devices microprocessor based or programmable consumer electronics and the like each of which can be operatively coupled to one or more associated devices.

The illustrated aspects can also be practiced in distributed computing environments where certain tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules can be located in both local and remote memory storage devices.

A computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by the computer and includes volatile and non volatile media removable and non removable media. By way of example and not limitation computer readable media can comprise computer storage media and communication media. Computer storage media includes volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital video disk DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the computer.

With reference again to the exemplary computing system for implementing various aspects includes a computer the computer including a processing unit a system memory and a system bus . The system bus provides an interface for system components including but not limited to the system memory to the processing unit . The processing unit can be any of various commercially available processors. Dual microprocessors and other multi processor architectures may also be employed as the processing unit .

The system bus can be any of several types of bus structure that may further interconnect to a memory bus with or without a memory controller a peripheral bus and a local bus using any of a variety of commercially available bus architectures. The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS is stored in a non volatile memory such as ROM EPROM EEPROM which BIOS contains the basic routines that help to transfer information between elements within the computer such as during start up. The RAM can also include a high speed RAM such as static RAM for caching data.

The computer further includes an internal hard disk drive HDD e.g. EIDE SATA which internal hard disk drive may also be configured for external use in a suitable chassis not shown a magnetic floppy disk drive FDD e.g. to read from or write to a removable diskette and an optical disk drive e.g. reading a CD ROM disk or to read from or write to other high capacity optical media such as the DVD . The hard disk drive magnetic disk drive and optical disk drive can be connected to the system bus by a hard disk drive interface a magnetic disk drive interface and an optical drive interface respectively. The interface for external drive implementations includes at least one or both of Universal Serial Bus USB and IEEE 1394 interface technologies.

The drives and associated computer readable media provide nonvolatile storage of data data structures computer executable instructions and so forth. For the computer the drives and media accommodate the storage of any data in a suitable digital format. Although the description of computer readable media above refers to a HDD a removable magnetic diskette and a removable optical media such as a CD or DVD it should be appreciated by those skilled in the art that other types of media which are readable by a computer such as zip drives magnetic cassettes flash memory cards cartridges and the like may also be used in the exemplary operating environment and further that any such media may contain computer executable instructions for performing novel methods of the disclosed architecture.

A number of program modules can be stored in the drives and RAM including an operating system one or more application programs other program modules and program data . The one or more application programs other program modules and program data can include the context component the analyzer component application context database context application DBMS server log file database sever events data access events application events presentation component single view and database and application profile in the view . This can further include the entities of the query component cost component and hints component of and the matching algorithm for example.

All or portions of the operating system applications modules and or data can also be cached in the RAM . It is to be appreciated that the disclosed architecture can be implemented with various commercially available operating systems or combinations of operating systems.

A user can enter commands and information into the computer through one or more wire wireless input devices for example a keyboard and a pointing device such as a mouse . Other input devices not shown may include a microphone an IR remote control a joystick a game pad a stylus pen touch screen or the like. These and other input devices are often connected to the processing unit through an input device interface that is coupled to the system bus but can be connected by other interfaces such as a parallel port an IEEE 1394 serial port a game port a USB port an IR interface etc.

A monitor or other type of display device is also connected to the system bus via an interface such as a video adapter . In addition to the monitor a computer typically includes other peripheral output devices not shown such as speakers printers etc.

The computer may operate in a networked environment using logical connections via wire and or wireless communications to one or more remote computers such as a remote computer s . The remote computer s can be a workstation a server computer a router a personal computer portable computer microprocessor based entertainment appliance a peer device or other common network node and typically includes many or all of the elements described relative to the computer although for purposes of brevity only a memory storage device is illustrated. The logical connections depicted include wire wireless connectivity to a local area network LAN and or larger networks for example a wide area network WAN . Such LAN and WAN networking environments are commonplace in offices and companies and facilitate enterprise wide computer networks such as intranets all of which may connect to a global communications network for example the Internet.

When used in a LAN networking environment the computer is connected to the local network through a wire and or wireless communication network interface or adapter . The adaptor may facilitate wire or wireless communication to the LAN which may also include a wireless access point disposed thereon for communicating with the wireless adaptor .

When used in a WAN networking environment the computer can include a modem or is connected to a communications server on the WAN or has other means for establishing communications over the WAN such as by way of the Internet. The modem which can be internal or external and a wire and or wireless device is connected to the system bus via the serial port interface . In a networked environment program modules depicted relative to the computer or portions thereof can be stored in the remote memory storage device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers can be used.

The computer is operable to communicate with any wireless devices or entities operatively disposed in wireless communication for example a printer scanner desktop and or portable computer portable data assistant communications satellite any piece of equipment or location associated with a wirelessly detectable tag e.g. a kiosk news stand restroom and telephone. This includes at least Wi Fi and Bluetooth wireless technologies. Thus the communication can be a predefined structure as with a conventional network or simply an ad hoc communication between at least two devices.

Referring now to there is illustrated a schematic block diagram of an exemplary computing environment for application server profiling in accordance with the disclosed architecture. The system includes one or more client s . The client s can be hardware and or software e.g. threads processes computing devices . The client s can house cookie s and or associated contextual information for example.

The system also includes one or more server s . The server s can also be hardware and or software e.g. threads processes computing devices . The servers can house threads to perform transformations by employing the architecture for example. One possible communication between a client and a server can be in the form of a data packet adapted to be transmitted between two or more computer processes. The data packet may include a cookie and or associated contextual information for example. The system includes a communication framework e.g. a global communication network such as the Internet that can be employed to facilitate communications between the client s and the server s .

Communications can be facilitated via a wire including optical fiber and or wireless technology. The client s are operatively connected to one or more client data store s that can be employed to store information local to the client s e.g. cookie s and or associated contextual information . Similarly the server s are operatively connected to one or more server data store s that can be employed to store information local to the servers .

What has been described above includes examples of the disclosed architecture. It is of course not possible to describe every conceivable combination of components and or methodologies but one of ordinary skill in the art may recognize that many further combinations and permutations are possible. Accordingly the novel architecture is intended to embrace all such alterations modifications and variations that fall within the spirit and scope of the appended claims. Furthermore to the extent that the term includes is used in either the detailed description or the claims such term is intended to be inclusive in a manner similar to the term comprising as comprising is interpreted when employed as a transitional word in a claim.

