---

title: Network storage appliance with integrated redundant servers and storage controllers
abstract: A network storage appliance includes a chassis, enclosing a storage controller and first and second servers. The storage controller has first and second I/O ports for coupling to first and second I/O links. The storage controller controls a plurality of physical disk drives and presents the plurality of physical disk drives as one or more logical disk drives on the first and second I/O links. The servers each have an I/O port for coupling to a respective one of the first and second I/O links. Each of the servers transmits packets to the storage controller over the respective I/O link. The packets include block-level protocol disk commands each identifying one of the logical disk drives, such as SCSI block level protocol commands each identifying one of said logical disk drives as a SCSI logical unit. The I/O links may be FibreChannel, Ethernet, or Infiniband links, for example.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07437604&OS=07437604&RS=07437604
owner: Dot Hill Systems Corporation
number: 07437604
owner_city: Carlsbad
owner_country: US
publication_date: 20070210
---
This application is a continuation of application Ser. No. 10 831 689 filed Apr. 23 2004 which claims the benefit of the following U.S. Provisional Application s 

This invention relates in general to the field of network storage in a computer network and particularly to the integration of server computers into a network storage appliance.

Historically computer systems have each included their own storage within the computer system enclosure or chassis or box. A typical computer system included a hard disk such as an IDE or SCSI disk directly attached to a disk controller which was in turn connected to the motherboard by a local bus. This model is commonly referred to as direct attached storage DAS .

However this model has certain disadvantages in an enterprise such as a business or university in which many computers are networked together each having its own DAS. One potential disadvantage is the inefficient use of the storage devices. Each computer may only use a relatively small percentage of the space on its disk drive with the remainder of the space being wasted. A second potential disadvantage is the difficulty of managing the storage devices for the potentially many computers in the network. A third potential disadvantage is that the DAS model does not facilitate applications in which the various users of the network need to access a common large set of data such as a database. These disadvantages among others have caused a trend toward more centralized shared storage in computer networks.

Initially the solution was to employ centralized servers such as file servers which included large amounts of storage shared by the various workstations in the network. That is each server had its own DAS that was shared by the other computers in the network. The centralized server DAS could be managed more easily by network administrators since it presented a single set of storage to manage rather than many smaller storage sets on each of the individual workstations. Additionally the network administrators could monitor the amount of storage space needed and incrementally add storage devices on the server DAS on an as needed basis thereby more efficiently using storage device space. Furthermore because the data was centralized all the users of the network who needed to access a database for example could do so without overloading one user s computer.

However a concurrent trend was toward a proliferation of servers. Today many enterprises include multiple servers such as a file server a print server an email server a web server a database server etc. and potentially multiple of each of these types of servers. Consequently the same types of problems that existed with the workstation DAS model existed again with the server DAS model.

Network attached storage NAS and storage area network SAN models were developed to address this problem. In a NAS SAN model a storage controller that controls storage devices typically representing a large amount of storage exists as a distinct entity on a network such as an Ethernet or FibreChannel network that is accessed by each of the servers in the enterprise. That is the servers share the storage controlled by the storage controller over the network. In the NAS model the storage controller presents the storage at a filesystem level whereas in the SAN model the storage controller presents the storage at a block level such as in the SCSI block level protocol. The NAS SAN model provides similar solutions to the fileserver DAS model problems that the fileserver DAS model provided to the workstation DAS problems. In the NAS SAN model the storage controllers have their own enclosures or chassis or boxes discrete from the server boxes. Each chassis provides its own power and cooling and since the chassis are discrete they require networking cables to connect them such as Ethernet or FibreChannel cables.

Another recent trend is toward storage application servers. In a common NAS SAN model one or more storage application servers resides in the network between the storage controller and the other servers and executes storage software applications that provided value added storage functions that benefit all of the servers accessing the common storage controller. These storage applications are also commonly referred to as middleware. Examples of middleware include data backup remote mirroring data snapshot storage virtualization data replication hierarchical storage management HSM data content caching data storage provisioning and file service applications. The storage application servers provide a valuable function however they introduce yet another set of discrete separately powered and cooled boxes that must be managed require additional space and cost and introduce additional cabling in the network.

Therefore what is needed is a way to improve the reliability and manageability and reduce the cost and physical space of a NAS SAN system. It is also desirable to obtain these improvements in a manner that capitalizes on the use of existing software to minimize the amount of software development necessary thereby achieving improved time to market and a reduction in development cost and resources.

In one aspect the present invention provides a network storage appliance that includes a chassis enclosing a storage controller and first and second servers. The storage controller has first and second I O ports for coupling to first and second I O links. The storage controller controls a plurality of physical disk drives and presents the plurality of physical disk drives as one or more logical disk drives on the first and second I O links. The servers each have an I O port for coupling to a respective one of the first and second I O links. Each of the servers transmits packets to the storage controller over the respective I O link. The packets include block level protocol disk commands each identifying one of the logical disk drives.

In another aspect the present invention provides a network storage appliance that includes a chassis enclosing first and second storage controllers and first and second servers. The first and second storage controller each have first and second I O ports for coupling to first and second I O links. Each of the storage controllers controls a plurality of physical disk drives and presents the plurality of physical disk drives as one or more logical disk drives on the first and second I O links. The first and second servers each have first and second I O ports. The first server I O ports couple to the respective first I O link coupled to the first and second storage controller. The second server I O ports couple to the respective second I O link coupled to the first and second storage controller. Each of the servers transmits to the storage controllers over the respective I O links block level disk commands each identifying one of the logical disk drives.

In another aspect the present invention provides a network storage appliance that includes a chassis and a backplane enclosed in the chassis. The backplane has a local bus. The network storage appliance also includes a first blade enclosed in the chassis and coupled to the backplane comprising a server a first portion of a storage controller and an I O link coupling the server and the first portion of the storage controller. The network storage appliance also includes a second blade enclosed in the chassis and coupled to the backplane comprising a second portion of the storage controller. The second portion of the storage controller controls a plurality of logical storage units. The server transmits packets to the first portion of the storage controller over the I O link. The packets include block level protocol disk commands each identifying one of the logical storage units. The first portion of the storage controller forwards the block level protocol disk commands to the second portion of the storage controller via the local bus.

An advantage of the storage appliance of the present invention is that it potentially reduces cost by reducing the number of enclosures needed and enables the servers and storage controllers to share common power and cooling systems within the enclosure. Also space savings may be enjoyed by integrating the servers and storage controllers what would heretofore typically have represented four distinct boxes into a single chassis. The cost and space savings may be significant particularly in an enterprise that incorporates many of the storage appliances since the cost and space savings would be multiplied. Yet another advantage is the potential increase in the reliability of the system in particular by enclosing the network connections between the servers and storage controllers within the chassis and eliminating damage prone inter enclosure cabling between the servers and storage controllers. Still further the network storage appliance presents an opportunity to improve the manageability of the server storage controller combination over the discrete enclosure prior art implementation. This may be particularly advantageous due to the intertwined nature of the storage controller function and the server function when the server is performing a value added storage function such as a storage virtualization function.

Referring now to a diagram of a prior art computer network is shown. The computer network includes a plurality of client computers coupled to a plurality of traditional server computers via a network . The network components may include switches hubs routers and the like. The computer network also includes a plurality of storage application servers coupled to the traditional servers via the network . The computer network also includes one or more storage controllers coupled to the storage application servers via the network . The computer network also includes storage devices coupled to the storage controllers .

The clients may include but are not limited to workstations personal computers notebook computers or personal digital assistants PDAs and the like. Typically the clients are used by end users to perform computing tasks including but not limited to word processing database access data entry email access internet access spreadsheet access graphic development scientific calculations or any other computing tasks commonly performed by users of computing systems. The clients may also include a computer used by a system administrator to administer the various manageable elements of the network . The clients may or may not include direct attached storage DAS such as a hard disk drive.

Portions of the network may include but are not limited to links switches routers hubs directors etc. performing the following protocols FibreChannel FC Ethernet Infiniband TCP IP Small Computer Systems Interface SCSI HIPPI Token Ring Arcnet FDDI LocalTalk ESCON FICON ATM Serial Attached SCSI SAS Serial Advanced Technology Attachment SATA and the like and relevant combinations thereof.

The traditional servers may include but are not limited to file servers print servers enterprise servers mail servers web servers database servers departmental servers and the like. Typically the traditional servers are accessed by the clients via the network to access shared files shared databases shared printers email the internet or other computing services provided by the traditional servers . The traditional servers may or may not include direct attached storage DAS such as a hard disk drive. However at least a portion of the storage utilized by the traditional servers comprises detached storage provided on the storage devices controlled by the storage controllers .

The storage devices may include but are not limited to disk drives tape drives or optical drives. The storage devices may be grouped by the storage application servers and or storage controllers into logical storage devices using any of well known methods for grouping physical storage devices including but not limited to mirroring striping or other redundant array of inexpensive disks RAID methods. The logical storage devices may also comprise a portion of a single physical storage device or a portion of a grouping of storage devices.

The storage controllers may include but are not limited to a redundant array of inexpensive disks RAID controller. The storage controllers control the storage devices and interface with the storage application servers via the network to provide storage for the traditional servers .

The storage application servers comprise computers capable of executing storage application software such as data backup remote mirroring data snapshot storage virtualization data replication hierarchical storage management HSM data content caching data storage provisioning and file service applications.

As may be observed from in the prior art computer network the storage application servers are physically discrete from the storage controllers . That is they reside in physically discrete enclosures or chassis. Consequently network cables must be run externally between the two or more chassis to connect the storage controllers and the storage application servers . This exposes the external cables for potential damage for example by network administrators thereby jeopardizing the reliability of the computer network . Also the cabling may be complex and therefore prone to be connected incorrectly by users. Additionally there is cost and space associated with each chassis of the storage controllers and the storage application servers and each chassis must typically include its own separate cooling and power system. Furthermore the discrete storage controllers and storage application servers constitute discrete entities to be configured and managed by network administrators. However many of these disadvantages are overcome by the presently disclosed network storage appliance of the present invention as will now be described.

Referring now to a diagram of a computer network according to the present invention is shown. In one embodiment the clients traditional servers storage devices and network are similar to like numbered elements of . The computer network of includes a network storage appliance which integrates storage application servers and storage controllers in a single chassis. The storage appliance is coupled to the traditional servers via the network as shown to provide detached storage such as storage area network SAN storage or network attached storage NAS for the traditional servers by controlling the storage devices coupled to the storage appliance . Advantageously the storage appliance provides the traditional servers with two interfaces to the detached storage one directly to the storage controllers within the storage appliance and another to the servers integrated into the storage appliance chassis which in turn directly access the storage controllers via internal high speed I O links within the storage appliance chassis. In one embodiment the servers and storage controllers in the storage appliance comprise redundant hot replaceable field replaceable units FRUs thereby providing fault tolerance and high data availability. That is one of the redundant FRUs may be replaced during operation of the storage appliance without loss of availability of the data stored on the storage devices . However single storage controller and single server embodiments are also contemplated. Advantageously the integration of the storage application servers into a single chassis with the storage controllers provides the potential for improved manageability lower cost less space better diagnosability and better cabling for improved reliability.

Referring now to a block diagram illustrating the computer network of including the storage appliance of is shown. The computer network includes the clients and or traditional servers of referred to collectively as host computers networked to the storage appliance . The computer network also includes external devices networked to the storage appliance i.e. devices external to the storage appliance . The external devices may include but are not limited to host computers tape drives or other backup type devices storage controllers or storage appliances switches routers or hubs. The computer network also includes the storage devices of coupled to the storage appliance . The storage appliance includes application servers coupled to storage controllers . The host computers are coupled to the application servers and the storage devices are coupled to the storage controllers . In one embodiment the application servers are coupled to the storage controllers via high speed I O links such as FibreChannel Infiniband or Ethernet links as described below in detail. The high speed I O links are also provided by the storage appliance external to its chassis of via port combiners shown in and expansion I O connectors shown in to which the external devices are coupled. The externalizing of the I O links advantageously enables the storage controllers to be directly accessed by other external network devices such as the host computers switches routers or hubs. Additionally the externalizing of the I O links advantageously enables the application servers to directly access other external storage devices such as tape drives storage controllers or other storage appliances as discussed below.

The application servers execute storage software applications such as those described above that are executed by the storage application servers of . However other embodiments are contemplated in which the application servers execute software applications such as those described above that are executed by the traditional servers of . In these embodiments the hosts may comprise clients such as those of networked to the storage appliance . The storage controllers control the storage devices and interface with the application servers to provide storage for the host computers and to perform data transfers between the storage devices and the application servers and or host computers . The storage controllers may include but are not limited to redundant array of inexpensive disks RAID controllers.

Referring now to a block diagram of one embodiment of the storage appliance of is shown. The storage appliance includes a plurality of hot replaceable field replaceable units FRUs referred to as modules or blades as shown enclosed in a chassis . The blades plug into a backplane or mid plane enclosed in the chassis which couples the blades together and provides a communication path between them. In one embodiment each of the blades plugs into the same side of the chassis . In one embodiment the backplane comprises an active backplane. In one embodiment the backplane comprises a passive backplane. In the embodiment of the blades include two power manager blades referred to individually as power manager blade A A and power manager blade B B two power port blades referred to individually as power port blade A A and power port blade B B two application server blades referred to individually as application server blade A A and application server blade B B two data manager blades referred to individually as data manager blade A A and data manager blade B B and two data gate blades referred to individually as data gate blade A A and data gate blade B B as shown.

The power manager blades each comprise a power supply for supplying power to the other blades in the storage appliance . In one embodiment each power manager blade comprises a 240 watt AC DC power supply. In one embodiment the power manager blades are redundant. That is if one of the power manager blades fails the other power manager blade continues to provide power to the other blades in order to prevent failure of the storage appliance thereby enabling the storage appliance to continue to provide the host computers access to the storage devices .

The power port blades each comprise a cooling system for cooling the blades in the chassis . In one embodiment each of the power port blades comprises direct current fans for cooling an integrated EMI filter and a power switch. In one embodiment the power port blades are redundant. That is if one of the power port blades fails the other power port blade continues to cool the storage appliance in order to prevent failure of the storage appliance thereby enabling the storage appliance to continue to provide the host computers access to the storage devices .

Data manager blade A A data gate blade A A and a portion of application server blade A A logically comprise storage controller A A of and the remainder of application server blade A A comprises application server A A of . Data manager blade B B data gate blade B B and a portion of application server blade B B comprise the other storage controller of and the remainder of application server blade B B comprises the other application server of .

The application servers comprise computers configured to execute software applications such as storage software applications. In one embodiment the application servers function as a redundant pair such that if one of the application servers fails the remaining application server takes over the functionality of the failed application server such that the storage appliance continues to provide the host computers access to the storage devices . Similarly if the application software executing on the application servers performs a function independent of the host computers such as a backup operation of the storage devices if one of the application servers fails the remaining application server continues to perform its function independent of the host computers . The application servers and in particular the application server blades are described in more detail below.

Each of the data gate blades comprises one or more I O interface controllers such as FC interface controllers and of for interfacing with the storage devices . In one embodiment each of the data gate blades comprises redundant interface controllers for providing fault tolerant access to the storage devices . In one embodiment the interface controllers comprise dual FibreChannel FC interface controllers for interfacing to the storage devices via a dual FC arbitrated loop configuration as shown in . However other embodiments are contemplated in which the data gate blades interface with the storage devices via other interfaces including but not limited to Advanced Technology Attachment ATA SAS SATA Ethernet Infiniband SCSI HIPPI ESCON FICON or relevant combinations thereof. The storage devices and storage appliance may communicate using stacked protocols such as SCSI over FibreChannel or Internet SCSI iSCSI . In one embodiment at least a portion of the protocol employed between the storage appliance and the storage devices includes a low level block interface such as the SCSI protocol. Additionally in one embodiment at least a portion of the protocol employed between the host computers and the storage appliance includes a low level block interface such as the SCSI protocol. The interface controllers perform the protocol necessary to transfer commands and data between the storage devices and the storage appliance . The interface controllers also include a local bus interface for interfacing to local buses shown as local buses of that facilitate command and data transfers between the data gate blades and the other storage appliance blades. In the redundant interface controller embodiment of each of the interface controllers is coupled to a different local bus as shown in and each data gate blade also includes a local bus bridge shown as bus bridge of for bridging the two local buses. In one embodiment the data gate blades function as a redundant pair such that if one of the data gate blades fails the storage appliance continues to provide the host computers and application servers access to the storage devices via the remaining data gate blade .

Each of the data manager blades comprises a processor such as CPU of for executing programs to control the transfer of data between the storage devices and the application servers and or host computers . Each of the data manager blades also comprises a memory such as memory in for buffering data transferred between the storage devices and the application servers and or host computers . The processor receives commands from the application servers and or host computers and responsively issues commands to the data gate blade interface controllers to accomplish data transfers with the storage devices . In one embodiment the data manager blades also include a direct memory access controller DMAC such as may be included in the local bus bridge memory controller shown in for performing data transfers to and from the buffer memory on the local buses. The processor also issues commands to the DMAC and interface controllers on the application server blades such as I O interface controllers of to accomplish data transfers between the data manager blade buffer memory and the application servers and or host computers via the local buses and high speed I O links . The processor may also perform storage controller functions such as RAID control logical block translation buffer management and data caching. Each of the data manager blades also comprises a memory controller such as local bus bridge memory controller in for controlling the buffer memory. The memory controller also includes a local bus interface for interfacing to the local buses that facilitate command and data transfers between the data manager blades and the other storage appliance blades. In one embodiment each of the data manager blades is coupled to a different redundant local bus pair and each data manager blade also includes a local bus bridge such as local bus bridge memory controller in for bridging between the two local buses of the pair. In one embodiment the data manager blades function as a redundant pair such that if one of the data manager blades fails the remaining data manager blade takes over the functionality of the failed data manager blade such that the storage appliance continues to provide the host computers and or application servers access to the storage devices . In one embodiment each data manager blade monitors the status of the other storage appliance blades including the other data manager blade in order to perform failover functions necessary to accomplish fault tolerant operation as described herein.

In one embodiment each of the data manager blades also includes a management subsystem for facilitating management of the storage appliance by a system administrator. In one embodiment the management subsystem comprises an Advanced Micro Devices Elan microcontroller for facilitating communication with a user such as a system administrator. In one embodiment the management subsystem receives input from the user via a serial interface such as an RS 232 interface. In one embodiment the management subsystem receives user input from the user via an Ethernet interface and provides a web based configuration and management utility. In addition to its configuration and management functions the management subsystem also performs monitoring functions such as monitoring the temperature presence and status of the storage devices or other components of the storage appliance and monitoring the status of other critical components such as fans or power supplies such as those of the power manager blades and power port blades .

The chassis comprises a single enclosure for enclosing the blade modules and backplane of the storage appliance . In one embodiment the chassis comprises a chassis for being mounted in well known 19 wide racks. In one embodiment the chassis comprises a one unit 1U high chassis.

In one embodiment the power manager blades power port blades data manager blades and data gate blades are similar in some aspects to corresponding modules in the RIO Raid Controller product sold by Chaparral Network Storage of Longmont Colo.

Although the embodiment of illustrates redundant modules other lower cost embodiments are contemplated in which some or all of the blade modules are not redundant.

Referring now to a block diagram of one embodiment of the storage appliance of illustrating the interconnection of the various local bus interconnections of the blade modules of is shown. The storage appliance in the embodiment of includes four local buses denoted local bus A A local bus B B local bus C C and local bus D D which are referred to collectively as local buses or individually as local bus . In one embodiment the local buses comprise a high speed PCI X local bus. Other embodiments are contemplated in which the local buses include but are not limited to a PCI CompactPCI PCI Express PCI X2 EISA VESA VME RapidIO AGP ISA 3GIO HyperTransport Futurebus MultiBus or any similar local bus capable of transferring data at a high rate. As shown data manager blade A A is coupled to local bus A A and local bus C C data manager blade B B is coupled to local bus B B and local bus D D data gate blade A A is coupled to local bus A A and local bus B B data gate blade B B is coupled to local bus C C and local bus D D application server blade A A is coupled to local bus A A and local bus B B application server blade B B is coupled to local bus C C and local bus D D. As may be observed the coupling of the blades to the local buses enables each of the application server blades to communicate with each of the data manager blades and enables each of the data manager blades to communicate with each of the data gate blades and each of the application server blades . Furthermore the hot pluggable coupling of the FRU blades to the backplane comprising the local buses enables fault tolerant operation of the redundant storage controllers and application servers as described in more detail below.

Referring now to a block diagram illustrating the logical flow of data through the storage appliance of is shown. The application server blades receive data transfer requests from the host computers of such as SCSI read and write commands over an interface protocol link including but not limited to FibreChannel Ethernet or Infiniband. The application server blades process the requests and issue commands to the data manager blades to perform data transfers to or from the storage devices based on the type of request received from the host computers . The data manager blades process the commands received from the application server blades and issue commands to the data gate blades such as SCSI over FC protocol commands which the data gate blades transmit to the storage devices . The storage devices process the commands and perform the appropriate data transfers to or from the data gate blades . In the case of a write to the storage devices the data is transmitted from the host computers to the application server blades and then to the data manager blades and then to the data gate blades and then to the storage devices . In the case of a read from the storage devices the data is transferred from the storage devices to the data gate blades then to the data manager blades then to the application server blades then to the host computers .

As shown in each of the application server blades has a path to each of the data manager blades and each of the data manager blades has a path to each of the data gate blades . In one embodiment the paths comprise the local buses of . Additionally in one embodiment each of the host computers has a path to each of the application server blades and each of the data gate blades has a path to each of the storage devices as shown. Because each of the stages in the command and data transfers is a redundant pair and a redundant communication path exists between each of the redundant pairs of each stage of the transfer a failure of any one of the blades of a redundant pair does not cause a failure of the storage appliance .

In one embodiment the redundant application server blades are capable of providing an effective data transfer bandwidth of approximately 800 megabytes per second MBps between the host computers and the redundant storage controllers .

Referring now to a block diagram of one embodiment of the storage appliance of illustrating the application server blades and data manager blades in more detail is shown. The data gate blades of are not shown in . In the embodiment of the local buses of comprise PCIX buses . illustrates application server blade A and B coupled to data manager blade A and B via PCIX buses A B C and D according to the interconnection shown in . The elements of the application server blades A and B are identical however their interconnections to the particular PCIX buses are different as shown therefore the description of application server blade A A is identical for application server blade B B except as noted below with respect to the PCIX bus interconnections. Similarly with the exception of the PCIX bus interconnections the elements of the data manager blades A and B are identical therefore the description of data manager blade A A is identical for data manager blade B B except as noted below with respect to the PCIX bus interconnections.

In the embodiment of application server blade A A comprises two logically distinct portions an application server portion and a storage controller portion physically coupled by the I O links of and integrated onto a single FRU. The application server portion includes a CPU subsystem Ethernet controller and first and second FC controllers which comprise a server computer employed to execute server software applications similar to those executed by the storage application servers and or traditional servers of . The storage controller portion of application server blade A A shown in the shaded area includes third and fourth FC controllers which are programmed by a data manager blade CPU and are logically part of the storage controller of . The storage controller portions of the application server blades may be logically viewed as the circuitry of a data gate blade integrated onto the application server blade to facilitate data transfers between the data manager blades and the application server portion of the application server blade . The storage controller portions of the application server blades also facilitate data transfers between the data manager blades and external devices of coupled to expansion I O connectors of the application server blade .

Application server blade A A includes a CPU subsystem described in detail below which is coupled to a PCI bus . The PCI bus is coupled to a dual port Ethernet interface controller whose ports are coupled to connectors on the application server blade faceplate shown in to provide local area network LAN or wide area network WAN access to application server blade A A by the host computers of . In one embodiment one port of the Ethernet interface controller of application server blade A A is coupled to one port of the Ethernet interface controller of application server blade B B to provide a heartbeat link such as heartbeat link of between the servers for providing redundant fault tolerant operation of the two application server blades as described below. In one embodiment the Ethernet controller ports may be used as a management interface to perform device management of the storage appliance . In one embodiment the application servers may function as remote mirroring servers and the Ethernet controller ports may be used to transfer data to a remote mirror site. The CPU subsystem is also coupled to a PCIX bus .

A first dual FibreChannel FC interface controller is coupled to the PCIX bus . The first FC interface controller ports also referred to as front end ports are coupled to the I O connectors on the application server blade faceplate shown in to provide the host computers NAS SAN access to the application servers . The first FC controller functions as a target device and may be connected to the host computers in a point to point arbitrated loop or switched fabric configuration. In and the remaining Figures a line connecting two FC ports or a FC port and a FC connector indicates a bi directional FC link i.e. an FC link with a transmit path and a receive path between the two FC ports or between the FC port and the FC connector.

A second dual FC interface controller is also coupled to the PCIX bus . The second FC controller functions as an initiator device. The second FC interface controller ports are coupled to the expansion I O connectors on the application server blade faceplate shown in to provide a means for the CPU subsystem of the application server blade to directly access devices of external to the storage appliance chassis such as other storage controllers or storage appliances tape drives host computers switches routers and hubs. In addition the expansion I O connectors provide the external devices direct NAS SAN access to the storage controllers rather than through the application servers as described in detail below. Advantageously the expansion I O connectors provide externalization of the internal I O links between the servers and storage controllers of as described in more detail below.

An industry standard architecture ISA bus is also coupled to the CPU subsystem . A complex programmable logic device CPLD is coupled to the ISA bus . The CPLD is also coupled to dual blade control interface BCI buses . Although not shown in one of the BCI buses is coupled to data manager blade A A and data gate blade A A and the other BCI bus is coupled to data manager blade B B and data gate blade B B as shown in . The BCI buses are a proprietary 8 bit plus parity asynchronous multiplexed address data bus supporting up to a 256 byte addressable region that interfaces the data manager blades to the data gate blades and application server blades . The BCI buses enable each of the data manager blades to independently configure and monitor the application server blades and data gate blades via the CPLD . The BCI buses are included in the backplane of . The CPLD is described in more detail with respect to and below.

Application server blade A A also includes a third dual FibreChannel interface controller coupled to PCIX bus A of whose FC ports are coupled to respective ones of the second dual FC interface controller . Application server blade A A also includes a fourth dual FibreChannel interface controller coupled to PCIX bus B of whose FC ports are coupled to respective ones of the second dual FC interface controller and to respective ones of the third dual FC interface controller . In the case of application server blade B B its third FC interface controller PCIX interface couples to PCIX bus C of and its fourth FC interface controller PCIX interface couples to PCIX bus D of . The third and fourth FC interface controllers function as target devices.

Data manager blade A A includes a CPU and a memory each coupled to a local bus bridge memory controller . In one embodiment the processor comprises a Pentium III microprocessor. In one embodiment the memory comprises DRAM used to buffer data transferred between the storage devices and the application server blade . The CPU manages use of buffer memory . In one embodiment the CPU performs caching of the data read from the storage devices into the buffer memory . In one embodiment data manager blade A A also includes a memory coupled to the CPU for storing program instructions and data used by the CPU . In one embodiment the local bus bridge memory controller comprises a proprietary integrated circuit that controls the buffer memory . The local bus bridge memory controller also includes two PCIX bus interfaces for interfacing to PCIX bus A and C of . The local bus bridge memory controller also includes circuitry for bridging the two PCIX buses A and C. In the case of data manager blade B B the local bus bridge memory controller interfaces to and bridges PCIX buses B and D of . The local bus bridge memory controller facilitates data transfers between each of the data manager blades and each of the application server blades via the PCIX buses .

Several advantages are obtained by including the third and fourth FC interface controllers on the application server blade . First the high speed I O links between the second FC controller and the third fourth FC controller are etched into the application server blade printed circuit board rather than being discrete cables and connectors that are potentially more prone to being damaged or to other failure. Second a local bus interface e.g. PCIX is provided on the application server blade backplane connector which enables the application server blades to interconnect and communicate via the local buses of the backplane with the data manager blades and data gate blades which also include a local bus interface on their backplane connector. Third substantial software development savings may be obtained from the storage appliance architecture. In particular the software executing on the data manager blades and the application server blades requires little modification to existing software. This advantage is discussed below in more detail with respect to .

Referring now to a block diagram illustrating one embodiment of the application server blade A A of is shown. The application server blade includes the CPU subsystem of comprising a CPU coupled to a north bridge by a Gunning Transceiver Logic GTL bus and a memory coupled to the north bridge by a double data rate DDR bus . The memory functions as a system memory for the application server blade . That is programs and data are loaded into the memory such as from the DOC memory described below and executed by the CPU . Additionally the memory serves as a buffer for data transferred between the storage devices and the host computers . In particular data is transferred from the host computers through the first FC controller and north bridge into the memory and vice versa. Similarly data is transferred from the memory through the north bridge second FC controller third or forth FC controller or and backplane to the data manager blades . The north bridge also functions as a bridge between the GTL bus DDR bus and the PCIX bus and the PCI bus of . The CPU subsystem also includes a south bridge coupled to the PCI bus . The Ethernet controller of is coupled to the PCI bus . In one embodiment the connectors of comprise RJ45 jacks denoted A and B in for coupling to respective ports of the Ethernet controller of for coupling to Ethernet links to the host computers . The south bridge also provides an IC bus by which temperature sensors are coupled to the south bridge . The temperature sensors provide temperature information for critical components in the chassis such as of CPUs and storage devices to detect potential failure sources. The south bridge also functions as a bridge to the ISA bus of .

A FLASH memory disk on chip DOC memory dual UART and the CPLD of are coupled to the ISA bus . In one embodiment the FLASH memory comprises a 16 MB memory used to store firmware to bootstrap the application server blade CPU . In one embodiment in which the application server blade conforms substantially to a personal computer PC the FLASH memory stores a Basic Input Output System BIOS . In one embodiment the DOC memory comprises a 128 MB NAND FLASH memory used to store among other things an operating system application software and data such as web pages. Consequently the application server blade is able to boot and function as a stand alone server. Advantageously the application server blade provides the DOC memory thereby alleviating the need for a mechanical mass storage device such as a hard disk drive for storing the operating system and application software. Additionally the DOC memory may be used by the storage application software executing on the application server blade as a high speed storage device in a storage hierarchy to cache frequently accessed data from the storage devices . In one embodiment the application server blade includes a mechanical disk drive such as a microdrive for storing an operating system application software and data instead of or in addition to the DOC memory . The two UART ports are coupled to respective 3 pin serial connectors denoted A and B for coupling to serial RS 232 links. In one embodiment the two serial ports function similarly to COM and COM ports of a personal computer. Additionally the RS 232 ports may be used for debugging and manufacturing support. The CPLD is coupled to a light emitting diode LED . The CPLD is coupled via the BCI buses of to a connector for plugging into the backplane of .

The CPLD includes a 2Kx8 SRAM port for accessing a shared mailbox memory region. The CPLD also provides the ability to program chip select decodes for other application server blade devices such as the FLASH memory and DOC memory . The CPLD provides dual independent BCI bus interfaces by which the data manager blades can control and obtain status of the application server blades . For example the CPLD provides the ability for the data manager blade to reset the application server blades and data gate blades such as in the event of detection of a failure. The CPLD also provides the ability to determine the status of activity on the various FibreChannel links and to control the status indicator LED . The CPLD also enables monitoring of the I O connectors and control of port combiners as described below. The CPLD also enables control of hot plugging of the various modules or blades in the storage appliance . The CPLD also provides general purpose registers for use as application server blade and data manager blade mailboxes and doorbells.

The first and second FC controllers of are coupled to the PCIX bus . In the embodiment of the I O connectors and of comprise FC small form factor pluggable sockets SFPs . The two ports of the first FC controller are coupled to respective SFPs A and B for coupling to FC links to the host computers . The two ports of the second FC controller are coupled to respective port combiners denoted A and B. The port combiners are also coupled to respective SFPs A and B for coupling to FC links to the external devices of . One port of each of the third and fourth FC controllers and of are coupled to port combiner A and one port of each of the third and fourth FC controllers and are coupled to port combiner B. The PCIX interface of each of the third and fourth FC controllers and are coupled to the backplane connector via PCIX bus A and B respectively of .

In one embodiment each of the port combiners comprises a FibreChannel arbitrated loop hub that allows devices to be inserted into or removed from an active FC arbitrated loop. The arbitrated loop hub includes four FC port bypass circuits PBCs or loop resiliency circuits LRCs serially coupled in a loop configuration as described in detail with respect to . A PBC or LRC is a circuit that may be used to keep a FC arbitrated loop operating when a FC L Port location is physically removed or not populated L Ports are powered off or a failing L Port is present. A PBC or LRC provides the means to route the serial FC channel signal past an L Port. A FC L Port is an FC port that supports the FC arbitrated loop topology. Hence for example if port of each of the second third and fourth FC controllers are all connected and operational and SFP A has an operational device coupled to it then each of the four FC devices may communicate with one another via port combiner A. However if the FC device connected to any one or two of the ports is removed or becomes non operational then the port combiner A will bypass the non operational ports keeping the loop intact and enabling the remaining two or three FC devices to continue communicating through the port combiner A. Hence port combiner A enables the second FC controller to communicate with each of the third and fourth FC controllers and consequently to each of the data manager blades additionally port combiner A enables external devices of coupled to SFP A to also communicate with each of the third and fourth FC controllers and consequently to each of the data manager blades . Although an embodiment is described herein in which the port combiners are FC LRC hubs other embodiments are contemplated in which the port combiners are FC loop switches. Because the FC loop switches are cross point switches they provide higher performance since more than one port pair can communicate simultaneously through the switch. Furthermore the port combiners may comprise Ethernet or Infiniband switches rather than FC devices.

In one embodiment the application servers substantially comprise personal computers without mechanical hard drives keyboard and mouse connectors. That is the application servers portion of the application server blade includes off the shelf components mapped within the address spaces of the system just as in a PC. The CPU subsystem is logically identical to a PC including the mappings of the FLASH memory and system RAM into the CPU address space. The system peripherals such as the UARTs interrupt controllers real time clock etc. are logically identical to and mapping the same as in a PC. The PCI PCIX ISA local buses and north bridge and south bridge are similar to those commonly used in high end PC servers. The Ethernet controller and first and second FC interface controllers function as integrated Ethernet network interface cards NICs and FC host bus adapters HBAs respectively. All of this advantageously potentially results in the ability to execute standard off the shelf software applications on the application server and the ability to run a standard operating system on the application servers with little modification. The hard drive functionality may be provided by the DOC memory and the user interface may be provided via the Ethernet controller interfaces and web based utilities or via the UART interfaces.

As indicated in the storage controller portion of the application server blade includes the third and fourth interface controllers and the SFPs the remainder comprises the application server portion of the application server blade .

Referring now to a diagram illustrating the physical layout of a circuit board of one embodiment of the application server blade of is shown. The layout diagram is drawn to scale. As shown the board is 5.040 inches wide and 11.867 inches deep. The elements of are included in the layout and numbered similarly. The first and second FC controllers each comprise an ISP2312 dual channel FibreChannel to PCI X controller produced by the QLogic Corporation of Aliso Viejo Calif. Additionally a 512Kx18 synchronous SRAM is coupled to each of the first and second FC controllers . The third and fourth FC controllers each comprise a JNIC 1560 Milano dual channel FibreChannel to PCI X controller. The south bridge comprises an Intel PIIX4E which includes internal peripheral interrupt controller PIC programmable interval timer PIT and real time clock RTC . The north bridge comprises a Micron PAD21 Copperhead. The memory comprises up to 1 GB of DDR SDRAM ECC protected memory DIMM. illustrates an outline for a memory DIMM to be plugged into a 184 pin right angle socket. The CPU comprises a 933 MHz Intel Tualatin low voltage mobile Pentium 3 with a 32 KB on chip L1 cache and a 512K on chip L2 cache. The FLASH memory comprises a 16 MBx8 FLASH memory chip. The DOC memory comprises two 32 MB each NAND FLASH memory chips that emulate an embedded IDE hard drive. The port combiners each comprise a Vitesse VSC7147 01. The Ethernet controller comprises an Intel 82546EB 10 100 1000 Mbit Ethernet controller.

Although an embodiment is described using particular components such as particular microprocessors interface controllers bridge circuits memories etc. other similar suitable components may be employed in the storage appliance .

Referring now to an illustration of one embodiment of the faceplate of the application server blade of is shown. The faceplate includes two openings for receiving the two RJ45 Ethernet connectors of . The faceplate also includes two openings for receiving the two pairs of SFPs and of . The face plate is one unit 1U high for mounting in a standard 19 inch wide chassis . The faceplate includes removal latches or removal mechanisms such as those well known in the art of blade modules that work together with mechanisms on the chassis to enable a person to remove the application server blade from the chassis backplane and to insert the application server blade into the chassis backplane while the storage appliance is operational without interrupting data availability on the storage devices . In particular during insertion the mechanisms cause the application server blade connector to mate with the backplane connector and immediately begin to receive power from the backplane conversely during removal the mechanisms cause the application server blade connector to disconnect from the backplane connector to which it mates thereby removing power from the application server blade . Each of the blades in the storage appliance includes removal latches similar to the removal latches of the application server blade faceplate shown in . Advantageously the removal mechanism enables a person to remove and insert a blade module without having to open the chassis .

Referring now to a block diagram illustrating the software architecture of the application server blade of is shown. The software architecture includes a loader . The loader executes first when power is supplied to the CPU . The loader performs initial boot functions for the hardware and loads and executes the operating system. The loader is also capable of loading and flashing new firmware images into the FLASH memory . In one embodiment the loader is substantially similar to a personal computer BIOS. In one embodiment the loader comprises the RedBoot boot loader product by Red Hat Inc. of Raleigh N.C. The architecture also includes power on self test POST diagnostics and manufacturing support software . In one embodiment the diagnostics software executed by the CPU does not diagnose the third and fourth FC controllers which are instead diagnosed by firmware executing on the data manager blades . The architecture also includes PCI configuration software which configures the PCI bus the PCIX bus and each of the devices connected to them. In one embodiment the PCI configuration software is executed by the loader .

The architecture also includes an embedded operating system and associated services . In one embodiment the operating system comprises an embedded version of the Linux operating system distributed by Red Hat Inc. Other operating systems are contemplated including but not limited to Hard Hat Linux from Monta Vista Software VA Linux an embedded version of Windows NT from Microsoft Corporation VxWorks from Wind River of Alameda Calif. Microsoft Windows CE and Apple Mac OS X 10.2. Although the operating systems listed above execute on Intel x86 processor architecture platforms other processor architecture platforms are contemplated. The operating system services include serial port support interrupt handling a console interface multi tasking capability network protocol stacks storage protocol stacks and the like. The architecture also includes device driver software for execution with the operating system . In particular the architecture includes an Ethernet device driver for controlling the Ethernet controller and FC device drivers for controlling the first and second FC controllers . In particular an FC device driver must include the ability for the first controller to function as a FC target to receive commands from the host computers and an FC device driver must include the ability for the second controller to function as a FC initiator to initiate commands to the storage controller and to any target external devices of connected to the expansion I O connectors . The architecture also includes a hardware abstraction layer HAL that abstracts the underlying application server blade hardware to reduce the amount of development required to port a standard operating system to the hardware platform.

The software architecture also includes an operating system specific Configuration Application Programming Interface CAPI client that provides a standard management interface to the storage controllers for use by application server blade management applications. The CAPI client includes a CAPI Link Manager Exchange LMX that executes on the application server blade and communicates with the data manager blades . In one embodiment the LMX communicates with the data manager blades via the high speed I O links provided between the second FC controller and the third and fourth FC controllers . The CAPI client also includes a CAPI client application layer that provides an abstraction of CAPI services for use by device management applications executing on the application server blade . The software architecture also includes storage management software that is used to manage the storage devices coupled to the storage appliance . In one embodiment the software architecture also includes RAID management software that is used to manage RAID arrays comprised of the storage devices controlled by the data manager blades .

Finally the software architecture includes one or more storage applications . Examples of storage applications executing on the application servers include but are not limited to the following applications data backup remote mirroring data snapshot storage virtualization data replication hierarchical storage management HSM data content caching data storage provisioning and file services such as network attached storage NAS . An example of storage application software is the IPStor product provided by FalconStor Software Inc. of Melville N.Y. The storage application software may also be referred to as middleware or value added storage functions. Other examples of storage application software include products produced by Network Appliance Inc. of Sunnyvale Calif. Veritas Software Corporation of Mountain View Calif. and Computer Associates Inc. of Islandia N.Y. similar to the FalconStor IPStor product.

Advantageously much of the software included in the application server blade software architecture may comprise existing software with little or no modification required. In particular because the embodiment of the application server blade of substantially conforms to the x86 personal computer PC architecture existing operating systems that run on an x86 PC architecture require a modest amount of modification to run on the application server blade . Similarly existing boot loaders PCI configuration software and operating system HALs also require a relatively small amount of modification to run on the application server blade . Furthermore because the DOC memories provide a standard hard disk drive interface the boot loaders and operating systems require little modification if any to run on the application server blade rather than on a hardware platform with an actual hard disk drive. Additionally the use of popular FC controllers and Ethernet controllers increases the likelihood that device drivers already exist for these devices for the operating system executing on the application server blade . Finally the use of standard operating systems increases the likelihood that many storage applications will execute on the application server blade with a relatively small amount of modification required.

Advantageously although in the embodiment of the storage appliance of the data manager blades and data gate blades are coupled to the application server blades via local buses as is typical with host bus adapter type or host dependent storage controllers the storage controllers logically retain their host independent or stand alone storage controller nature because of the application server blade architecture. That is the application server blade includes the host bus adapter type second interface controller which provides the internal host independent I O link to the third fourth interface controllers which in turn provide an interface to the local buses for communication with the other blades in the chassis via the backplane . Because the third fourth interface controllers are programmable by the data manager blades via the local buses the third fourth interface controllers function as target interface controllers belonging to the storage controllers . This fact has software reuse and interoperability advantages in addition to other advantages mentioned. That is the storage controllers appear to the application servers and external devices coupled to the expansion I O connectors as stand alone storage controllers. This enables the application servers and external devices to communicate with the storage controllers as a FC device using non storage controller specific device drivers rather than as a host bus adapter storage controller which would require development of a proprietary device driver for each operating system running on the application server or external host computers .

Notwithstanding the above advantages another embodiment is contemplated in which the second third and fourth FC controllers of are not included in the application server blade and are instead replaced by a pair of PCIX bus bridges that couple the CPU subsystem directly to the PCIX buses of the backplane . One advantage of this embodiment is potentially lower component cost which may lower the cost of the application server blade . Additionally the embodiment may also provide higher performance particularly in reduced latency and higher bandwidth without the intermediate I O links. However this embodiment may also require substantial software development which may be costly both in time and money to develop device drivers running on the application server blade and to modify the data manager blade firmware and software. In particular the storage controllers in this alternate embodiment are host dependent host bus adapters rather than host independent stand alone storage controllers. Consequently device drivers must be developed for each operating system executing on the application server blade to drive the storage controllers.

Referring now to a block diagram illustrating the storage appliance of in a fully fault tolerant configuration in the computer network of is shown. That is illustrates a storage appliance in which all blades are functioning properly. In contrast illustrate the storage appliance in which one of the blades has failed and yet due to the redundancy of the various blades the storage appliance continues to provide end to end connectivity thereby maintaining the availability of the data stored on the storage devices . The storage appliance comprises the chassis of for enclosing each of the blades included in . The embodiment of includes a storage appliance with two representative host computers A and B of redundantly coupled to the storage appliance via I O connectors . Each of the host computers includes two I O ports such as FibreChannel Ethernet Infiniband or other high speed I O ports. Each host computer has one of its I O ports coupled to one of the I O connectors of application server blade A A and the other of its I O ports coupled to one of the I O connectors of application server blade B B. Although the host computers are shown directly connected to the application server blade I O connectors the host computers may be networked to a switch router or hub of network that is coupled to the application server blade I O connectors .

The embodiment of also includes two representative external devices of redundantly coupled to the storage appliance via expansion I O connectors . Although the external devices are shown directly connected to the application server blade I O connectors the external devices may be networked to a switch router or hub that is coupled to the application server blade I O connectors . Each external device includes two I O ports such as FibreChannel Ethernet Infiniband or other high speed I O ports. Each external device has one of its I O ports coupled to one of the expansion I O connectors of application server blade A A and the other of its I O ports coupled to one of the expansion I O connectors of application server blade B B. The external devices may include but are not limited to other host computers a tape drive or other backup type device a storage controller or storage appliance a switch a router or a hub. The external devices may communicate directly with the storage controllers via the expansion I O connectors and port combiners of without the need for intervention by the application servers . Additionally the application servers may communicate directly with the external devices via the port combiners and expansion I O connectors without the need for intervention by the storage controllers . These direct communications are possible advantageously because the I O link between the second interface controller ports of the application server and the third interface controller ports of storage controller A A and the I O link between the second interface controller ports of the application server and the fourth interface controller ports of storage controller B B are externalized by the inclusion of the port combiners . That is the port combiners effectively create a blade area network BAN on the application server blade that allows inclusion of the external devices in the BAN to directly access the storage controllers . Additionally the BAN enables the application servers to directly access the external devices .

In one embodiment the storage application software executing on the application server blades includes storage virtualization provisioning software and the external devices include storage controllers and or other storage appliances that are accessed by the second interface controllers of the application servers via port combiners and expansion I O port connectors . Advantageously the virtualization provisioning servers may combine the storage devices controlled by the external storage controllers appliances and the storage devices controlled by the internal storage controllers when virtualizing provisioning storage to the host computers .

In another embodiment the storage application software executing on the application server blades includes storage replication software and the external devices include a remote host computer system on which the data is replicated that is accessed by the second interface controllers of the application servers via port combiners and expansion I O port connectors . If the remote site is farther away than the maximum distance supported by the I O link type then the external devices may include a repeater or router to enable communication with the remote site.

In another embodiment the storage application software executing on the application server blades includes data backup software and the external devices include a tape drive or tape farm for backing up the data on the storage devices which is accessed by the second interface controllers of the application servers via port combiners and expansion I O port connectors . The backup server may also back up to the tape drives data of other storage devices on the network such as direct attached storage of the host computers .

In another embodiment the external devices include host computers or switches or routers or hubs to which host computers are networked which directly access the storage controllers via the third fourth interface controllers via expansion I O connectors and port combiners . In one embodiment the storage controllers may be configured to present or zone two different sets of logical storage devices or logical units to the servers and to the external host computers .

The embodiment of includes two groups of physical storage devices A and B each redundantly coupled to the storage appliance . In one embodiment each physical storage device of the two groups of storage devices A and B includes two FC ports for communicating with the storage appliance via redundant FC arbitrated loops. For illustration purposes the two groups of physical storage devices A and B may be viewed as two groups of logical storage devices A and B presented for access to the application servers and to the external devices . The logical storage devices A and B may be comprised of a grouping of physical storage devices A A and or physical storage devices B B using any of well known methods for grouping physical storage devices including but not limited to mirroring striping or other redundant array of inexpensive disks RAID methods. The logical storage devices A and B may also comprise a portion of a single physical storage device or a portion of a grouping of physical storage devices. In one embodiment under normal operation i.e. prior to a failure of one of the blades of the storage appliance the logical storage devices A A are presented to the application servers and to the external devices by storage controller A A and the logical storage devices B B are presented to the application servers and external devices by storage controller B B. However as described below if the data manager blade of one of the storage controllers fails the logical storage devices A or B previously presented by the failing storage controller will also be presented by the remaining i.e. non failing storage controller . In one embodiment the logical storage devices are presented as SCSI logical units.

The storage appliance physically includes two application server blades A and B of two data manager blades A and B of and two data gate blades A and B of . is shaded to illustrate the elements of application server A A application server B B storage controller A A and storage controller B B of based on the key at the bottom of . Storage controller A A comprises data manager blade A A the first interface controllers of the data gate blades and the third interface controllers of the application server blades storage controller B B comprises data manager blade B B the second interface controllers of the data gate blades and the fourth interface controllers of the application server blades application server A A comprises CPU subsystem and the first and second interface controllers of application server blade A A application server B B comprises CPU subsystem and the first and second interface controllers of application server blade B B. In one embodiment during normal operation each of the application server blades accesses the physical storage devices via each of the storage controllers in order to obtain maximum throughput.

As in each of the application server blades includes first second third and fourth dual channel FC controllers . Port of the first FC controller of each application server blade is coupled to a respective one of the I O ports of host computer A A and port of the first FC controller of each application server blade is coupled to a respective one of the I O ports of host computer B B. Each of the application server blades also includes a CPU subsystem coupled to the first and second FC controllers . Port of each of the second third and fourth FC controllers of each application server blade are coupled to each other via port combiner A of and port of each controller of each application server blade are coupled to each other via port combiners B of . As in the third FC controller of application server blade A A is coupled to PCIX bus A the fourth FC controller of application server blade A A is coupled to PCIX bus B the third FC controller of application server blade B B is coupled to PCIX bus C and the fourth FC controller of application server blade B B is coupled to PCIX bus D. The Ethernet interface controllers CPLDs and BCI buses of are not shown in .

As in data manager blade A A includes a bus bridge memory controller that bridges PCIX bus A and PCIX bus C and controls memory and data manager blade B B includes a bus bridge memory controller that bridges PCIX bus B and PCIX bus D and controls memory . Hence the third FC controllers of both application server blades A and B are coupled to transfer data to and from the memory of data manager blade A A via PCIX buses A and C respectively and the fourth FC controllers of both application server blades A and B are coupled to transfer data to and from the memory of data manager blade B B via PCIX buses B and D respectively. Additionally the data manager blade A A CPU of is coupled to program the third FC controllers of both the application server blades A and B via PCIX bus A and C respectively and the data manager blade B B CPU of is coupled to program the fourth FC controllers of both the application server blades A and B via PCIX bus B and D respectively.

Each of data gate blades A and B include first and second dual FC controllers and respectively. In one embodiment the FC controllers each comprise a JNIC 1560 Milano dual channel FibreChannel to PCI X controller developed by the JNI Corporation that performs the FibreChannel protocol for transferring FibreChannel packets between the storage devices and the storage appliance . The PCIX interface of the data gate blade A A first FC controller is coupled to PCIX bus A the PCIX interface of the data gate blade A A second FC controller is coupled to PCIX bus B the PCIX interface of the data gate blade B B first FC controller is coupled to PCIX bus C and the PCIX interface of the data gate blade B B second FC controller is coupled to PCIX bus D. The first and second FC controllers function as FC initiator devices for initiating commands to the storage devices . In one embodiment such as the embodiment of one or more of the first and second FC controllers ports may function as FC target devices for receiving commands from other FC initiators such as the external devices . In the embodiment of a bus bridge of data gate blade A A couples PCIX buses A and B and a bus bridge of data gate blade B B couples PCIX buses C and D. Hence the first FC controllers of both data gate blades A and B are coupled to transfer data to and from the memory of data manager blade A A via PCIX buses A and C respectively and the second FC controllers of both data gate blades A and B are coupled to transfer data to and from the memory of data manager blade B B via PCIX buses B and D respectively. Additionally the data manager blade A A CPU of is coupled to program the first FC controllers of both the data gate blades A and B via PCIX bus A and C respectively and the data manager blade B B CPU of is coupled to program the second FC controllers of both the data gate blades A and B via PCIX bus B and D respectively.

In the embodiment of port of each of the first and second interface controllers of data gate blade A A and of storage devices B B is coupled to a port combiner of data gate blade A A similar to the port combiner of for including each of the FC devices in a FC arbitrated loop configuration. Similarly port of each of the first and second interface controllers of data gate blade A A and of storage devices A A is coupled to a port combiner of data gate blade A A port of each of the first and second interface controllers of data gate blade B B and of storage devices A A is coupled to a port combiner of data gate blade B B port of each of the first and second interface controllers of data gate blade B B and of storage devices B B is coupled to a port combiner of data gate blade B B. In another embodiment the storage devices are coupled to the data gate blades via point to point links through a FC loop switch. The port combiners are coupled to external connectors to connect the storage devices to the data gate blades . In one embodiment the connectors comprise FC SFPs similar to SFPs A and B of for coupling to FC links to the storage devices .

Advantageously the redundant storage controllers and application servers of the embodiment of of the storage appliance provide active active failover fault tolerance as described below with respect to and through such that if any one of the storage appliance blades fails the redundant blade takes over for the failed blade to provide no loss of availability to data stored on the storage devices . In particular if one of the application server blades fails the primary data manager blade deterministically kills the failed application server blade and programs the I O ports of the third and fourth interface controllers of the live application server blade to take over the identity of the failed application server blade such that the application server second interface controller coupled to the third or fourth interface controllers via the port combiners and the external devices coupled to the third or fourth interface controllers via the port combiners and expansion I O connectors continue to have access to the data on the storage devices additionally the live application server blade programs the I O ports of the first interface controller to take over the identity of the failed application server blade such that the host computers continue to have access to the data on the storage devices as described in detail below.

Referring now to a block diagram illustrating the computer network of in which data gate blade A has failed is shown. is similar to except that data gate blade A is not shown in order to indicate that data gate blade A has failed. However as may be seen storage appliance continues to make the data stored in the storage devices available in spite of the failure of a data gate blade . In particular data gate blade B B continues to provide a data path to the storage devices for each of the data manager blades A and B. Data manager blade A A accesses data gate blade B B via PCIX bus C and data manager blade B B accesses data gate blade B B via PCIX bus D through the chassis backplane . In one embodiment data manager blade A A determines that data gate blade A A has failed because data manager blade A A issues a command to data gate blade A A and data gate blade A A has not completed the command within a predetermined time period. In another embodiment data manager blade A A determines that data gate blade A A has failed because data manager blade A A determines that a heartbeat of data gate blade A A has stopped.

If data gate blade A A fails the data manager blade A A CPU programs the data gate blade B B first interface controller via data manager blade A A bus bridge and PCIX bus C to access storage devices A A via data gate blade B B first interface controller port and data is transferred between storage devices A A and data manager blade A A memory via data gate blade B B port combiner data gate blade B B first interface controller port PCIX bus C and data manager blade A A bus bridge . Similarly data manager blade A A CPU programs the data gate blade B B first interface controller via data manager blade A A bus bridge and PCIX bus C to access storage devices B B via data gate blade B B first interface controller port and data is transferred between storage devices B B and data manager blade A A memory via data gate blade B B port combiner data gate blade B B first interface controller port PCIX bus C and data manager blade A A bus bridge . Advantageously the storage appliance continues to provide availability to the storage devices data until the failed data gate blade A A can be replaced by hot unplugging the failed data gate blade A A from the chassis backplane and hot plugging a new data gate blade A A into the chassis backplane .

Referring now to a block diagram illustrating the computer network of in which data manager blade A A has failed is shown. is similar to except that data manager blade A A is not shown in order to indicate that data manager blade A A has failed. However as may be seen storage appliance continues to make the data stored in the storage devices available in spite of the failure of a data manager blade . In particular data manager blade B B provides a data path to the storage devices for the application server blade A A CPU subsystem and the external devices via the application server blade A A fourth interface controller and PCIX bus B additionally data manager blade B B continues to provide a data path to the storage devices for the application server blade B B CPU subsystem and external devices via the application server blade B B fourth interface controller and PCIX bus D as described after a brief explanation of normal operation.

In one embodiment during normal operation i.e. in a configuration such as shown in prior to failure of data manager blade A A data manager blade A A owns the third interface controller of each of the application server blades and programs each of the ports of the third interface controllers with an ID for identifying itself on its respective arbitrated loop which includes itself the corresponding port of the respective application server blade second and fourth interface controllers and any external devices connected to the respective application server blade corresponding expansion I O connector . In one embodiment the ID comprises a unique world wide name. Similarly data manager blade B B owns the fourth interface controller of each of the application server blades and programs each of the ports of the fourth interface controllers with an ID for identifying itself on its respective arbitrated loop. Consequently when a FC packet is transmitted on one of the arbitrated loops by one of the second interface controllers or by an external device the port of the third interface controller or fourth interface controller having the ID specified in the packet obtains the packet and provides the packet on the appropriate PCIX bus to either data manager blade A A or data manager blade B B depending upon which of the data manager blades owns the interface controller.

When data manager blade B B determines that data manager blade A A has failed data manager blade B B disables the third interface controller of each of the application server blades . In one embodiment data manager blade B B disables or inactivates the application server blade third interface controllers via the BCI bus and CPLD of such that the third interface controller ports no longer respond to or transmit packets on their respective networks. Next in one embodiment data manager blade B B programs the fourth interface controllers to add the FC IDs previously held by respective ports of the now disabled respective third interface controllers to each of the respective ports of the respective fourth interface controllers of the application server blades . This causes the fourth interface controllers to impersonate or take over the identity of the respective now disabled third interface controller ports. That is the fourth interface controller ports respond as targets of FC packets specifying the new IDs programmed into them which IDs were previously programmed into the now disabled third interface controller ports. In addition the fourth interface controllers continue to respond as targets of FC packets with their original IDs programmed at initialization of normal operation. Consequently commands and data previously destined for data manager blade A A via the third interface controllers are obtained by the relevant fourth interface controller and provided to data manager blade B B. Additionally commands and data previously destined for data manager blade B B via the fourth interface controllers continue to be obtained by the relevant fourth interface controller and provided to data manager blade B B. This operation is referred to as a multi ID operation since the ports of the non failed data gate blade fourth interface controllers are programmed with multiple FC IDs and therefore respond to two FC IDs per port rather than one. Additionally as described above in one embodiment during normal operation data manager blade A A and data manager blade B B present different sets of logical storage devices to the application servers and external devices associated with the FC IDs held by the third and fourth interface controllers . Advantageously when data manager blade A A fails data manager blade B B continues to present the sets of logical storage devices to the application servers and external devices associated with the FC IDs according to the pre failure ID assignments using the multi ID operation.

Data manager blade B B CPU programs the application server blade A A fourth interface controller via data manager blade B B bus bridge and PCIX bus B and programs the application server blade B B fourth interface controller via data manager blade B B bus bridge and PCIX bus D data is transferred between application server blade A A CPU subsystem memory of and data manager blade B B memory via application server blade A A second interface controller port combiner A or B application server blade A A fourth interface controller PCIX bus B and data manager blade B B bus bridge data is transferred between application server blade B B CPU subsystem memory of and data manager blade B B memory via application server blade B B second interface controller port combiner A or B application server blade B B fourth interface controller PCIX bus D and data manager blade B B bus bridge data may be transferred between the application server blade A A expansion I O connectors and data manager blade B B memory via port combiner A or B application server blade A A fourth interface controller PCIX bus B and data manager blade B B bus bridge data may be transferred between the application server blade B B expansion I O connectors and data manager blade B B memory via port combiner A or B application server blade B B fourth interface controller PCIX bus D and data manager blade B B bus bridge .

Furthermore if data manager blade A A fails data manager blade B B continues to provide a data path to the storage devices via both data gate blade A A and data gate blade B B via PCIX bus B and D respectively for each of the application server blade CPU subsystems and for the external devices . In particular the data manager blade B B CPU programs the data gate blade A A second interface controller via data manager blade B B bus bridge and PCIX bus B to access the storage devices via data gate blade A A second interface controller and data is transferred between the storage devices and data manager blade B B memory via data gate blade A A port combiner or data gate blade A A second interface controller PCIX bus B and data manager blade B B bus bridge . Similarly the data manager blade B B CPU programs the data gate blade B B second interface controller via data manager blade B B bus bridge and PCIX bus D to access the storage devices via data gate blade B B second interface controller and data is transferred between the storage devices and data manager blade B B memory via data gate blade B B port combiner or data gate blade B B second interface controller PCIX bus D and data manager blade B B bus bridge . Advantageously the storage appliance continues to provide availability to the storage devices data until the failed data manager blade A A can be replaced by removing the failed data manager blade A A from the chassis backplane and hot plugging a new data manager blade A A into the chassis backplane .

In one embodiment the backplane includes dedicated out of band signals used by the data manager blades to determine whether the other data manager blade has failed or been removed from the chassis . One set of backplane signals includes a heartbeat signal generated by each of the data manager blades . Each of the data manager blades periodically toggles a respective backplane heartbeat signal to indicate it is functioning properly. Each of the data manager blades periodically examines the heartbeat signal of the other data manager blade to determine whether the other data manager blade is functioning properly. In addition the backplane includes a signal for each blade of the storage appliance to indicate whether the blade is present in the chassis . Each data manager blade examines the presence signal for the other data manager blade to determine whether the other data manager blade has been removed from the chassis . In one embodiment when one of the data manager blades detects that the other data manager blade has failed e.g. via the heartbeat signal the non failed data manager blade asserts and holds a reset signal to the failing data manager blade via the backplane in order to disable the failing data manager blade to reduce the possibility of the failing data manager blade disrupting operation of the storage appliance until the failing data manager blade can be replaced such as by hot swapping.

Referring now to a block diagram illustrating the computer network of in which application server blade A A has failed is shown. is similar to except that application server blade A A is not shown in order to indicate that application server blade A A has failed. However as may be seen storage appliance continues to make the data stored in the storage devices available in spite of the failure of an application server blade . In particular application server blade B B provides a data path to the storage devices for the host computers and external devices .

If application server blade A A fails application server blade B B continues to provide a data path to the storage devices via both data manager blade A A and data manager blade B B via PCIX bus C and D respectively for the application server blade B B CPU subsystem and the external devices . In particular the data manager blade A A CPU programs the application server blade B B third interface controller via bus bridge and PCIX bus C data is transferred between the data manager blade A A memory and the application server blade B B CPU subsystem memory of via data manager blade A A bus bridge PCIX bus C application server blade B B third interface controller port combiner A or B and application server blade B B second interface controller data is transferred between the data manager blade A A memory and the external devices via data manager blade A A bus bridge PCIX bus C application server blade B B third interface controller and port combiner A or B data is transferred between the application server blade B B memory and host computer A A via port of the application server blade B B first interface controller and data is transferred between the application server blade B B memory and host computer B B via port of the application server blade B B first interface controller .

Host computer A A for example among the host computers re routes requests to application server blade B B I O connector coupled to port of the first interface controller in one of two ways.

In one embodiment host computer A A includes a device driver that resides in the operating system between the filesystem software and the disk device drivers which monitors the status of I O paths to the storage appliance . When the device driver detects a failure in an I O path such as between host computer A A and application server A A the device driver begins issuing I O requests to application server B B instead. An example of the device driver is software substantially similar to the DynaPath agent product developed by FalconStor Software Inc.

In a second embodiment application server blade B B detects the failure of application server blade A A and reprograms the ports of its first interface controller to take over the identity of the first interface controller of now failed application server blade A A via a multi ID operation. Additionally the data manager blades reprogram the ports of the application server blade B B third and fourth interface controllers to take over the identities of the third and fourth interface controllers of now failed application server blade A A via a multi ID operation. This embodiment provides failover operation in a configuration in which the host computers and external devices are networked to the storage appliance via a switch or router via network . In one embodiment the data manager blades detect the failure of application server blade A A and responsively inactivate application server blade A A to prevent it from interfering with application server blade B B taking over the identity of application server blade A A. Advantageously the storage appliance continues to provide availability to the storage devices data until the failed application server blade A A can be replaced by removing the failed application server blade A A from the chassis backplane and hot replacing a new application server blade A A into the chassis backplane . The descriptions associated with provide details of how the data manager blades determine that an application server blade has failed how the data manager blades inactivate the failed application server blade and how the identity of the failed application server blade is taken over by the remaining application server blade .

Referring now to a diagram of a prior art computer network is shown. The computer network of is similar to the computer network of and like numbered items are alike. However the computer network of also includes a heartbeat link coupling the two storage application servers which are redundant active active failover servers. That is the storage application servers monitor one another s heartbeat via the heartbeat link to detect a failure in the other storage application server . If one of the storage application servers fails as determined from the heartbeat link then the remaining storage application server takes over the identify of the other storage application server on the network and services requests in place of the failed storage application server . Typically the heartbeat link is an Ethernet link or FibreChannel link. That is each of the storage application servers includes an Ethernet or FC controller for communicating its heartbeat on the heartbeat link to the other storage application server . Each of the storage application servers periodically transmits the heartbeat to the other storage application server to indicate that the storage application server is still operational. Similarly each storage application server periodically monitors the heartbeat from the other storage application server to determine whether the heartbeat stopped and if so infers a failure of the other storage application server . In response to inferring a failure the remaining storage application server takes over the identity of the failed storage application server on the network such as by taking on the MAC address world wide name or IP address of the failed storage application server .

As indicated in a situation may occur in which both storage application servers are fully operational and yet a failure occurs on the heartbeat link . For example the heartbeat link cable may be damaged or disconnected. In this situation each server infers that the other server has failed because it no longer receives a heartbeat from the other server . This condition may be referred to as a split brain condition. An undesirable consequence of this condition is that each server attempts to take over the identity of the other server on the network potentially causing lack of availability of the data on the storage devices to the traditional server and clients .

A means of minimizing the probability of encountering the split brain problem is to employ dual heartbeat links. However even this solution is not a deterministic solution since the possibility still exists that both heartbeat links will fail. Advantageously an apparatus system and method for deterministically solving the split brain problem are described herein.

A further disadvantage of the network of will now be described. A true failure occurs on one of the storage application servers such that the failed server no longer transmits a heartbeat to the other server . In response the non failed server sends a command to the failed server on the heartbeat link commanding the failed server to inactivate itself i.e. to abandon its identity on the network namely by not transmitting or responding to packets on the network specifying its ID. The non failed server then attempts to take over the identity of the failed server on the network . However the failed server may not be operational enough to receive and perform the command to abandon its identity on the network yet the failed server may still be operational enough to maintain its identity on the network namely to transmit and or respond to packets on the network specifying its ID. Consequently when the non failed server attempts to take over the identity of the failed server this may cause lack of availability of the data on the storage devices to the traditional server and clients . Advantageously an apparatus system and method for the non failed server to deterministically inactivate on the network a failed application server integrated into the storage appliance of is described herein.

Referring now to a block diagram illustrating the storage appliance of is shown. The storage appliance of includes application server blade A A application server blade B B data manager blade A A data manager blade B B and backplane of . The storage appliance also includes a heartbeat link coupling application server blade A A and application server blade B B. The heartbeat link of serves a similar function as the heartbeat link of . In one embodiment the heartbeat link may comprise a link external to the storage appliance chassis of such as an Ethernet link coupling an Ethernet port of the Ethernet interface controller of of each of the application server blades or such as a FC link coupling a FC port of the first FC interface controller of of each of the application server blades or any other suitable communications link for transmitting and receiving a heartbeat. In another embodiment the heartbeat link may comprise a link internal to the storage appliance chassis and in particular may be comprised in the backplane . In this embodiment a device driver sends the heartbeat over the internal link. By integrating the application server blades into the storage appliance chassis the heartbeat link advantageously may be internal to the chassis which is potentially more reliable than an external heartbeat link . Application server blade A A transmits on heartbeat link to application server blade B B an A to B link heartbeat and application server blade B B transmits on heartbeat link to application server blade A A a B to A link heartbeat . In one of the internal heartbeat link embodiments the heartbeat link comprises discrete signals on the backplane .

Each of the data manager blades receives a blade present status indicator for each of the blade slots of the chassis . Each of the blade present status indicators indicates whether or not a blade such as the application server blades data manager blades and data gate blades are present in the respective slot of the chassis . That is whenever a blade is removed from a slot of the chassis the corresponding blade present status indicator indicates the slot is empty and whenever a blade is inserted into a slot of the chassis the corresponding blade present status indicator indicates that a blade is present in the slot.

Application server blade A A generates a health A status indicator which is provided to each of the data manager blades to indicate the health of application server blade A A. In one embodiment the health comprises a three bit number indicating the relative health 7 being totally healthy 0 being least healthy of the application server blade A A based on internal diagnostics periodically executed by the application server blade A A to diagnose its health. That is some subsystems of application server blade A A may be operational but others may not resulting in the report of a health lower than totally healthy. Application server blade B B generates a similar status indicator denoted health B status indicator which is provided to each of the data manager blades to indicate the health of application server blade B B.

Application server blade A A also generates a direct heartbeat A status indicator corresponding to the A to B link heartbeat but which is provided directly to each of the data manager blades rather than to application server blade B B. That is when application server blade A A is operational it generates a heartbeat both to application server blade B B via A to B link heartbeat and to each of the data manager blades via direct heartbeat A . Application server blade B B generates a similar direct heartbeat B status indicator which is provided directly to each of the data manager blades .

Application server blade A A generates an indirect heartbeat B to A status indicator which is provided to each of the data manager blades . The indirect heartbeat B to A status indicator indicates the receipt of B to A link heartbeat . That is when application server blade A A receives a B to A link heartbeat application server blade A A generates a heartbeat on indirect heartbeat B to A status indicator thereby enabling the data manager blades to determine whether the B to A link heartbeat is being received by application server blade A A. Application server blade B B generates an indirect heartbeat A to B status indicator similar to indirect heartbeat B to A status indicator which is provided to each of the data manager blades to indicate the receipt of A to B link heartbeat . The indirect heartbeat B to A status indicator and indirect heartbeat A to B status indicator in conjunction with the direct heartbeat A status indicator and direct heartbeat B status indicator enable the data manager blades to deterministically detect when a split brain condition has occurred i.e. when a failure of the heartbeat link has occurred although the application server blades are operational.

Data manager blade B B generates a kill A by B control provided to application server blade A A to kill or inactivate application server blade A A. In one embodiment killing or inactivating application server blade A A denotes inactivating the I O ports of the application server blade A A coupling the application server blade A A to the network particularly the ports of the interface controllers of . The kill A by B control is also provided to application server blade B B as a status indicator to indicate to application server blade B B whether data manager blade B B has killed application server blade A A. Data manager blade B B also generates a kill B by B control provided to application server blade B B to kill application server blade B B which is also provided to application server blade A A as a status indicator. Similarly data manager blade A A generates a kill B by A control provided to application server blade B B to kill application server blade B B which is also provided to application server blade A A as a status indicator and data manager blade A A generates a kill A by A control provided to application server blade A A to kill application server blade A A which is also provided to application server blade B B as a status indicator.

Advantageously the kill controls deterministically inactivate the respective application server blade . That is the kill controls inactivate the application server blade without requiring any operational intelligence or state of the application server blade in contrast to the system of in which the failed storage application server must still have enough operational intelligence or state to receive the command from the non failed storage application server to inactivate itself.

In one embodiment a data manager blade kills an application server blade by causing power to be removed from the application server blade specified for killing. In this embodiment the kill controls are provided on the backplane to power modules such as power manager blades of and instruct the power modules to remove power from the application server blade specified for killing.

The status indicators and controls shown in are logically illustrated. In one embodiment logical status indicators and controls of correspond to discrete signals on the backplane . However other means may be employed to generate the logical status indicators and controls. For example in one embodiment the blade control interface BCI buses and CPLDs shown in and may be employed to generate and receive the logical status indicators and controls shown in . Operation of the status indicators and controls of will now be described with respect to .

Referring now to a flowchart illustrating fault tolerant active active failover of the application server blades of the storage appliance of is shown. primarily describes the operation of the data manager blades whereas primarily describe the operation of the application server blades . Flow begins at block .

At block one or more of the data manager blades is reset. The reset may occur because the storage appliance is powered up or because a data manager blade is hot plugged into a chassis slot or because one data manager blade reset the other data manager blade . Flow proceeds to block .

At block the data manager blades establish between themselves a primary data manager blade . In particular the primary data manager blade is responsible for monitoring the health and heartbeat related status indicators of from the application server blades and deterministically killing one of the application server blades in the event of a heartbeat link failure or application server blade failure in order to deterministically accomplish active active failover of the application server blades . Flow proceeds to decision block .

At decision block the data manager blades determine whether the primary data manager blade has failed. If so flow proceeds to block otherwise flow proceeds to block .

At block the secondary data manager blade becomes the primary data manager blade in place of the failed data manager blade . Flow proceeds to block .

At block the primary data manager blade and secondary data manager blade if present receives and monitors the status indicators from each application server blade . In particular the primary data manager blade receives the health A health B indirect heartbeat B to A indirect heartbeat A to B direct heartbeat A and direct heartbeat B status indicators of . Flow proceeds to decision block .

At decision block the primary data manager blade determines whether direct heartbeat A has stopped. If so flow proceeds to block otherwise flow proceeds to decision block .

At block the primary data manager blade kills application server blade A A. That is if data manager blade A A is the primary data manager blade then data manager blade A A kills application server blade A A via the kill A by A control and if data manager blade B B is the primary data manager blade then data manager blade B B kills application server blade A A via the kill A by B control . As described herein various embodiments are described for the primary data manager blade to kill the application server blade such as by resetting the application server blade or by removing power from it. In particular the primary data manager blade causes the application server blade to be inactive on its network I O ports thereby enabling the remaining application server blade to reliably assume the identity of the killed application server blade on the network . Flow proceeds to decision block .

At decision block the primary data manager blade determines whether direct heartbeat B has stopped. If so flow proceeds to block otherwise flow proceeds to decision block .

At block the primary data manager blade kills application server blade B B. That is if data manager blade A A is the primary data manager blade then data manager blade A A kills application server blade B B via the kill B by A control and if data manager blade B B is the primary data manager blade then data manager blade B B kills application server blade B B via the kill B by B control . Flow proceeds to decision block .

At decision block the primary data manager blade determines whether both indirect heartbeat B to A and indirect heartbeat A to B have stopped i.e. the heartbeat link has failed or both servers have failed . If so flow proceeds to decision block otherwise flow returns to block .

At decision block the primary data manager blade examines the health A status and health B status to determine whether the health of application server blade A A is worse than the health of application server blade B B. If so flow proceeds to block otherwise flow proceeds to block .

At block the primary data manager blade kills application server blade A A. Flow proceeds to decision block .

At block the primary data manager blade kills application server blade B B. It is noted that block is reached in the case that both of the application server blades are operational and totally healthy but the heartbeat link is failed. In this case as with all the failure cases the system management subsystem of the data manager blades notifies the system administrator that a failure has occurred and should be remedied. Additionally in one embodiment status indicators on the faceplates of the application server blades may be lit to indicate a failure of the heartbeat link . Flow proceeds to decision block .

At decision block the primary data manager blade determines whether the killed application server blade has been replaced. In one embodiment the primary data manager blade determines whether the killed application server blade has been replaced by detecting a transition on the blade present status indicator of the slot corresponding to the killed application server blade from present to not present and then to present again. If decision block was arrived at because of a failure of the heartbeat link then the administrator may repair the heartbeat link and then simply remove and then re insert the killed application server blade . If the killed application server blade has been replaced flow proceeds to block otherwise flow returns to decision block .

At block the primary data manager blade unkills the replaced application server blade . In one embodiment unkilling the replaced application server blade comprises releasing the relevant kill control in order to bring the killed application server blade out of a reset state. Flow returns to block .

Other embodiments are contemplated in which the primary data manager blade determines a failure of an application server blade at decision blocks and by means other than the direct heartbeats . For example the primary data manager blade may receive an indication such as from temperature sensors of that the temperature of one or more of the components of the application server blade has exceeded a predetermined limit. Furthermore the direct heartbeat status indicator of an application server blade may stop for any of various reasons including but not limited to a failure of the CPU subsystem or a failure of one of the I O interface controllers .

Referring now to a flowchart illustrating fault tolerant active active failover of the application server blades of the storage appliance of is shown. Flow begins at block .

At block application server blade A A provides it s A to B link heartbeat to application server blade B B and application server blade B B provides it s B to A link heartbeat to application server blade A A of . Additionally application server blade A A provides health A indirect heartbeat B to A and direct heartbeat A to the data manager blades and application server blade B B provides health B indirect heartbeat A to B and direct heartbeat B to the data manager blades . In one embodiment the frequency with which the application server blades provide their health may be different from the frequency with which the direct heartbeat and or link heartbeats are provided. Flow proceeds to block .

At block application server blade A A monitors the B to A link heartbeat and application server blade B B monitors the A to B link heartbeat . Flow proceeds to decision block .

At decision block each application server blade determines whether the other application server blade link heartbeat has stopped. If so flow proceeds to decision block otherwise flow returns to block .

At decision block each application server blade examines the relevant kill signals to determine whether the primary data manager blade has killed the other application server blade . If so flow proceeds to block otherwise flow returns to decision block .

At block the live application server blade takes over the identity of the killed application server blade on the network . In various embodiments the live application server blade takes over the identity of the killed application server blade on the network by assuming the MAC address IP address and or world wide name of the corresponding killed application server blade I O ports. The I O ports may include but are not limited to FibreChannel ports Ethernet ports and Infiniband ports. Flow ends at block .

In an alternate embodiment a portion of the I O ports of each of the application server blades are maintained in a passive state while other of the I O ports are active. When the primary data manager blade kills one of the application server blades one or more of the passive I O ports of the live application server blade take over the identity of the I O ports of the killed application server blade at block .

As may be seen from the storage appliance advantageously deterministically performs active active failover from the failed application server blade to the live application server blade by ensuring that the failed application server blade is killed i.e. inactive on the network before the live application server blade takes over the failed application server blade identity thereby avoiding data unavailability due to conflict of identity on the network.

Referring now to a flowchart illustrating fault tolerant active active failover of the application server blades of the storage appliance of according to an alternate embodiment is shown. is identical to and like numbered blocks are alike except that block replaces decision block . That is if at decision block it is determined that the other application server blade heartbeat stopped then flow proceeds to block rather than decision block and flow unconditionally proceeds from block to block .

At block the live application server blade pauses long enough for the primary data manager blade to kill the other application server blade . In one embodiment the live application server blade pauses a predetermined amount of time. In one embodiment the predetermined amount of time is programmed into the application server blades based on the maximum of the amount of time required by the primary data manager blade to detect a failure of the link heartbeats via the indirect heartbeats and to subsequently kill an application server blade or to detect an application server blade failure via the direct heartbeats and to subsequently kill the failed application server blade .

As may be seen from the storage appliance advantageously deterministically performs active active failover from the failed application server blade to the live application server blade by ensuring that the failed application server blade is killed i.e. inactive on the network before the live application server blade takes over the failed application server blade identity thereby avoiding data unavailability due to conflict of identity on the network.

Referring now to a block diagram illustrating the interconnection of the various storage appliance blades via the BCI buses of is shown. includes data manager blade A A data manager blade B B application server blade A A application server blade B B data gate blade A A data gate blade B B and backplane of . Each application server blade includes CPLD of coupled to CPU of and I O interface controllers via ISA bus of . The CPLD generates a reset signal which is coupled to the reset input of CPU and I O interface controllers in response to predetermined control input received from a data manager blade on one of the BCI buses coupled to the CPLD . Each of the data manager blades includes CPU of coupled to a CPLD via an ISA bus . Each data gate blade includes I O interface controllers of coupled to a CPLD via an ISA bus . The CPLD generates a reset signal which is coupled to the reset input of the I O interface controllers in response to predetermined control input received from a data manager blade on one of the BCI buses coupled to the CPLD . The backplane includes four BCI buses denoted BCI A A BCI B B BCI C C and BCI D D. BCI A A couples the CPLDs and of data manager blade A A application server blade A A and data gate blade A A respectively. BCI B B couples the CPLDs and of data manager blade A A application server blade B B and data gate blade B B respectively. BCI C C couples the CPLDs and of data manager blade B B application server blade A A and data gate blade A A respectively. BCI D D couples the CPLDs and of data manager blade B B application server blade B B and data gate blade B B respectively.

In the embodiment of the application server blade CPUs generate the health and heartbeat statuses via CPLDs on the BCI buses which are received by the data manager blade CPLDs and conveyed to the CPUs via ISA buses thereby enabling the primary data manager blade to deterministically distinguish a split brain condition from a true application server blade failure. Similarly the data manager blade CPUs generate the kill controls via CPLDs on the BCI buses which cause the application server blade CPLDs to generate the reset signals to reset the application server blades thereby enabling a data manager blade to deterministically inactivate an application server blade so that the other application server blade can take over its network identity as described above. Advantageously the apparatus of does not require the application server blade CPU or I O interface controllers to be in a particular state or have a particular level of operational intelligence in order for the primary data manager blade to inactivate them.

Referring now to a block diagram illustrating the interconnection of the various storage appliance blades via the BCI buses of and discrete reset signals according to an alternate embodiment is shown. is identical to and like numbered elements are alike except that reset signals of are not present in . Instead a reset A signal is provided from the backplane directly to the reset inputs of the application server blade A A CPU and I O interface controllers and a reset B signal is provided from the backplane directly to the reset inputs of the application server blade B B CPU and I O interface controllers . Application server blade A A CPU also receives the reset B signal as a status indicator and application server blade B B CPU also receives the reset A signal as a status indicator. Data manager blade A A generates a reset A by A signal to reset application server blade A A and generates a reset B by A signal to reset application server blade B B. Data manager blade B B generates a reset B by B signal to reset application server blade B B and generates a reset A by B signal to reset application server blade A A. The reset A signal is the logical OR of the reset A by A signal and the reset A by B signal . The reset B signal is the logical OR of the reset B by B signal and the reset B by A signal .

In the embodiment of the application server blade CPUs generate the health and heartbeat statuses via CPLDs on the BCI buses which are received by the data manager blade CPLDs and conveyed to the CPUs via ISA buses thereby enabling the primary data manager blade to deterministically distinguish a split brain condition from a true application server blade failure. Similarly the data manager blade CPUs generate the reset signals via CPLDs which reset the application server blades thereby enabling a data manager blade to deterministically inactivate an application server blade so that the other application server blade can take over its network identity as described above. Advantageously the apparatus of does not require the application server blade CPU or I O interface controllers to be in a particular state or having a particular level of operational intelligence in order for the primary data manager blade to inactivate them.

Referring now to a block diagram illustrating an embodiment of the storage appliance of comprising a single application server blade is shown. Advantageously the storage appliance embodiment of may be lower cost than the redundant application server blade storage appliance embodiment of . is similar to and like numbered elements are alike. However the storage appliance of does not include application server blade B B. Instead the storage appliance of includes a third data gate blade similar to data gate blade B B denoted data gate blade C C in the chassis slot occupied by application server blade B B in the storage appliance of . The data gate blade C C first interface controller is logically a portion of storage controller A A and the second interface controller is logically a portion of storage controller B B as shown by the shaded portions of data gate blade C C. In one embodiment not shown data gate blade C C comprises four I O port connectors rather than two.

Data manager blade A A communicates with the data gate blade C C first interface controller via PCIX bus C and data manager blade B B communicates with the data gate blade C C second interface controller via PCIX bus D. Port of external device A A is coupled to the data gate blade C C I O connector coupled to port combiner and port of external device B B is coupled to the data gate blade C C I O connector coupled to port combiner thereby enabling the external devices to have redundant direct connections to the storage controllers and in particular redundant paths to each of the data manager blades via the redundant interface controllers . The data manager blades program the data gate blade C C interface controllers as target devices to receive commands from the external devices .

In one embodiment if application server blade A A fails the data manager blades program the data gate blade C C interface controller ports to take over the identities of the application server blade A A third fourth interface controller ports. Conversely if data gate blade C C fails the data manager blades program the application server blade A A third fourth interface controller ports to take over the identities of the data gate blade C C interface controller ports. The embodiment of may be particularly advantageous for out of band server applications such as a data backup or data snapshot application in which server fault tolerance is not as crucial as in other applications but where high data availability to the storage devices by the external devices is crucial.

Referring now to a block diagram illustrating an embodiment of the storage appliance of comprising a single application server blade is shown. Advantageously the storage appliance embodiment of may be lower cost than the redundant application server blade storage appliance embodiment of or then the single server embodiment of . is similar to and like numbered elements are alike. However the storage appliance of does not include application server blade B B. Instead the storage devices A A and storage devices B B are all coupled on the same dual loops thereby leaving the other data gate blade I O connectors available for connecting to the external devices . That is port of external device A A is coupled to one I O connector of data gate blade B B and port of external device B B is coupled to one I O connector of data gate blade A A thereby enabling the external devices to have redundant direct connections to the storage controllers and in particular redundant paths to each of the data manager blades via the redundant interface controllers . The data manager blades program the data gate blade interface controllers as target devices to receive commands from the external devices .

In one embodiment if application server blade A A fails the data manager blades program port of the data gate blade A A interface controllers to take over the identities of port of the application server blade A A third fourth interface controllers and the data manager blades program port of the data gate blade B B interface controllers to take over the identities of port of the application server blade A A third fourth interface controllers . Additionally if data gate blade A A fails the data manager blades program port of the application server blade A A third fourth interface controllers to take over the identities of port of the data gate blade A A interface controller ports. Furthermore if data gate blade B B fails the data manager blades program port of the application server blade A A third fourth interface controllers to take over the identities of port of the data gate blade B B interface controller ports. As with the embodiment of may be particularly advantageous for out of band server applications such as a data backup or data snapshot application in which server fault tolerance is not as crucial as in other applications but where high data availability to the storage devices by the external devices is crucial.

I O interfaces typically impose a limit on the number of storage devices that may be connected on an interface. For example the number of FC devices that may be connected on a single FC arbitrated loop is . Hence in the embodiment of a potential disadvantage of placing all the storage devices on the two arbitrated loops rather than four arbitrated loops as in is that potentially half the number of storage devices may be coupled to the storage appliance . Another potential disadvantage is that the storage devices must share the bandwidth of two arbitrated loops rather than the bandwidth of four arbitrated loops. However the embodiment of has the potential advantage of being lower cost than the embodiments of and or .

Referring now to a block diagram illustrating the computer network of and portions of the storage appliance of and in detail one embodiment of the port combiner of is shown. The storage appliance includes the chassis of enclosing various elements of the storage appliance . The storage appliance also illustrates one of the application server blade expansion I O connectors of . also includes an external device of external to the chassis with one of its ports coupled to the expansion I O connector . The expansion I O connector is coupled to the port combiner by an I O link . The I O link includes a transmit signal directed from the expansion I O connector to the port combiner and a receive signal directed from the port combiner to the expansion I O connector .

The storage appliance also includes the application server blade CPU subsystem coupled to an application server blade second interface controller via PCIX bus the data manager blade A A CPU coupled to the application server blade third interface controller via PCIX bus and the data manager blade B B CPU coupled to the application server blade fourth interface controller via PCIX bus all of . The storage appliance also includes the application server blade CPLD of . One port of each of the I O interface controllers is coupled to the port combiner by a respective I O link .

In the embodiment of the port combiner comprises a FibreChannel arbitrated loop hub. The arbitrated loop hub includes four FC port bypass circuits PBCs or loop resiliency circuits LRCs denoted A B C D. Each LRC includes a 2 input multiplexer. The four multiplexers are coupled in a serial loop. That is the output of multiplexer A is coupled to one input of multiplexer B the output of multiplexer B is coupled to one input of multiplexer C the output of multiplexer C is coupled to one input of multiplexer D and the output of multiplexer D is coupled to one input of multiplexer A. The second input of multiplexer A is coupled to receive the transmit signal of the I O link coupled to the second interface controller port the second input of multiplexer B is coupled to receive the transmit signal of the I O link coupled to the third interface controller port the second input of multiplexer C is coupled to receive the transmit signal of the I O link coupled to the fourth interface controller port and the second input of multiplexer D is coupled to receive the transmit signal of the I O link coupled to the expansion I O connector . The output of multiplexer D is provided as the receive signal of the I O link to the second I O interface controller port the output of multiplexer A is provided as the receive signal of the I O link to the third I O interface controller port the output of multiplexer B is provided as the receive signal of the I O link to the fourth I O interface controller port the output of multiplexer C is provided as the receive signal of the I O link to the expansion I O connector .

Each multiplexer also receives a bypass control input that selects which of the two inputs will be provided on the output of the multiplexer . The application server blade CPU subsystem provides the bypass control to multiplexer A the data manager blade A A CPU provides the bypass control to multiplexer B the data manager blade B B CPU provides the bypass control to multiplexer C and the application server blade CPLD provides the bypass control to multiplexer D. A value is generated on the respective bypass signal to cause the respective multiplexer to select the output of the previous multiplexer i.e. to bypass its respective interface controller I O port if the I O port is not operational otherwise a value is generated on the bypass signal to cause the multiplexer to select the input receiving the respective I O link transmit signal i.e. to enable the respective I O port on the arbitrated loop. In particular at initialization time the application server blade CPU data manager blade A A CPU and data manager blade B B CPU each diagnose their respective I O interface controller to determine whether the respective I O port is operational and responsively control the bypass signal accordingly. Furthermore if at any time during operation of the storage appliance the CPU determines the I O port is not operational the CPU generates a value on the bypass signal to bypass the I O port.

With respect to multiplexer D the CPLD receives a presence detected signal from the expansion I O connector to determine whether an I O link such as a FC cable is plugged into the expansion I O connector . The port combiner also includes a signal detector coupled to receive the transmit signal of the I O link coupled to the expansion I O connector . The signal detector samples the transmit signal and generates a true value if a valid signal is detected thereon. The CPLD generates a value on its bypass signal to cause multiplexer D to select the output of multiplexer C i.e. to bypass the expansion I O connector and consequently to bypass the I O port in the external device that may be connected to the expansion I O connector if either the presence detected signal or signal detected signal are false otherwise the CPLD generates a value on its bypass signal to cause multiplexer D to select the input receiving the transmit signal of the I O link coupled to the expansion I O connector i.e. to enable the external device I O port on the FC arbitrated loop . In one embodiment the CPLD generates the bypass signal in response to the application server blade CPU writing a control value to the CPLD .

Although describes an embodiment in which the port combiner of is a FibreChannel hub other embodiments are contemplated. The port combiner may include but is not limited to a FC switch or hub an Infiniband switch or hub or an Ethernet switch or hub.

The I O links advantageously enable redundant application servers to be coupled to architecturally host independent or stand alone redundant storage controllers . As may be observed from and various of the other Figures the port combiner advantageously enables the I O links between the application servers and storage controllers to be externalized beyond the chassis to external devices . This advantageously enables the integrated application servers to access the external devices and enables the external devices to directly access the storage controllers .

Although embodiments have been described in which the I O links between the second I O interface controller and the third and fourth I O interface controllers is FibreChannel other interfaces may be employed. For example a high speed Ethernet or Infiniband interface may be employed. If the second interface controller is an interface controller that already has a device driver for the operating system or systems to be run on the application server blade then an advantage is gained in terms of reduced software development. Device drivers for the QLogic ISP2312 have already been developed for many popular operating systems for example. This advantageously reduces software development time for employment of the application server blade embodiment described. Also it is advantageous to select a link type between the second interface controller and the third and fourth interface controllers which supports protocols that are frequently used by storage application software to communicate with external storage controllers such as FibreChannel Ethernet or Infiniband since they support the SCSI protocol and the internet protocol IP for example. A link type should be selected which provides the bandwidth needed to transfer data according to the rate requirements of the application for which the storage appliance is sought to be used.

Similarly although embodiments have been described in which the local buses between the various blades of storage appliance is PCIX other local buses may be employed such as PCI CompactPCI PCI Express PCI X2 bus EISA bus VESA bus Futurebus VME bus MultiBus RapidIO bus AGP bus ISA bus 3GIO bus HyperTransport bus or any similar local bus capable of transferring data at a high rate. For example if the storage appliance is to be used as a streaming video or audio storage appliance then the sustainable data rate requirements may be very high requiring a very high data bandwidth link between the controllers and and very high data bandwidth local buses. In other applications lower bandwidth I O links and local buses may suffice. Also it is advantageous to select third and fourth interface controllers for which storage controller firmware has already been developed such as the JNIC 1560 in order to reduce software development time.

Although embodiments have been described in which the application server blades execute middleware or storage application software typically associated with intermediate storage application server boxes which have now been described as integrated into the storage appliance as application servers it should be understood that the servers are not limited to executing middleware. Embodiments are contemplated in which some of the functions of the traditional servers may also be integrated into the network storage appliance and executed by the application server blade described herein particularly for applications in which the hardware capabilities of the application server blade are sufficient to support the traditional server application. That is although embodiments have been described in which storage application servers are integrated into the network storage appliance chassis it is understood that the software applications traditionally executed on the traditional application servers may also be migrated to the application server blades in the network storage appliance chassis and executed thereon.

Although the present invention and its objects features and advantages have been described in detail other embodiments are encompassed by the invention. For example although embodiments have been described employing dual channel I O interface controllers other embodiments are contemplated using single channel interface controllers. Additionally although embodiments have been described in which the redundant blades of the storage appliance are duplicate redundant blades other embodiments are contemplated in which the redundant blades are triplicate redundant or greater. Furthermore although active active failover embodiments have been described active passive embodiments are also contemplated.

Also although the present invention and its objects features and advantages have been described in detail other embodiments are encompassed by the invention. In addition to implementations of the invention using hardware the invention can be implemented in computer readable code e.g. computer readable program code data etc. embodied in a computer usable e.g. readable medium. The computer code causes the enablement of the functions or fabrication or both of the invention disclosed herein. For example this can be accomplished through the use of general programming languages e.g. C C JAVA and the like GDSII databases hardware description languages HDL including Verilog HDL VHDL Altera HDL AHDL and so on or other programming and or circuit i.e. schematic capture tools available in the art. The computer code can be disposed in any known computer usable e.g. readable medium including semiconductor memory magnetic disk optical disk e.g. CD ROM DVD ROM and the like and as a computer data signal embodied in a computer usable e.g. readable transmission medium e.g. carrier wave or any other medium including digital optical or analog based medium . As such the computer code can be transmitted over communication networks including Internets and intranets. It is understood that the invention can be embodied in computer code and transformed to hardware as part of the production of integrated circuits. Also the invention may be embodied as a combination of hardware and computer code.

Finally those skilled in the art should appreciate that they can readily use the disclosed conception and specific embodiments as a basis for designing or modifying other structures for carrying out the same purposes of the present invention without departing from the spirit and scope of the invention as defined by the appended claims.

