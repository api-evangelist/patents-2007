---

title: Method and apparatus for filtering video data using a programmable graphics processor
abstract: Video filtering using a programmable graphics processor is described. The programmable graphics processor may be programmed to complete a plurality of video filtering operations in a single pass through a fragment-processing pipeline within the programmable graphics processor. Video filtering functions such as deinterlacing, chroma up-sampling, scaling, and deblocking may be performed by the fragment-processing pipeline. The fragment-processing pipeline may be programmed to perform motion adaptive deinterlacing, wherein a spatially variant filter determines, on a pixel basis, whether a “bob”, a “blend”, or a “weave” operation should be used to process an interlaced image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07876378&OS=07876378&RS=07876378
owner: NVIDIA Corporation
number: 07876378
owner_city: Santa Clara
owner_country: US
publication_date: 20071214
---
This application is a divisional of co pending U.S. patent application Ser. No. 10 448 590 filed May 29 2003 which is herein incorporated by reference

The present invention relates to graphics processors and more particularly to filtering video data using said graphics processors.

Current graphics data processors have processing units that perform specific operations on graphics data including linear interpolation bilinear interpolation and trilinear interpolation. Video filtering for processing video data and displaying high quality images requires filtering the video data using more than two taps or using non linear filters. Current video filtering systems use dedicated processors to perform video filtering operations and generate high quality images. The high quality images are integrated with data output by a graphics data processor using an overlay technique. Recently graphics data processors may be programmed using shader programs and vertex programs permitting a wider variety of functions to be performed using these programmable graphics processors.

Accordingly video filtering has computational requirements that it would be desirable to meet by application of the computational resources of a programmable graphics processor.

New systems and methods for processing of video data are disclosed. The systems include a programmable graphics processor configured to produce filtered video data. In various embodiments the video data is stored in a format suitable as an input to a shading pipeline within the programmable graphics processor. For example in some embodiments video data is saved in a texture data format. Video data may therefore be manipulated using the shading pipeline to produce filtered video data.

The shading pipeline receives an input video signal having a plurality of odd and even fields. The shading pipeline applies a spatially variant deinterlacing function on each pixel within a frame containing an odd field and an even field to produce deinterlaced video frames.

Alternatively the shading pipeline receives an input video signal having a plurality of odd and even frames into a shading pipeline. The shading pipeline applies a spatially variant deinterlacing function on the input video signal in a single pass through the shading pipeline to produce deinterlaced video frames.

Alternatively the shading pipeline receives pixel data associated with a pixel within a video frame. The shading pipeline extracts fractional portions from texture coordinates associated with the pixel to use as a filter kernel phase. The shading pipeline accesses a plurality of filter values using the filter kernel phase. The shading pipeline applies the plurality of filter values to the pixel data to produce scaled pixel data within a scaled video frame.

Alternatively the shading pipeline receives an input video signal into a shading pipeline. The shading pipeline applies a deblocking function on the input video signal in a single pass through the shading pipeline to produce deblocked video data.

Alternatively the shading pipeline receives an input video signal having a plurality of odd and even frames into a shading pipeline. The shading pipeline applies a spatially variant deinterlacing function on the input video signal using a shader program to produce deinterlaced video data.

Alternatively the shading pipeline receives pixel data associated with a pixel within a video frame into a shading pipeline. The shading pipeline extracts fractional portions from texture coordinates associated with the pixel to produce a filter kernel phase within the shading pipeline using a shader program. The shading pipeline accesses a plurality of filter values using the filter kernel phase using the shader program. The shading pipeline applies the plurality of filter values to the pixel data to produce scaled pixel data within a scaled video frame using the shader program.

Alternatively the shading pipeline receives an input video signal into a shading pipeline. The shading pipeline applies a deblocking function on the input video signal using a shader program.

The programmable graphics processor includes a digital video interface configured to receive digital video image data a memory interface configured to read digital image data from a graphics memory and the shading pipeline configured to perform at least one video filtering operation specified by shader program instructions on the digital video image data to produce each pixel in a filtered video frame.

A computing system includes a host processor a host memory a system interface and the programmable graphics processor. The host memory stores programs for the host processor. The system interface is configured to interface with the host processor. The programmable graphics processor includes a memory interface and the shading pipeline. The memory interface is configured to read digital video image data from a graphics memory and write digital video image data to the graphics memory. The shading pipeline is configured to generate filtered video data by performing at least one video filtering operation on the digital video image data specified by shader program instructions.

A data structure stored on a computer readable medium includes a video data location field a video data format field a deinterlacing enable field a scaling factor enable field a deblocking enable field a color space conversion enable field and a destination address field.

An application programming interface for a graphics processor includes a function call to configure a shading pipeline within the programmable graphics processor to filter video data.

A shader program comprises an instruction to extract a fractional portion of a source register and stores the fractional portion extracted in a destination register.

A computer program embodied in a computer readable medium for sequencing graphics processing includes a code segment for receiving a plurality of bits indicative of the location of video data a code segment for identifying a format used to represent the video data a code segment to enable a deinterlacing function to process the video data a code segment to enable a scaling function to process the video data a code segment to enable a deblocking function to process the video data and a code segment to enable a color space conversion function to process the video data.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

Host Computer may include a Video Unit that receives video image data and outputs digital video image data to System Interface . In one embodiment Video Unit includes a storage resource e.g. register file FIFO cache and the like and a synchronization unit to output successive frames of video data. Host computer communicates with Graphics Subsystem via System Interface and an Interface . Graphics Subsystem includes a Local Memory and a Programmable Graphics Processor . Programmable Graphics Processor uses memory to store graphics data and program instructions where graphics data is any data that is input to or output from the computation units within the Programmable Graphics Processor .

In addition to Interface Programmable Graphics Processor includes a Graphics processing Pipeline a Memory Controller and an Output Controller . Programmable Graphics Processor may also include Video Unit that receives video image data and outputs digital video image data to Memory Controller or Interface . Data and program instructions received at Interface may be passed to a Geometry Processor within Graphics processing Pipeline or written to Local Memory through a Memory Interface within Memory Controller . Memory Interface includes read interfaces and write interfaces that each generate address and control signals to Local Memory SR Storage Resources Interface and optional Video Unit . SR may include register files caches FIFOs first in first out and the like. In addition to communicating with Local Memory SR Video Unit and Interface Memory Controller also communicates with Graphics processing Pipeline and Output Controller through read and write interfaces in Graphics processing Pipeline and a read interface in Output Controller . The read and write interfaces in Graphics processing Pipeline and the read interface in Output Controller generate address and control signals to Memory Controller .

Within Graphics processing Pipeline Geometry Processor and a shading pipeline Fragment processing Pipeline perform a variety of computational functions. Some of these functions are table lookup scalar and vector addition multiplication division coordinate system mapping calculation of vector normals tessellation calculation of derivatives interpolation and the like. Program instructions specify the functions and the precision of computations to perform the functions. Geometry Processor and Fragment processing Pipeline are optionally configured such that data processing operations are performed in multiple passes through Graphics processing Pipeline or in multiple passes through Fragment processing Pipeline . Each pass through Programmable Graphics Processor Graphics processing Pipeline or Fragment processing Pipeline concludes with optional processing by a Raster Analyzer . A pass is considered completed when processed data is output by Raster Analyzer .

Geometry Processor receives a stream of program instructions state bundles and data from Interface Memory Controller or SR and performs vector floating point operations or other processing operations using the data. State bundles are microcoded instructions used to configure subunits within Geometry Processor Rasterizer and Fragment processing Pipeline . Codewords are also microcoded instructions containing information used to configure subunits within Geometry Processor Rasterizer and Fragment processing Pipeline . Geometry Processor generates codewords from vertex program instructions. Codewords generated in Geometry Processor are used by subunits within Geometry Processor and Rasterizer . State bundles are used by subunits within Geometry Processor Rasterizer and Fragment processing Pipeline . Alternatively configuration information is written to registers within Geometry Processor Rasterizer and Fragment processing Pipeline using program instructions encoded with the data or the like.

The program instructions state bundles and data are stored in graphics memory. Graphics memory is any memory used to store graphics data or program instructions to be executed by the graphics processor. Graphics memory may include portions of system memory local memory directly coupled to the graphics processor register files coupled to the computation units within the graphics processor and the like such as portions of Host Memory Local Memory or SR within Programmable Graphics Processor . When a portion of Host Memory is used to store program instructions state bundles and data the portion of Host Memory may be uncached so as to increase performance of access by Programmable Graphics Processor .

Data processed by Geometry Processor together with state bundles shader program instructions and codewords are passed from Geometry Processor to a Rasterizer . Shader programs are sequences of shader program instructions compiled by Host Processor for execution within Fragment processing Pipeline . In a typical embodiment Rasterizer performs scan conversion on the data processed by Geometry Processor and outputs fragment data pixel data or sample data state bundles and shader program instructions to Fragment processing Pipeline . Each fragment is at least a portion of a pixel. Therefore a shader program programs Fragment processing Pipeline to operate on fragment pixel or sample digital image data. For simplicity the remainder of this description will use the term fragment data to refer to pixel data sample data and fragment data.

Shader programs program as discussed below the Fragment processing Pipeline to process fragment data by specifying computations and computation precision. A Fragment Shader within Fragment processing Pipeline generates codewords from shader program instructions. Fragment Shader optionally is configured by codewords generated in Fragment Shader from shader program instructions such that fragment data processing operations are performed in multiple passes within Fragment Shader . Fragment Shader and Fragment processing Pipeline may receive digital video image data from optional Video Unit through Memory Controller .

Fragment data processed by Fragment Shader is passed to Raster Analyzer which optionally performs near and far plane clipping color space conversion and raster operations such as stencil z test and the like and saves the results i.e. filtered image data in graphics memory. Raster Analyzer includes a read interface and a write interface to Memory Controller through which Raster Analyzer accesses data stored in Local Memory or Host Memory . In various embodiments the data output from Raster Analyzer is represented in a high precision data format specified by shader program instructions such as 16 32 64 128 bit or higher precision fixed point or floating point. Therefore in various embodiments Programmable Graphics Processor is a high precision programmable graphics processor.

Video filtering operations as hereinafter described may use high precision data formats to generate filtered images including filtered video data filtered frames intended for output e.g. read from graphics memory or output by Output Controller such as one frame in a film sequence of frames. Several intermediate filtered images may be generated before the final filtering operation to generate the filtered frame is performed. Furthermore each filtered image that is output as a filtered frame may be further filtered to generate additional filtered frames. Each filtered frame may be represented in a video format for display or further processing as a video frame. Each pass through Graphics processing Pipeline or Fragment processing Pipeline is considered complete when data is output by Raster Analyzer to be stored in graphics memory.

In various embodiments Memory Controller Local Memory and Geometry Processor are configured such that data generated at various points along Graphics processing Pipeline may be output via Raster Analyzer during a pass and provided to Geometry Processor or Fragment processing Pipeline as input during a subsequent pass. Since the output of Raster Analyzer may include floating point data formats data is optionally provided to Geometry Processor or Fragment processing Pipeline without loss of precision. Furthermore data is optionally processed in multiple passes through Graphics processing Pipeline without a loss of precision.

When processing is completed an Output of Graphics Subsystem is provided using Output Controller . Alternatively Host Processor reads a frame stored in Local Memory through Memory Controller Interface and System Interface . Output Controller reads a frame stored in Local Memory through Memory Controller to deliver data to a display device network electronic control system other Computing System other Graphics Subsystem or the like. Alternatively the data such as video frames are output to a film recording device or written to a peripheral device e.g. disk drive tape compact disk or the like.

When specified by the codewords received from Shader Core to do so Texture Unit reads map data height field bump texture video field data filter coefficients and the like and shader program instructions from Local Memory or Host Memory via Memory Controller using a RI Read Interface within Texture Unit . The map data stored in graphics memory may be generated by Programmable Graphics Processor by Host Processor by another device by a human or the like. The map data or shader program instructions are received by Texture Unit from Memory Controller . Texture Unit processes the map data using filtering functions such as trilinear interpolation bilinear interpolation anisotropic filtering poly phase filtering and the like as described further herein. The processed map data is output to a Remap along with the shader program instructions. The shader program instructions specify the computations precision of the computations and the precision of the output s of the computations performed by PCUs . An IPU within Remap interprets the shader program instructions and generates codewords that control the processing completed by PCUs in Fragment processing Pipeline .

When multi pass operations are being performed within Fragment Shader Remap also reads the data fed back from Combiners via a Quad Loop Back synchronizing the fed back data with the processed map data and shader program instructions received from Texture Unit . Remap formats the processed map data and fed back data outputting codewords and formatted data to Shader Back End . Shader Back End receives fragment data from Shader Core via Core Back End FIFO and triangle data from Gate Keeper the triangle data was received by Gate Keeper from Shader Triangle Unit . Shader Back End synchronizes the fragment and triangle data with the formatted data from Remap . Shader Back End performs computations in PCUs using the input data formatted data fragment data and triangle data based on codewords received from Remap . Shader Back End outputs codewords and shaded fragment data.

The output of Shader Back End is input to Combiners where the codewords are executed by PCUs within Combiners that in turn output combined fragment data. Operations performed to produce the combined fragment data include selection subtraction addition and multiplication. The codewords executing in the current pass control whether the combined fragment data will be fed back within Fragment Shader to be processed in a subsequent pass. Combiners optionally output codewords to be executed by Shader Core and Texture Unit in a subsequent pass to Gate Keeper using Feedback Output . Combiners also optionally output combined fragment data to Quad Loop Back to be used by Remap in a subsequent pass. Finally Combiners optionally output combined fragment data e.g. x y color depth configuration control other parameters to Raster Analyzer .

Raster Analyzer includes a RI and a WR write interface to Memory Controller . Raster Analyzer performs raster operations such as stencil test z test blending and the like using the combined fragment data and pixel data stored in a location in graphics memory corresponding to the x y coordinates associated with the combined fragment data. The output data from Raster Analyzer is written back to the location in graphics memory corresponding to the x y coordinates associated with the output data. The output data is represented in one or more formats as specified by the codewords. For example color data may be written as 16 32 64 or 128 bit per pixel fixed point or floating point values to be scanned out for display. Specifically four 16 bit floating point components are combined forming 64 bits of color data for each pixel. Likewise four 32 bit floating point components are combined forming 128 bits of color data for each pixel. Combinations of various numbers of bits of floating point or fixed point components may be combined to form 16 32 64 or 128 or more bit color data formats.

The color data may represent RGBA red green blue and alpha YIQ YUVA YCbCr or the like. The Y component in YUV YIQ and YCrCb representations is luminance and U V I Q Cr and Cb are chrominance components. R G and B components are a combination of luminance and chrominance. Color data may be stored in graphics memory to be used as a texture map by a shader program executed in a subsequent pass through Fragment processing Pipeline or through Graphics processing Pipeline . Alternatively color and depth data may be written and later read and processed by Raster Analyzer to generate the final pixel data prior to being scanned out for display via Output Controller .

During the transfer of film to video each film frame is decomposed into an odd field including odd horizontal scan lines from a film frame and an even field including even horizontal scan lines from the film frame for display on an interlaced display such as an NTSC video display. Information may be encoded with the data for use during deinterlacing or chroma up sampling e.g. a 3 2 pulldown flag a progressive frame flag a repeat first field flag. Each field within a film frame is captured at the same moment in time therefore when odd and even field pairs for film frames are displayed simultaneously as on a progressive display spatial distortion visual artifacts such as blurring are not introduced. In contrast each field within a video frame is captured at a different point in time for interlaced display. When video fields are displayed simultaneously spatial distortion visual artifacts may be introduced. When an odd and an even field from different film frames are displayed simultaneously spatial distortion visual artifacts may also be introduced for example during progressive display of 3 2 pulldown processed film frames. A video filtering technique known in the art as deinterlacing may be used to process video frames or 3 2 pulldown processed film frames for progressive display.

Fragment processing Pipeline may be programmed to perform video filtering functions such as deinterlacing deblocking scaling and the like on multiple video fields stored in graphics memory. In some embodiments of Programmable Graphics Processor the video filtering functions are performed in real time. The multiple video fields are filtered in Fragment processing Pipeline to generate filtered frames e.g. deinterlaced frames deblocked frames scaled frames and the like. Depending on the number of pixels video fields frames to be filtered or the number of filtering operations to be performed multiple passes within Fragment Shader may be needed to execute the shader program. Multiple passes within Fragment Shader may be completed during a single pass through Fragment processing Pipeline . At the completion of a pass through Fragment processing Pipeline the filtered frame is output by Raster Analyzer and stored in graphics memory. Other filtering functions include gamma correction reverse gamma correction chroma up sampling color space conversion and the like encoded in a shader program.

Chroma up sampling is used in the art to convert 4 2 0 format video data into 4 2 2 format video data or 4 4 4 format video data. For example chrominance data e.g. Cr and Cb or U and V for two scanlines within 4 2 2 format video data is interpolated to generate an additional scaline of chrominance data positioned between the two scanlines for 4 4 4 format video data. In another example chrominance data for two pixels within a scanline is interpolated to generate chrominance data for an additional pixel positioned between the two pixels. Chrominance data is generated to double the chrominance resolution converting 4 2 0 format video data into 4 2 2 format video data. The 4 2 2 format video data may be converted to 4 4 4 format video data as described above. In some embodiments chroma up sampling is performed on video frames and in other embodiments chroma up sampling is performed on video fields that are combined to produce chroma up sampled video frames.

Continuing in step Shader Back End determines if the codewords specify to perform a deinterlacing function and if so in step at least one PCU in Shader Back End processes the video image data or the chroma up sampled video data to produce deinterlaced video data and proceeds to step . If in step Shader Back End determines the codewords do not specify to perform a deinterlacing function Shader Back End also proceeds to step . In step Shader Back End determines if the codewords specify to perform a deblocking function and if so in step at least one PCU in Shader Back End processes the video image data the chroma up sampled video data or the deinterlaced video data to produce deblocked video data and proceeds to step . If in step Shader Back End determines the codewords do not specify to perform a deblocking function Shader Back End also precedes to step . In an alternate embodiment steps and precede steps and .

In step Shader Back End determines if the codewords specify to perform a scaling function and if so in step at least one PCU in Shader Back End processes the video image data the chroma up sampled video data the deinterlaced video data or the deblocked video data applying a video filter to produce scaled video data and proceeds to step . In an alternate embodiment in step Shader Back End outputs the video image data the chroma up sampled video data the deinterlaced video data or the deblocked video data to Combiners and Combiners determines if the codewords specify to perform a scaling function and if so in step at least one PCU in Combiners converts the video image data the chroma up sampled video data the deinterlaced video data or the deblocked video data from one color format to another color format to produce the scaled video data and proceeds to step . If in step Shader Back End determines the codewords do not specify to perform a scaling function Shader Back End also precedes to step . In the alternate embodiment if in step Combiners determines the codewords do not specify to perform a scaling function Combiners also proceed to step .

In step Shader Back End determines if the codewords specify to perform a color space conversion function and if so in step at least one PCU in Shader Back End converts the video image data the chroma up sampled video data the deinterlaced video data the deblocked video data or the scaled video data from one color format to another color format to produce color converted video data and proceeds to step . In the alternate embodiment in step Combiners determines if the codewords specify to perform a color space conversion function and if so in step at least one PCU in Combiners converts the video image data the chroma up sampled video data the deinterlaced video data the deblocked video data or the scaled video data from one color format to another color format to produce color space converted video data and proceeds to step . In a further alternate embodiment in step Combiners outputs the video image data the chroma up sampled video data the deinterlaced video data the deblocked video data or the scaled video data to Raster Analyzer and Raster Analyzer determines if the codewords specify to perform a color space conversion function and if so in step Raster Analyzer converts the video image data the chroma up sampled video data the deinterlaced video data the deblocked video data or the scaled video data from one color format to another color format to produce color space converted video data and proceeds to step . If in step Shader Back End determines the codewords do not specify to perform a color space conversion function Shader Back End also precedes to step . In the alternate embodiment if in step Combiners determines the codewords do not specify to perform a color space conversion function Combiners also precedes to step . In the further alternate embodiment if in step Raster Analyzer determines the codewords do not specify to perform a color space conversion function Raster Analyzer also precedes to step .

Steps and may be completed in a single pass through Fragment processing Pipeline to produce the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color space converted video data. The single pass through Fragment processing Pipeline may include multiple passes within Fragment Shader through Quad Loop Back or Feedback Output to produce the deinterlaced video data the deblocked video data the scaled video data or the color space converted video data.

In step output frames of the video image data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color converted video data are output by Shader Back End through Combiners and Raster Analyzer to Memory Controller . In the alternate embodiment in step the output frames of the video image data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color converted video data are output by Combiners through Raster Analyzer to Memory Controller . In the further alternate embodiment in step the output frames of the video image data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color converted video data are output by Raster Analyzer to Memory Controller . The output frames are stored in graphics memory and may be output for display or to a film recording device by Output Controller or written to a peripheral device e.g. disk drive tape compact disk or the like using Interface .

Gamma correction is conventionally used to convert color data from a linear scale to a non linear scale for display on a monitor. Pre gamma corrected data should be reverse gamma corrected prior to performing any operation that affects light intensity such as filtering overlaying transforming and the like. Performing operations that affect light intensity using gamma corrected data may result in visual artifacts particularly on boundaries between regions of high color contrast. Processed reverse gamma corrected data should be gamma corrected prior to display on a non linear display device such as a monitor.

In step Shader Back End determines if the codewords specify to perform a chroma up sampling function and if so in step PCUs within Shader Back End are configured to chroma up sample the video fields producing chroma up sampled video data and proceeds to step . In step Shader Back End determines if the codewords specify to perform a deblocking function and if so in step at least one PCU in Shader Back End processes the video image data or the chroma up sampled video data to produce deblocked video data and proceeds to step . If in step Shader Back End determines the codewords do not specify to perform a deblocking function Shader Back End also precedes to step .

In step Shader Back End determines if the codewords specify to perform a deinterlacing function and if so in step at least one PCU in Shader Back End processes the video image data the chroma up sampled video data or the deblocked video data to produce deinterlaced video data and proceeds to step . If in step Shader Back End determines the codewords do not specify to perform a deinterlacing function Shader Back End also proceeds to step . In an alternate embodiment steps and precede steps and .

Processing continues with steps and as described in relation to with the inclusion of the reverse gamma corrected video data optionally produced in step . In step Shader Back End determines if the codewords specify to perform a gamma correction function and if so in step at least one PCU in Shader Back End gamma corrects the video image data the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color converted video data to produce gamma corrected video data and proceeds to step .

In the alternate embodiment in step Combiners determines if the codewords specify to perform the gamma correction function and if so in step at least one PCU in Combiners gamma corrects the video image data the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color space converted video data to produce gamma corrected video data and proceeds to step .

In a further alternate embodiment in step Combiners outputs the video image data the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color space converted video data to Raster Analyzer and Raster Analyzer determines if the codewords specify to perform the gamma correction function and if so in step Raster Analyzer gamma corrects the video image data the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data or the color space converted video data to produce gamma corrected video data and proceeds to step .

If in step Shader Back End determines the codewords do not specify to perform a color space conversion function Shader Back End also precedes to step . In the alternate embodiment if in step Combiners determines the codewords do not specify to perform the gamma correction function Combiners also precedes to step . In the further alternate embodiment if in step Raster Analyzer determines the codewords do not specify to perform a gamma correction function Raster Analyzer also precedes to step .

Steps and may be completed in a single pass through Fragment processing Pipeline to produce the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data the color space converted video data or the gamma corrected video data. The single pass through Fragment processing Pipeline may include multiple passes within Fragment Shader through Quad Loop Back or Feedback Output to produce the deinterlaced video data the deblocked video data the scaled video data the color space converted video data or the gamma corrected video data.

In step output frames of the video image data the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data the color converted video data or the gamma corrected video data are output by Shader Back End through Combiners and Raster Analyzer to Memory Controller .

In the alternate embodiment in step the output frames of the video image data the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data the color converted video data or the gamma corrected video data are output by Combiners through Raster Analyzer to Memory Controller .

In the further alternate embodiment in step the output frames of the video image data the reverse gamma corrected video data the chroma up sampled video data the deinterlaced video data the deblocked video data the scaled video data the color converted video data or the gamma corrected video data are output by Raster Analyzer to Memory Controller . The output frames are stored in graphics memory and may be output for display or to a film recording device by Output Controller or written to a peripheral device e.g. disk drive tape compact disk or the like using Interface .

An application programming interface API includes a function call used to configure Fragment processing Pipeline to filter video data. A graphics application executed by Host Processor in may issue the function call to configure Fragment processing Pipeline within Programmable Graphics Processor to process the video data using one or more of a variety of video filtering functions. The video filtering functions may include a reverse gamma correction function a deinterlacing function a scaling function a deblocking function a color space conversion function a chroma up sampling function or a gamma correction function. In one embodiment the function call is used to configure Fragment processing Pipeline to process the video data using a deinterlacing function and a deblocking function. In another embodiment the function call is used to configure Fragment processing Pipeline to process the video data using a scaling function a color space conversion function and a chroma up sampling function.

A device driver executed by Host Processor communicates between Programmable Graphics Processor and the API. In one embodiment the function call communicates with Programmable Graphics Processor via the device driver to modify bits in a register that is readable by Fragment processing Pipeline and the bits correspond to the variety of video filtering functions. In another embodiment the function call is a data structure including fields containing source and destination locations and enable fields for video filtering functions. A video data location field specifies a location in graphics memory where source video data is stored. A destination address field specifies a location in graphics memory where filtered video data produced by processing the source video data is stored. A video data format field specifies the data format used to represent the source video data. Enable fields such as a deinterlacing enable field a scaling enable field a deblocking enable field a color space conversion enable field and the like each correspond to a video filtering function and are used to control whether or not the corresponding video filtering function is used to process the source video data.

In step Texture Unit reads two adjacent vertically aligned pixels from graphics memory using RI as specified by the shader program. One pixel prevPixOdd is read from an odd scanline in a previous frame of video data e.g. Frame and another vertically aligned pixel prevPixEven is read from an even scanline from the previous frame of video data. Texture Unit outputs prevPixOdd and prevPixEven to Shader Back End . In step Texture Unit reads two more adjacent vertically aligned pixels from graphics memory using RI as specified by the shader program. One pixel CurPixOdd is read from an odd scanline in a frame of video data e.g. Frame and another vertically aligned pixel CurPixEven is read from an even scanline from the frame of video data. Texture Unit outputs curPixOdd and curPixEven to Shader Back End .

In step Shader Back End is configured by codewords to compute diffCur by subtracting the luminance of the pixCurEven and the luminance of the pixPrevEven to produce a difference and taking the absolute value of the difference to produce diffCur. In step Shader Back End is configured by codewords to compute diffCurChroma by subtracting the chrominance of the pixCurEven and the chrominance of the pixPrevEven to produce a difference and taking the absolute value of the difference to produce diffCurChroma. In step Shader Back End is configured by codewords to compute diffDl by subtracting the luminance of the pixCurOdd and the luminance of the pixPrevOdd to produce a difference and taking the absolute value of the difference to produce diffDl. In step Shader Back End is configured by codewords to compute diffDlChroma by subtracting the chrominance of the pixCurOdd and the chrominance of the pixPrevOdd to produce a difference and taking the absolute value of the difference to produce diffDlChroma.

In step Shader Back End is configured by codewords to compute diffHighFreq. pixPrevOdd is subtracted from an average of pixCurOdd and pixCurEven to produce a difference and taking the absolute value of the difference to produce diffHighFreq. In step Shader Back End is configured by codewords to determine the greatest of diffCur diffDl half of diffCurChroma and half of diffDlChroma and set maxDiff equal to that value. In step Shader Back End is configured by codewords to compare maxDiff to a diffThreshold. diffThreshold may be programmed or fixed. If maxDiff is not less than diffTHreshold in step Shader Back End is configured by codewords to perform a bob operation averaging pixCurEven and pixCurOdd to produce a color value for a corresponding pixel in the deinterlaced frame. Shader Back End outputs the deinterlaced pixel to graphics memory through Combiners and Raster Analyzer . In an alternate embodiment Shader Back End is configured by the codewords to process the deinterlaced pixel to produce a deblocked pixel a scaled pixel a color space converted pixel or a gamma corrected pixel.

In step Fragment Shader determines if the shader program specifies that there is another pixel to be deinterlaced and if so returns to step . Steps through are repeated for the other pixel. In step Shader Back End is configured by codewords to compare maxDiff to a diffThreshold and If maxDiff is less than diffTHreshold in step Shader Back End is configured by codewords to compare diffHighFreq to a diffHFThreshold. diffHFThreshold may be programmed or fixed. If diffHighFreq is not less than diffTHreshold in step Shader Back End is configured by codewords to perform a blend operation averaging a corresponding deinterlaced pixel in the previous frame with an average of pixCurEven and pixCurOdd to produce a color value for a corresponding pixel in the current deinterlaced frame. Shader Back End outputs the deinterlaced pixel to graphics memory through Combiners and Raster Analyzer . In an alternate embodiment Shader Back End is configured by the codewords to process the deinterlaced pixel to produce a deblocked pixel a scaled pixel a color space converted pixel or a gamma corrected pixel.

In step Fragment Shader determines if the shader program specifies that there is another pixel to be deinterlaced and if so returns to step . Steps through are repeated for the other pixel. In step Shader Back End is configured by codewords to compare maxDiff to a diffThreshold and If maxDiff is less than diffTHreshold in step Shader Back End is configured by codewords to compare diffHighFreq to a diffHFThreshold. diffHFThreshold may be programmed or fixed. If diffHighFreq is less than diffTHreshold in step Shader Back End is configured by codewords to perform a weave operation selecting the color of a corresponding deinterlaced pixel in the previous frame as the color of the pixel in the current deinterlaced frame. Shader Back End outputs the pixel in the current deinterlaced frame to graphics memory through Combiners and Raster Analyzer . In an alternate embodiment Shader Back End is configured by the codewords to process the pixel to produce a deblocked pixel a scaled pixel a color space converted pixel or a gamma corrected pixel.

In step Fragment Shader determines if the shader program specifies that there is another pixel to be deinterlaced and if not in step the current deinterlaced frame is read from graphics memory by Output Controller for display or for output to a film recording device a peripheral device e.g. disk drive tape compact disk or the like using Interface .

In an alternate embodiment the one or more motion vectors are converted into texture coordinate offsets that are applied to translate the texture coordinates s and t used by Texture Unit to read Frame stored in graphics memory as a texture map in step . In a further alternate embodiment in step a plurality of motion vectors are computed by Host Computer or Programmable Graphics Processor for Frame relative to Frame and stored as a texture coordinate displacement map in graphics memory. In the further alternate embodiment in step texture coordinate displacements are read from the texture coordinate displacement map and applied to texture coordinates associated with Frame to map Frame to a geometry e.g. rectangle polygon mesh or the like such that Region aligns with Region in Frame when Frame is read as a texture map by Texture Unit in step .

Continuing in step Texture Unit reads two adjacent vertically aligned pixels within Frame stored as a texture map. Steps through and step are completed as described in relation to to perform motion adaptive filtering to deinterlace Frame using a motion compensated Frame .

A pixel in a source image stored as a texture map may be filtered with the filter function centered on the pixel to produce a pixel in an up scaled image e.g. destination image using a filter kernel phase of zero. Pixels neighboring the pixel in the destination image may be produced by applying the filter function to the pixel in the source image using a non zero filter kernel phase. The filter function is effectively stepped across the source image using different filter kernel phases to produce neighboring pixels in the destination image. Likewise during down scaling the filter function is effectively stepped in larger size steps across the source image reading and filtering multiple pixels to produce each pixel in the destination image.

In step a filter kernal phase is obtained for a pixel in the destination image. In one embodiment the filter kernal phase for each pixel is determined by Host Computer and stored as a filter kernal phase texture map in graphics memory. Texture Unit reads the filter kernal phase for each pixel in the destination image from the filter kernal phase texture map to obtain the filter kernel phase for each pixel in the destination image. In another embodiment the filter kernal phase for each pixel is computed by Fragment processing Pipeline as specified by the shader program. For example texture coordinates ranging in value from zero to less that one may be computed by configuring one or more PCUs in either Shader Core or Shader Back End to interpolate texture map coordinates across a geometry as specified by the shader program. The computed texture coordinates are the filter kernal phases needed to access the filter texture map. The computed filter kernal phases may be stored in graphics memory as a filter kernal phase texture map to be read by Texture Unit or fed back within Fragment Shader to Texture Unit using Feedback Output .

In yet another embodiment a fractional portion of each scaling filter texture map coordinate for each destination pixel is extracted within Texture Unit using an FRC fraction program instruction. The fractional portions are used as the filter kernel phase for accessing the filter texture map. The FRC program instruction sets a destination register to a fractional portion of a source register such as a source register containing a texture coordinate. The fractional portion represents a value greater than or equal to zero and less than one. The data in the source register may be represented in a floating point format or a point format.

In step Texture Unit is configured to read filter function coefficients f f f f and the like stored in the filter texture map using the filter kernel phase phase obtained in step as specified by the program instructions. In step Texture Unit is configured to read the source image pixels source pixel source pixel source pixel source pixel and the like from the source image texture map stored in graphics memory as specified by the program instructions. In step PCUs within Shader Back End are configured by the codewords to apply the scaling filter function to the source image pixels read by Texture Unit in step to compute an output pixel color. For example the following equation may be evaluated to determine the output pixel color Output pixel source pixel phase source pixel phase source pixel phase 

Alternatively in step PCUs within Combiners are configured by the codewords to apply the scaling filter function to the source image pixels read by Texture Unit in step to compute an output pixel color.

In alternate embodiments the output pixel color is computed using two or more passes within Fragment Shader using Feedback Output to pass partially computed output pixel colors filter function texture map coordinates and source image texture map coordinates to Texture Unit to read additional filter function coefficients and source image pixels.

In step Fragment Shader determines if the shader program specifies to compute another destination image pixel and if so Texture Unit repeats steps and to compute the other destination image pixel. In step Fragment Shader determines if the shader program specifies that there is another destination pixel to be computed and if not in step the destination image scaled source image is read from graphics memory by Output Controller for display or for output to a film recording device a peripheral device e.g. disk drive tape compact disk or the like using Interface .

In step if Fragment Shader determines the first source pixel lies along a vertical boundary in step Texture Unit is configured by the codewords to read the first source pixel and neighboring source pixels as specified by the program instructions and output the first source pixel and neighboring source pixels to Shader Back End . Shader Back End is configured by the codewords to compute the color of a first destination pixel by filtering the first source pixel and the neighboring source pixels forming a j k region. For example the color of Destination Pixel may be computed by filtering a 2 2 region including source pixels associated with x y coordinates of Destination Pixel Destination Pixel Destination Pixel and Destination Pixel . In another example the color of Destination Pixel may be computed by filtering a 3 2 region including source pixels associated with x y coordinates of Destination Pixel Destination Pixel Destination Pixel Destination Pixel Destination Pixel and Destination Pixel .

In step Fragment Shader determines if the shader program specifies that there is another source pixel to be deblocked and if so returns to step . Step is repeated for the second source pixel and in step Fragment Shader determines a second source pixel does not lie along a vertical boundary. In step Texture Unit is configured by the codewords to read the second source pixel and neighboring vertically aligned source pixels as specified by the program instructions and output the second source pixel and neighboring vertically aligned source pixels to Shader Back End . Shader Back End is configured by the codewords to compute the color of a second destination pixel by filtering the second source pixel and the neighboring vertically aligned source pixels forming a 1 j region. For example the color of a destination pixel within Region may be computed by filtering the two source pixels within Region . In another example the color of a destination pixel within Region may be computed by filtering the eight source pixels within Region .

In step Fragment Shader determines if the shader program specifies that there is another source pixel to be deblocked and if so returns to step . In step Fragment Shader is configured by the codewords to determine if a third source pixel lies along a horizontal block encoding boundary and if not in step Fragment Shader determines if the third source pixel lies along a vertical block encoding boundary. In step if Fragment Shader determines the third source pixel does not lie along a vertical block encoding boundary Texture Unit outputs the third source pixel to Shader Back End . Shader Back End is configured by the codewords to output the third source pixel as the third destination pixel and Fragment Shader proceeds to step .

In step if Fragment Shader determines the third source pixel lies along a vertical block encoding boundary in step Texture Unit is configured by the codewords to read the third source pixel and neighboring horizontally aligned source pixels as specified by the program instructions and output the third source pixel and neighboring horizontally aligned source pixels to Shader Back End . Shader Back End is configured by the codewords to compute the color of a third destination pixel by filtering the third source pixel and the neighboring horizontally aligned source pixels forming a j 1 region. For example the color of a destination pixel within Region may be computed by filtering the two source pixels within Region .

In step Fragment Shader determines if the shader program specifies that there is another pixel to be deblocked and if not in step the destination image deblocked source image is read from graphics memory by Output Controller for display or for output to a film recording device a peripheral device e.g. disk drive tape compact disk or the like using Interface .

In an alternate embodiment Fragment Shader compares color data associated with a source pixel to color data associated with several neighboring pixels on either side of the block encoding boundary to produce the color difference value. If in step Fragment Shader determines the color difference value is not within the tolerance in step Fragment Shader determines if the first source pixel lies along a vertical block encoding boundary. If in step Fragment Shader determines the color difference value is within the tolerance Fragment Shader proceeds to step as described further herein.

Continuing in step if Fragment Shader determines the first source pixel lies along a vertical boundary in step Fragment Shader compares color data associated with the first source pixel to color data associated with an adjacent source pixel on the other side of the vertical block encoding boundary to produce a color difference value. Fragment Shader determines if the color difference value is within the tolerance. If in step Fragment Shader determines the color difference value is not within the tolerance in step Texture Unit is configured by the codewords to read the first source pixel and neighboring source pixels as specified by the program instructions and output the first source pixel and neighboring source pixels to Shader Back End . Shader Back End is configured by the codewords to compute the color of a first destination pixel by filtering the first source pixel and the neighboring source pixels forming a j k region. If in step Fragment Shader determines the color difference value is within the tolerance Texture Unit outputs the first source pixel to Shader Back End . Shader Back End is configured by the codewords to output the first source pixel as the first destination pixel.

In step Fragment Shader determines if the shader program specifies that there is another source pixel to be deblocked and if so returns to step . Step is repeated for the second source pixel and in step Fragment Shader compares color data associated with the second source pixel to color data associated with an adjacent source pixel on the other side of the horizontal block encoding boundary to produce a color difference value. If in step Fragment Shader determines the color difference value is not within the tolerance in step Fragment Shader determines if the second source pixel lies along a vertical block encoding boundary.

In step Fragment Shader determines a second source pixel does not lie along a vertical boundary and in step Texture Unit is configured by the codewords to read the second source pixel and neighboring vertically aligned source pixels as specified by the program instructions. Texture Unit outputs the second source pixel and neighboring vertically aligned source pixels to Shader Back End . Shader Back End is configured by the codewords to compute the color of a second destination pixel by filtering the second source pixel and the neighboring vertically aligned source pixels forming a 1 j region. For example the color of a destination pixel within Region may be computed by filtering the two source pixels within Region .

In step Fragment Shader determines if the shader program specifies that there is another source pixel to be deblocked and if so returns to step . In step Fragment Shader is configured by the codewords to determine if a third source pixel lies along a horizontal block encoding boundary and if not in step Fragment Shader determines if the third source pixel lies along a vertical block encoding boundary. In step if Fragment Shader determines the third source pixel does not lie along a vertical block encoding boundary Texture Unit outputs the third source pixel to Shader Back End . Shader Back End is configured by the codewords to output the third source pixel as the third destination pixel and Fragment Shader proceeds to step .

In step if Fragment Shader determines the third source pixel lies along a vertical block encoding boundary in step Fragment Shader compares color data associated with the third source pixel to color data associated with an adjacent source pixel on the other side of the vertical block encoding boundary to produce a color difference value. Fragment Shader determines if the color difference value is within the tolerance.

If in step Fragment Shader determines the color difference value is not within the tolerance in step Fragment Shader is configured by the codewords to read the third source pixel and neighboring horizontally aligned source pixels as specified by the program instructions and output the third source pixel and neighboring horizontally aligned source pixels to Shader Back End . Shader Back End is configured by the codewords to compute the color of a third destination pixel by filtering the third source pixel and the neighboring horizontally aligned source pixels forming a j 1 region. If in step Fragment Shader determines the color difference value is within the tolerance Texture Unit outputs the third source pixel to Shader Back End . Shader Back End is configured by the codewords to output the third source pixel as the third destination pixel.

In step Fragment Shader determines if the shader program specifies that there is another pixel to be deblocked and if not in step the destination image deblocked source image is read from graphics memory by Output Controller for display or for output to a film recording device a peripheral device e.g. disk drive tape compact disk or the like using Interface .

While foregoing is directed to embodiments in accordance with one or more aspects of the present invention other and further embodiments of the present invention may be devised without departing from the scope thereof which is determined by the claims that follow. Claims listing steps do not imply any order of the steps unless such order is expressly indicated.

