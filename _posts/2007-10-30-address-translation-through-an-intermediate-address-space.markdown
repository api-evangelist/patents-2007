---

title: Address translation through an intermediate address space
abstract: In a data processing system capable of concurrently executing multiple hardware threads of execution, an intermediate address translation unit in a processing unit translates an effective address for a memory access into an intermediate address. A cache memory is accessed utilizing the intermediate address. In response to a miss in cache memory, the intermediate address is translated into a real address by a real address translation unit that performs address translation for multiple hardware threads of execution. The system memory is accessed with the real address.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08966219&OS=08966219&RS=08966219
owner: International Business Machines Corporation
number: 08966219
owner_city: Armonk
owner_country: US
publication_date: 20071030
---
The present invention relates in general to data processing and in particular to address translation in a data processing system employing memory virtualization.

A computer system typically includes one or more processors coupled to a hierarchical data storage system. The computer system s hierarchy of data storage devices often comprises processor registers cache memory and system memory e.g. SRAM or DRAM as well as additional data storage devices such as hard disks optical media and or magnetic tapes.

Regardless of the computer system architecture that is employed it is typical that each processor accesses data residing in memory mapped storage locations whether in physical system memory cache memory or another system resource by utilizing real or physical addresses to identify the storage locations of interest. An important characteristic of real or physical addresses is that there is a unique real address for each memory mapped physical storage location.

Because the one to one correspondence between memory mapped physical storage locations and real addresses necessarily limits the number of storage locations that can be referenced to 2 where N is the number of bits in the real address the processors of most commercial computer systems employ memory virtualization to enlarge the number of addressable locations. In fact the size of the virtual memory address space can be orders of magnitude greater than the size of the real address space. Thus in a conventional systems processors internally reference memory locations by the effective addresses and then perform effective to real address translations often via one or more virtual address spaces to access the physical memory locations identified by the real addresses.

In a virtual memory system a page frame and or block table is commonly maintained at least partially in system memory in order to track the mapping between the logical address space s and the physical address space. A typical entry in a page or block table includes a valid bit which indicates whether the page block is currently resident in system memory a dirty bit which indicates whether a program has modified the block protection bits which control access to the page block and a real page block number i.e. the physical address for the page block of virtual memory if the page block is resident in system memory.

To minimize the latency of address translation processors typically contain a number of address translation data structures that cache address translations for recently accessed memory pages. For example an exemplary computer system employing two level translation from effective addresses to virtual addresses to real addresses may include data and instruction effective to real address translation ERAT tables that buffer only the most recent translations to facilitate direct effective to real address translation a software managed segment lookaside buffer SLB that buffers recently used effective to virtual address translations and a hardware managed translation lookaside buffer TLB that buffers recently used virtual to real address translations. In addition some virtual memory systems provide an additional address translation buffer called a block address translation BAT buffer which serves as a TLB for variable sized memory blocks.

In operation when a processor generates the effective address of a memory access the processor performs an ERAT lookup. If the effective address hits in the ERAT the real address can be obtained relatively quickly. However if the effective address misses in the ERAT the SLB and TLB or BAT are accessed to perform a full effective to virtual to real address translation. If a miss occurs at this second level of address translation the translation hardware invokes a page table walk engine to access the required translation entry from cache or system memory. Once the real address is obtained the memory access is performed in cache memory or system memory.

As real memory capacities program footprints and user working sets continue to grow it is beneficial to increase the coverage of translation information buffered in a processor. Common approaches to increasing the translation coverage include increasing the number of ERAT SLB and TLB entries and supporting larger memory pages. For example in addition to conventional 4 kilobyte 4 KB and 16 KB pages many systems now additionally support page sizes of 1 megabyte MB 16 MB and 16 gigabyte GB . However increasing the number of ERAT SLB and TLB entries becomes expensive both in terms of chip area power dissipation and the latency to perform a search for a matching translation entry in a large translation data structure. In addition use of multiple memory page sizes and providing support for larger page sizes injects additional complexity into processor designs and can cause increased memory fragmentation.

According to one embodiment a data processing system capable of concurrently executing multiple hardware threads of execution includes an intermediate address translation unit in a processing unit translates an effective address for a memory access into an intermediate address. A cache memory is accessed utilizing the intermediate address. In response to a miss in cache memory the intermediate address is translated into a real address by a real address translation unit that performs address translation for multiple hardware threads of execution. The system memory is accessed with the real address.

With reference now to the figures wherein like reference numerals refer to like and corresponding parts throughout and in particular with reference to there is illustrated a high level block diagram depicting a first view of an exemplary data processing system in accordance with the present invention. In the exemplary embodiment data processing system includes multiple processing nodes for processing data and instructions. Processing nodes are coupled via a host fabric interface HFI to an interconnect fabric that supports data communication between processing nodes in accordance with one or more interconnect and or network protocols. Interconnect fabric may be implemented for example utilizing one or more buses switches and or networks.

Each processing node may be implemented for example as a single integrated circuit chip e.g. system on a chip SOC multi chip module MCM or circuit board which contains one or more processing units e.g. processing units for processing instructions and data. Each processing unit typically contains instruction sequencing logic one or more execution units for executing instruction as well as various buffers registers and other circuitry all realized in integrated circuitry. In many embodiments each processing unit can concurrently execute multiple concurrent hardware threads of execution.

As shown each processing unit is supported by cache memory which contains one or more levels of in line or lookaside cache. As is known in the art cache memories provide processing units with low latency access to instructions and data received from source s within the same processing node and or remote processing node s . The processing units within each processing node are further coupled to a local interconnect which may be implemented for example with one or more buses and or switches. Local interconnect is further coupled to HFI to support data communication between processing nodes .

As further illustrated in processing nodes typically include at least one memory controller which may be coupled to local interconnect to provide an interface to a respective physical system memory . In alternative embodiments of the invention one or more memory controllers can be coupled to interconnect fabric or directly to a processing unit rather than a local interconnect .

According to the present invention data processing system implements memory virtualization utilizing at least three address spaces. These address spaces include a first address space employed by software referred to herein as an effective address EA space a second address space utilized to index cache memories referred to herein as an intermediate address IA space and a third address space utilized to address locations in system memory referred to herein as a real address RA space. In common embodiments the IA space will be larger than or equal in size to the RA space.

As will be appreciated data processing system may employ additional address spaces in addition to the three address spaces previously enumerated. For example memory controller may employ a further level of address translation to map RAs to physical locations within the storage devices e.g. DIMMs comprising system memory and storage controllers not illustrated may employ a further level of address translation to map RAs to physical locations within the physical data storage media e.g. solid state drives optical or magnetic disks tape etc. .

In order to support translation of effective addresses EAs to intermediate addresses IAs each processing unit preferably includes a hardware intermediate address translation unit IATU that receives EAs as inputs and generates corresponding system wide unique IAs utilized to access cache memories . IATU is preferably able to translate the address for any cache line held in the associated cache memory without a miss or the need to access any other translation facility. In some embodiments IATU is implemented as a translation cache that caches page table entries from an operating system managed page table in system memory . In such embodiments the page table entry required to perform an EA to IA translation is retrieved from system memory as needed together with requested data. In other embodiments IATU may simply hash the EA utilizing a predetermined or software controlled hashing algorithm to obtain an IA without accessing a page table . In yet other embodiments IATU may perform translation by concatenating EAs with a prefix e.g. supplied by hardware software or firmware to obtain IAs.

Translation from intermediate addresses to real addresses is performed by a real address translation facility such as real address translation unit RATU . In at least some embodiments RATU is a software managed facility that is shared by multiple or all processing units in a processing node and is in communication with memory controller . RATU may be implemented in any of or a combination of hardware software and or firmware. Thus in the embodiment of RATU is implemented as hardware e.g. special purpose hardware or a dedicated processing unit that executes program code to perform translation of intermediate addresses into real addresses. In the alternative embodiment depicted in RATU is instead implemented in hardware and or firmware within memory controller . In the alternative embodiment illustrated in IA to RA translation is implemented in software for example as RATU program code executing on one or more processing units . For example if multiple concurrent hardware threads of execution are supported RATU program code may execute as one of the hardware threads of execution of a processing unit in order to provide IA to RA translation for one or more other hardware threads. These various embodiments of the real address translation facility are generally referred to herein as RATU .

The translation performed by RATU may employ any known or future developed technique or multiple techniques of address translation. These techniques may include translation by references to software managed page tables accesses to hardware managed translation caches address hashing etc. Following translation RATU outputs the real address for use in a memory access request targeting system memory . As will be appreciated the form in which the real address is output by RATU depends upon the selected implementation of RATU . If RATU is implemented as hardware as depicted as RATU of RATU may output the real address in a memory access request on local interconnect . If RATU is alternatively implemented in memory controller as depicted with RATU of RATU may directly utilize the real address to access system memory or utilize the real address to perform a further translation to a physical e.g. DIMM address. If RATU is alternatively implemented in software as depicted with RATU of RATU may output the real address in a processor register in a specified memory location e.g. in cache memory or in a message to cache memory . In this case hardware in processing unit and or cache memory can initiate a memory access request on local interconnect that specifies the real address as the request address.

Those skilled in the art will appreciate that processing system as depicted in various embodiments in can include many additional non illustrated components such as interconnect bridges non volatile storage ports for connection to networks or attached devices etc. Because such additional components are not necessary for an understanding of the present invention they are not illustrated in or discussed further herein.

Referring now to there is depicted a more detailed block diagram of a cache memory in accordance with the present invention. In the depicted embodiment cache memory includes one or more levels of data storage where each such level includes a data array for storing cache lines of data and or instructions a cache directory that records the tag portion of the intermediate address IA and state information associated with each cache line held in data array and a cache controller that controls operation of cache memory .

During operation of data processing system cache memory can receive processor memory access requests from its associated processor and in embodiments implementing snoop based coherency can also snoop interconnect memory access requests on local interconnect . As illustrated in each such memory access request includes a request address specified as an intermediate address IA which includes an address tag formed of the high order address bit an index formed on the mid order address bits and low order select bits . As indicated the index of each intermediate address received by cache is utilized to select a particular one of a plurality of sets in cache directory and data array . As is known in the art the address tag of the intermediate address is the utilized to determine which of the cache lines in the selected set if any is associated with requested intermediate address . Depending upon the type of memory access request and whether the request address hits in cache directory cache controller may for example supply requested data to the associated processor store specified data in data array issue a request on local interconnect and or update a cache line status within cache directory . In the case of a memory access request issued on local interconnect to access system memory the request address of the memory access request is translated into a real address for example by RATU .

With reference now to there is illustrated a software layer diagram of an exemplary software configuration of data processing system as embodied in any of . As illustrated software configuration has at its lowest level a system supervisor or hypervisor that allocates resources among one or more possibly heterogeneous operating systems concurrently executing within data processing system . The resources allocated to each operating system image may include hardware resources such as processing units network adapters non volatile storage etc. as well as specified ranges of effective intermediate and real or address spaces.

As further shown in each operating system image allocates addresses and other resources from the pool of resources allocated to it by hypervisor to various application programs and or middleware MW . Application programs which can be programmed to perform any of a wide variety of computational control communication data management and presentation functions comprise a number of user level processes . Each operating system image independently controls the operation of the hardware allocated to it creates and manages a page table if present governing EA to IA and or IA to RA translation and provides various application programming interfaces API through which operating system services can be accessed by application programs and middleware .

Referring now to there is depicted an exemplary address translation schema in accordance with the preset invention. In the depicted embodiment the effective address EA space comprises a plurality of memory pages . Memory pages may have any of a number of possibly different page sizes e.g. 4 KB 64 KB 1 MB 16 MB 1 GB 16 GB etc. or alternatively may all have uniform page size e.g. 16 GB . As described above and as shown in EAs in EA space that are specified by memory access requests of processing units are translated by IATUs into intermediate addresses IAs within IA space .

IAs within IA space can be translated to real addresses RAs within RA space in multiple ways. For example as shown in in some embodiments IA space includes a direct mapped region containing IAs that map directly with or without a hash to RAs in RA space with one to one correspondence. In such cases the request addresses of memory access requests to system memory can readily be obtained by processing units and or cache memories directly from the IAs generated by IATUs without translation by RAU . The portion of IA space within direct mapped region may be indicated for example by one or more address range registers in processing units and or cache memories . Additionally for IAs in a translated region of IA space RATU is invoked to perform IA to RA translation. It should be noted that in preferred embodiments translation by RATU is only employed as needed for memory access requests to system memory and is therefore generally performed for only a small percentage of the total number of memory access requests. As with EA to IA translation the IA to RA translations performed by RATU can be performed with any desired address granularity e.g. 4 KB 64 KB 1 MB 16 MB 1 GB 16 GB etc. .

As has been described in at least some embodiments the present invention provides an improved address translation schema in which an intermediate address translation unit is utilized to translate effective addresses of processing unit memory accesses into intermediate addresses utilized to access cache memory and a real address translation unit which may be shared by multiple hardware threads of execution is utilized to translate intermediate addresses into real addresses utilized to access system memory. This translation schema simplifies and reduces the die area of the circuitry utilized to implement address translation by moving the hardware real address translation structures e.g. the SLB and TLB out of each processing core and centralizing their functionality in a real address translation unit.

While embodiments of the present invention have been particularly shown and described it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention. For example although aspects of the present invention have been described with respect to a data processing system executing program code that directs the functions of the present invention it should be understood that present invention may alternatively be implemented as a program product for use with a data processing system. Program code defining the functions of the present invention can be delivered to a data processing system via a variety of signal bearing media which include without limitation data storage media e.g. CD ROM hard disk drive static memory and communication media such as digital and analog networks. It should be understood therefore that such signal bearing media when carrying or encoding computer readable instructions that direct the functions of the present invention represent alternative embodiments of the present invention.

