---

title: Distributed overlay multi-channel media access control for wireless ad hoc networks
abstract: Systems and methods for distributed overlay multi-channel MAC for wireless ad hoc networks are described. In one aspect, the systems and methods divide channel frequencies defined by a wireless network protocol into a single home channel and multiple guest channels that are orthogonal to the home channel. Each of the network nodes in the ad hoc network operates on the home channel for respective variable and overlapping amounts of time to maintain network connectivity with other respective network nodes. Additionally, each of the network nodes determines whether and when to switch from the home channel to a particular guest channel of the guest channels for a variable amount of time to increase data throughput over one or more corresponding communication links in the ad hoc network with other network node(s).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07720086&OS=07720086&RS=07720086
owner: Microsoft Corporation
number: 07720086
owner_city: Redmond
owner_country: US
publication_date: 20070319
---
IEEE 802.11 is a technology standard for WLAN Wireless LAN which has been widely used for both infrastructure and ad hoc networks. Nodes in an ad hoc network are typically configured to communicate with other node s in the ad hoc network over a single same channel frequency. With an increase in transmitting network nodes channel capacity eventually becomes a data throughput bottleneck.

To mitigate mutual data transmission interference on a same spectrum band the IEEE 802.11 standard provides for multiple orthogonal frequency channels essentially increasing network data throughput capacity. For example 802.11b defines eleven 11 channels three 3 of which are orthogonal non overlapped and 801.11a defines thirteen 13 orthogonal channels. shows ad hoc network with node pairs communicating on different orthogonal channels to reduce mutual transmission interference. In this example node pairs operate over channel node pairs operate over channel and node pairs operate over channel . Each of the arrows showing traffic patterns through include different fill e.g. hatching to represent different traffic volume across respective ones of the links between the illustrated nodes. In this scenario although each node pair can simultaneously transmit data a node communicating over a particular channel will not be able to communicate with any node configured to communicate over a different channel.

Software controlled channel assignment and switching techniques have been developed to address some of the described limitations. These conventional techniques allows nodes to communicate on a respective channel frequency and at the same time maintain connectivity to nodes configured to transmit over different channels to accommodate different traffic patterns by allowing. An exemplary such scenario is shown in where node on channel maintains link connectivity to both node on channel see traffic pattern and node on channel see traffic pattern . However existing channel switching techniques typically use a fixed pattern to emulate all possible arbitrary traffic and connectivity patterns in an ad hoc network. Such emulation is substantially problematic because it assumes that all nodes will follow similar same behavior which is not always the case.

One conventional channel switching technique for example provides for a transmitting node to switch channels after each packet transmission. Such packet by packet switching operations often result in prohibitive amounts of processing overhead due to the corresponding time consuming channel switching latencies. For instance packet transmission time in 802.11 is approximately 300 microseconds us . Even if a NIC can switch as fast as 100 us and two nodes can synchronize with ms accuracy packet switching overhead in such a packet b packet level channel switching scheme is still an excessive 33 . In another example a component based conventional channel switching technique configures all nodes in a particular communication session to transmit on a same channel. In this technique the channel does not change for the duration of the communication session. Although this essentially reduces channel switching latencies as compared to the packet by packet switching schemes this technique is least flexible in using channel diversity.

For example existing component based channel assignment techniques would not be able to achieve throughput gain for ad hoc network configurations such as shown in which represents a combination pattern of . Referring to traffic patterns through across respective ones of channels and are illustrated. As shown some nodes always communicate on the same channel. For instance nodes and in this example communicate on only channel see traffic pattern and nodes and communicate only on channel see traffic pattern . Other nodes work across multiple channels. For instance node in this example sometimes works on channel for instance to communicate with node see traffic pattern . Node of this example is also configured to operate over channel to communicate with node see traffic pattern and over channel to communicate with node see traffic pattern . Although existing MAC level channel switching schemes can achieve throughput gain and maintain node connectivity for the illustrative ad hoc network of existing MAC level channel switching schemes are somewhat limited in such scenarios.

For instance conventional MAC level channel switching schemes switch channels only at edges of predetermined and fixed time slots wherein all packets in a particular time slot are transmitted on the same channel. Such conventional super frame level schemes are limited. For instance existing MAC level channel switching schemes generally require a node to change channels consistently on a per super frame or time slot basis making such schemes too inflexible to accommodate large channel switching latencies. Additionally existing MAC level channel switching schemes divide a super frame into control and data time which may be substantially problematic. For instance such dividing a super frame in this manner may cause a underutilization of data time b substantial data throughput delays decreasing response time per hop due to a one service opportunity per super frame and c a control channel time bottleneck because each node is typically required to send a signaling message to contend access on a per super frame basis. Furthermore such MAC level schemes are incompatible with legacy 802.11 based systems either requiring modifications to the 802.11 MAC e.g. extending the power saving mode or use of a proprietary MAC.

Systems and methods for distributed overlay multi channel MAC for wireless ad hoc networks are described. In one aspect the systems and methods divide channel frequencies defined by a wireless network protocol into a single home channel and multiple guest channels that are orthogonal to the home channel. Each of the network nodes in the ad hoc network operates on the home channel for respective variable and overlapping amounts of time to maintain network connectivity with other respective network nodes. Additionally each of the network nodes determines whether and when to switch from the home channel to a particular guest channel of the guest channels for a variable amount of time to increase data throughput over one or more corresponding communication links in the ad hoc network with other network node s .

This Summary is provided to introduce in a simplified form a selection of concepts that are further described below in the detailed description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Systems and methods for distributed overlay multi channel MAC OMMAC for ad hoc networks are described. The distributed OMMAC provides a framework to maintain node to node connectivity and optimize data throughput i.e. performance in an ad hoc network. To this end the systems and methods identify a channel frequency over which network nodes nodes can periodically synchronize to maintain node to node connectivity. This synchronization channel is the home channel which is the same for all nodes in the ad hoc network. In this implementation the home channel represents the channel frequency over which nodes first synchronized when in the ad hoc network was initially configured i.e. established . The systems and methods then identify multiple channel frequencies orthogonal to the home channel. These orthogonal channels are guest channels . The nodes collectively implement a distributed and time synchronized channel scheduling and switching algorithm based on the home channel and the guest channels. Specifically all of the nodes to operate on the home channel for variable and overlapping amounts of time to periodically broadcast traffic and channel information to other ones of the nodes and thereby maintain network connectivity. Additionally each node in the ad hoc network independently determines whether and when to switch from the home channel to a particular guest channel for a variable amount of time to locally optimize data throughput in the ad hoc network.

These and other aspects of the systems and methods for distributed OMMAC for ad hoc networks are now described in greater detail.

Each node includes one or more processors coupled to system memory comprising computer program modules executable by respective ones of the processor s to implement the distributed OMMAC for ad hoc networks. Such system memory also includes program data generated and or used by respective ones of the computer program instructions during program module execution. For example node includes one or more processors coupled to system memory representing volatile random access memory RAM and non volatile read only memory ROM . System memory includes program modules comprising computer program instructions executable by processor s . System memory also includes program data generated and or used by respective ones of the computer program instructions during program module execution. In this implementation for example program models includes overlay multi channel MAC OMMAC module a Network Interface Control NIC driver and other program modules such as an operating system to provide a runtime environment a packet routing module to respectively route and receive information to from OMMAC module etc.

OMMAC maintains node to node connectivity in network and enforces a distributed channel switching policy in view of precise synchronization requirements specified by the underlying wireless modulation techniques of a protocol. For purposes of illustrating one exemplary embodiment operations of OMMAC are described with respect to the exemplary protocol of IEEE 802.11. To maintain node to node connectivity and enforce a distributed channel switching policy in view of the underlying synchronization requirements OMMAC divides the available communication channel frequencies into a single home channel and multiple guest channels. The single home channel and the multiple guest channels are collectively represented in as channel set . Each node uses the home channel to maintain network connectivity with other node s . Guest channels are selectively used according to the below described channel switching and scheduling algorithm by respective ones of nodes to increase network data throughput performance .

In this implementation the home channel is determined as follows. An initiator node scans all available channel frequencies and selects one with the least noise as the home channel. All nodes can be reached by a node over the home channel via a broadcast message. In this implementation each node votes for or against the home channel via broadcast messages ACK NACK messages based on local observations of noise on the home channel. For example in one implementation if the number of broadcast ACK message is received are greater than some percentage e.g. 50 etc. of the stations in timeout mode for some configurable number of time cycles then a new home channel is determined and voted on by nodes . In this implementation the home channel is not constrained to a fixed channel frequency and can be dynamically adjusted migrated by nodes according to communication link status. In a different implementation criteria other than channel noise is used to select a home channel.

Channel frequencies orthogonal to the home channel are guest channels. A node communicates with a different node over a guest channel to increase data throughput i.e. performance of network . In this implementation OMMAC sorts the guest channels according to interference observed on that channel. Such interference for a particular node is the averaged observation result by all other nodes in broadcast range of the particular node . Many OMMAC operations are performed at a node level based on OMMAC information from other node s and thus the term distributed OMMAC . In this implementation each node detects a respective interference level on the home channel and guest channels over which the node respectfully operates. In this scenario the node communicates such information in periodic broadcast messages to other nodes in its broadcast range. In this implementation a node s local OMMAC implements channel switching and scheduling operations such that a guest channel with least interference is utilized for node to node communications first.

The amount of time that a node stays on the home channel is a variable b partially overlapped with the time that neighboring node s covering a particular network beacon are also on the home channel and c a function of bidirectional traffic over a communication link between the node and other node s within broadcast range of the node . When a node is not on the home channel the node is on a guest channel. In view of the above each node is respectively configured by its local OMMAC to determine if and when the node should switch from the home channel to a scheduled guest channel to improve data throughput. Each node s local OMMAC defines a channel switching point i.e. a pole point for the particular node . A node s pole point is defined with respect to a periodic time cycle T that is based on the particular beacon interval of the wireless protocol being utilized e.g. IEEE 802.11 etc. . For purposes of exemplary illustration a nodes pole point is shown as pole point .

In this implementation the length of a cycle is an integer n determined by multiplying a beacon interval associated with the underlying wireless protocol. For example a default beacon interval for nodes configured to implement the IEEE 802.11 standard is 100 time units TU which is 1024 ms.

As shown in time cycle Tis divided into a multiple number m of slots. A slot represents the granularity of time that a node is scheduled on a channel. For example if a periodic cycle is 102.4 ms n 1 and m 8 i.e. each cycle is divided into 8 slots then OMMAC adjusts home and guest channel time by a granularity of 12.8 ms. The pole point indicates a particular time for a node to jump switch from one channel to another channel. Different nodes may define their own pole points but the sender and receiver pair of a particular wireless link synchronize respective pole points and jump to a same channel at the same time described in greater detail below in the section titled Exemplary Time Synchronization . In this implementation pole points are restricted at the boundaries of slots. Therefore an integer value represents a pole point. For example if m 8 then 1 byte 8 bits can be used to denote a pole point. In this implementation for example there can be multiple pole points in one cycle. In addition the pole point of a link between two nodes is adjusted according to the traffic volume on the link as discussed in greater detail below.

At the beginning of a cycle all nodes return to the home channel and stay on home channel for at least T which is called broadcast time. OMMAC schedules broadcast packets to transmit within this time period of each cycle. In this implementation for example a node may stay at home channel longer than T i.e. T T. Let T T T. During that time the node is still on home channel but for unicast messages. The pole point in slot i.e. T is valid in range b m . If Tequals to m then the corresponding node remains on the home channel for the entire cycle and no guest channel is scheduled for this particular node. In this implementation the value of b is chosen so that the transmission opportunities of beacon and broadcast packets are reserved e.g. b 1 .

We now describe the distributed algorithm for each node to schedule the guest channels to switch. Aspects of scheduling guest channel scheduling at communication links are first generalized in view of a combination of a Maximal Independent Set MIS of guest channels and a coloring problem. Then an exemplary distributed implementation of the algorithm is presented in view of the above described exemplary framework based on a tradeoff between signaling overhead and benefits by MIS for which a greedy algorithm is used.

As indicated above the amount of time that a node remains on the home channel is variable which is partially overlapped with that of neighboring nodes covering beacon period. The node station spends the rest of the time in a cycle Ton guest channel in each cycle. If source and destination nodes of a link switch to the same guest channel at the same time then the traffic on this link is orthogonal with traffic on the home channel and other guest channels. Since 802.11 provides CSMA CA Carrier Sense Multiple Access Collision Avoidance to handle contention at the link layer two way traffic on a link is treated as an atom and scheduled to the same slots in the cycle T. Thus channel switching is according to the traffic on a link a link is bidirectional.

OMMAC guest channel scheduling operations determine which communication link to select and also determine how long to communicate over respective guest channel s to substantially maximize total data throughput without deterioration . If there is no constraint for the number of channels and the traffic on each link is greedy and since each 802.11 station involves only one link at a time OMMAC selects a maximal number of non primary conflict active links. Two links are in primary conflict if both links require a same node for communication. A communication link is active if there is certain volume of traffic on that link per time cycle T.

Let V and E denote an entire node and link set links are shown as respective communication links through N in network respectively and G denotes an original traffic graph for the network. Assume the total number of nodes in G is N which are indexed by I 1 2 . . . N so that we use Vto denote node i in G. Let ldenote a bidirectional link between node i and node j so that we have l l. OMMAC converts the original traffic graph G node graph for the network into a link graph G where each node represented in the link graph is an active link in the original node graph. There is a link between two nodes in G if their corresponding links in G are primary conflict. Thus the Maximum Independent Set in graph G corresponds to the non primary conflict link set in G such that the largest number of links is selected maximizing throughput .

The Maximum Independent Set problem is well known NP complete and thus lots of heuristics algorithms have been proposed to solve MIS Maximal Independent Set where no more nodes can be added into it. In this implementation a MIS in G corresponds to a link set in G that is maximal and no more links can be scheduled. One well known heuristic for MIS selection is greedy algorithm which sorts all the nodes in graph G by its degree and selects the node with least degree first removing the selected node and all its neighbor nodes. This least degree node selection continues until all nodes are removed. All selected nodes represent a MIS and the number of MIS may be more than one if multiple nodes have the same degree.

After a MIS has been generated OMMAC addresses the constraint of number of channels by coloring. Specifically OMMAC converts the MIS in link graph G into a graph G where each node in G is a node in the MIS of G and there is a link between the two nodes in G if the two corresponding links in G are secondary conflict. Here in the original channel graph G two links are secondary conflict when the two links cannot work in parallel on the same channel at the same time. Therefore if two links are primary conflict they must be secondary conflict also but not vise versa. The graph G is called channel graph and the coloring algorithm is utilized to schedule the channel for the nodes represented in G . OMMAC selects the MIS of G and assigns the first represented guest channel to these nodes in MIS. OMMAC then removes the nodes from G and the used guest channel in guest channel set.

OMMAC continues MIS selection and channel assignment until all nodes are assigned or until the guest channel set is empty. In this implementation the last MIS is assigned to the home channel since it is also orthogonal to all guest channels. As described above links are assigned to different guest channels to improve performance. Since channel switching introduces some switching latencies and to reduce such latencies a set of links may be configured to stay on the home channel since the home channel is also orthogonal to all guest channels. In view of this and in the centralized algorithm the MIS selected to work on home channel could be any MIS e.g. the first MIS. However to break a deadlock in the distributed scenario the last MIS is selected. Such a deadlock may occur for example if multiple nodes links simultaneously regard themselves as the one s working over the home channel and no guest channel is used.

For each node OMMAC locally addresses the overhead introduced by channel switching and determines whether the node is a data throughput bottleneck in network . Specifically let Tand Tdenote the time overhead of switching channel and the channel occupied time of node Nper cycle then channel switching affects the throughput in node Nif T T T. shows an exemplary node graph a converted to a link graph b that shows a data throughput bottleneck at a particular network node according to one embodiment. Specifically illustrates a situation where data throughput cannot be increased but rather data throughput somehow decreased due to the overhead of channel switching. is illustrative of a similar scenario . For example referring to please note that all nodes are on the same channel so there is no cost for channel switching. Referring to the link graph shows that there is only one node in the corresponding MIS either link or link . If MIS link is chosen to work on a guest channel then there is certain overhead at node . However both link and require node to be involved. Therefore total throughput is decreased if node is implementing channel switching operations. Please note that with one link scheduled data throughput cannot be increased. In such a scenario OMMAC removes the channel switching scheduling for the node .

The preceding paragraphs have described how OMMAC establishes when a communication link is scheduled on a particular channel. We now describe how OMMAC determines a pole point for a nodes channel switching operations. For node N T total channel occupation time consists of both transmission time and receiving time on all channels. For a link l let Tdenote the channel occupation time on all channels. We have 0. 2 The time weight of link lon node Nis . 3 Thus we define the time weight of link las min . 4 For link l the pole point is scheduled so that . 5 The time to select a guest channel according to equation 5 guarantees that a time on guest channel is adjusted according to traffic volume on that link so that more time is allocated if traffic increases and b the proportional to cycle time ensures the goal that the requirement of other traffic on the corresponding node is not affected.

In one implementation for example OMMAC implements a centralized algorithm to schedule channel switching for each node . The centralized algorithm always provides complete and updated information for ad hoc network traffic and nodes while a distributed algorithm provides localized such information. Centralized algorithm MIS selection may have multiple solutions whereas only one MIS solution will be selected. In contrast using a distributed algorithm results are determined by multiple steps taken at neighbor nodes. This means that using the distributed algorithm each node s decision affects the entire result but does not control the entire result. For instance compared with the centralized algorithm the steps taken in such a distributed algorithm are asynchronous and could be parallel conflict or even result in a deadlock scenario. This latter scenario as described above is addressed by using the home channel for the last MIS and a channel negotiation protocol .

In this implementation for example the centralized algorithm to schedule channel switching for each node is as follows 

OMMAC is designed in view of the following 1 each node has only partial updated information of network so OMMAC locally optimizes the channel switching decision in view of information acquired from one hop neighbor nodes this decision is independent of information from nodes further than a one hop range 2 each link is scheduled on a guest channel asynchronously so in centralized mode OMMAC link scheduling is adjusted in view of other scheduled links and 3 OMMAC implements a protocol for guest channel negotiation for distributed channel switching and adapts to traffic. These aspects are described in greater detail below.

Each node in a wireless network typically only obtains information of traffic and channel in its communication range. Thus the network traffic graph G generated observed by each node is partial. Accordingly MIS selection on each node may be different. In this subsection we discuss updating a node s information with information from neighbor node s . Specifically OMMAC framework provides for link layer broadcast over the home channel. As such each node is configured to communicate periodic broadcast messages to exchange traffic and channel information with other node s . In this implementation for example each node communicates such a broadcast message once per second although other periodic time frames could be used. To reduce opportunities for delayed messages possibly resulting in outdated information each node broadcasts its traffic and channel information rather than relaying information for other nodes. In another implementation such information exchange is implemented independent of broadcast messages. For example in one implementation a node s traffic and channel information is piggy backed in an ongoing traffic header.

In centralized mode OMMAC calculates a MIS for the whole link graph of network . OMMAC then assigns all the links using a greedy algorithm least degree first . The greedy least degree first algorithm is an example of a known coloring algorithm. In practical distributed mode each node negotiates channel switching scheduling asynchronously which means a node makes schedule decision based on observed channel switching schedule s received from neighbor node s . Such asynchronous scheduling smoothes any message surge over network by negotiation messages and balances data throughput load on home channel time. In one implementation and to further reduce overhead of negotiation messages OMMAC generates channel switching schedule s that cover channel switching activities for multiple time cycles e.g. please see .

In one implementation OMMAC implements a distributed algorithm for node Nto schedule channel switching pattern as follows.

Referring to blocks and and in this implementation respective message sending and receiving nodes start respective timer s responsive to sending receiving message s to avoid synchronized channel switching schedule negotiation for nodes in a neighborhood of nodes . Exemplary timer logic is described in greater detail below in the section titled Timer Logic at paragraph 0062 . Responsive to receiving the request message node Nchecks whether T TT. If passed i.e. T T

In one implementation a node may provide feedback to other node s for example with ACK NACK messages to confirm reject the switching request. If the result is NACK or no feedback is received at node Nafter a configurable amount of time has passed a timeout then node Nregards the channel switching request as a failed request. Otherwise in this implementation both nodes Nand node Nbroadcast a SC notification message twice a first such message is broadcast right after the SC response message is sent or received at node Nor Nrespectively. In this implementation such double notification substantially prevents any impact of a notification broadcast being lost. If nodes implement the channel switching negotiation on broadcast channel time in a current time cycle the second SC notification message is communicated at the broadcast time in a next time cycle. This latter scenario is shown in . Please note that in this implementation it is possible that two nodes will negotiate a new switching schedule for example due to pole point changes or schedule extending on a previously scheduled guest channel. In such a scenario and in this particular implementation local OMMAC s respectively schedule the two broadcasts based on a broadcast time in next and third cycles respectively.

Referring to as discussed above each node implements a respective portion of the channel switching policy of system . In one implementation this channel switching policy is centralized. In another implementation this channel switching policy is distributed. Specifically OMMAC includes logic to control NIC driver manage operating system non real time software interrupts and comply with any legacy NIC driver scheduling buffering requirements. For example in one implementation OMMAC includes traffic measurement logic information exchange logic channel switching scheduler logic scheduling on switching logic timer logic and packet buffering logic . This exemplary ordering does not imply or limit architectural order or dataflow between various portions of this logic. Rather the presented order of logic through is for purposes of description. The exemplary aspects of these modules are now described.

Traffic measurement logic for example generates log s of sent and received packets. Traffic measurement logic also calculates associated traffic volume for incoming and outgoing packets. In this implementation traffic measured at logic implements such calculations using a known time sliding window average algorithm to provide an averaged traffic volume for a previous configurable time period. Information exchange logic propagates channel switching schedules for the corresponding node . For purposes of exemplary illustration such logs and channel switching schedules are shown as respective portion of other program data . Information exchange logic also organizes traffic information and channel switching schedules received from neighboring nodes . Scheduling and switching logic utilizes any such received channel switching schedules to implement exemplary channel switching scheduling operations.

Scheduling and switching logic performs for example the greedy algorithm described above based on the node specific information self information collected by traffic measurement logic and neighbor node information collected by information exchange logic . Scheduling and switching logic also accesses utilizes Application Programming Interface API Exposed by the NIC Driver . API provides an interface into the NIC driver to switch channels and synchronize channel switching schedules so that respective ones of the nodes switch to a same channel at a same time. We now describe an exemplary implementation of node channel switching synchronization.

Synchronizing channel switching operations between nodes across multiple channels is challenging since nodes working on different channels are more likely to lose synchronization. Using conventional channel switching techniques a difference in channel switching times between network nodes often results packet retransmissions and may even result in packet loss. Multiple factors affect channel switching synchronization. Such factors include for example the particular synchronization protocol utilized clock drift temperature etc. Scheduling and switching logic addresses these challenges with a practical node channel switching synchronization scheme that allows nodes to switch to a particular channel frequency at the same time.

In this implementation scheduling and switching logic operates on a periodic time cycle that is based on the 802.11 superframe. Please also refer to the above description associated with describing such a periodic time cycle . The 802.11 protocol beacons at the beginning of broadcast message periods. A received beacon indicates an 802.11 MAC time. Responsive to receiving a beacon each node updates its respective 802.11 timestamp i.e. at the broadcast time for each cycle . Each node then determines any difference of the local machine time and the MAC time per cycle. When scheduling and switching logic calculates a channel switching schedule for a node the scheduling and switching logic translates times associated with the schedule into the 802.11 MAC time. Such translations are performed before any communication by information exchange logic of the schedule to neighbor node s . Responsive to receiving such channel switching schedule s from other node s scheduling and switching logic translates the indicated 802.11 MAC time s to associated local machine time to schedule the channel switching event.

In this implementation NIC driver exposes API allowing switching logic to request NIC driver to send the requesting node the MAC time e.g. 802.11 MAC time . This respective portion of API allows a requesting node to calculate the difference of machine time and MAC time. In this implementation granularity of MAC time is 1 microsecond and synchronization error of MAC time is the propagation delay error of approximately 1 also microsecond. Machine time granularity for example is typically 100 nanoseconds. Therefore and in this particular implementation switching logic provides node channel switching synchronization at a microsecond level.

Moreover because each node operates over a same home channel and 802.11 MAC broadcasts beacons only on the home channel when a new node is added to network the new node uses the 802.11 scan phase to implement network discovery operations.

For purposes of exemplary description timer aspects of the WINDOWS operating system are described. These described aspects are analogous to other operating system implementations. The default timer granularity on WINDOWS is 10 ms and an application can typically obtain accurate time measurements at 100 ns. For stringent timer requirements in a kernel WINDOWS provides an API to adjust timer granularity to approximately 1 ms. However after adjusting timer granularity and although average timer error is less than 1 ms timer error can still be greater than 10 ms when operating system OS load is very high. One reason for this is because software and hardware events are typically placed into priority based queues for processing. Hardware events typically have the highest priority for removal from the event queue for processing whereas software events have many possible priorities. Thus there are practical limitations of implementing a channel switching schedule using a software timer based on operating system timer interrupts.

In contrast to conventional channel switching timers OMMAC implements software timer logic timer based on multiprocessing time slices associated with the operating system. Timer logic of system addresses the limitations of conventional software timers to provide timers for packet and channel switching scheduling. Specifically timer logic sets priority of timer threads to a highest software priority level so that they will be promptly removed from the event queue and processed. Such priority level adjustments substantially reduce channel switching latencies evident in conventional channel switching implementations. For packet and channel switching scheduling timer logic responsive to expiration firing of a timer thread for a packet schedule notifies packet buffering logic to send packets to specific neighbor nodes . Such buffering logic is described in the following section titled Exemplary Packet Buffering Logic . In another example when a channel switch timer fires timer logic notifies switching logic to communicate a channel switch command request to NIC driver e.g. via API 

Referring to OMMAC comprises a data plane and a control plane. The data plane receives packets from conventional packet routing logic as shown by data traffic arrow . The data plane also receives messages e.g. negotiation messages signaling messages etc. from the control plane portion of OMMAC . Messages from the control plane are represented with a black filled in arrow that trends from left to right from the control plane to the data plane . As shown data plane classifier logic puts received packets messages collectively referred to as packets into node destination queues based on packet destination and based on whether the packet is to be communicated over the home channel or a particular guest channel. Such destination queues are illustrated in for example as Dest through Dest n. Node destination and channel identification are provided by respective packet headers. Arrow represents incoming uplink messages from NIC driver the interface to NIC hardware . As shown OMMAC transfers incoming messages representing data to routing logic . OMMAC transfers incoming messages that are not data to the control plane.

As indicated in the previous sections a network node s local OMMAC includes timer logic to trigger pole point s for the corresponding node to switch from the home channel to a corresponding guest channel during a periodic time cycle at the end of which the node switches back to the home channel . A nodes pole point is based on the node s traffic provided by traffic measurements of traffic measurement logic as well as traffic measurements provided by other nodes. To this end this node exchanges its particular traffic measurements with other nodes so that the other nodes can make their own pole point decisions. In the example the left to right arrow from the control plane to the data plane represents data traffic at least a portion of which is this particular node traffic measurements for queuing and communication to other respective nodes in the ad hoc network. Such communication is shown by arrow . Additionally and as described in the previous sections distributed channel schedule logic schedules channel switching times e.g. based on the greedy algorithm for this node and communicates these schedules to other nodes for channel switching synchronization operations.

We now describe operations of the data plane of also represented by packet buffering logic of in greater detail. The packet buffering logic buffers packets from classifier logic and from the control plane for packet transmissions to other network nodes in the ad hoc network. In this implementation buffering logic buffers packets in priority sorted destination queues . Such priority sorting is not a first in first out priority sorting but rather based on specific priorities provided by one or more respective portions of OMMAC e.g. timer logic and or other program modules . For example in a priority queue OMMAC provides signaling messages such as the channel switching request s described above with higher respective priorities than messages associated with other application traffic.

In this implementation for example each node implements a per node neighbor destination queue to control transmission of packets to different destination node s scheduled at different time s . If multiple neighbor node queues are active e.g. an active queue exhibits a threshold amount of traffic and valid for the same time e.g. on the home channel the packets are served according to a round robin order. As shown the packet buffering and scheduling logic maintains the priority queue not NIC driver or corresponding hardware . This allows switching and scheduling logic to control all packet transmission times as corresponding channels are scheduled.

Serving packets according to a round robin order is different from serving packets according to first in first out FIFO scheduling. This is because a node buffers packets according to respective priorities in different queues. A queue with higher priority is always served first then a queue with a lower priority. Queues with a same priority are served by round robin scheduling. For FIFO packets are served according to arrival time. In contrast and in this implementation higher priority packet s are served from a queue even if the higher priority packet s arrive later than packets queued in lower priority. Round robin operations are also different than FIFO queue handling operations. For instance please assume queue and queue has some priority and queue s packet arrival rate is twice of queue s. With FIFO queue is given twice opportunities than queue while with round robin queue is given the same opportunity as queue.

When conventional channel switching schemes switch channels packets buffered by the MAC or in hardware are typically dropped because of hardware reset s occurring at a switching channel. Additionally MAC or hardware buffer space allocated to data throughput performance is generally limited in size often resulting in buffer overflow conditions. To address such limitations of conventional systems the packet buffering and scheduling logic sends packets to NIC driver only when a previously communicated packet has been served by NIC driver . To provide this determination NIC driver communicates a success status please see uplink arrow to information exchange module . Information exchange model then communicates the status to the packet buffering and scheduling logic. In such a scenario and when a previous packet is pending at the NIC driver the packet buffering and scheduling logic holds on to any next packet until such a success status is received. Additionally in this implementation switching and scheduling logic uses timer logic to configure a packet drain time before any scheduled channel switching event. This packet drain time provides the wireless NIC hardware with time to communicate any buffered packets prior to the channel switching operations. In this implementation the packet drain time is 1 ms.

Referring to operations of block determine a home channel frequency home channel for network nodes in an ad hoc wireless network. Operations of block identify a set of guest channel frequencies guest channels orthogonal to the home channel. The home and guest channels are selected i.e. determined and identified from multiple channel frequencies provided by a wireless network protocol. Such a wireless network protocol includes for example IEEE 802.11. Operations of block operate by each node in the ad hoc wireless network over the home channel for respective variable and overlapping amounts of time to maintain network connectivity with other network nodes. Operations of block determine by each node whether and when to switch from the home channel to a respective assigned guest channel for variable amount of time to locally optimize data throughput in the ad hoc network.

Operations of block configure each network node in the ad hoc network to maintain connectivity over a single home channel frequency of multiple available channel frequencies during the connectivity phase. The multiple available channel frequencies are provided and according to the wireless network protocol. Operations of block configure during the connectivity phase at least two nodes to communicate over a guest channel of multiple guest channels that are orthogonal to the home channel. Operations of block maintain by each network node connectivity in the ad hoc network between respective notes using the home channel during the connectivity phase. Operations of block optimize by least a subset of the network nodes data throughput in the ad hoc network using one or more respective assigned guest channels during the data throughput improvement phase. In this manner network nodes in the ad hoc network maintain node to node network connectivity and improve performance of the ad hoc network.

Although the above sections describe distributed overlay multi channel MAC for wireless ad hoc networks in language specific to structural features and or methodological operations or actions the implementations defined in the appended claims are not necessarily limited to the specific features or actions described. For example OMMAC please see channel switching latency can be reduced by replacing the software timer provided by a non real time OS with a hardware timer. Alternatively in an alternate implementation timer accuracy is improved by adding real time support to the OS. Thus the specific features and operations for distributed overlay multi channel MAC for wireless ad hoc networks described above are exemplary forms of implementing the claimed subject matter.

