---

title: System and method for bump mapping setup
abstract: One embodiment of the present invention sets forth a technique to setup efficient bump mapping using a geometry shader. This approach uses a vertex shader, a primitive assembly unit, and a geometry shader. The vertex shader performs vertex operations, such as calculating a per-vertex normal vector, and emits vertex data. The primitive assembly unit processes the vertex data and constructs primitives. Each primitive includes a series of one or more vertices, each of which may be shared amongst multiple primitives, and state information defining the primitive. The geometry shader processes each primitive, calculating an object-space to texture-space mapping for each vertex of the primitive and, subsequently, using this mapping to transform the object-space view vector and the object-space light vectors associated with each vertex of the primitive to texture-space equivalents. Advantageously, this approach to setting up bump mapping fully utilizes the GPU, thereby optimizing both hardware resources and performance.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07843463&OS=07843463&RS=07843463
owner: NVIDIA Corporation
number: 07843463
owner_city: Santa Clara
owner_country: US
publication_date: 20070924
---
This application claims the priority benefit of the United States provisional patent application having Ser. No. 60 941 606 and filed on Jun. 1 2007. The subject matter of this provisional patent application is hereby incorporated herein by reference.

The present invention relates generally to the field of graphics processing and more specifically to a system and method for bump mapping setup.

A typical computing system includes a central processing unit CPU and a graphics processing unit GPU . Some GPUs are capable of very high performance using a relatively large number of small parallel execution threads on dedicated programmable hardware processing units. The specialized design of such GPUs usually allows these GPUs to perform certain tasks such as rendering 3 D scenes much faster than a CPU. However the specialized design of these GPUs also limits the types of tasks that the GPU can perform. The CPU is typically a more general purpose processing unit and therefore can perform most tasks. Consequently the CPU usually executes the overall structure of the software application and configures the GPU to perform specific tasks in the graphics pipeline the collection of processing steps performed to transform 3 D images into 2 D images .

One task that may be performed when transforming 3 D scenes into 2 D images is bump mapping. As is well known bump mapping is a rendering approach for simulating lighting effects caused by surface irregularities such as bumps wrinkles and divots. For example a golf ball may have dimples which we visually perceive because of the way that light interacts with the surface of the golf ball. Although such surface irregularities may be modeled as geometries in the graphics model of a given object such a graphics model would be very complex and time consuming to process and display. Furthermore some surface irregularities may be smaller than the size of a pixel and therefore the corresponding geometry could not be accurately rendered by the GPU. To address these issues the object may instead be modeled using simpler geometries to convey the overall shape of the object and any surface irregularities may be captured as texture maps. Bump mapping techniques use such texture maps in conjunction with the geometries of the object to simulate the lighting effects of the surface irregularities thereby improving the realism of the graphics image of the object.

One approach to bump mapping divides the bump mapping process into two tasks. The first task is to generate a perturbed surface normal map representing the texture of an object. In this approach each normal in the perturbed surface normal map may be a direction vector that points up and away from the surface. Typically the normal 0 0 1 represents a flat surface and any surface variation such as a dimple is represented by a normal with a corresponding tilt. The perturbed surface normal map is often generated in a coordinate system called the texture space.

The second task in the bump mapping process is to perform lighting calculations using the perturbed surface normal map to render and illuminate the object. One approach to this task performs the lighting calculations using the dot product of each of the light vectors and view vectors of each of the pixels of the geometries of the object with the perturbed surface normal map. However to calculate the dot product the light vectors and view vectors must share a consistent coordinate system with the normal map. The light vectors and view vectors are typically defined in another coordinate system such as the object space. Therefore bump mapping programs often construct an object space to texture space mapping to transform the object space light vectors and view vectors to texture space equivalents. Once the light vectors and view vectors are defined in texture space the lighting calculations are performed and the object may be rendered with the texture data.

To create realistic lighting effects the object space to texture space mapping is constructed for each vertex of a given primitive. Both information specific to a graphics primitive such as a triangle and information specific to a particular vertex in a graphics primitive such as the vertex encompassing the upper left point in a triangle may be used to create the object space to texture space mapping. For example the object space to texture space mapping may include three normalized vectors the per primitive tangent vector the per vertex normal the mean of the surface normals of all the primitives that share the particular vertex and the cross product of the tangent vector and the per vertex normal. Furthermore the object space to texture space mapping is dynamic as the object represented in object space animates or morphs the object space to texture space mapping also changes.

In one approach to bump mapping the CPU is used to calculate the object space to texture space mapping. Since the object space to texture space mapping may be different for each primitive the CPU constructs individual primitives consisting of vertices that are unique to each primitive. One drawback to this approach is that the CPU has to replicate each vertex shared by two or more different primitives in order to construct the individual primitives. Replicating data in such a fashion is inefficient. Furthermore since the CPU constructs the primitives and calculates the object space to texture space mapping the vertices must be available to the CPU. Therefore in this approach the CPU is usually used to perform the vertex processing tasks in addition to the object space to texture space mapping to avoid having to pass vertex data from the vertex shader in the GPU to the CPU. Since the CPU performs vertex processing tasks much less efficiently than the vertex shader the capabilities of the vertex shader in this approach are oftentimes not properly leveraged. As indicated this problem can be addressed by using the vertex shader to perform the vertex processing tasks but such a solution would require the vertices to be passed from the vertex shader back to the CPU and therefore would not necessarily improve the overall efficiency of the bump mapping setup.

As the foregoing illustrates what is needed in the art is a more effective technique for bump mapping setup.

One embodiment of the present invention sets forth a geometry processing engine for processing primitives in a graphics rendering pipeline. The geometry processing unit is configured to receive vertex data related to each vertex of a primitive compute a first vector based on at least a portion of the vertex data to generate an object space to texture space mapping for the primitive and transform a second vector associated with one of the vertices of the primitive from an object space representation to a texture space representation by transforming the object space representation by the object space to texture space mapping. The object space to texture space mapping is typically represented as a three by three orthonormal matrix when 3 D vectors are involved so the texture space vector is computed by multiplying the matrix by the object space vector.

One advantage of the disclosed geometry processing engine is that using the geometry shader to perform coordinate space transformations is more efficient than using the CPU for such purposes. Objects are typically represented as a mesh of vertices. Vertices can be shared by multiple primitives in the mesh. Bump map setup computes vertex attributes dependent on each assembled primitive. So if bump mapping setup is performed on the CPU mesh vertices can no longer be shared by multiple primitives. Instead a per primitive instance of each vertex must be transferred and processed. This generates substantially more vertices to be transferred and processed compared to performing the bump mapping setup with a geometry processing engine subsequent to vertex processing. Furthermore the disclosed approach implements the vertex shader for vertex shading operations thereby more fully exploiting the processing efficiencies of the graphics rendering pipeline. Moreover since the foregoing operations are executed completely by the GPU there is no need to send data to the CPU as with prior art approaches further increasing overall processing efficiency.

The system data bus connects the CPU the input devices the system memory and the graphics processing subsystem . In alternate embodiments the system memory may connect directly to the CPU . The CPU receives user input from the input devices executes programming instructions stored in the system memory operates on data stored in the system memory and configures the graphics processing subsystem to perform specific tasks in the graphics pipeline. For example the CPU may read a rendering method and corresponding textures from one or more files and configure the graphics processing subsystem to implement this rendering method. The system memory typically includes dynamic random access memory DRAM used to store programming instructions and data for processing by the CPU and the graphics processing subsystem . The graphics processing subsystem receives instructions transmitted by the CPU and processes the instructions in order to render and display graphics images on the display devices .

The system memory includes an application program an application programming interface API high level shader programs and a graphics processing unit GPU driver . The application program generates calls to the API in order to produce a desired set of results typically in the form of a sequence of graphics images. The application program also transmits one or more high level shading programs to the API for processing within the GPU driver . The high level shading programs are typically source code text of high level programming instructions that are designed to operate on one or more shaders within the graphics processing subsystem . The API functionality is typically implemented within the GPU driver . The GPU driver is configured to translate the high level shading programs into machine code shading programs that are typically optimized for a specific type of shader e.g. vertex geometry or fragment .

The graphics processing subsystem includes a graphics processing unit GPU a GPU local memory and a GPU data bus . The GPU is configured to communicate with the GPU local memory via the GPU data bus . The GPU may receive instructions transmitted by the CPU process the instructions in order to render graphics data and images and store these images in the GPU local memory . Subsequently the GPU may display certain graphics images stored in the GPU local memory on the display devices .

The GPU includes one or more streaming multiprocessors . Each of the streaming multiprocessors is capable of executing a relatively large number of threads concurrently. Advantageously each of the streaming multiprocessors can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying of physics to determine position velocity and other attributes of objects and so on. Furthermore each of the streaming multiprocessors may be configured as one or more programmable shaders e.g. vertex geometry or fragment each executing a machine code shading program i.e. a thread to perform image rendering operations. The GPU may be provided with any amount GPU local memory including none and may use GPU local memory and system memory in any combination for memory operations.

The GPU local memory is configured to include machine code shader programs storage buffers and a frame buffer . The machine code shader programs may be transmitted from the GPU driver to the GPU local memory via the system data bus . The machine code shader programs may include a machine code vertex shading program a machine code geometry shading program a machine code fragment shading program or any number of variations of each. The storage buffers are typically used to store shading data generated and used by the shading engines in the graphics pipeline. The frame buffer stores data for at least one two dimensional surface that may be used to drive the display devices . Furthermore the frame buffer may include more than one two dimensional surface so that the GPU can render to one two dimensional surface while a second two dimensional surface is used to drive the display devices .

The display devices are one or more output devices capable of emitting a visual image corresponding to an input data signal. For example a display device may be built using a cathode ray tube CRT monitor a liquid crystal display or any other suitable display system. The input data signals to the display devices are typically generated by scanning out the contents of one or more frames of image data that is stored in the frame buffer .

The data assembly unit is a fixed function unit that collects vertex data from the application program for high order surfaces primitives and the like and passes the vertex data to the vertex shader . The data assembly unit may gather data from buffers stored within system memory and the GPU local memory as well as from API calls from the application program used to specify vertex attributes. The vertex shader is a programmable execution unit that is configured to execute a machine code vertex shading program processing vertex data as specified by the vertex shading program. For example the vertex shader may be programmed to perform skinning operations transform the vertex data from an object based coordinate representation object space to an alternatively based coordinate system such as world space or normalized device coordinates NDC space or any combination thereof. The vertex processing unit may access data that is stored in GPU local memory .

The primitive assembly unit is a fixed function unit that receives processed vertex data from vertex shader and constructs primitives e.g. points lines triangles or the like for processing by the geometry shading engine . The constructed primitives may include a series of one or more vertices each of which may be shared amongst multiple primitives and state information such as a primitive identifier defining the primitive. In alternative embodiments a second primitive assembler not shown may be included subsequent to the geometry shader in the data flow through the GPU .

The geometry shader is a programmable execution unit that is configured to execute a machine code geometry shading program processing primitives received from the primitive assembly unit as specified by the geometry shading program. The geometry shader may traverse the vertices of a given primitive using the state information associated with the primitive to operate on those vertices. For example the geometry shader may be configured to traverse the vertices of an input primitive using the state information of the primitive to construct object space to texture space mappings and to transform the object space view vector and light vectors of each vertex of the primitive to texture space equivalents. In addition to well known per primitive operations such as clipping the geometry shader may be programmed to generate one or more new graphics primitives and calculate per vertex parameters for generated vertices that are used when the new graphics primitives are rasterized. The geometry shader may access data that is stored in the GPU local memory .

The geometry shader outputs the parameters and primitives to the rasterizer . The rasterizer is a fixed function unit that scans the primitives and outputs fragments and coverage data to the fragment shader .

The fragment shader is a programmable execution unit that is configured to execute a machine code fragment shading program processing fragments received from rasterizer as specified by the machine code fragment shading program. The fragment shader may be programmed to perform operations such as bump mapping perspective correction shading blending and the like to produce shaded fragments that are output to the raster operations unit . For example the fragment shader may use texture space view vectors and light vectors in conjunction with a texture space perturbed surface normal map to perform bump mapping calculations. The fragment shading engine may access data that is stored in the GPU local memory . The raster operations unit optionally performs fixed function computations such as near and far plane clipping and raster operations such as stencil z test blending and the like and outputs pixel data as processed graphics data for storage in a buffer in the GPU local memory such as the frame buffer .

The position coordinates may be a three component vector e.g. x y z representing the position of the vertex in object space. The texture coordinates may be a two component vector e.g. s t representing the two dimensional position of the corresponding texture data in a texture that is defined in texture space. For example the texture coordinates may define the location of a perturbed surface normal in a perturbed surface normal map in texture space. The normal vector is a per vertex normal. The vertex shader of may be configured to generate the normal vector by calculating and normalizing the combination of the normals from multiple key frames or normal skinning transforms associated with the position coordinates . The view vector may be a three component directional vector that points from the vertex represented by the positions coordinates to the eye of a viewer. Similarly each of the light vectors may be a three component directional vector that points from the vertex represented by the positions coordinates to a light source such as the sun or a lamp.

Typically the view vector and the light vectors are specified in object space consistent with the position coordinates . However as discussed previously to perform effective bump mapping the view vector and the light vectors should share a consistent coordinate system with the perturbed surface normal map which may be defined in texture space. Accurately mapping these vectors from object space to texture space involves operating on the collection of vertices making up a given primitive. As set forth herein the geometry shader of is advantageously optimized to operate on the collection of vertices and state information defining a particular primitive to efficiently transform the view vector and the light vectors from object space to texture space.

As shown the object space to texture space mapping includes three three component vectors the normal vector of a tangent vector and a binormal vector . The tangent vector represents how the object space coordinates change with respect to one of the texture coordinates for a given primitive. As is well known the tangent vector may be calculated using the position coordinates and one of the texture coordinates of the vertices of a given primitive. The geometry shader may be configured to calculate and normalize the tangent vector in any technically feasible manner. As is also well known the binormal vector is the cross product of the normal vector and the tangent vector . Furthermore the normal vector the tangent vector and the binormal vector may be assembled as shown into the object space to texture space mapping .

Although the tangent vector may apply to all of the vertices that make up a given primitive since the normal vector may differ for each vertex of a given primitive the object space to texture space mapping may also differ for each vertex of a given primitive. The geometry shader may be configured to iterate over each of the vertices in a given primitive calculating the binormal vector creating the object space to texture space mapping and using the object space to texture space mapping to transform each of the view vectors and the light vectors associated with a particular vertex of a primitive e.g. view vector and light vectors associated with vertex from an object space representation to a texture space equivalent.

In alternative embodiments a second tangent vector may be calculated using the second of the texture coordinates to represent how the object space coordinates change with respect to the second texture coordinate. This approach applies when per vertex normals are not available or the underlying surface has a faceted appearance. An object space to texture space mapping may then be created using the tangent vector the second tangent vector and the cross product of the tangent vector and the second tangent vector. This object space to texture space mapping varies by primitive but not by the vertices within the primitive and may be used to transform the view vector and the light vectors of each of the vertices within the primitive from object space to texture space equivalents. Since only one mapping is calculated for each primitive this approach may be quicker and eliminates the need to transfer and process per vertex normals.

As shown the method begins at step where the vertex shader processes vertices . During step as discussed above in conjunction with the vertex shader calculates the position coordinates and normal vector in any technically feasible fashion. The vertex shader then emits the position coordinates the texture coordinates the normal vector the object space view vector in object space and the light vectors also in object space . In step the primitive assembly unit receives the vertices and constructs primitives such as triangles. Each primitive may include a series of one or more vertices and primitive state information defining the primitive. Advantageously a given vertex may be shared by one or more of the primitives constructed by the primitive assembly unit throughout the graphics pipeline . For example a given vertex may be shared by three triangles in a triangle strip without replicating any of the data such as the normal vector included in the vertex .

In step the geometry shader receives the primitives and performs bump mapping setup operations. Note that step is described in greater detail below in conjunction with . The geometry shader operates on each primitive transforming the view vector and the light vectors of each vertex of the given primitive from object space representations to texture space equivalents. During step the geometry shader emits the processed primitives including the transformed vectors. In step the rasterizer converts the processed primitives to fragments. In step the fragment shader uses the texture space representation of the view vector and the texture space representations of the light vectors generated by the geometry shader in step in conjunction with a texture such as a perturbed surface normal map to perform lighting calculations in texture space.

In addition to the light and view vectors the lighting calculations could involve additional or alternative texture space vectors such as tangent reflection or half angle vectors. A person skilled in the art will recognize that the geometry shader could transform any such vectors into texture space.

As shown the method begins at step where the geometry shader receives an input primitive. In step the geometry shader uses the position coordinates and the texture coordinates of multiple vertices in the primitive to calculate the tangent vector representing how the object space coordinates change with respect to one of the texture space coordinates. The geometry shader may perform this calculation in any technically feasible fashion. In step the geometry shader initializes a traversal of the vertices in the primitive by setting a current vertex to a first vertex in the primitive. In step the geometry shader calculates the binormal vector for the current vertex by computing the cross product of the per primitive tangent vector calculated in step and the per vertex normal vector calculated by the vertex shader in step of method . In step the geometry shader creates the object space to texture space mapping for the current vertex by assembling the tangent vector the binormal vector and the normal vector into a three by three matrix. In step the geometry shader iterates through each of the object space view vector and the object space light vectors associated with the current vertex. The geometry shader multiplies the object space to texture space mapping matrix created in step by each of these vectors thereby generating the texture space representations of these vectors. In step the geometry shader emits the current vertex with the texture space view vector and the texture space light vectors calculated in step .

At step if the geometry shader determines that the current vertex is not the final vertex of the primitive i.e. the geometry shader has not yet iterated over all the vertices in the primitive then the method proceeds to step . In step the geometry shader sets the current vertex to the next vertex in the primitive and the method returns to step where the geometry shader calculates the binormal vector for the new current vertex. The method continues in this fashion looping through steps until the geometry shader has transformed all of the object space view vectors and the object space light vectors associated with all of the vertices in the primitive to their texture space equivalents. Then the method terminates and the geometry shader may process another primitive.

In alternative embodiments the geometry shader may also be programmed to compensate for texture mirroring. As is well known texture mirroring is a technique which may be implemented to re use textures. The step of method in which the geometry shader calculates the tangent vector may be modified to identify and account for texture mirroring. For example the geometry shader may be configured to identify in any technically feasible fashion when texture mirroring is occurring using the position coordinates and the texture coordinates of multiple vertices in the primitive. If the geometry shader establishes that mirroring is occurring then the geometry shader negates the tangent vector and the method will continue to step using the negated version of the tangent vector .

In sum more efficient per fragment lighting may be achieved by using a geometry shader to perform coordinate space transformations necessary for efficient texture space lighting and bump mapping calculations. In one embodiment a vertex shader generates vertex data a primitive assembly unit constructs primitives and a geometry shader performs coordinate space transformations on vectors in the primitives. The vertex shader processes vertices and emits vertex data including a per vertex normal vector. The primitive assembly unit receives the processed vertex data from the vertex shader and constructs primitives. Each primitive includes a series of one or more vertices which may be shared amongst multiple primitives and primitive state information. Upon receiving a primitive from the primitive assembly unit the geometry shader uses the position coordinates and the texture coordinates of the vertices of the given primitive to calculate a tangent vector representing how the object space coordinates change with respect to one of the texture space coordinates. Then for each vertex in the primitive the geometry shader calculates an object space to texture space mapping using the normalized tangent vector and the per vertex normal vector and uses this mapping to transform the object space view vector and the object space light vectors of the vertex to texture space equivalents.

Advantageously using the geometry shader to perform coordinate space transformations for bump mapping setup is more efficient than using the CPU for such purposes. Furthermore the disclosed approach implements the vertex shader for vertex shading operations thereby more fully exploiting the processing efficiencies of the graphics rendering pipeline. Moreover since bump mapping setup operations are executed completely by the GPU there is no need to send data to the CPU as with prior art approaches. And since the geometry shader is configured to recognize individual primitives that share vertex data there is no need for the primitive assembly unit to replicate vertex data when constructing the primitives.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims.

