---

title: Data de-duplication using thin provisioning
abstract: A system for de-duplicating data includes providing a first volume including at least one pointer to a second volume that corresponds to physical storage space, wherein the first volume is a logical volume. A first set of data is detected as a duplicate of a second set of data stored on the second volume at a first data chunk. A pointer of the first volume associated with the first set of data is modified to point to the first data chunk. After modifying the pointer, no additional physical storage space is allocated for the first set of data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07822939&OS=07822939&RS=07822939
owner: EMC Corporation
number: 07822939
owner_city: Hopkinton
owner_country: US
publication_date: 20070925
---
This application relates to computer storage devices and more particularly to the field of managing data stored on a computer storage device.

Host processor systems may store and retrieve data using storage devices containing a plurality of host interface units host adapters disk drives and disk interface units disk adapters . Such storage devices are provided for example by EMC Corporation of Hopkinton Mass. and disclosed in U.S. Pat. No. 5 206 939 to Yanai et al. U.S. Pat. No. 5 778 394 to Galtzur et al. U.S. Pat. No. 5 845 147 to Vishlitzky et al. and U.S. Pat. No. 5 857 208 to Ofek which are incorporated herein by reference. The host systems access the storage device through a plurality of channels provided therewith. Host systems provide data and access control information through the channels of the storage device and the storage device provides data to the host systems also through the channels. The host systems do not address the disk drives of the storage device directly but rather access what appears to the host systems as a plurality of logical volumes. Different sections of the logical volumes may or may not correspond to the actual disk drives.

Information Lifecycle Management ILM concerns the management of data throughout the data s lifecycle. The value of data may change over time and accordingly the needs for the storage and accessibility of the data may change during the lifecycle of the data. For example data that is initially accessed often may over time become less valuable and the need to access that data become more infrequent. It may not be efficient for such data infrequently accessed to be stored on a fast and expensive storage device. On the other hand older data may suddenly become more valuable and where once accessed infrequently become more frequently accessed. In this case it may not be efficient for such data to be stored on a slower storage system when data access frequency increases. Further during data s lifecycle there may be instances when multiple copies of data exist at a particular point of time on a storage system and unnecessarily take up additional storage space.

Accordingly it would be desirable to provide a system that allows for the efficient automatic management of data in a storage system throughout the data s lifecycle on each section of a logical volume.

According to the system described herein a method of de duplicating data may include providing a first volume including at least one pointer to a second volume that corresponds to physical storage space. The first volume may be a logical volume. A first set of data may be detected as a duplicate of a second set of data stored on the second volume at a first data chunk. A pointer of the first logical volume associated with the first set of data may be modified to point to the first data chunk wherein after modifying the pointer no additional physical storage space is allocated for the first set of data. A counter associated with the first data chunk that identifies a number of pointers pointing to the data chunk may be updated. The first volume may include a thin device.

The second volume may be a physical storage area that contains the physical storage space or a logical volume that maps to allocations of the physical storage space. The first set of data may previously exist on the second volume and the method may further include de allocating physical storage space associated with the first set of data that previously existed on the second logical volume. The first set of data may be associated with a new write request and the method may further include intercepting the new write request after detecting that the first set of data of the new write request is a duplicate of the second set of data stored on the second logical volume. The first volume may include a first thin chunk and a second thin chunk wherein the first thin chunk includes the pointer to the first data chunk of the second volume and the second thin chunk may include a pointer that points to the first thin chunk. A write request to change data associated with a particular one of the first set of data and the second data set may be intercepted. The particular one of the first set of data and the second set of data may be copied to a different data chunk. The pointer of the volume associated with the particular one of the first set of data and the second set of data may be modified to point to the different data chunk. A counter associated with the first data chunk may be decremented. The write request to change the particular one of the first set of data and the second data set may be performed at the different data chunk.

According further to the system described herein a computer program product stored on a computer readable medium for de duplicating data includes executable code that provides a first logical volume including at least one pointer to a second volume that corresponds to physical storage space. Executable code may detect a first set of data as a duplicate of a second set of data stored on the second volume at a first data chunk. Executable code may modify a pointer of the first volume associated with the first set of data to point to the first data chunk wherein after modifying the pointer no additional physical storage space is allocated for the first set of data. Executable code may update a counter associated with the first data chunk that identifies a number of pointers pointing to the data chunk. The second volume may be a physical storage area that contains the physical storage space or a logical volume that maps to allocations of the physical storage space. The first set of data may previously exist on the second volume and the computer program product may further include executable code that de allocates physical storage space associated with the first set of data that previously existed on the second logical volume. The first set of data may be associated with a new write request and the computer program product may further include executable code that intercepts the new write request after detecting that the first set of data of the new write request is a duplicate of the second set of data stored on the second volume.

According further to the system described herein a computer storage device includes a plurality of interconnected directors wherein at least some of the directors handle reading and writing data for the computer storage device and a plurality of disk drives coupled to at least some of the directors that store data for the computer storage device. Computer software provided on a computer readable medium of at least one of the directors may be included having executable code that provides a first volume including at least one pointer to a second volume that corresponds to physical storage space wherein the first volume is a logical volume. Executable code may detect a first set of data as a duplicate of a second set of data stored on the second volume at a first data chunk. Executable code may modify a pointer of the first volume associated with the first set of data to point to the first data chunk wherein after modifying the pointer no additional physical storage space is allocated for the first set of data. Executable code may update a counter associated with the first data chunk that identifies a number of pointers pointing to the data chunk. The second volume may be a physical storage area that contains the physical storage space or a logical volume that maps to allocations of the physical storage space. The first set of data may previously exist on the second volume and the computer software may further include executable code that de allocates physical storage space associated with the first set of data that previously existed on the second logical volume. The first set of data may be associated with a new write request and the computer software may further include executable code that intercepts the new write request after detecting that the first set of data of the new write request is a duplicate of the second set of data stored on the second volume.

According further to the system described herein a computer storage system includes a remote storage device and a local storage device coupled to the remote storage device via a data link the local storage device having at least one processor that controls a first logical volume of the local storage device the first logical volume including at least one pointer to a second volume of the local storage device that corresponds to physical storage space where in response to the local storage device detecting duplicate data prior to transmission of the data to the second storage device the local storage device transmits information indicative of the duplicate data instead of the data. The second volume may be a physical storage area that contains the physical storage space and or a logical volume that maps to allocations of the physical storage space. The first set of data may be associated with a new write request and the processor may intercept the new write request after detecting that the first set of data of the new write request is a duplicate of the second set of data stored on the second volume. The remote storage device may update a counter associated with the data that identifies a number of pointers pointing to the data chunk. Following transmitting the information indicative of duplicate data the local storage device may provide an acknowledgement.

According further to the system described herein cloning data includes providing a first volume including at least one pointer to a second volume that corresponds to physical storage space where the least one pointer of the first volume points to a data chunk stored on the second volume and where the first volume is a logical volume providing a third volume including at least one pointer where the third volume is another logical volume and modifying the at least one pointer of the third volume to point to the data chunk without allocating any additional physical storage space. Cloning data may also include updating a counter associated with the data chunk to indicate that more than one logical volume points to the data chunk. The second volume may be a physical storage area that contains the physical storage space and or a logical volume that maps to allocations of the physical storage space.

A system may be provided having at least one processor that performs any of the above noted steps. A computer program product stored on a computer readable medium for de duplicating data may be provided including executable code for performing any of the above noted steps.

Referring now to the figures of the drawings the figures comprise a part of this specification and illustrate exemplary embodiments of the described system. It is to be understood that in some instances various aspects of the system may be shown schematically or may be shown exaggerated or altered to facilitate an understanding of the system.

Each of the HA s of the storage device may be coupled to one or more host computers that access the storage device . The host computers hosts access data on the disk drives through the HA s and the DA s . The global memory contains a cache memory that holds tracks of data read from and or to be written to the disk drives as well as storage for tables that may be accessed by the HA s the DA s the RA and the EA . Note that for the discussion herein blocks of data are described as being a track or tracks of data. However it will be appreciated by one of ordinary skill in the art that the system described herein may work with any appropriate incremental amount or section of data including possibly variable incremental amounts of data and or fixed incremental amounts of data and or logical representations of data including but not limited to compressed data encrypted data or pointers into de duplicated data dictionaries.

In some embodiments one or more of the directors may have multiple processor systems thereon and thus may be able to perform functions for multiple directors. In some embodiments at least one of the directors having multiple processor systems thereon may simultaneously perform the functions of at least two different types of directors e.g. an HA and a DA . Furthermore in some embodiments at least one of the directors having multiple processor systems thereon may simultaneously perform the functions of different types of director and perform other processing with the other processing system. Generally the system described herein could work with any appropriate hardware configuration including configurations where at least some of the memory is distributed among at least some of the directors and in configurations where a number of core processors are coupled to a generic interface card.

The system described herein is suitable for use with the technique of thin provisioning. Thin provisioning allows for the creation of logical volumes of storage space where allocation of physical storage space occurs only when space is actually needed e.g. when data is written in the first time to the storage space . Logical storage space may be identified to a user as being available even though no physical storage space has been committed at least initially. When data is written to the logical storage space physical storage space is drawn for use from a pool of physical storage space as further described elsewhere herein. In addition as described in more detail elsewhere herein stored data may be moved between physical locations using the storage infrastructure described herein.

The storage device may also include one or more thin devices . Each of the thin devices may appear to a host coupled to the storage device as a logical volume logical device containing a contiguous block of data storage. Each of the thin devices may contain tables that point to some or all of the data devices or portions thereof as further discussed elsewhere herein. In some instances the thin devices may be concatenated to form a metavolume of thin devices. In some embodiments only one thin device may be associated with the same data device while in other embodiments multiple thin devices may be associated with the same data device as illustrated in the figure with arrows having broken lines.

In some embodiments it may be possible to implement the system described herein using storage areas instead of storage devices. Thus for example the thin devices may be thin storage areas the data devices may be standard logical areas and so forth. In some instances such an implementation may allow for hybrid logical devices where a single logical device has portions that behave as a data device and or portions that behave as a thin device. Accordingly it should be understood that in appropriate instances references to devices in the discussion herein may also apply to storage areas that may or may not correspond directly with a storage device.

Each of the entries of the table correspond to another table that may contain information for one or more logical volumes such as thin device logical volumes. For example the entry may correspond to a thin device table . The thin device table may include a header that contains overhead information such as information identifying the corresponding thin device information concerning the last used data device and or other information including counter information such as a counter that keeps track of used group entries described below . The header information or portions thereof may be available globally to the storage device .

The thin device table may include one or more group elements that contain information corresponding to a group of tracks on the data device. A group of tracks may include one or more tracks the number of which may be configured as appropriate. In an embodiment herein each group has sixteen tracks although this number may be configurable or dynamically adjustable based on criteria described elsewhere herein.

One of the group elements for example the group element of the thin device table may identify a particular one of the data devices having a track table that contains further information such as a header having overhead information and a plurality of entries corresponding to each of the tracks of the particular one of the data devices . The information in each of the entries may include a pointer either direct or indirect to the physical address on one of the disk drives of the storage device or a remote storage device if the system is so configured that maps to the logical address es of the particular one of the data devices . Thus the track table may be used in connection with mapping logical addresses of the logical devices corresponding to the tables to physical addresses on the disk drives of the storage device .

The tables may be stored in the global memory of the storage device . In addition the tables corresponding to particular logical devices accessed by a particular host may be stored cached in local memory of the corresponding one of the HA s . In addition the RA and or the DA s may also use and locally store cache portions of the tables .

Accordingly a thin device presents a logical storage space to one or more applications running on a host where different portions of the logical storage space may or may not have corresponding physical storage space associated therewith. However the thin device is not mapped directly to physical storage space. Instead portions of the thin storage device for which physical storage space exists are mapped to data devices which are logical devices that map logical storage space of the data device to physical storage space on the disk drives . Thus an access of the logical storage space of the thin device results in either a null pointer or equivalent indicating that no corresponding physical storage space has yet been allocated or results in a reference to a data device which in turn references the underlying physical storage space.

If it is determined at the step that there is physical data corresponding to the logical tracks being read then processing proceeds to a step where one or more of the data devices associated with the logical tracks being read are identified from the group table . After the step processing proceeds to a step where the track table is read from the identified one or more of the data devices and the corresponding location of the physical data i.e. cylinder and track is determined. As further discussed elsewhere herein physical storage space may be provided in connection with one data device and or by a concatenation of multiple data devices or portions thereof. Logical storage space of the physical devices maps to logical storage space. After the step processing proceeds to a step where a request may be sent to one or more disk adapters corresponding to disk drives that provide the physical storage space associated with the identified one of the data devices and corresponding location information. After the step processing proceeds to a step where the physical data is read. Note that the data may be stored in a cache or other memory for example the memory in connection with being read. In some cases if the data being read is already in the cache then the processing at the step and following steps may not be necessary. Note also that reading the data may include updating any metadata used to provide the processing described herein such as the time last accessed the host user making the request frequency of use and or any other appropriate metric. After the step processing proceeds to a step where the data may be received by an appropriate one of the host adapters e.g. by reading the memory . After the step processing is complete.

Following the step is a test step where it is determined whether physical space had been previously allocated i.e. in a prior write operation for the logical tracks being written. If so then processing proceeds to a step where the data device that includes the logical tracks is identified. After the step is a step where the track table is read from the identified one or more of the data devices and the corresponding location of the physical data i.e. cylinder and track is determined. As further discussed elsewhere herein physical storage space may be provided in connection with one data device and or by a concatenation of multiple data devices or portions thereof. Logical storage space of the physical devices maps to logical storage space. Following the step processing proceeds to a step where the data being written is directed to the appropriate physical storage space. The incoming data overwrites the appropriate portion of the data where directed. After the step processing is complete.

If it is determined at the step that there is no physical storage that has been allocated for the logical track s being written then control transfers from the step to a step where a next available data device identifier i.e. the data device is determined. This information may be obtained from the header of the device table . In an embodiment herein data device identifiers are provided by numbers so that a next available data device identifier is simply one more than a last allocated data device. However as discussed in more detail elsewhere herein selection of a data device at the step may include application of other criteria.

After the step processing proceeds to a step where available physical storage space on the disk drives is determined. In an embodiment herein available physical storage space is allocated sequentially from one or more of the disk drives . Following the step is a step where a request may be sent to a disk adapter or possibly the RA and or the EA to allocate the physical storage space for the write. Also at the step header info is updated to reflect the newly allocated data device and physical tracks. After the step processing proceeds to the step discussed above where the data being written is directed to the one or more data devices. After the step processing is complete.

After the read and write processes illustrated in information concerning access of the data such as access frequency time of last access or use and or other characteristics and statistics may be updated and stored by the system described herein. The updated data access information or other characteristic information of the data and or any portion of the data may for example be stored as an entry in a group element of the thin device table for example the entry of the group element as shown in . Alternatively the data characteristic information may be stored in a memory such as the global memory of the storage device and a pointer to this information stored in the group element . Other implementations for storing and access of the data characteristic information are possible.

The allocation of the physical storage space for a thin device at the time of writing the data as well as the policies that govern the allocation may be transparent to a user. For example a user s inquiry into how much storage space is available on a particular thin device may indicate a maximum amount of physical storage space that could be allocated for a thin storage device provisioned storage space even though the corresponding physical storage space had not yet been allocated. In an alternative embodiment the policy for the thin device may be to report something less than the total maximum that could be allocated. In some embodiments used physical storage space may not exceed 30 of the provisioned storage space.

In an embodiment herein different portions of the physical data may be automatically moved between different physical disk drives or other storage devices with the same or different characteristics according to one or more policies. For example data may be initially allocated to a particular fast disk drive but a portion of the data that has not been used over a period of time for example three weeks may be automatically moved according to the system described herein to a slower and perhaps less expensive disk drive. The physical data may then be automatically moved back to the faster disk drive if the data is subsequently used and or accessed according to a policy or other criteria for example accessed twice in any given week as further described herein. Thus the system described herein may operate to automatically move data between disk drives or other storage devices within the same machine according to the one or more policies.

A policy may be configured by an administrator on a system wide level or may be specific to a particular user on a specific logical device. The system described herein allows for the remapping of physical data based on policy criteria or other statistics. For example the policy may be based on the last time data was used and or accessed. Alternatively the policy may be based on anticipated use of data over specific times and or dates. For example data that is expected to be used at a particular time may be stored on or relocated to relatively fast disk drives and then moved to relatively slow disk drives when it is expected that the data will not be used again for a lengthy period of time. Moreover different policies and or criteria may be implemented corresponding to different users and or different levels of importance or security of data. For example it may be known that user A accesses particular data more frequently than user B and accordingly the policy for moving physical data according to the system described herein may be to leave more data associated with user A on the relatively fast disk drive as compared with the data associated with user B. Alternatively user A may access data that is generally of a higher level of importance or requires higher security than that of user B and accordingly the system described herein may maintain and or move more data associated with user A on a disk drive that is relatively more reliable available and or secure as compared with the data associated with user B.

In an embodiment herein data may be moved between physical disk drives or other physical storage having different characteristics such as speed cost reliability availability security and or other characteristics. As discussed elsewhere herein logical data devices may be established having different classes corresponding to characteristics of the physical disk drives to which the data devices are mapped. Further it should be noted that any section of the logical device may be moved according to the system described herein based on the characteristics of the data and governed by default or specific policies .

After the step processing proceeds to a step where the policy is applied to the stored data. The policy may include criteria used for managing stored data such as criteria concerning frequency of use of data and or criteria with respect to specific users and or other criteria such as file name file type file path requesting application expected time to re use of the data temporary storage only life expectancy of the data data type e.g. compressed encrypted de duped and or protection requirements of the data e.g. store on an encrypted tier . The policy may be applied to identify data for lifecycle management according to characteristics of entire data volumes or any portions thereof. The policy may also consider the access history effective performance or other characteristics about the data that might be utilized to optimize the performance cost availability or retention requirements of the data.

After the step processing proceeds to a step where the data for which characteristics have been determined is managed according to the policy and based on the characteristics of the data. For example data that is frequently used may be moved to a relatively fast storage device whereas data that has not been used over a certain period of time may be moved to a relatively slow storage device according to the data processing as discussed elsewhere herein. As noted herein the data that is moved may be entire data volumes or portions thereof.

After the step processing proceeds to a test step where it is determined if another policy with other criteria should be applied to the stored data being managed. If an additional policy is to be applied then processing proceeds to the step . If no further policies are to be applied then processing proceeds to a test step where it is determined whether there is more data to be managed according to the system described herein. If there is further stored data to manage then processing proceeds back to the step . If no further stored data is to be managed then after the test step processing is complete. In some cases tracking avoiding and resolving conflicting priorities would be handled to ensure that two policies do not create a ping pong effect moving data blocks up and down in a never ending cycle.

As discussed elsewhere herein the data devices may be associated with physical storage areas e.g. disk drives tape solid state storage etc. having different characteristics. In various embodiments the physical storage areas may include multiple tiers of storage in which each tier of storage areas and or disk drives that may be ordered according to different characteristics and or classes such as speed technology and or cost. The thin devices may appear to a host coupled to the storage device as a logical volume logical device containing a contiguous block of data storage as discussed herein. Each thin device may correspond to a particular data device a portion thereof and or multiple data devices. Accordingly each thin device may map to storage areas across multiple storage tiers. As a result although each thin device may appear as containing a logically contiguous block of storage each thin device may allow for blocks of data to be transparently stored and or retrieved from discontiguous storage pools made up of the varying classes of data storage devices. In this way the granularity at which the system for tiered storage described herein operates may be smaller than at the file level for example potentially as small as a single byte but more practically at the granularity of a single logical block or collection of sequential data blocks. A data block may be of any size including file system or database logical block size physical block track or cylinder and or other size. Multiple data blocks may be substantially the same size or different sizes such as different size data blocks for different storage tiers or different sized data blocks within a single storage tier.

The thin device may map to different storage areas devices across multiple tiers. As discussed herein the granularity of the system described herein may be less than at the file level and allow for blocks of data of any size to be stored across multiple storage tiers of the storage device in a process that is transparent to the host and or host application. For example in the illustrated embodiment the thin device may map blocks of data to storage areas devices such as a storage area in the pool of storage of the top storage tier a storage area in the pool of storage of the next storage tier storage areas in pool of storage of the next storage tier and storage areas in the pool of storage of the next storage tier . As discussed elsewhere herein the last storage tier may include external storage and the system described herein may map to a storage area in the pool of storage in the tier .

At least one storage tier e.g. the lowest storage tier may include redundant data elimination RDE de duplication storage. RDE de duplication technology involves identifying multiple instances of data and storing only a single instances of that data e.g. files blocks chunks tracks etc. thereby eliminating redundant storage of the same data. RDE de duplication technologies may be particularly applicable for data that is accessed and or changed infrequently. For example RDE de duplication may be applied only for data blocks that have not been modified in a particular number of hours days etc. and or may be applied in conjunction with a new write and or other I O action as further discussed elsewhere herein. Further the system may include processing to separate data blocks files etc. that have been de duped in the case where the data is no longer suitable for RDE de duplication. For example parts of two separate data files may initially be redundant and subjected to RDE to reduce storage space usage however if a write is subsequently requested for only one of the initially redundant data files then the initially redundant data files may require separate processing and no longer be appropriate for RDE. RDE de duplication may allow fewer physical data blocks to support reads from multiple different users applications hosts etc. RDE de duplication is described in more detail elsewhere herein in accordance with embodiments of the system described herein. Reference is also made to U.S. Pat. No. 6 704 730 to Moulton et al. which is incorporated by reference herein which discloses systems for eliminating or screening redundant copies of aggregate blocks of data.

Mirroring backup of data may also be facilitated by tiered storage across multiple tiers according to the system described herein. For example data that is accessed frequently may be stored on a fast storage device tier while a mirrored copy of the data that is not expected to be accessed may be stored on a slower storage device in one or more other tiers e.g. tiers . Accordingly the same data may be stored on storage devices of multiple tiers of storage pools.

In an embodiment herein a write target policy may be applied to data that is being written according to the system described herein. For example data that is expected to be used frequently for example database indices may be initially written directly to fast storage e.g. tier flash SSD storage whereas data that is not expected to be accessed frequently for example backup or archived data may be initially written to slower storage devices e.g. tier MAID or external storage . In this manner data is efficiently stored by targeting the write to storage areas and devices according to the estimated or expected access frequency of the data beginning with the initial write of the data and also applying to subsequent data writes that jump across multiple tiers.

The process for determining the appropriate target storage location of the write of the data may be made based on the logical unit number LUN ID of the device from which the data is being written where the storage device may have or obtain information about the types of data stored on specific logical units. Alternatively additional policies and capabilities may be enabled by adding host resident extension software for example to tag I O requests with information about the requesting application or user so that the determination may be made based on other information provided by the host and or entity accessing the storage device e.g. a target policy indicator provided with each write or class of writes . Other possible criteria include the time of day the size of the incoming write operation e.g. very large sequential writes vs. smaller random writes file name file type host OS type data type access patterns inter dependent accesses to other data etc. It is also possible that hints from the host could also be used particularly relating to performance and availability requirements of the data etc.

The system described herein may include autonomic promotion and demotion policies to facilitate optimization of performance storage availability and power. For example a least recently used LRU policy may be used to demote data blocks in order to pro actively make room for new writes of data blocks and or promotions of data blocks within the system. A most frequently used MRU policy may be used to promote data blocks that are frequently used to faster storage tiers. Predictive policies may be used to recognize that data blocks that will be needed before they are actually needed and promote the data blocks accordingly for example nightly batch jobs etc. . Alternatively the system described herein may include an application programming interface API that allows a hosts users applications to inform the storage that certain blocks should be promoted or demoted to different tiers.

Other special purpose policies may also be used. As discussed elsewhere herein mirroring of data blocks across multiple tiers may be used. For example for frequently used data blocks one copy may be written to flash SSD memory at a top storage tier and a second copy mirrored to another storage tier e.g. tier or tier . Another policy may include promoting and or demoting a data block but not deleting the data block from its pre promoted or demoted location until the data block is modified. This policy offers advantages including when subsequently demoting the block if unmodified a copy may already exist on a slower storage tier and an additional copy does not need to be made only the copy on the faster storage tier deleted . When a data block is modified the previous copy on a different storage tier may be deleted.

Other policies may include manual or automatic pre promotion and post demotion policies. For example blocks may be promoted in the background immediately before batch runs e.g. billing runs etc. . Additionally writes for such processes as back ups may required the fastest possible write but never or only infrequently read. In this case writes may be written to a top storage tier and immediately scheduled for demotion to a lower storage tier. With MAID storage data blocks rarely or never used may be consolidated onto individual spindles that may then be powered off providing a reduction in power consumption for storage of data blocks infrequently accessed. Further sequential contiguous blocks may be coalesced and relocated in an optimization process that may include other advanced strategies including aligning indices near to data being indexed. It is also possible to have a de duplication policy in which nothing is deleted from storage in a de dup tier. Data blocks in storage pools of a de dup storage tier may be promoted to fast storage tiers as needed but block and index metadata in the de dup storage may be maintained even if a data block is promoted to a faster storage tier and modified or deleted. Maintenance of de dup storage tiers may involve use counters and other mechanisms that may be used with known data cleaning processes such as garbage collection etc.

After the step processing proceeds to a step where the determined information associated with the data is processed according to the target policy and the data block is written to a storage location in the storage device according thereto. Accordingly the data block may initially be written to a storage area device in a pool of storage of a storage tier corresponding to the anticipated frequency of use of the data block and or according to other criteria. After the step processing proceeds to a step where information concerning the location of the data block is updated in a table of information in the thin device as further discussed elsewhere herein. After the step processing is complete.

In some cases there may be insufficient available free space to write data to the storage tier corresponding to the storage policy at the step . This may be addressed in a number of ways. One possibility is to maintain the data in cache memory until space becomes available which can occur when data is moved from the target tier as a result deletion of promotion demotion based on storage policies. Note also that it is possible to temporarily store the data in a lower tier and then schedule the data for promotion to the appropriate tier using any appropriate mechanism such as setting a flag that causes the data to be promoted before any other data.

If it is determined at the test step that the storage tier is full then control passes from the test step to a step where wait processing is performed. The wait at the step could be for any appropriate amount of time. Following the step control passes back to the test step for a new iteration.

If it is determined at the test step that the storage tier is full then control passes from the test step to a step where the data is written to a different storage area such as a lower or higher storage tier or to global memory of the storage device e.g. cache memory as further discussed herein. The data may be placed in the different storage area temporarily. Following the step is a step where the data is scheduled to be moved to the appropriate storage area the originally destined storage tier . Following the step processing is complete.

In an embodiment at the step the write data may be temporarily stored in a global memory such as the global memory until memory in the particular requested tier becomes available that is sufficient to handle the write request. At the step scheduling for the movement of the data may include relocating data in the particular requested tier e.g. faster storage tier to a lower tier e.g. slower storage tier to make memory available for the data temporarily stored in the global memory. In another embodiment at the step data for the requested write may be immediately written to a lower tier than that requested for the write and at the step a future promotion of the data to the particular requested higher tier originally destined storage tier may be scheduled. The embodiments discussed herein provide for the dynamic re allocation and re ordering of data to accommodate write policies usage patterns and the like.

At the step data blocks are to be promoted and or demoted according to the one or more policies. If a data block is promoted the data block is moved to a storage area device in a pool of storage of a higher storage tier for example faster storage. If a data block is to be demoted the data block is moved to a storage area device in a pool of storage of a lower storage tier for example slower storage. As further discussed elsewhere herein in some cases the promotion and or demotion procedure may include moving copies of data blocks to other storage tiers and the deleting the old data blocks from their original storage location and or copies of data blocks previously stored at the subsequent storage tiers may be used and movement of the data block is to make the previously stored version of the data block become again the current accessible data block.

After the step processing proceeds to a step where information concerning the location of the data block is updated in a table of information in the thin device as further discussed elsewhere herein. After the step processing proceeds to a test step where it is determined whether additional stored data is to be managed according to the system described herein. If more stored data is to be managed promoted demoted processing proceeds back to the step . Otherwise processing is complete.

As described elsewhere herein it may be appropriate to store data blocks in multiple locations and or on multiple different tiers in order to implement and maintain the performance or availability policies. For example should a policy determine that a particular block should be moved to a MAID tier where drives are powered down for lengthy periods a copy may also be kept on otherwise unused space in a higher tier. Should a request arrive for the block it could thus be serviced from the copy in the higher tier instead of requiring to power up the MAID storage device. In such situations the secondary copy might be identified as expendable. In the event a new write arrives that requires space utilized by the secondary expendable block it could be overwritten with the new data and the appropriate information regarding the location of the old and new blocks updated appropriately. Further such expendable blocks could be deleted on other trigger events or on a timer based schedule.

The system described herein may be used in conjunction with garbage collection processing for memory management. For example for data blocks that are being kept in two separate tiers either temporarily or for longevity it may be desirable to delete one copy to make room for more new data e.g. when a tier is nearly full or when the pool itself is getting full . As further discussed herein one or more lists of potential delete candidates may be maintained and garbage collection then becomes a process of traversing the one or more delete lists and removing redundant copies according to policy e.g. keep the fastest keep the slowest cheapest base decision on which one more recently frequently utilized accessed etc. .

In an embodiment of the system described herein a list of data sections that may be deleted delete list may be maintained as a linked list such as a singly linked list or a doubly linked list although other known data structures may be used. Candidates for deletion may be inserted and removed from the list using known linked list manipulation techniques or other data structure manipulation techniques. Entries in the delete list may be generated during promotion or demotion processing as discussed elsewhere herein. For example a data block may be promoted from a lower tier to a higher tier with the data block being copied to the higher tier and the copy of the data block on the lower tier being added to the delete list. Other techniques for generating entries in the delete list are possible.

If the delete list contains entries indicating that there are data block candidates for deletion then processing proceeds to step where the delete list is traversed to select a data block candidate for garbage collection i.e. deletion. The delete list may be configured as appropriate such as with the oldest data block candidate for deletion at the head of the linked list and the most recent data block candidate for deletion at the end. Other traversal configurations may also be used. Accordingly for a delete list configured as noted above the head of the list containing the oldest data block candidate for deletion may be selected for garbage collection. After the step processing may proceed to a step at which the selected data block candidate is deleted or otherwise marked for deletion. After the step processing proceeds to a step where the delete list is updated for example using appropriate linked list manipulation techniques. After the step processing may proceed back to the test step to determine whether there is sufficient memory now available for the write request.

In various embodiments the system described herein may allow for restore capabilities for data block candidates on the delete list. Although such data block candidates may no longer be written to the data block candidates marked for deletion as discussed herein may still be read to provide restore capability of older data. The system described herein may also include processing to monitor when data block candidates on the delete list are no longer redundant copies of data. For example data may be initially maintained as a redundant copy of data promoted to a different storage tier but become outdated if the promoted data is changed by additional writes. In this instance the older version of the data may be moved to the top of the delete list and or immediately marked for deletion. The system described herein may allow for splitting of data blocks for garbage collection that includes splitting portions of files datasets and or databases as further discussed elsewhere herein.

According to another embodiment of the system described herein data de duplication or redundant data elimination RDE may be used to reduce the amount of actual storage space consumed by a device and or the amount of data copied locally inside a storage device and or the amount of data sent over a link if a device is protected by a remote storage system such as an RDF product by EMC Corporation of Hopkinton Mass.

A data de duplication algorithm may detect duplicate data on a chunk by chunk basis where a chunk may represent a plurality of bytes tracks sections etc. of data. In an embodiment duplicate data may be detected as it is written to a thin device however other detection processes may be used including periodic duplicate data detection that is performed as a background process. The data de duplication algorithm may use a Cyclical Redundancy Check CRC signature and or other high reliability signature that is calculated and updated for a data chunk to detect instances of duplicate data. Examples of algorithms for detecting data duplication including the use of hashing are described in U.S. Pat. No. 6 704 730 to Moulton et al. which is incorporated by reference herein. If data of a write request is found to be a duplicate of an already existing chunk no new storage space is allocated when the write request is made to the thin device. Instead the thin device may be configured to point to the data device that contains the already existing copy of the data. In an embodiment a reference counter may be maintained on the data device for example as part of a data structure used in connection with a data device chunk indicating that the particular data chunk has more than one owner including multiple thin chunks on one thin device and or multiple thin chunks among multiple thin devices as further discussed elsewhere herein. In another embodiment a reference counter may be maintained on a thin device that indicates other of the thin devices may be linked to this thin device concerning ownership of a particular data chunk as further discussed elsewhere herein. In this way the system described herein provides for date duplication with data owners within one thin device and or with data owners across multiple thin devices. Note that any other appropriate technique for detecting duplicate data be used including for example byte by byte comparison.

When a write request is made to the thin device a data de duplication detection algorithm as discussed elsewhere herein may be used to detect whether the data of the write request is a duplicate of data already stored on one of the data devices . If it is determined by the data de duplication detection algorithm that the data of the write request is a duplicate of data already on one of the data devices for example a data chunk on the data device that is pointed to by thin chunk of the thin device then according to the system described herein the new data may not be written again to one of the data devices . Instead a new thin chunk may be configured on the thin device to also point to the data chunk

A reference counter may be included that indicates how many owners there are of a data chunk stored on a data device. An owner of a data chunk on a data device may represent a thin chunk of a thin device pointing to the data chunk. The reference counter may be maintained on a data chunk of the data device. In various embodiments the reference counter may be included as part of data in the data chunk that the reference counter is maintaining for example data chunk and or may be part of a different data chunk. Alternatively the reference counter may be part of data external to the data devices for example as part of data of the thin device. The reference counter may be additional metadata that keeps track of the number of owners and or users of the data. The size and counting range of the counter may be balanced against the storage space savings. For example having a byte counter may allow creation of 255 data clones.

In other embodiments the system described herein may operate with only one logical volume that maps to physical storage space on a physical volume. is a schematic illustration of storage system showing a logical device including logical sections that both point directly to a section of a physical storage area e.g. one or more physical drives and indicate de duplication of data stored on the section of the physical storage area . In this embodiment the logical device may be similar to the thin device but which maps directly to the physical volume without intervening data devices.

In a thin chunk points to a thin chunk and accordingly in the event of data being mirrored or just moved from one storage tier to another storage tier the de duplication process according to the illustrated embodiment of the system described herein allows for only the pointers from the thin chunk to be modified to point to the data chunk of the data device and or the new data chunk of the new data device . It should be noted that a storage device like that described herein for the storage device may also be used with the above described embodiment however for such a storage device in the event of mirroring or moving data to a data device of a different storage tier both thin chunks such as the thin chunks and described herein would each be modified to point to the new data chunk of the new data device.

The system described herein may also be implemented in a device having RDF protection. An RA may detect the de duplicated chunks and generate a type of I O indicating that the chunk is the same as another chunk already transferred. The de duplication mechanism may be used in conjunction with a local storage device on an RDF copy of data that is sent to a remote storage device and or on host I O data flow and may be beneficial in reducing the amount of duplicate data that is sent via the data link from the local storage device to the remote storage device see . Additionally the remote storage device may include also include a de duplication system as described herein. As further discussed elsewhere herein de duplication processes may be performed as writes are made to a storage device and or as periodic background processes. For example in the case of an RDF system de duplication may be performed in conjunction with daily or nightly synchronization of the local and remote storage devices.

After the step processing proceeds to a step where the first owner i.e. the first thin chunk pointing to the non selected data chunk is identified. After the step processing proceeds to a step where a pointer from the identified thin chunk owner pointing to the non selected data chunk is modified to instead point to the selected operable data chunk. After the step processing proceeds to a step where a reference counter for the selected data chunk is incremented to reflect that an additional thin chunk is now pointing to the selected data chunk. After the step processing proceeds to a step where a reference counter for the non selected data chunk is decremented to reflect that a thin chunk is no longer pointing to the non selected data chunk. Alternatively is should be noted that the order of steps and may be reversed. After the step processing proceeds to a test step where it is determined if the reference counter of the non selected data chunk is zero. If the reference counter of the non selected counter is not zero indicating value of one or greater then processing proceeds to a step where another owner of the data in the non selected data chunk is identified. After the step processing proceeds back to the step .

If after test step it is determined that the reference counter of the non selected data chunk is zero then processing proceeds to a step where the storage space of the non selected data chunk is de allocated and otherwise freed for subsequent use. After the step processing is complete.

It should also be noted that de duplication processing like that described with respect to the local storage device may also be conducted independently at the remote storage device. For example the de duplication may be performed as part of background processing procedures at the remote storage device. Note also that the system described herein does not depend on any particular physical distance between devices e.g. between a thin device and any corresponding data devices and or between a data device and any corresponding physical storage area so that in appropriate instances the devices may be physically separated into different geographic areas e.g. U.S. east coast and U.S. west coast .

The above noted steps and other steps described herein may be performed automatically by the system described herein. For example steps described herein may be performed periodically at designated times and or after particular trigger events such as receipt of a write request access by a particular user to the system log in and or log out after assessment of space usage on the disk drives for example space usage on the fast disk drive and or after some other suitable I O action. Alternatively the steps described herein may be activated manually by a user and or a system administrator. In an embodiment the system described herein may include a system having at least one processor that performs any of the above noted steps. Further computer software stored in a computer readable medium may be provided according to the system described herein including executable code for carrying out any of the steps and processes described herein.

The system described herein provides an architecture for dynamically deploying applying managing and or optimizing tiers of block storage is provided in a manner that is transparent and non intrusive to standard host operating systems file systems databases and or other data repositories. The system provides for data management using thin provisioning techniques to manage and relocate data among multiple storage tiers offering at least the following advantages a does not require special host resident file system replacement or extension software b may operate at a granularity smaller than the file level c may transparently support any host application that utilizes the data block device file system databases etc. and d may allow for the dynamic relocation of data blocks including portions of files datasets and or databases based on application demand and or policy. The system further provides for de duplication of data to eliminate instances of redundant data and reduce the unnecessary use of storage space and for sparsely cloning of data to facilitate data management.

Other embodiments of the invention will be apparent to those skilled in the art from a consideration of the specification or practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only with the true scope and spirit of the invention being indicated by the following claims.

