---

title: Various methods and apparatus for moving thumbnails with metadata
abstract: Various methods, apparatuses, and systems are described for a moving thumbnail generator. The moving thumbnail generator generates one or more moving thumbnails that are tagged and time-stamped to arbitrary video trigger events that take place in an associated original video file. Each of the moving thumbnails has two or more moving frames derived from its associated original video file at a portion in the original video file that is tagged and time-stamped to one or more relevant video trigger events in order for the generated moving thumbnail to summarize a visual content of the associated original piece of video file to a user. The tag carries information about content that takes place in the original video file and metadata about that content including a time reference to frames that are contextually relevant to the reason why a viewer of the thumbnail might be interested in viewing that thumbnail.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08196045&OS=08196045&RS=08196045
owner: Blinkx UK Limited
number: 08196045
owner_city: Cambridge, England
owner_country: GB
publication_date: 20070123
---
This application claims the benefit of U.S. provisional application No. 60 850 115 filed on Oct. 5 2006.

A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the software engine and its modules as it appears in the Patent and Trademark Office Patent file or records but otherwise reserves all copyright rights whatsoever.

Some embodiments of the invention generally relate to a search engine that retrieves moving thumbnails. More particularly an aspect of an embodiment of the invention relates to video search engines that retrieve moving thumbnails.

An explosive volume of video is available within an on demand context. In an on demand system any piece of video can be viewed by any user at any point of time. Unlike a linear or sub linear system where some centralized scheduling is used and the users are somewhat limited in their viewing choices at a given point in time an on demand system makes no guess as to what a given user will want to watch when. A number of challenges exist in facilitating such a system there is a considerable bandwidth requirement if the system is to be distributed in any way and storage technologies that are able to stream large volumes of data perhaps in parallel are required for example. A further challenge that has become apparent only as these systems get larger is that of navigation. Put simply when the average consumer s video experience was limited to fewer than a hundred explicit linear channels a simple Programming Guide and channel selector buttons on a remote control may have provided sufficient user interface to the content available. As the corpus of available content gets larger however fixed channel and other hierarchical choice systems become increasingly unwieldy to operate and free form textual search is used instead.

When free form search is used over a large corpus however it will still often return voluminous amounts of possible search results. A user needs to quickly and efficiently sort through these many options sometimes numbering in their hundreds in order to choose a video segment to actually watch. This search problem exists outside the video context and has been solved in various ways textual search engines e.g. www.Google.com www.altavista.com etc already make use of so called contextual summarization that displays a portion of the text of the matching textual document so that a user is able to quickly assess not just that a given document matched their search but also why and how that document matched their search.

Various methods apparatuses and systems are described for a moving thumbnail generator. The moving thumbnail generator generates one or more moving thumbnails that are tagged and time stamped to arbitrary video trigger events that take place in an associated original video file. Each of the moving thumbnails has two or more moving frames derived from its associated original video file at a portion in the original video file that is tagged and time stamped to one or more relevant video trigger events in order for the generated moving thumbnail to summarize a visual content of the associated original piece of video file to a user. The tag carries information about content that takes place in the original video file and metadata about that content including a time reference to frames that are contextually relevant to the reason why a viewer of the thumbnail might be interested in viewing that thumbnail. Each moving thumbnail has a relational link back to the original video file associated with the moving thumbnail that can be used to retrieve the original video file.

While the invention is subject to various modifications and alternative forms specific embodiments thereof have been shown by way of example in the drawings and will herein be described in detail. The invention should be understood to not be limited to the particular forms disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the invention.

In the following description numerous specific details are set forth such as examples of specific signals named components types of filters etc. in order to provide a thorough understanding of the present invention. It will be apparent however to one of ordinary skill in the art that the present invention may be practiced without these specific details. In other instances well known components or methods have not been described in detail but rather in a block diagram in order to avoid unnecessarily obscuring the present invention. The specific details set forth are merely exemplary. The specific details may be varied from and still be contemplated to be within the spirit and scope of the present invention. The moving thumbnail generator will be discussed in the context of being implemented in an example video search engine and web page however as will be readily apparent the moving thumbnails may be implemented in other uses.

In general various methods apparatuses and systems are described for a moving thumbnail generator. The moving thumbnail generator generates one or more moving thumbnails that are visually and aurally representative of the content that takes place in an associated original video file. Each moving thumbnail may be tagged and time stamped to arbitrary video trigger events that take place in an associated original video file. Each of the moving thumbnails has two or more moving frames derived from its associated original video file at a portion in the original video file that is tagged and time stamped to one or more relevant video trigger events in order for the generated moving thumbnail to summarize a visual content of the associated original piece of video file to a user. The tag carries information about content that takes place in the original video file and metadata about that content including a time reference to frames that are contextually relevant to the reason why a viewer of the thumbnail might be interested in viewing that thumbnail. Each moving thumbnail is stored with a relational link back to the original video file in order so that the moving thumbnail can be used as a linkage back to the original video file by for example clicking on the moving thumbnail itself or a link that is displayed near the moving thumbnail.

The moving thumbnail generator in connection with the attribute filters analyzes video audio and textual content from an original video file from a number of sources such as news broadcast internet broadcast etc. The moving thumbnail generator analyzes the content of the video audio and textual content of the source file to create metadata about the subject matter of the source video file . Note the moving thumbnail generator may also analyze source text files and source audio files as well but the remainder of this description will discuss video files as the source file under analysis. However the analysis and processing of those purely audio and text source files will be similar.

After analyzing the content in the original video file and the source of the original video file the moving thumbnail generator generates metadata about key content in the video file such as persons of interest referenced from a list number of times each person of interest appears in that video file and length of time that person of interest appears in that video file subject matter that person speaks about in the video file etc. The tag contains the information including metadata about the events that occur in the original video file in a searchable format.

The moving thumbnail generator module then generates moving thumbnails that are tagged and time stamped to arbitrary video trigger events that take place in the original source video file . The moving thumbnail generator module creates event driven contextual moving thumbnails from the corpus or incoming stream of these original video files . The moving thumbnails are tagged with metadata that captures the video trigger events to which they were originally attached. Each original video file may have one or more animated video moving thumbnails associated with that original video file. Each moving thumbnail is tied to a separate video trigger event from that original video file. In an embodiment multiple moving thumbnails may be associated with each original video file .

Each animated video thumbnail may be stored in the thumbnail data store library database or other form of storage data structure to be retrieved by the query control module if relevant to search parameters of the query. These animated moving thumbnails can also be stored in any standard search index or relational database. The thumbnail data store stores one or more animated moving thumbnail segments for each video clip where each animated thumbnail contains content based on different key metadata parameters. As well as the thumbnail itself the moving thumbnail data store contains data information that ties the thumbnail to the point in time within the video file that the two or more frames were extracted from and any relevant metadata from the video file that occurred in or around that moment in time. For example a particular face may have appeared in the video at that point or certain words may have been uttered by speakers.

The search engine via the query control module is able to use these time stamp offsets and metadata video triggers to retrieve a moving thumbnail for every relevant clip not just a truly visual summary for the clip but also a visual summary that most closes matches the point in time within the original video file that is most relevant to the supplied query content. Note the animated thumbnails are created for an original video file prior to a query being submitted to the search engine by the user. Upon a search the query control module receives input from one or more of the search retrieval engines on the search results and displays the most relevant of these video trigger events at a portion in a video clip that is tagged and time stamped to the one or more relevant video trigger events to help the user decide which result is most appropriate for their particular purpose. The query control module selects the appropriate thumbnail associated with the original video file in the moving thumbnail data store to display the most relevant of the video trigger events at a portion in a video clip that is tagged and time stamped to the one or more relevant video trigger events.

Viewing a moving thumbnail showing frames at the point in time that are most relevant to the supplied query content allows the user to immediately assess aurally visually and potentially textually not just that the original video file result is relevant but why and how it is. In an embodiment when the user hovers the cursor over a moving thumbnail the sounds from the sequence of audio visual video frames will be piped to the user. In an embodiment when the moving thumbnail plays on the Direct Play Screen the sounds from the sequence of audio visual video frames will be piped to the user.

The search engine contains code scripted to present a moving thumbnail preview of multiple scene sequences of two or more frames of a video clip to a user. The search engine returns a search query of the most relevant video files based on the content supplied to the query input box and search parameter icon qualifiers of the query and presents an animated thumbnail containing a sequence of two or more frames of the relevant sections of the most relevant files based on the query input. Thus each moving thumbnail presents a sequence of two or more frames of the actual relevant sections of that original video file visually to the user based on the user s query input.

The search engine uses one or more attribute filters to identify various video trigger events relevant to the content in the original video file. The video trigger events in a video may also be manually identified and added to the metadata tag included with each moving thumbnail . The attribute filters of the video file being analyzed may include an image analysis tool a facial image recognition tool a speech to text converter a scene change analysis tool video optical character recognition tool and other video audio or text analysis software that may automatically evaluate content contained in the original video file. The relevant sections are identified by metadata about the content of the original video file such as meta data about particular objects in the video file identified through the image analysis tool and the number of times each identified object appears in that clip such as facial recognition with a facial image recognition tool each word spoken via a transcript of the video file identified through the speech to text converter and thus the number of times spoken words of a particular query term or phrase is used that clip as well as indexing particular words to frames of the video file in which they occur scene change meta data such as length of a particular segment on a specific topic of content identified by a scene change analysis tool text appearing in a video identified through video optical character recognition tool and much more metadata items derived from the content of the video file or its source location.

The moving thumbnail generator may generate an index of the relevant metadata. An index of the created thumbnails may be created by the thumbnail store . The metadata about each generated moving thumbnail and the weighed essence of the content of the original video file are stored in the thumbnail store . The metadata about each generated moving thumbnail may be searched by the one or more search retrieval engines .

As discussed the multiple moving thumbnails for each original video file each contain scenes of multiple frames of the original video file. Each of the scenes is mapped to a time line of defined sub sections of the original video file that are automatically identified. Each of the scenes has its own set of metadata tied to video triggers occurring in that original video file. The time line may be a frame count a measurement of time in seconds microseconds or some other time indication of a start time and a duration in which the content of the video file was conveyed. Alternatively each identified attribute of the video file has one or more time codes associated with when that identified attribute occurs in the video file and the time codes are synchronized to the time reference used by the original video file.

Thus the moving thumbnail generator uses plug in attribute filters to detect attributes such aural information textual information source information visual information on things that appear etc. from a video file under analysis and identify the attributes an indexer to index the attributes and to assign a time ordered indication with each of the identified attributes and a tag generator to generates a metadata tag that includes the key identified attributes from this video file and relative weights assigned to each of these identified attributes.

Thus two or more moving thumbnails may exist for each original video file and the tag associated with each thumbnail carries a timestamp with the metadata to show frames that are contextually relevant to the reason why the viewer of the thumbnail might be interested in viewing that thumbnail. For example the displayed frames in the thumbnail may be relevant to search query terms supplied by the viewer relevant to theme a web page is displaying and the viewer chose to go to that web site relevant to user s selection for shopping etc. relevant to the context that a user history suggests the user would be interested in etc. The metadata contains the terms to match the relevance to and the timestamps associated to when those relevant trigger events occur in the original video file. Also each thumbnail has a relational link back to the original video file that can be activated to retrieve the original video file . In an embodiment the relational link in the moving thumbnail retrieves the original video file at the corresponding offset. Thus the relational link does not just bring back the video source file but the video source file plays from the point that particular moving thumbnail was taken.

In another embodiment the moving thumbnails may visually and aurally present representative content that takes place in an associated original video file and not have an associated tag. Either way the moving thumbnail data store contains the one or more moving thumbnails and the thumbnails are retrievable in response to a query being submitted. The two or more frames for the moving thumbnail can be selected to be between a range of frames such as two to twenty frames. The sequences of two or more frames of the moving animated thumbnails are usually scenes within the original video file that are tied to particular points in time in the original video file. In an embodiment the range of frames is selected from the beginning of the original video clip when the video file starts to display the essence of its video content. The sequences of the two or more frames create a moving thumbnail that conveys visually and aurally why a particular video scene from the search results may be of interest to the user. One or more relational links exist between a displayed moving thumbnail and its associated original video file. In an embodiment the relational link may be embedded in the moving thumbnail associated with a hyperlink displayed near the moving thumbnail such as the title of the original video clip or associated with the moving thumbnail in a similar manner. The one or more relational links between the displayed moving thumbnail and its associated original video file can be activated to retrieve the original video file by clicking a mouse arrow on the moving thumbnail itself highlighting the moving thumbnail and depressing an enter key or clicking a mouse arrow on a link that is displayed near the moving thumbnail. The relational link may bring back the video source file and start playing the video source file from the offset point from the beginning of the video source file that particular moving thumbnail was taken.

The search engine may have two or more types of search retrieval engines to search audio as well as video content files. Seemingly the search engine has one query input box and some helpful search parameter icons to click on to a user. The user need not conscientiously select a particular type of search engine to perform the search for the user. However two or more different types of search retrieval engines are employed depending upon the content of the query input supplied by the user and or the helpful search parameter icons qualifiers attached to content of the query itself by the user. The query control module of the video search engine seamlessly selects from the user s standpoint from the two or more types of search retrieval engines based on the above query content and query qualifier factors attached to the content of the query itself such as 1 the amount and type of query terms supplied by the user to the query input box 2 whether the user has selected various qualifying search parameter icons presented by the graphic user interface to qualify the search parameters whether the user has historical selection profile etc. The one or more selectable icons act as qualifiers that can modify search parameters associated with the content of the query supplied by a user. In an embodiment the query control module of the video search engine selects from the conceptual search retrieval engine the Boolean keyword search retrieval engine and the database search retrieval engine or a similar search retrieval engine.

The conceptual search retrieval engine may process large amounts of structured and unstructured video and audio files. In an embodiment conceptual search retrieval engine is based on advanced pattern matching technology that exploits high performance Bayesian probabilistic techniques. The conceptual search retrieval engine forms a conceptual understanding of text in any format and automates key operations and processing tasks upon it. The conceptual search retrieval engine may form its conceptual understanding of a video file by extracting a set of key terms describing the essence of the video content and then having a set of mathematical values associated with each term in the set of key terms. The set of key terms are cross referenced to semantically similar terms and the set of mathematical values associated with each term in an instance of the representation is adjusted based on a historical mathematical value for that term in similar representations. All of the key terms weights and links are put into a tag such as an XML tag associated with the moving thumbnail. The conceptual search retrieval engine uses information theoretic algorithms and Bayesian algorithms to determine statistics for each attribute extracted from the video content. The set of terms may include single terms higher order terms noun phrases proper names and other similar types of information as well as relational links and metadata about each term. In an embodiment for a given key term the engine produces multiple statistics associated with each term. The statistics include position information those derived from the frequency of occurrence length of time devoted to a segment on that key term from scene change to scene change and other statistical information associated with the key terms at both the individual word used level and sum of the corpus of video level. In an embodiment key terms may also have their weighted values modified by statistical correlation. The conceptual search retrieval engine implements automated information operations including concept matching agent creation agent retraining agent matching information categorization information summarization and other similar information operations. In an embodiment the conceptual search retrieval engine is an Autonomy IDOL server available from Autonomy Corporation Cambridge Business Park Cowley Rd Cambridge United Kingdom.

The query control module selects the conceptual search retrieval engine to find search results to the user supplied query by 1 default 2 if the user has established a search history with the search engine 3 if the user supplies a natural language query of five or more terms or 4 if the search engine automatically generates a suggested set of returned similar topics the user may want may find of interest based on the content of the supplied query terms to the query box in addition to the search results presented to the user based directly on the supplied query terms. The more query terms and information the user supplies to the conceptual search retrieval engine the more relevant the returned search results will be to supplied query content. Because the conceptual search retrieval engine cross references semantically similar query terms the user need not worry that supplying additional search terms will exclude relevant videos because that relevant video uses some word different than a supplied query term to convey the desired concept.

The Boolean key word search retrieval engine uses applies Boolean logic operators searches that use things like AND and OR to the content of the query input to return search result hits for the supplied query terms. Typically the query control module selects the Boolean key word search retrieval engine for query inputs of four or less query terms to the query input box .

The database search retrieval engine uses a database organizational structure to organize the search for selected sources as well as exclude returns from various sources or search for content arranged by date type etc.

As will be discussed later in more detail the query control module has the graphic user interface that presents a user a Date Relevance Slider icon a Safe Filter button one or more content categorizer buttons a tracking module and other qualifiers that can be attached to the query content itself by the user. These search parameter icon qualifiers help the database search retrieval engine to exclude types of video and sources of video from the search results . These search parameter icon qualifiers also help the query control module to prioritize the display of listed search results as date sensitive or content related search results . The search engine lists the search results in order of relevance to the query.

The query control module allows the user to select several example qualifiers to be attached to the content search terms of the query itself by the user. The query control module causes the graphic user interface to display a Date Relevance Slider icon a safe filter button a tracking module and a content categorizer to qualify the parameters surrounding the user supplied query search terms.

The user interface of the search engine allows the user to specify the extent to which date and relevance are of relative importance to that user through some form of biasing mechanism such as a Date Relevance Slider or the ability to set a date bias value and weights the overall results generated by the search engine based on this setting. The query control module may display a Date Relevance Slider icon. The Date Relevance Slider icon allows a user to supply input into weighing search results by whether the user is more interested in 1 content that closely matches the search query terms or 2 content that appeared more recently than other content. If the user is looking for news or other breaking information the query control module allows the user to move the Date Relevance Slider icon towards date. If user does not care when the video files were created but prefers the best conceptual search results that match the supplied query content then the user moves the Date Relevance Slider towards relevance instead.

The query control module displays a safe filter button to ensure that a search engine such as on TV.blinkx.com only returns non pornographic content. The query control module displays a content categorizer such as TV.blinkx.com Channels to allow a user to select exactly which category of content from the search results will come from. Multiple example categories of source files can be included excluded from the search results such content source content theme topic as date of content production popularity of content the number of times the content has been viewed whether the content is pay per view or free and other similar ways to categorize content by simply selecting de selecting that category as a possible source of video files. The query control module offers all kinds of content from various content sources ranging in content theme topic from news to sports and entertainment video as well as radio podcast vlog content. The user can click on the main heading to choose or eliminate all the channels under that category or choose individual channels to be included or excluded from the user s search. For example the user can deactivate the main icon for all of the news sources or individually deactivate individual new sources such as Euronews.

The search engine has code scripted for Really Simple Syndication RSS support so a user can save any search as an RSS feed. The search engine then automatically alerts the user every time relevant content to the user s query criteria appears on the World Wide Web. The search engine also has a tracking module such as a Smart Folders to allow a user and the search engine to track a user s search over time. The tracking module is bit like RSS except without needing an RSS Reader. In addition the Smart Folders will automatically download content if it s hosted in the retrieval data store to the user s computer so that the user can watch the video file even if the user is not connected to the internet.

The query control module displays a Direct Play Screen. The Direct Play Screen automatically appears if any of the content the user s search returns is hosted in the retrieval data store. If any relevant content is hosted in the retrieval data store the query control module automatically shows this screen and begins to play back the relevant clips from the moving thumbnails one after another without the user having to prompt it in any way. The Direct Play Screen also allows the display of the actual video files. The graphic user interface displays user controls such as a Play button a Pause button a fast forward button a rewind button a Stop button a Volume control button and other similar user controls.

The query control module retrieves moving thumbnails that match the user s interest from the thumbnail data store . The query control module uses the tag that carries information about the content that takes place in the original video file and ties the moving thumbnail to the point in time within the original video file that the two or more frames were extracted from and any relevant metadata from the video file that occurred in or around that moment in time. The more details the user supplies in what the user is interested in viewing to the query control module the more relevant the retrieved moving thumbnails will be because each of the moving thumbnails has two or more moving frames derived from its associated original video file at a portion in the original video file that is tagged and time stamped to one or more relevant video trigger events and those relevant trigger events match the qualifiers the user wants to see. The moving thumbnail displays these frames from the source video file that are contextually relevant to the reason why the viewer of the moving thumbnail might be interested in viewing that original video file.

The video wall of two or more animated video thumbnails includes for example twenty four moving thumbnails including a first moving thumbnail and a second moving thumbnail . The video wall of moving thumbnails is capable of showing many search results allowing a user to view potentially hundreds of potentially relevant moving thumbnail results in the same window space generally taken to represent around ten search results. The video wall is configured to enlarge a thumbnail when a cursor hovers over that thumbnail. The moving thumbnail when enlarged will also aurally project the voices and sounds from the sequence of frames being played for the user. The GUI may also display a textual description of the content of that video thumbnail when a cursor hovers over a moving thumbnail.

The components of the search engine including the moving thumbnail generator may be a combination of hardware logic and or software. The software portion may be stored on a machine readable medium. A machine readable medium includes any mechanism that provides e.g. stores and or transmits information in a form readable by a machine e.g. a computer . For example a machine readable medium includes read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices a hard drive in a server or any other computing device.

The moving thumbnail summarization technique can be used in systems other than a search engine. Whenever a large corpus of video exists and it is necessary to allow a user to rapidly analyze and understand what content is available the moving thumbnails can be used to provide that visual summarization. Example 1 A hierarchical directory system where videos are organized into some form of taxonomy created either by a central source or body or by users at large. Once a user navigates to a node or end point within this taxonomy all matching videos can be shown using moving thumbnails that summarize their visual and aural qualities. Example 2 the summarizing of items in a feed. Where an RSS Really Simple Syndication or other feed technology is used to syndicate a number of pieces of video content or video files perhaps over a period of time the moving thumbnail system can be used to summarize each of those video files. Example 3 in a user generated tagsonomy of content where content is organized by tags that are applied by users to the content when a user selects a given tag or group of tags matching video files could be displayed and summarized by their moving thumbnails representations.

While some specific embodiments of the invention have been shown the invention is not to be limited to these embodiments. For example several specific modules have been shown. Each module performs a few specific functions. However all of these functions could be grouped into one module or even broken down further into scores of modules. Most functions performed by electronic hardware components may be duplicated by software emulation and vice versa. The search engine may have a graphical user interface as described above a textual user interface or an application programming interface. The invention is to be understood as not limited by the specific embodiments described herein but only by scope of the appended claims.

