---

title: Systems and methods for reading objects in a file system
abstract: Embodiments of the disclosure relate generally to file systems. Specifically, certain embodiments include systems and methods for reading objects in a file system. In some embodiments, a first processing thread traverses a portion of a file system and represents file system objects in a data structure. A portion of the data and/or metadata associated with the represented objects may be prefetched to a memory. In some embodiments, a second processing thread consumes the objects represented in the queue. For example, in a file system backup embodiment, the second processing thread may transfer data and/or metadata associated with the objects to an archive target.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07966289&OS=07966289&RS=07966289
owner: EMC Corporation
number: 07966289
owner_city: Hopkinton
owner_country: US
publication_date: 20070821
---
A portion of the disclosure of this patent document includes material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyrights whatsoever.

This application was filed on the same day as the following applications U.S. patent application Ser. No. 11 894 756 entitled SYSTEMS AND METHODS FOR ADAPTIVE COPY ON WRITE and U.S. patent application Ser. No. 11 894 739 entitled SYSTEMS AND METHODS FOR PORTALS INTO SNAPSHOT DATA all of which are hereby incorporated by reference in their entirety herein.

The systems and methods disclosed herein relate generally to file systems and more specifically to systems and methods for reading objects in a file system.

The amount of data stored on digital computing systems has increased dramatically in recent years. Accordingly users have become increasingly reliant on the storage devices of these computing systems to safely store this data. In order to preserve a copy of the data in case of loss many users routinely copy some or all of the contents of the storage devices to a backup or archival storage device.

The data stored on the storage devices may be organized as electronic files in a file system. The files may be grouped into directories with some directories including other directories and or files. During a backup process the system typically traverses some or all of the file system to read individual files for transfer to the backup device. However problems may occur when reading a file system from the storage devices. For example if the file system includes a large number of relatively small files the backup system may be latency bound while waiting for each of the individual files to be read from the storage device. Because of the foregoing challenges and limitations there is a need to provide systems and methods for reading files in a file system.

In general embodiments of the invention relate to file systems. More specifically systems and methods embodying the invention provide support for reading objects such as for example files in a file system.

An embodiment of the present invention includes a method of traversing objects in a file system. The method may include traversing a portion of the file system to identify an object to be read and to determine a size representative of the object and determining whether to represent the object in a data structure based at least in part on one or more factors including the size representative of the object and a cumulative size of objects currently represented in the data structure. The method may also include prefetching at least a portion of the objects currently represented in the data structure.

Another embodiment of the present invention includes a computer readable medium on which are stored executable instructions that when executed by a processor cause the processor to perform a method for traversing objects in a file system. The method may comprise traversing a portion of the file system to identify an object to be read and to determine a size representative of the object. The method may also comprise determining whether to represent the object in a data structure based at least in part on one or more factors including the size representative of the object and a cumulative size of objects currently represented in the data structure. The method may further include prefetching at least a portion of the objects currently represented in the data structure.

A further embodiment of the present invention includes a system for managing reading of a portion of a file system. The system may comprise a storage device capable of accessing a file system a memory operably coupled to the storage device and a processing module operably coupled to the memory and the storage device. The processing module may comprise a prefetch module a working module and a data structure capable of representing files in the file system. The prefetch module may be configured to traverse data related to a portion of the file system and to represent a file in the data structure based at least in part on a size of the file and a cumulative size of files currently represented in the data structure. The prefetch module may be further configured to open the file and to prefetch at least a portion of the file. The working module may be configured to read the files represented in the data structure so as to transfer the files from the storage device to the memory.

For purposes of this summary certain aspects advantages and novel features of the invention are described herein. It is to be understood that not necessarily all such advantages may be achieved in accordance with any particular embodiment of the invention. Thus for example those skilled in the art will recognize that the invention may be embodied or carried out in a manner that achieves one advantage or group of advantages as taught herein without necessarily achieving other advantages as may be taught or suggested herein.

These and other features will now be described with reference to the drawings summarized above. The drawings and the associated descriptions are provided to illustrate embodiments of the invention and not to limit the scope of the invention. Throughout the drawings reference numbers may be reused to indicate correspondence between referenced elements. In addition the first digit of each reference number generally indicates the figure in which the element first appears.

Systems and methods which represent one embodiment of an example application of the invention will now be described with reference to the drawings. Variations to the systems and methods which represent other embodiments will also be described.

For purposes of illustration some embodiments will be described in the context of a file system which may be a distributed file system. The present invention is not limited by the type of environment in which the systems and methods are used however and systems and methods may be used in other environments such as for example other file systems other distributed systems the Internet the World Wide Web a private network for a hospital a broadcast network for a government agency and an internal network for a corporate enterprise an Intranet a local area network a wide area network a wired network a wireless network and so forth. Some of the figures and descriptions however relate to an embodiment of the invention wherein the environment is that of a distributed file system. It is also recognized that in other embodiments the systems and methods may be implemented as a single module and or implemented in conjunction with a variety of other modules and the like. Moreover the specific implementations described herein are set forth in order to illustrate and not to limit the invention. The scope of the invention is defined by the appended claims and their equivalents.

One example of a distributed file system in which embodiments of systems and methods described herein may be implemented is described in U.S. patent application Ser. No. 10 007 003 entitled SYSTEMS AND METHODS FOR PROVIDING A DISTRIBUTED FILE SYSTEM UTILIZING METADATA TO TRACK INFORMATION ABOUT DATA STORED THROUGHOUT THE SYSTEM filed Nov. 9 2001 which claims priority to Application No. 60 309 803 filed Aug. 3 2001 U.S. Pat. No. 7 146 524 entitled SYSTEMS AND METHODS FOR PROVIDING A DISTRIBUTED FILE SYSTEM INCORPORATING A VIRTUAL HOT SPARE filed Oct. 25 2002 and U.S. patent application Ser. No. 10 714 326 entitled SYSTEMS AND METHODS FOR RESTRIPING FILES IN A DISTRIBUTED FILE SYSTEM filed Nov. 14 2003 which claims priority to Application No. 60 426 464 filed Nov. 14 2002 all of which are hereby incorporated by reference herein in their entirety.

For purposes of illustration some embodiments will also be described with reference to updating data structures in a file system using information stored in related data structures of the file system. Embodiments of a file system capable of updating data structures with information stored in related data structures of a file system are disclosed in U.S. patent application Ser. No. 11 255 337 titled SYSTEMS AND METHODS FOR ACCESSING AND UPDATING DISTRIBUTED DATA and is hereby incorporated by reference in its entirety.

For purposes of illustration certain embodiments of the systems and methods disclosed herein will be described in the example context of backing up a file system to a storage medium. The scope of the disclosure is not limited to file system backups and in other embodiments the systems and methods advantageously may be used for example for replicating a disk indexing and or searching file systems and or data on a search engine generating a cryptographic hash function for example an md5sum and so forth. The specific examples described below are set forth to illustrate and not to limit various aspects of the disclosure.

In some embodiments a distributed file system is used to store the file system data. The distributed file system may comprise one or more physical nodes that are configured to intercommunicate via hard wired connections via a suitable data network for example the Internet via wireless communications or by any suitable type of communication as known by those of ordinary skill in the art. In one example a node of the distributed file system comprises the storage device . The archive target may comprise data storage on the same node or on a different node of the distributed file system or may comprise another storage device as discussed above for example a tape drive .

In some embodiments the backup system is configured to transfer a copy of the file system data to a cache before transferring the cached data through a communication medium to the archive target . The cache thereby buffers the data waiting to be transferred to the archive target via the communication medium . In some embodiments the cache comprises volatile and or non volatile memory with fast data access times. For example in one embodiment 1 G RAM cache is used. The communication medium may comprise a wired or wireless communication medium. In some embodiments the communication medium comprises a data network such as a wide area network or local area network the Internet or the World Wide Web. The communication medium may support communications protocols such as TCP IP backup protocols such as NDMP and or standards for file access such as NFS or CIFS.

In some backup system embodiments file system data is read from the storage device to the cache and then the cached data is transferred to the archive target . The input output I O performance of the cache typically is much better than the I O performance of the storage device for example disk I O in and or the communication medium for example network I O in . Accordingly in such embodiments because the disk I O and the network I O are performed serially the overall performance of the backup system typically is limited by the lesser of the disk I O or the network I O. Additionally if the file system data includes numerous files that are small in size the backup performance may be further reduced if the communication medium has to wait for each of the small files to be read from the storage device for example the communication medium may be latency bound .

Accordingly some embodiments of the backup system advantageously may read ahead or prefetch portions of the file system data before this data is requested by the target archive stream. The file system data may include file data and or metadata. The prefetched data may be stored on the cache cached so that it is available when needed for transfer by the communication medium to the archive target . Although caching prefetched file system data consumes storage in the cache the caching may improve the performance of the backup system by reducing latency in the disk I O. Additionally in certain embodiments portions of data in each file in a group of files may be prefetched by the backup system . The size of the data portions and or the number of files in the group may be selected so that the network I O stream does not stall or become latency bound. Such embodiments of the system advantageously may improve the backup performance particularly when the file system includes numerous small files. Further in certain embodiments multiple processing threads handle the data and file prefetch and the data transfer to the archive target . An advantage of such embodiments is that the prefetch and the data transfer may perform in parallel rather than in a serial fashion.

In order to more efficiently utilize cache resources embodiments of the backup system may optionally implement a drop behind procedure in which data for example file data and or metadata is dropped from the cache after the data has been transferred to the archive target . Such embodiments advantageously improve cache utilization and reduce the impact of the prefetch process on other processing threads that also may be attempting to store data in the cache .

In the backup system illustrated in a processor may be configured to execute one or more program modules that carry out embodiments of the methods described herein. The word module refers to logic embodied in hardware or firmware or to a collection of software instructions possibly having entry and exit points written in a programming language such as for example C or C . A software module may be compiled and linked into an executable program installed in a dynamically linked library or may be written in an interpreted programming language such as for example BASIC Perl or Python. It will be appreciated that software modules may be callable from other modules or from themselves and or may be invoked in response to detected events or interrupts. Software instructions may be embedded in firmware such as an EPROM. It will be further appreciated that hardware modules may be comprised of connected logic units such as gates and flip flops and or may be comprised of programmable units such as programmable gate arrays or processors. The modules described herein are preferably implemented as software modules but may be represented in hardware or firmware. Moreover although in some embodiments a module may be separately compiled in other embodiments a module may represent a subset of instructions of a separately compiled program and may not have an interface available to other logical program units.

In some embodiments the processor is remote from the storage device cache and or the archive target . In other embodiments the processor or processors may be included in one or more of the components of the backup system . For example a node of a distributed file system may comprise the processor the storage device and the cache . Multiple processors are used in certain embodiments.

The processor may be a general purpose computer using one or more microprocessors such as for example a Pentium processor a Pentium II processor a Pentium Pro processor a Pentium IV processor an x86 processor an 8051 processor a MIPS processor a Power PC processor a SPARC processor an Alpha processor and so forth. In other embodiments the processor may be a special purpose computer comprising one or more integrated circuits such as application specific integrated circuits ASICs field programmable gate arrays FPGAs and so forth.

The backup system may be configured to operate with a variety of one or more operating systems that perform standard operating system functions such as accessing opening reading writing and closing a file. It is recognized that other operating systems may be used such as for example Microsoft Windows 3. X Microsoft Windows 98 Microsoft Windows 2000 Microsoft Windows NT Microsoft Windows Vista Microsoft Windows CE Microsoft Windows ME Palm Pilot OS Apple MacOS Disk Operating System DOS UNIX IRIX Solaris SunOS FreeBSD Linux IBM OS 2 operating systems and so forth.

In general various embodiments of the disclosed systems and methods relate to reading objects in a file system. In some embodiments the objects may include files and or directories. As used herein a file is a collection of data stored in one unit under a filename. A directory similar to a file is a collection of data stored in one unit under a directory name. A directory however is a specialized collection of data regarding elements in a file system. In one embodiment a file system is organized in a tree like structure. Directories are organized like the branches of trees. Directories may begin with a root directory and or may include other branching directories for example subdirectories . Files resemble the leaves or the fruit of the tree. Files typically do not include other elements in the file system such as files and directories. In other words files do not typically branch.

In some embodiments of a file system metadata structures also referred to as inodes are used to monitor and manipulate the files and directories within the file system. An inode is a data structure that describes a file or directory and may be stored in a variety of locations including in the storage device other storage devices and or in memory. In some embodiments each inode points to one or more locations on a physical disk that store the data associated with a file or directory. The inode in memory may include a copy of the on disk data plus additional data used by the system including fields associated with the data structure. Although in certain illustrated embodiments an inode represents either a file or a directory in other embodiments an inode may include metadata for other elements in a distributed file system in other distributed systems in other file systems or in other systems.

In embodiments described herein data in the file system may be organized into data blocks. Conceptually a data block may be any size of data such as a single bit a byte a gigabyte or even larger. In general a data block is the smallest logical unit of data storage in the file system. In some embodiments a file system may use data block sizes that are different from the native block size of a physical disk. For example a physical disk may have a native size of 512 bytes but a file system may address 4096 bytes or 8192 bytes.

In other embodiments the portion of the file system may have a different number of files and or directories than is shown in . Additionally the example file sizes may be different in other embodiments. In some cases the portion of the file system may include the entire file system for a computer system for example the directory dir may be the root directory . In some cases the portion of the file system may include files and directories having for example particular permissions or access rights for example user and or group IDs . Many variations in the file system are possible.

As described above certain embodiments of the backup system may utilize multiple processing threads in order to at least partially parallelize prefetching file system data from the storage device and transferring the data to the archive target via the communication medium . One possible advantage of such embodiments is that performance of the backup process may be improved particularly when the file system includes many small files and or when disk I O performance is less than network I O performance see for example .

In certain embodiments two separate processing threads are utilized by the system a prefetch thread and a work thread. In certain such embodiments the prefetch thread and or the work thread may be executed by the processor shown in . In brief the prefetch thread traverses the portion of the file system being backed up and prefetches data for example file data and or metadata associated with objects in the file system for example files and directories . The prefetched data may be stored in the cache . The work thread transfers the prefetched data to the archive target via the communication medium . In some embodiments the prefetch thread represents file system objects that are to be prefetched and stored in a data structure such as for example a queue. The work thread is a consumer of the data in the data structure. When the work thread accesses an object represented in the data structure the prefetch thread advantageously has already prefetched at least a portion of the data for the object.

In some embodiments the prefetch thread sleeps when the amount of prefetched data exceeds a first threshold referred to as a high water mark HWM . As the work thread transfers the prefetched data the amount of untransferred cached data decreases. In some embodiments the prefetch thread wakes when the amount of untransferred cached data decreases below a second threshold referred to as a low water mark LWM . The high water mark and or the low water mark may be selected for example so that the prefetch thread uses a reasonable amount of memory in the cache for storing the prefetched data and or so that the work thread does not stall while waiting for data to transfer. In some embodiments a drop behind procedure is used to drop data from the cache after the file system object has been transferred to the archive target .

Although certain embodiments of the backup system use two processing threads other embodiments may use a different number of threads including one three four six fifteen or more threads. Also other embodiments of the backup system may combine or allocate differently some or all of the functions performed by the prefetch thread and the work thread. Additional details of these and other embodiments will be further described below.

In the example shown in the backup system uses a high water mark HWM and a low water mark LWM to control the cumulative size of the file system objects represented in the queue. In this embodiment if the cumulative size of the file system objects represented in the queue exceeds the HWM a sufficient amount of file system objects have been represented in the queue and the prefetch thread sleeps. If the cumulative size of the file system objects represented in the queue falls below the LWM for example due to operation of the work thread the prefetch thread wakes and begins to prefetch additional objects. The cumulative size of the queue may be based on one or more metrics including for example cumulative physical size of the data and or metadata of the objects represented in the queue cumulative number of objects represented in the queue or any other suitable measure of the size of the queue. In some embodiments the HWM and LWM include more than one of these factors. For example in the embodiment shown in the high and low water marks include values for both the cumulative physical size of the objects in megabytes and cumulative number of objects. Embodiments using both cumulative physical size and cumulative number of objects may have several advantages. For example if the file system includes numerous small files then many files will be represented in the queue which reduces the likelihood the file transfer to the archive target will stall or become latency bound. If the file system includes files having large sizes then relatively few files will be represented in the queue but the cumulative size of these files will be sufficiently large to reduce the latency of the transfer.

In certain embodiments the HWM and or the LWM can be adjusted by a user for example to tune the system so that the backup process uses a reasonable amount of system resources for example cache and or has a sufficiently low latency. In certain such embodiments the HWM and or the LWM may be dynamically adjustable based on for example current CPU and memory usage transfer speed of the communication medium transfer speed of the storage device and so forth. In some embodiments default values for the LWM may include 10 MB for the physical size of the data and or 1000 for the number of files. Default values for the HWM may include 20 MB for the physical size of the data and or 2000 for the number of files.

The prefetch thread traverses a portion of the file system to identify an object to be prefetched. In the example illustrated in the prefetch thread uses a depth first method to traverse the portion of the file system that is to be backed up. In other embodiments other file system traversal methods may be used such as for example a breadth first search. In the example file system in a depth first traversal the prefetch thread begins at dir and continues through the file system tree in the order indicated by the circled numerical indices.

During the traversal the prefetch thread determines a size that is representative of the object for example file or directory data and or metadata . As discussed above the size may represent the physical size of the data and or metadata associated with the object on the storage device . Because the physical size of a directory s metadata generally is relatively small compared to files physical sizes of directories are not taken into account in some embodiments. In other embodiments the size may represent a numerical count associated with the object. In the example shown in a file and a directory are each assigned a numerical count of one. In other embodiments the numerical count may be weighted to reflect the size or other properties of the object. In some embodiments including the embodiment shown in both physical size and a numerical count are determined by the prefetch thread. In other embodiments different measures of the size of a file system object may be used.

The prefetch thread determines whether to represent the object in the queue based at least in part on one or more factors including the size determined to be representative of the object and a cumulative size of objects currently represented in the queue. For example if the cumulative size is less than a threshold for example the HWM the prefetch thread represents the object in the queue. If the cumulative size of the queue exceeds the threshold the prefetch thread does not add the object to the queue. In the example shown in the physical size of a directory s metadata is taken to be zero bytes and the physical size of a particular file is the file size shown in . For each state of the backup process the cumulative size of objects currently represented in the queue including both number of objects and physical size is shown on the right side of . The HWM and the LWM are indicated at the top of . In this example the HWM includes the values 2000 objects and 20 MB and the LWM includes the values 1000 objects and 10 MB.

Various states of the queue and the prefetch and work threads will now be described with reference to and the portion of the file system shown in . In state i the work thread initially is asleep. The prefetch thread begins to traverse the file system starting at the root dir of the portion of the file system that is to be backed up. Because the queue initially is empty the prefetch thread adds a representation of dir index in to the queue. To indicate that the prefetch thread has added dir to the queue the prefetch pointer is shown in at the end of the queue. Because the queue is now non empty the work thread wakes and begins to transfer to the target archive data and or

The prefetch thread continues to traverse the file system and identifies the next object which in a depth first traversal is dir . The current size of the queue 1 object 0 MB is below the HWM so in state ii the prefetch thread adds a representation of dir index in to the queue. Continuing in state iii the next object identified in the traversal of the file system is file index in which is represented in the queue because the cumulative size of the queue is below the HWM. Similarly in state iv the prefetch thread represents file index in in the queue because the cumulative queue size is below the HWM. As can be seen in states i iv shown in the prefetch pointer moves to the right to represent the growth of the queue.

After the prefetch thread represents a particular object in the queue the prefetch thread may prefetch at least a portion of the data and or metadata associated with the object. The prefetch thread may prefetch locks inodes data blocks and so forth. The prefetch thread may store the prefetched data and or metadata in the cache or in any other suitable memory. In some embodiments the prefetch thread may not prefetch all the data for example file data and or metadata associated with the object. For example the prefetch thread may prefetch only the data blocks at the beginning of the object for example the first 512 kB .

In state iv after the representation of file index in in the queue the cumulative size of files and directories represented in the queue is 4 objects and 51 MB. In this example although the HWM for number of objects has not been exceeded the HWM for physical size has been exceeded. In certain embodiments the prefetch thread sleeps when any of the various HWM values is exceeded because a sufficient amount of objects for example number and or physical size has been represented in the queue.

Returning to state i after dir has been represented in the queue the queue is non empty and the work thread wakes and begins transferring the prefetched data to the archive target . In embodiments in which the prefetch thread did not prefetch all the data associated with the object the work thread may issue routine operating system prefetch or read ahead calls for the data blocks that were not prefetched by the prefetch thread. As shown in states vi ix the work thread pointer moves to the right to indicate that objects in the queue have been consumed for example transferred to the target archive . As the work thread consumes the queue the work thread may update the cumulative size of the objects currently represented in the queue. In some embodiments the work thread updates the cumulative size whenever a certain amount of data has been transferred. In one embodiment the update is performed each time 512 kB of data is transferred by the work thread. As can be seen in the example in the cumulative size of objects represented in the queue decreases from state vi to state ix as the work thread consumes the queue.

The file index in is the last object in the queue because the HWM was reached in state iv and the prefetch thread went to sleep. In the state ix the work thread has previously transferred the data and or metadata associated with all the other objects in the queue and is currently transferring the data and or metadata associated with file. At the point shown for state ix in the data and or metadata of file that has already been transferred is schematically illustrated by the dashed line portion of the square representing file and the portion of file remaining to be transferred is schematically illustrated by the solid line portion of the square. Further details related to the transfer of the data in file will be described below with reference to .

As file is consumed by the work thread the cumulative size of the untransferred portion of the queue decreases. To reduce the likelihood that the queue will be entirely consumed by the work thread in certain embodiments the work thread wakes the prefetch thread when the cumulative size of the untransferred objects represented in the queue decreases below the LWM threshold. The prefetch thread begins again to traverse the file system starting from the place in the file system at which the prefetch thread last went to sleep. In some embodiments a token such as a cookie is used to indicate the place in the file system where the traversal is to start. In a similar fashion as described above the prefetch thread continues to traverse the file system and to represent objects in the queue until for example the HWM is reached or all of the data being transferred has been transferred .

In the LWM is reached in state ix when 10 MB of file remain to be transferred and the work thread wakes the prefetch thread. By the point illustrated in state x the prefetch thread has represented in the queue file index dir index dir index file index and file index . The physical size of the objects represented in the queue is 30 MB which exceeds the HWM so the prefetch thread again sleeps while the work thread continues to consume the queue. This process of building and consuming the queue may be repeated as needed until the entire portion of the file system is transferred to the archive target .

In this example when the work thread reaches a point in file where 10 MB remain to be transferred the amount of untransferred data represented in the queue reaches the LWM. In some embodiments as described above with reference to step x of the work thread wakes the prefetch thread which starts again to traverse the file system and represent additional files and directories in the queue.

In some embodiments the backup system utilizes a drop behind procedure in which some or all of the data and or metadata blocks that have been transferred to the archive stream are dropped from the cache. These embodiments advantageously allow the freed memory to be used to store additional data prefetched by the prefetch thread or to be used by other processing threads. In the example shown in the data blocks unshaded at the beginning of the file have been dropped from the cache. In certain embodiments metadata for example inodes are not dropped from the cache until all the data blocks of the file have been transferred.

Although certain embodiments described herein relate to backing up objects in a file system to a target archive for example a tape drive other embodiments of the systems and methods may advantageously be used for replicating a disk indexing and or searching file systems generating a cryptographic hash and so forth.

In state the prefetch thread traverses the file system and gets the next object in a portion of the file system. The portion may include the entire file system. The prefetch thread may use any suitable traversal method such as for example a depth first traversal. The objects in the file system may include files and or directories. In state the prefetch thread calculates the quantity of data to represent in a data structure such as for example a queue. In some embodiments the prefetch thread determines a size for the object and a cumulative size for objects currently represented the data structure. If the cumulative size is below one or more thresholds which may be for example the high water mark HWM described above the prefetch thread represents the object in the data structure. The cumulative size represented by the data structure may be updated to reflect addition of the object. In certain embodiments the update is performed by the work thread see for example state in . In some embodiments the prefetch thread may prefetch a portion of the file data and or metadata associated with the objects represented in the data structure. The prefetched data and or metadata advantageously may be stored in a memory with fact access times such as for example a cache.

In state the prefetch thread determines whether the work thread is sleeping. If the work thread is asleep in state the prefetch thread wakes the work thread to begin using the objects represented in the data structure. If the work thread is not asleep the prefetch thread determines in state whether the cumulative size of the objects represented in the data structure has reached the HWM. If the cumulative size equals or exceeds the HWM the prefetch thread sleeps state until awoken by the work thread see for example state in . When awoken the prefetch thread returns to state and gets the next object in the file system. If the cumulative size represented in the data structure is below the HWM the prefetch thread continues in state and determines if there are additional objects in the portion of the file system being traversed. If there are no more objects the prefetch thread exits in state . If there are more objects the prefetch thread returns to state and gets the next object.

The following is an example of pseudocode for an embodiment of a prefetch thread. It will be appreciated by one of ordinary skill in the art that there are many ways to implement a prefetch thread.

Some embodiments of the work thread implement a drop behind procedure. For example in state the work thread may drop from the cache data and or metadata that has been transferred to the target stream. As discussed above in some embodiments metadata associated with an object is not dropped from the cache until all the data and metadata associated with the object has been transferred.

In state the work thread updates the cumulative size of the queue. In some embodiments the cumulative size is updated based on factors including for example the amount of data metadata transferred since the last update and or the elapsed time since the last update. In state the work thread determines whether the updated cumulative size of the queue has decreased below one or more thresholds which may be for example the low water mark LWM described above. If the cumulative size is below the LWM in state the work thread determines whether the prefetch thread is sleeping. If the prefetch thread is asleep in state the work thread wakes up the prefetch thread so that the prefetch thread can continue traversing the file system to represent additional objects in the queue. If the cumulative size of the queue exceeds the LWM in state the work thread determines whether there are any remaining objects represented in the queue. If the queue is empty in state the work thread sleeps until awoken for example by the prefetch thread in state of . When awoken the work thread returns to state and dequeues the next object. In state if the queue is not empty the work thread returns to state and dequeues the next object.

The following is an example of pseudocode for an embodiment of a work thread. It will be appreciated by one of ordinary skill in the art that there are many ways to implement a work thread.

While certain embodiments of the invention have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the present invention.

