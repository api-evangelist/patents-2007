---

title: Using message passing interface (MPI) profiling interface for emulating different MPI implementations
abstract: In one embodiment, the present invention includes a method for receiving an application linked against a first application binary interface (ABI), providing an ABI wrapper associated with the application, and binding the application to a native message passing interface (MPI) library using the ABI wrapper and the profiling message passing interface (PMPI). Other embodiments are described and claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07966624&OS=07966624&RS=07966624
owner: Intel Corporation
number: 07966624
owner_city: Santa Clara
owner_country: US
publication_date: 20070822
---
Processes typically communicate through internode or intranode messages. There are many different types of standards that have been formed to attempt to simplify the communication of messages between processes. One such standard is the message passing interface called MPI . MPI A Message Passing Interface Standard Message Passing Interface Forum May 5 1994 and MPI 2 Extension to the Message Passing Interface Message Passing Interface Forum Jul. 18 1997. MPI is essentially a standard library of routines that may be called from programming languages such as FORTRAN and C. MPI is portable and typically fast due to optimization of the platform on which it is run.

Even though the MPI standards strictly define the Application Programming Interface API of all MPI functions every MPI library ships with its own Application Binary Interface ABI . Due to this applications linked against a particular MPI implementation generally cannot run on top of another MPI implementation. Accordingly extensive steps are needed to transition an application from one MPI implementation to another including re compilation and re linking.

Various embodiments allow an MPI library of a given MPI implementation to emulate other MPI implementations without modification of the main library. Still further embodiments may provide special MPI interfaces without such modification. To enable these features embodiments provide a so called foreign ABI wrapper on top of a particular native MPI implementation where the native MPI implementation is called by the foreign ABI wrapper through the standard MPI profiling interface PMPI .

As shown in an arrangement of an application wrapper and MPI library may enable transition of an application written against one MPI library to execute using a second MPI library without re compilation or re linking of the application. As shown in an application is present. Application may be linked against a foreign MPI library using a first MPI ABI i.e. MPI ABI A . To enable execution using a second MPI library namely MPI library which may be written in accordance with a native MPI implementation i.e. MPI ABI B a foreign wrapper may be presented on top of MPI library . MPI wrapper may be written to the first MPI namely MPI ABI A. Referring now to Table 1 shown is an example ABI wrapper in C language.

To enable communication between these different code layers a first ABI namely MPI ABI A may enable communication between application and MPI wrapper . In turn the standard profiling interface namely PMPI ABI B which enables MPI wrapper to call MPI library through the standard MPI profiling interface i.e. PMPI ABI B is used for ABI conversion. While shown with this particular implementation in the embodiment of the scope of the present invention is not limited in this regard. For example in other implementations multiple wrappers may be written on top of MPI library . Furthermore such wrappers each may be of a different foreign MPI e.g. of different MPI implementations to enable dynamic binding and thus easy transition for applications written in a given MPI implementation to execute on an MPI library of a different implementation. The ABI differences may include different calling conventions i.e. the method of passing the arguments entering the function and leaving it. The foreign ABI wrapper itself may select and dynamically load the appropriate native MPI library i.e. the optimized or debugging version or perform other actions related to the adjustment of the runtime environment for the current execution. The ABI wrapper may be restricted to include only those MPI functions that are actually called by the target application. Still further in some embodiments rather than using the standard profiling interface for communication with the application an equivalent non standard interface may be used. In this way the standard PMPI  profiling interface may be left open for use by third party tracing tools or the like.

Note that every MPI implementation is required to provide a profiling interface. This interface is implemented by universally shifting all mainline MPI function names MPI  into profiling name space PMPI  and making the mainline MPI function names MPI  say weak symbols that reference the respective profiling strong symbols PMPI  . Due to this the user of the respective MPI implementation can replace the implementation of the mainline MPI functions MPI  by some wrappers that perform trace generation statistics gathering logging memory checking correctness checking or any other additional functions before calling the underlying MPI implementation through the profiling interface PMPI  .

In this way the original library can be made looking externally like another MPI implementation. One just needs to implement a foreign ABI in the wrapper functions and use the native ABI through the profiling interface PMPI  . The wrappers may have to take care of converting foreign data into native format and back on return of data to the application but this may be unnecessary if the size of the respective opaque MPI data entities is equal to each other in both MPI implementations involved.

Referring now to shown is a flow diagram of a method in accordance with one embodiment of the present invention. As shown in method may begin by receiving an application linked against a foreign ABI block . For example an application may be present in a platform that has a first i.e. native MPI library where the application was written on a different system having a different i.e. foreign MPI library implementation.

Next control passes to block where a foreign ABI wrapper may be provided that is associated with the application. Specifically a foreign ABI wrapper may be written to enable communication between the application and the native MPI library. This wrapper may enable conversion of data from a format associated with the application to a format of the MPI library if the formats differ e.g. in width or another such measure. Note that this foreign ABI wrapper may be written on top of the native MPI library. In this way an application may be readied for execution on the native MPI library.

Thus still referring to dynamic binding of the application to the native MPI library may be performed block . The dynamic binding may occur using the foreign ABI wrapper and a native profiling interface. More specifically the native MPI library may be called by the foreign ABI wrapper using a standard profiling interface i.e. PMPI. In this way the application may execute using the native MPI library block . While shown with this particular implementation in the embodiment of the scope of the present invention is not limited in this regard. Note that one does not need to rely on dynamic linkage to use an embodiment. One embodiment may relink statically in the absence of the application source code. In this case the ABI wrapper library is mentioned before the native MPI library in the linker run string and the resulting statically linked application will use the native MPI library without recompilation of the application.

Thus rather than emulating a foreign MPI library using a given native MPI library by re compiling the native library with the respective foreign MPI headers files mpi.h and fmpi.h embodiments resolve ABI conversion in an elegant and standard conforming way and reduce transition to a given native MPI using dynamic binding instead of re compilation or re linkage.

Embodiments thus allow a native MPI library to emulate other MPI implementations and provide special purpose MPI interfaces without modification of the main library. For example some customers have special needs such as an interface in which all integer values are 64 bits rather than 32 bits long. While transforming the whole of the MPI library to work with 64 bit integers can be burdensome providing an extra interface i.e. a foreign wrapper that uses 64 bit integers and maps them to the 32 bit integers internally may help customers meet their goals. This reduces transition of applications from a foreign MPI implementation to a native MPI library to dynamic binding at runtime without re compilation or re linkage.

Shown in is a block diagram of the interrelation between multiple processes in accordance with an embodiment of the present invention. As shown in a plurality of processors generically processor are present. Each processor may include a process or application generically application . In some embodiments the system of is an exemplary distributed application which is cooperatively implemented via generally contemporaneous execution of machine accessible instructions of multiple processors . In particular a first process i.e. software application may be executed on first processor and a second software application may be executed by second processor which cooperatively realize the example distributed application using any variety of distributed computing algorithms techniques and or methods. In the example system of the example software applications implement different machine accessible instructions. Alternatively the example software applications may implement similar and or identical machine accessible instructions.

For simplicity and ease of understanding the example two processor system of is referenced. However distributed applications may be implemented by systems incorporating any number and or variety of processors. For example one or more processes of a distributed application may be implemented by a single processor a single process may be implemented by each processor etc. Applications may be developed using any variety of programming tools and or languages and may be used to implement any variety of distributed applications. In the example system of processors may be implemented within a single computing device system and or platform or may be implemented by separate devices systems and or platforms. Further processors may execute any variety of operating system s .

For purposes of discussion assume that each application is written and linked to a MPI implementation different than that of an associated MPI library generally . To enable easy transition to the corresponding native MPI an ABI wrapper generically wrapper written to the same MPI implementation as application intercepts MPI calls made by the process to library generically library of which facilitates the exchange of for example distributed application messages between applications . ABI wrapper thus calls MPI library using PMPI calls. In turn these MPI libraries may perform requested operations for application which may be transmitted via an interconnect which in one embodiment may be a fast interconnect such as a point to point interconnect although the scope of the present invention is not limited in this regard.

Embodiments may be suited for many different types of platforms. Referring now to shown is a block diagram of a multiprocessor system in which embodiments of the present invention may be implemented. As shown in multiprocessor system is a point to point interconnect system and includes a first processor and a second processor coupled via a point to point interconnect . However in other embodiments the multiprocessor system may be of another bus architecture such as a multi drop bus or another such implementation. As shown in each of processors and may be multi core processors including first and second processor cores i.e. processor cores and and processor cores and although other cores and potentially many more other cores may be present in particular embodiments.

Still referring to first processor further includes a memory controller hub MCH and point to point P P interfaces and . Similarly second processor includes a MCH and P P interfaces and . As shown in MCH s and couple the processors to respective memories namely a memory and a memory which may be portions of main memory e.g. a dynamic random access memory DRAM locally attached to the respective processors.

First processor and second processor may be coupled to a chipset via P P interconnects and respectively. As shown in chipset includes P P interfaces and . Furthermore chipset includes an interface to couple chipset with a high performance graphics engine via a bus .

As shown in various I O devices may be coupled to first bus along with a bus bridge which couples first bus to a second bus . In one embodiment second bus may be a low pin count LPC bus. Various devices may be coupled to second bus including for example a keyboard mouse communication devices and a data storage unit which may include code in one embodiment. Further an audio I O may be coupled to second bus .

Embodiments may be implemented in code and may be stored on a storage medium having stored thereon instructions which can be used to program a system to perform the instructions. The storage medium may include but is not limited to any type of disk including floppy disks optical disks compact disk read only memories CD ROMs compact disk rewritables CD RWs and magneto optical disks semiconductor devices such as read only memories ROMs random access memories RAMs such as dynamic random access memories DRAMs static random access memories SRAMs erasable programmable read only memories EPROMs flash memories electrically erasable programmable read only memories EEPROMs magnetic or optical cards or any other type of media suitable for storing electronic instructions.

While the present invention has been described with respect to a limited number of embodiments those skilled in the art will appreciate numerous modifications and variations therefrom. It is intended that the appended claims cover all such modifications and variations as fall within the true spirit and scope of this present invention.

