---

title: Controlling a document based on user behavioral signals detected from a 3D captured image stream
abstract: A computer-implemented method, system, and program product comprises a behavior processing system for capturing a three-dimensional movement of a user within a particular environment, wherein the three-dimensional movement is determined by using at least one image capture device aimed at the user. The behavior processing system identifies a three-dimensional object properties stream using the captured movement. The behavior processing system identifies a particular defined behavior of the user from the three-dimensional object properties stream by comparing the identified three-dimensional object properties stream with multiple behavior definitions each representing a separate behavioral signal for directing control of the document. A document control system selects at least one document element to represent the at least one particular defined behavior and inserts the selected document element into the document.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07877706&OS=07877706&RS=07877706
owner: International Business Machines Corporation
number: 07877706
owner_city: Armonk
owner_country: US
publication_date: 20070112
---
The present invention relates in general to improved automated document creation transcription and editing. In particular the present invention relates to controlling a document based on behaviors of a user identified from a three dimensional image stream captured by one or more image capture devices indicative of behavioral signals which trigger control of the document.

Many people create review and edit multiple documents each day whether through generating an email document searching for a web document creating a document in a word processor editing a spreadsheet presenting documents through a document presentation application or coding an application for example. Creating reviewing and editing multiple types of documents typically requires a user to enter multiple commands by typing on a keyboard selecting options with a mouse or speaking commands into a microphone connected to a voice command system.

In creating a document transcribing an event a person watching the event such as a court reporter records a transcript of the event through typing a log representing the person s observations of the words spoken during an event. A person watching the event may observe a behavior of another person and type a reference to the behavior in the log. Voice transcription systems are often used in place of a person to record a transcript of an event where the voice transcription system automatically converts detected voice signals into text to generate a transcript of an event. Voice transcription systems are limited however in that the behaviors of the speaker are not recorded in the textual transcript. While a video record of an event may be recorded to provide a video record of the behaviors of a speaker the combination of an automated transcript from voice converted to text and a video record do not provide a complete textual transcript of both the spoken words and behaviors of speakers and others during an event.

In addition in creating and editing documents it is common that a user may be speaking to another person or listening to another person while creating and editing a document. Thus while voice commands provide a way for a user to reduce the number of keystrokes or mouse clicks that the user performs in a day a user who multi tasks by conversing while editing a document may not be able to use voice commands to edit a document.

Therefore in view of the foregoing there is a need for a method system and program for capturing three dimensional movement of a user predicting types of behaviors from the captured three dimensional movement and selecting whether control a document including adding an element to a document or performing a function on the document is triggered based on the identified type of behavior.

Therefore the present invention provides improved automated document creation transcription and editing. In particular the present invention provides for controlling a document based on behaviors of a user identified from a three dimensional image stream captured by one or more image capture devices indicative of behavioral signals which trigger control of the document

In one embodiment a computer implemented method system and program product comprises a behavior processing system for capturing a three dimensional movement of a user within a particular environment wherein the three dimensional movement is determined by using at least one image capture device aimed at the user. The behavior processing system identifies a three dimensional object properties stream using the captured movement. The behavior processing system identifies a particular defined behavior of the user from the three dimensional object properties stream by comparing the identified three dimensional object properties stream with multiple behavior definitions each representing a separate behavioral signal for directing control of the document. A document control system selects at least one document element to represent the at least one particular defined behavior and inserts the selected document element into the document.

In capturing a three dimensional movement of the user the behavior processing system may capture the three dimensional movement using a stereoscopic image device to identify and track a particular three dimensional movement of the user. In addition in capturing a three dimensional movement of the user the behavior processing system may capture at least one image frame of a user at a first point in time and compare at least one additional image frame from a second point in time with the image frame from the first point in time to detect three dimensional movement.

In selecting at least one document element to represent the at least one particular defined behavior the document control system selects at least one textual indicator of the at least one particular defined behavior to insert into a transcript recorded in the document at a location specified by a time stamp associated with the captured three dimensional object properties stream. In addition in selecting at least one document element to represent the at least one particular defined behavior the document control system may convert the at least one particular defined behavior into a programming code element equation or module.

In addition in the embodiment the document control system selects at least one function to perform on the document to represent at least one particular defined behavior and triggers the application to perform the at least one function on the document. In addition the document control system may select function to perform on a document and a document element to insert in the document.

With reference now to a block diagram illustrates a flow of information in a behavior processing method system and program product. It will be understood that provides one embodiment of information flow for capturing image streams of one or more users and processing those image streams to predict behaviors of users and to predict which behaviors represent behavioral signals which trigger control of a document however other information flows may be implemented to process captured data and predict behaviors and whether the behaviors represent behavioral signals for trigger control of a document.

It is important to note that as used throughout the term behavior may include user actions typically labeled as behavior or body language such as gesturing and facial expressions and may also include but is not limited to any detectable body movements detectable body posture detectable eye movements changes in skin surface characteristics such as color temperature tone and level of perspiration changes in muscle contraction and other types of non verbal communication. The term document as used throughout may include but is not limited to a computer file with at least one text or graphical element created with an application or a computer file that contains data for use by an application. Examples of documents may include but are not limited to a word processor file an email a web page a code module and a graphical file.

In the example a behavior processing system includes a three dimensional 3D object detector . 3D object detector represents multiple systems for capturing images and other data about moving and stationary objects streamlining the captured data tracking particular objects within the captured movement streaming the properties of the particular objects and combining the streamed properties into a three dimensional representation of the 3D characteristics of the captured objects as illustrated by 3D object properties . 3D object properties may include but are not limited to positions color size and orientation representative of movement by users within an environment objects within an environment and the background identifying attributes of an environment for example.

In the example 3D object detector captures images within a focus area represented as detectable behavior movement . In addition 3D object detector may detect other types of data within a focus area. In particular 3D object detector detects detectable behavior movement through multiple types of image and data detection including but not limited to capturing video images detecting body part movement detecting skin texture detecting eye movement detecting skin surface characteristics and capturing thermal images. For supporting multiple types of image and data detection 3D object detector may include multiple types of image capture devices including one or more video cameras arranged for stereoscope video image capture and other types of sensors for capturing at least one other characteristic of one or more objects such as thermal body imaging sensors skin texture sensors laser sensing devices sound navigation and ranging SONAR devices or synthetic laser or sonar systems. In particular a sensor may be implemented using a separate sensor unit or may be implemented through a logic unit that operates on a captured image stream. For example a logic unit may process the captured image stream to detect facial skin textures distinguishable from non skin textures such as a smooth wall or textured foliage within a focus area.

Portions of detectable behavior movement may include images and other data representative of actual behaviors and other portions of detectable behavior movement may include images and data not representative of behaviors. Some behaviors within detectable behavior movement may represent behavioral signals which trigger control of a document.

3D object detector translates detectable behavior movement into a stream of 3D properties of detected objects and passes the stream of 3D object properties to behavior interpreter . Behavior interpreter maps the streamed 3D object properties into one or more behaviors for each detected user and estimates for each predicted behavior of a detected user the probability that the actual behavior in detectable behavior movement is correctly predicted by behavior interpreter . In addition behavior interpreter predicts whether a particular predicted behavior represents a behavioral signal which triggers control of a document.

Behavior interpreter outputs each predicted behavior percentage probability and whether the predicted behavior represents a behavioral signal which triggers control of a document as predicted behavior output . Behavior interpreter may pass predicted behavior output to one or more behavior enabled applications at one or more systems.

In particular in processing detectable behavior movement and generating predicted behavior output 3D object detector and behavior interpreter may access a definition database of previously accumulated and stored behavior definitions to better track and detect within 3D object properties those monitored objects representative of behaviors to better recognize people separate from other objects within detectable behavior movement and to better track recognize and predict behaviors representative of behavioral signals which triggers control of a document from 3D object properties .

Further in processing behavior movement and generating predicted behavior output 3D object detector and behavior interpreter attempt to identify objects representative of user behaviors and predict the type of behavior in view of the overall interaction in which the behavior is made. Thus 3D object detector and behavior interpreter attempt to determine not just a behavior but a level of emphasis included in a behavior that would effect the meaning of the behavior a background of a detected user making a behavior that would effect the meaning of the behavior the environment in which the detected user makes the behavior that would effect the meaning of the behavior combinations of behaviors made together that effect the meaning of each behavior and other detectable factors that effect the meaning of a behavior. In addition 3D object detector and behavior interpreter determine whether a particular user is the user whose behavior should trigger control of a document. Thus definition database includes behaviors definitions corresponding to different types of people environments and other factors that may affect the meaning of a behavior. In addition definition database includes behavior definitions adjusted according to a corresponding facial expression or other corresponding behavior. Further definition database may be trained to more accurately identify objects representing particular people animals places or things.

In addition in generating predicted behavior output behavior interpreter identifies behaviors and identifies the time range over which a behavior occurs or the speed or intensity at which a behavior occurs. In one example where the predicted behavior triggers control of a document the predicted behavior triggers editing a document to include a textual indicator of the behavior such that factors such as the speed of intensity of movement effect the level of detail in the textual indicator. In another example where the predicted behavior triggers control of a document the predicted behavior triggers a function of an application to be performed on the document such that factors such as the speed and intensity of movement effect which function of the application may be triggered by the predicted behavior.

Further in processing behavior movement multiple separate systems of image capture devices and other sensors may each capture image and data about separate or overlapping focus areas from different angles. The separate systems of image capture devices and other sensors may be communicatively connected via a wireless or wired connection and may share captured images and data with one another between 3D behavior detectors or between behavior interpreters such that with the combination of data behavior interpreter may interpreter behaviors with greater accuracy.

Referring now to an illustrative diagram depicts an example of an environment in which a 3D object detector captures and generates the 3D object properties representative of captured behavior movement. It will be understood that detectable movement environment is one example of an environment in which 3D object detector detects images and data representative of detectable behavior movement as described with reference to behavior processing system in .

In the example detectable movement environment includes a stereoscopic image device comprising an image capture device and an image capture device each positioned to detect movement of one or more objects including people within a combined 3D focus area . In the depicted embodiment image capture device and image capture device may each be positioned on one stationary axis or separate stationary axis such that the area represented by 3D focus area remains constantly focused upon. In addition in the depicted embodiment image capture device and image capture device and any other sensors may be positioned in parallel at tangents or at any other angles to control the span of and capture images within 3D focus area .

In another embodiment image capture device and image capture device may each be positioned on a position adjustable axis or the actual focus point of image capture device and image capture device may be adjustable such that the area represented by 3D focus area may be repositioned. In one example each of image capture device and image capture device may be coupled with one or more thermal imaging devices that detect thermal imaging based movement within a broad area and directs the repositioning of the focus area of each of image capture device and image capture device to track the thermal movement within the focus area of each camera.

Further in the present embodiment image capture device and image capture device may be affixed to an apparatus that is carried by or worn by a person. For example image capture device and image capture device may be affixed to a pair of glasses or other headwear for a person such that 3D focus area changes as the person moves. In another example image capture device and image capture device may be affixed to a moving machine such as a vehicle such that 3D focus area changes as the vehicle moves.

Although not depicted in another embodiment only a single video camera such as image capture device may be implemented as a stereoscopic image device. The single video camera is placed on a track or other adjustable axis and a controller adjusts the position of the single video camera along the track wherein the single video camera then captures a stream of video images within a focus area at different positioned points along the track and 3D behavior detector combines the stream of images into a 3D object property stream of the properties of detectable objects. In one example the 3D object property stream can be generated from comparing the changes in luminance and shadowing across the frames as the camera changes in position. Alternatively a stereoscopic image device may be implemented using a single fixed camera coupled with a sensor that detects depth. In addition alternatively a single camera enabled to process images and detect depth from a fixed position may function as a stereoscopic image device. For example the single camera may process images and detect depth from detecting the movement of a light source and comparing changes in luminance and shadowing across the captured image frames. In particular the single camera system may first map a model of a person s face focusing on the eyes mouth and nose and then detect changes in luminance and shadowing across image frames to detect depth characteristics of the face. In other examples a system may process a captured stream of video images to extract depth from other characteristics of the stream of images.

For purposes of example 3D focus area includes a first capture plane captured by image capture device and a second capture plane captured by image capture device . First capture plane detects movement within the plane illustrated by reference numeral and second capture plane detects movement within the plane illustrated by reference numeral . Thus for example image capture device detects movement of an object side to side or up and down and image capture device detects movement of an object forward and backward within 3D focus area . It is important to note that when the movement of an object is tracked at a fine granularity even small adjustments in the body movement such as a raised eyebrow a constricted muscle or a finger bending of a person are tracked and can then be interpreted as behavior representing a behavioral signal which triggers control of a document.

In the example within 3D focus area a hand represents a moving object and a box represents a stationary object. In the example hand is the portion of a person s hand within 3D focus area . A monitored person may make any number of movements some representative of behavioral signals which trigger control of a document by moving hand .

As a person moves hand within 3D focus area each of image capture device and image capture device capture a video stream of the movement of hand within capture plane and capture plane . From the video streams 3D object detector detects hand as a moving object within 3D focus area and generates a 3D property stream representative of 3D object properties of hand over a period of time.

In addition a person may move hand in relation to box or another object. For example a person may point or make another type of behavior directed to box . As the person moves hand within 3D focus area the video streams captured by image capture device and image capture device include the movement of hand and box . From the video streams 3D object detector detects hand as a moving object and box as a stationary object within 3D focus area and generates 3D object property streams indicating the 3D properties of box and the 3D properties of hand in relation to box over a period of time.

It is important to note that by capturing different planes of movement within 3D focus area using multiple cameras more points of movement are captured than would occur with a typical stationary single camera. By capturing more points of movement from more than one angle 3D object detector can more accurately detect and define a 3D representation of stationary objects and moving objects including behaviors within 3D focus area . In addition the more accurately that 3D object detector defines a 3D representation of a moving object the more accurately behavior interpreter can predict a behavior from the 3D model. For example a behavior could consist of a user making a motion directly towards or away from one of image capture device and image capture device which would not be able to be captured in a two dimensional frame 3D behavior detector detects and defines a 3D representation of the behavior as a moving object and behavior interpreter predicts the behavior made by the movement towards or away from a video camera from the 3D model of the movement. Further by capturing more points in different planes of movement within 3D focus area the processing load required to generate 3D object properties is reduced in comparison to the processing load needed to generate 3D object properties from points gathered from only a single 2D plane of movement.

In addition it is important to note that while illustrates a gesturing hand and a stationary box in alternate embodiments 3D focus area may include multiple separate monitored users and other elements such that image capture device and image capture device capture images of the behavior of multiple people and images of behavior of multiple people in relation to each other or one or more elements and 3D object detector detects each behavior by each person as a separate object. In particular 3D object detector may detect from the captured video images from image capture device and image capture device behaviors with more motion such as behaviors made with hands and behaviors made with less motion such as facial expressions to accurately generate 3D object properties of a person s non verbal communication.

In the example in addition to capturing images within focus area within detectable movement environment other sensors may detect information relevant to an environment but outside of focus area . For example sensor may detect information within a sensor area . Sensor area may overlap be incorporated within incorporate or be separate from focus area . 3D object detector combines the sensed information with captured images to more accurately generate 3D object properties and to provide additional information about an environment to a document control system.

In one example sensor may perform facial recognition or other identification recognition from the captured image streams. In one example sensor may access a database of facial mappings for users and identify a particular user facial mapping matching a particular facial mapping from the database. In another example sensor may analyze the captured image stream for text that may identify a user. For example sensor may detect a badge number the captured image stream of a badge worn by a user. By sensor detecting a user identity object detector may more accurately generate 3D object properties and movement interpreter may more accurately predict types of user behavioral movement from definitions specified for the identified user in definition database .

Additionally in another example sensor may supplement user and other object recognition by detecting information broadcast from RFID chips placed on objects within sensor area where the RFID of an object broadcasts the object type the object location and any warning conditions associated with the object. In on example a portable telephone carried by a user an identification card carried by a user or other item carried by a user may include an RFID chip that broadcasts one or more of an identifier for the user and an identifier for the item. By combining sensed information about the location of a particular object with captured images from which the image is identified object detector may more accurately generate 3D object properties and behavior interpreter may more accurately predict the types of 3D objects the user associated with 3D objects the behaviors representing behavioral signals which trigger control of a document detected within 3D object properties .

In another example sensor may track the relative location of a tracked object within sensor area . Although not depicted sensor area may track a person move from a first focus area to a second focus area within sensor area . By tracking movement across multiple focus areas sensor provides additional tracking information of a location of a monitored person so that data gathered in different focus areas can be shared when generating 3D object properties .

In yet another example sensor may detect additional information about the depth surface area color temperature or other characteristic of an object to more accurately predict whether the object is representative of a particular behavior and whether that represents a behavioral signal which triggers control of a document. In particular by detecting additional information about the depth surface area or other characteristic of an object data collected by sensor is combined with images captured by image capture device and image capture device to generate additional detail and granularity in a 3D image of an object representing user movement

With reference now to a block diagram illustrates one embodiment of a 3D object detector system for generating 3D object properties for enabling a behavior interpreter to interpret from 3D object properties behaviors of monitored users and whether these behaviors represent behavioral signals which trigger control of a document. It is important to note that the multiple components depicted within 3D object detector may be incorporated within a single system or distributed via a network other communication medium or other transport medium across multiple systems. In addition it is important to note that additional or alternate components from those illustrated may be implemented in 3D object detector for capturing images and data and generating a stream of 3D object positions .

Initially multiple image capture devices such as image capture device image capture device and sensor represent a stereoscopic image device for acquiring the data representative of detectable movement within a 3D focus area and sensor area such as 3D focus area and sensor area . As previously described image capture device and image capture device may represent video cameras for capturing video images. In addition image capture device and image capture device may represent a camera or other still image capture device. In addition image capture device and image capture device may represent other types of devices capable of capturing data representative of detectable behavior movement . Image capture device and image capture device may be implemented using the same type of image capture system or different types of image capture systems. In addition the scope size and location of the capture area and plane captured by each of image capture device and image capture device may vary.

Sensor may represent one or more different types of sensors as described with reference to . Sensor may gather independent data about an object or may process the images captured by image capture device and image capture device .

Each of image capture device image capture device and sensor transmit captured images and data to one or more computing systems enabled to initially receive and buffer the captured images and data. In the example image capture device transmits captured images to image capture server image capture device transmits captured images to image capture server and sensor transmits captured data to sensor server . Image capture server image capture server and sensor server may be implemented within one or more server systems.

Each of image capture server image capture server and sensor server streams the buffered images and data from image capture device image capture device and sensor device to one or more processors. In the example image capture server streams images to a video processor image capture server streams images to a video processor and sensor server streams the sensed data to sensor processor . It is important to note that video processor video processor and sensor processor may be implemented within one or more processors in one or more computer systems.

In one example image capture server and image capture server each stream images to video processor and video processor respectively where the images are streamed in frames. Each frame may include but is not limited to a camera identifier ID of the image capture device a frame number a time stamp and a pixel count.

Video processor video processor and sensor processor are programmed to detect and track objects within image frames. In particular because video processor video processor and sensor processor receive streams of complex data and process the data to identify three dimensional objects including objects representing monitored users and characteristics of the three dimensional objects video processor video processor and sensor processor may implement the Cell Broadband Engine Cell BE architecture Cell Broadband Engine is a registered trademark of Sony Computer Entertainment Inc. . The Cell BE architecture refers to a processor architecture which includes a base processor element such as a Power Architecture based control processor PPE connected to multiple additional processor elements also referred to as Synergetic Processing Elements SPEs and implementing a set of DMA commands for efficient communications between processor elements. In particular SPEs may be designed to handle certain types of processing tasks more efficiently than others. For example SPEs may be designed to more efficiently handle processing video streams to identify and map the points of moving objects within a stream of frames. In addition video processor video processor and sensor processor may implement other types of processor architecture that enables efficient processing of video images to identify in three dimensions moving and stationary objects within video images from which behavior of monitored users and whether the behavior represents behavioral signals which trigger control of a document can be predicted.

In the example video processor video processor and sensor processor each create and stream the properties including positions color size shape and orientation of the detected objects to a geometry processor . In one example each processed frame streamed to geometry processor may include but is not limited to a camera ID a frame number a time stamp and combinations of two or more of X axis coordinates x loc Y axis coordinates y loc and Z axis coordinates z loc . It is important to note that x loc y loc and z loc may each include multiple sets of points and other data that identify all the properties of an object. If multiple objects are detected and tracked within a single frame the X axis coordinates and Y axis coordinates for each object may be included in a single streamed object property record or in multiple separate streamed object property records. In addition a streamed property frame such as the frame from sensor processor for a SONAR detected position may include Z axis location coordinates listed as z loc for example.

Geometry processor receives the 2D streamed object properties from video processor and video processor and the other object data from sensor processor . Geometry processor matches up the streamed 2D object properties and other data and constructs 3D object properties from the streamed 2D object properties and other data. In particular geometry processor constructs 3D object properties that include the depth of an object. In one example each 3D object property record constructed by geometry processor may include a time stamp an object or user movement label X axis coordinates x loc Y axis coordinates y loc and Z axis coordinates z loc and additional information collected from sensors. For example additional information collected from sensors may include a location identifier received from an RFID or GPS detected location coordinates.

At any of video processor video processor sensor processor and geometry processor property records may include at least one identifier to enable persistence in tracking the object. For example the identifier may include a unique identifier for the object itself and an identifier of a class or type of object including an object identified as user movement.

In particular by video processor video processor and sensor processor identifying and classifying object properties each of the processors may access definition database for accessing previously processed inputs and behavior mappings to more accurately identify and classify 2D object properties to detect and match the streamed 2D object properties to an object. In addition geometry processor may more accurately construct 3D properties of objects based on the streamed 2D object properties based on previously matched and constructed 3D properties of objects accessed from definition database . Further object database may store the streamed 2D object properties and 3D object properties for future reference.

In addition by video processor video processor and sensor processor identifying and classifying object properties and by geometry processor constructing 3D object properties each of the processors may identify detected objects including behaviors of monitored people. For example video processor video processors sensor processor and geometry processor may access definition database which includes behavior definitions for use in mapping facial expressions and other body movements performing facial and other body movement recognition and performing additional processing to identify an object representing a behavior. In addition video processor video processors sensor processor and geometry processor may access definition database which includes behavior definitions for different types of environments for use in identifying a particular environment in which a user is located based on detected objects and background. Further in constructing 3D object properties video processor video processors sensor processor and geometry processor may identify multiple detected objects in the environment and therefore identify multiple behaviors of a single monitored person or one or more interactions between multiple people. By monitoring and identifying interactions between objects detected in the environment in which the object is located more accurate prediction of a behavior in the context in which the behavior is made may be performed.

Referring now to a block diagram illustrates one embodiment of a behavior interpreter system. It is important to note that the multiple components depicted within behavior interpreter may be incorporated within a single system or distributed via a network across multiple systems. In the example a 3D properties record includes time stamp x loc y loc and z loc data elements. It will be understood that 3D properties record may include additional or alternate data elements as determined by geometry processor of . For example 3D properties record may include additional information identifying a particular or relative location of a user within a sensor area and not just within the focus area colors and other data collected by image capture devices and sensors and processed within 3D object detector .

Behavior interpreter includes a behavior interpreter controller where behavior interpreter controller may include one or more processors programmed to perform behavior interpretation. For example behavior interpreter controller may include a processor with the CellBE architecture programmed to efficiently process 3D object properties data streams predict behaviors of monitored people from the 3D object properties streams and predict whether the behaviors represent behavioral signals which trigger the control of a document. In addition behavior interpreter controller may include processors upon which software runs where the software directs processing of 3D object properties streams predicts behaviors of monitored people from the 3D object properties streams and predicts whether the behaviors represent behavioral signals which trigger the control of a document.

In processing 3D object properties streams predicting behaviors and predicting whether behaviors represent behavioral signals which trigger control of a document behavior interpreter controller maps 3D object properties to one or more behavior definitions with a percentage probability that the streamed 3D object properties represent the mapped behavior definitions and with a percentage probability that the predicted behavior represents a behavioral signal which triggers control of a document. In particular behavior interpreter controller accesses one or more behavior definitions for one or more behaviors and determines whether the 3D object properties match one or more characteristics of one or more behaviors as defined in one or more of the behavior definitions. Behavior definitions may include mapped 3D models of one or more types of behaviors. In addition behavior definitions may define the parameters of identifying characteristics of a behavior including but not limited to body part detected type of movement surface characteristics shape speed of movement frequency span of movement depth of movement temperature and color.

In addition behavior definitions are specified to enable behavior interpreter controller to determine whether characteristics of a behavior indicate that the behavior is representative of a behavioral signal which trigger control of a document. For example once behavior interpreter controller determines that an object stream represents a person scratching one s nose behavior interpreter controller determines whether a person scratching one s nose represents a behavioral signal which triggers control of a particular document based on the type of document the type of application the type of environment or other factors. For example a person scratching one s nose during a meeting may not represent a behavioral signal which triggers inserting a textual indicator of the behavior in the text transcript of the meeting. Further a nose scratch may be determined as a type of behavior which a user often performs involuntary and therefore is not a behavioral signal. In contrast a user holding up two fingers may be determined as a type of behavior which a user performs intentionally and for which a textual indicator should be added to a transcript or which should trigger a function to be performed by an application on a document.

It is important to note that in interpreting 3D object properties streams behavior interpreter controller performs an aggregate analysis of all the tracked objects in one or more 3D object properties streams identified for a particular focus area by one or more behavior processing systems. In one example behavior interpreter controller aggregates the 3D object property streams for a particular focus area and particular sensor area. In another example behavior interpreter controller may receive multiple 3D object properties streams from areas overlapping a focus area and sensor area analyze the 3D object properties streams for similarities location indicators and orientation indicators and construct the 3D object properties streams into a 3D aggregate representation of an area.

In one embodiment behavior interpreter controller may map the aggregate of the tracked objects directly into a single behavior definition. In another embodiment behavior interpreter controller maps multiple aggregated tracked objects into multiple behavior definitions. For example a person may simultaneously communicate through facial behavior and a hand behavior where in predicting the actual behaviors communicated through the tracked movement of the facial behavior and hand behavior behavior interpreter controller analyzes the 3D object properties of the facial behavior in correlation with the 3D object properties of the hand behavior and accesses behavior definitions to enable prediction of each of the behaviors and to enable prediction of whether the behaviors in combination represent behaviors representing a behavioral signal which triggers control of a document. Additionally behavior interpreter controller may aggregate the tracked objects representative of behavior by multiple monitored people within an environment. Behavior interpreter controller then predicts whether the combination of behaviors by multiple monitored people is representative of behavioral signals Which trigger control of a document.

In the example behavior interpreter controller accesses behavior definitions from definition database which includes general behavior definitions environment specific behavior definitions application specific definitions and user specific behavior definitions . It will be understood that definition database may include additional or alternate types of behavior definitions. In addition it is important to note that each of the groupings of behavior definitions illustrated in the example may reside in a single database or may be accessed from multiple database and data storage systems via a network.

General behavior definitions include behavior definitions for common behaviors. For example general behavior definitions may include behaviors definitions for common behaviors such as a person pointing a person waving a person nodding yes or shaking one s head no or other types of common behaviors.

Environment specific behavior definitions include behavior definitions and factors for predicting a behavior and predicting whether a behavior represents a behavioral signal for which triggers control of a document that are specific to the context in which the behavior is being detected. Examples of contexts may include but are not limited to the current location of a monitored person the type of document currently accessed by the user the time of day the cultural meanings behind gestures and other behaviors within the context the languages spoken within the context and other factors that influence the context in which behavior could be interpreted. Behavior interpreter controller may detect current context from accessing a GPS indicator of a monitored person location from performing speech analysis of the monitored person s speech to detect variations in language and dialect from detecting objects within the image data indicative of particular types of locations or from receiving additional data from other systems monitoring the context in which a monitored person is monitored.

Application specific definitions include behavior definitions specific to the application which will be triggered to control the document based on predicted behavior output . For example if predicted behavior will trigger an application to insert a textual indicator of a behavior to a document application specific definitions specify behavior definitions for the types of behaviors that would be relevant to addition as a textual indicator in a document. In another example if predicted behavior will trigger an application to perform a function on a document then application specific definitions specify behavior definitions for the types of behaviors that would be relevant to triggering a function to be performed on a document. In addition application specific definitions may include definitions for detecting current processes performed by an application that would affect the meaning of a behavior or would effect whether a particular behavior is relevant to triggering a function to be performed on a document.

User specific behavior definitions include behavior definitions specific to a particular person being monitored. In one example behavior interpreter controller accesses an identifier for a user from facial or voice recognition performed by a sensor or by behavior interpreter controller . In addition behavior interpreter controller may not identify the actual identity of a monitored person but may identify attributes of a person that identify the person as a type of person whose behaviors trigger document control. In another example facial recognition may be supplemented or identification may be performed solely from scanning for an RFID on identification carried by the monitored person detecting a particular item worn by a user or accessing other types of identification from a system monitoring the identification of a user.

Definition database may also include behavior definitions and other factors specified according to a level of experience of a monitored person within a particular environment. For example the characteristics of a behavior in a behavior definition and the specific behavior definitions grouped within definition database may be specified according to the level of experience of a user in triggering document control based on behavioral signals.

Further within the available behavior definitions a behavior definition may be associated with a particular area of movement or a particular depth of movement within a 3D focus area. In particular the three dimensional focus area in which movement is detected may be divided into three dimensional portions where movements made in each of the portions may be interpreted under different selections of behavior definitions. For example one three dimensional portion of a focus area may be considered an active region where movement detected within the area is compared with a selection of behavior definitions associated with that particular active region such as a region within a particular distance of a keyboard or other area in which a user may trigger document control based on behavioral signals.

Additionally within behavior definitions included within definition database the predictability of a behavior may be increased by the presence of associated audio signatures or translated text from audio. In particular sensor or another audio sensor may detect audio from an environment or from a specific person. Behavior interpreter controller may determine whether sensed audio increases the probability that a detected movement represents a particular type of behavior. In one example behavior interpreter controller may convert audio signals into text and determine whether the text matches types of words typically associated with a behavioral signal.

The behavior definitions included within definition database may be added to or adjusted based on user feedback of a monitored person in a similar manner as a speech recognition system is trained to more accurately map and predict behaviors. For example definition database may learn additional behavior definitions and adjust the parameters of already learned behavior definitions through a monitored person indicating whether the specific type of document control trigger responsive to a behavior is the type of document control intended to be triggered by the user s behavioral signal.

Behavior interpreter controller may output predicted behavior output in the form of one or more behavior records such as behavior record . Behavior record indicates the behavior type probability as the behavior and behavioral signal type . The behavioral signal type may indicate for example a percentage probability of the behavior as a behavioral signal. In another example the behavioral signal type may indicate an identifier for the user the type of environment an application or a document to which the behavioral signal is associated. In addition the behavioral signal type may include additional or alternate types of data gathered by behavior interpreter in predicting a behavior type and predicting whether that behavior type represents a behavioral signal which triggers control of a document. Alternatively a behavior interpreter controller may transmit any predicted behavior to a document control system in predicted behavior record and enable the document control system to determine whether the predicted behavior type triggers control of a document by the document control system.

In addition as illustrated behavior record includes the start X Y and Z axis properties and ending X Y and Z axis properties of the detected behavior indicative of the location direction of movement and speed of movement of the behavior listed as start x pos end x pos start y pos end y pos start z pos end z pos and the time stamp range indicating the times over which the behavior is detected. In addition or alternatively behavior record may include indicators of the location direction of movement intensity of movement and speed of movement of the monitored person. Further additional information acquired from sensors such as RFID data GPS coordinates skin surface characteristics and other sensed data may be associated with a particular behavior record or included in a separate object record.

In passing behavior record behavior interpreter controller may filter out particular types of behavior records. For example behavior interpreter controller may not pass records where the predictability of a behavior as a behavior type is less than a particular percentage. In addition in passing behavior record behavior interpreter controller may filter one type of behavior records for passing to one type of document control system and filter other types of behavior records for passing to another type of document control system.

With reference now to a block diagram depicts one embodiment of a computing system in which the present invention may be implemented. The controllers and systems of the present invention may be executed in a variety of systems including a variety of computing systems such as computer system communicatively connected to a network such as network .

Computer system includes a bus or other communication device for communicating information within computer system and at least one processing device such as processor coupled to bus for processing information. Bus preferably includes low latency and higher latency paths that are connected by bridges and adapters and controlled within computer system by multiple bus controllers. When implemented as a server computer system may include multiple processors designed to improve network servicing power. Where multiple processors share bus an additional controller not depicted for managing bus access and locks may be implemented.

Processor may be a general purpose processor such as IBM s PowerPC processor that during normal operation processes data under the control of an operating system application software middleware not depicted and other code accessible from a dynamic storage device such as random access memory RAM a static storage device such as Read Only Memory ROM a data storage device such as mass storage device or other data storage medium. In one example processor may further implement the CellBE architecture to more efficiently process complex streams of data in 3D. It will be understood that processor may implement other types of processor architectures. In addition it is important to note that processor may represent multiple processor chips connected locally or through a network and enabled to efficiently distribute processing tasks.

In one embodiment the operations performed by processor may control 3D behavior detection from captured images and data for an environment in which a person may communicative through behaviors recognizing the behaviors determining which behaviors represent behavioral signals which trigger control of a document and controlling a document responsive to behavioral signals as depicted in the operations of flowcharts of and other operations described herein. Operations performed by processor may be requested by operating system application software middleware or other code or the steps of the present invention might be performed by specific hardware components that contain hardwired logic for performing the steps or by any combination of programmed computer components and custom hardware components.

The present behavior processing system and behavior enabled document control system may be provided as a computer program product included on a computer or machine readable medium having stored thereon the executable instructions of a computer readable program that when executed on computer system cause computer system to perform a process according to the present invention. The terms computer readable medium or machine readable medium as used herein includes any medium that participates in providing instructions to processor or other components of computer system for execution. Such a medium may take many forms including but not limited to storage type media such as non volatile media and volatile media and transmission media. Common forms of non volatile media include for example a floppy disk a flexible disk a hard disk magnetic tape or any other magnetic medium a compact disc ROM CD ROM or any other optical medium punch cards or any other physical medium with patterns of holes a programmable ROM PROM an erasable PROM EPROM electrically EPROM EEPROM a flash memory any other memory chip or cartridge or any other medium from which computer system can read and which is suitable for storing instructions. In the present embodiment an example of a non volatile medium is mass storage device which as depicted is an internal component of computer system but will be understood to also be provided by an external device. Volatile media include dynamic memory such as RAM . Transmission media include coaxial cables copper wire or fiber optics including the wires that comprise bus . Transmission media can also take the form of acoustic or light waves such as those generated during radio frequency or infrared data communications.

Moreover the present invention may be downloaded or distributed as a computer program product wherein the computer readable program instructions may be transmitted from a remote computer such as a server to requesting computer system by way of data signals embodied in a carrier wave or other propagation medium via network to a network link e.g. a modem or network connection to a communications interface coupled to bus . In one example where processor includes multiple processor elements then a processing task distributed among the processor elements whether locally or via a network may represent a computer program product where the processing task includes program instructions for performing a process or program instructions for accessing Java Java is a registered trademark of Sun Microsystems Inc. objects or other executables for performing a process. Communications interface provides a two way data communications coupling to network link that may be connected for example to a local area network LAN wide area network WAN or directly to an Internet Service Provider ISP . In particular network link may provide wired and or wireless network communications to one or more networks such as network . Further although not depicted communication interface may include software such as device drivers hardware such as adapters and other controllers that enable communication. When implemented as a server computer system may include multiple communication interfaces accessible via multiple peripheral component interconnect PCI bus bridges connected to an input output controller for example. In this manner computer system allows connections to multiple clients via multiple separate ports and each port may also support multiple connections to multiple clients.

Network link and network both use electrical electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link and through communication interface which carry the digital data to and from computer system may be forms of carrier waves transporting the information.

In addition computer system may include multiple peripheral components that facilitate input and output. These peripheral components are connected to multiple controllers adapters and expansion slots such as input output I O interface coupled to one of the multiple levels of bus . For example input device may include for example a microphone a video capture device a body scanning system a keyboard a mouse or other input peripheral device communicatively enabled on bus via I O interface controlling inputs. In addition for example an output device communicatively enabled on bus via I O interface for controlling outputs may include for example one or more graphical display devices audio speakers and tactile detectable output interfaces but may also include other output interfaces. In alternate embodiments of the present invention additional or alternate input and output peripheral components may be added.

Those of ordinary skill in the art will appreciate that the hardware depicted in may vary. Furthermore those of ordinary skill in the art will appreciate that the depicted example is not meant to imply architectural limitations with respect to the present invention.

Referring now to a block diagram depicts one example of a distributed network environment in which a behavior enabled document control method system and program may be implemented. It is important to note that distributed network environment is illustrative of one type of network environment in which the behavior enabled document control method system and program may be implemented however the behavior enabled document control method system and program may be implemented in other network environments. In addition it is important to note that the distribution of systems within distributed network environment is illustrative of a distribution of systems however other distributions of systems within a network environment may be implemented. Further it is important to note that in the example the systems depicted are representative of the types of systems and services that may be accessed or request access in implementing a behavior processing system and a behavior enabled document control system. It will be understood that other types of systems and services and other groupings of systems and services in a network environment may implement the behavior processing system and behavior enabled document control system.

As illustrated multiple systems within distributed network environment may be communicatively connected via network which is the medium used to provide communications links between various devices and computer communicatively connected. Network may include permanent connections such as wire or fiber optics cables and temporary connections made through telephone connections and wireless transmission connections for example. Network may represent both packet switching based and telephony based networks local area and wide area networks public and private networks. It will be understood that is representative of one example of a distributed communication network for supporting a behavior processing system and behavior enabled document control system however other network configurations and network components may be implemented.

The network environment depicted in may implement multiple types of network architectures. In one example the network environment may be implemented using a client server architecture where computing systems requesting data or processes are referred to as clients and computing systems processing data requests and processes are referred to as servers. It will be understood that a client system may perform as both a client and server and a server system may perform as both a client and a server within a client server architecture. In addition it will be understood that other types of network architectures and combinations of network architectures may be implemented.

In the example distributed network environment includes a client system with an image capture system and a client system with an image capture system . In one example image capture systems and represent stereoscopic image devices implementing one or more image capture devices such as image capture devices and and may include one or more sensors such as sensor . Image capture systems and capture images and other data and stream the images and other data to other systems via network for processing. In addition stereoscope image capture systems and may include video processors for tracking object properties such as video processor and video processor described with reference to and a geometry processor for generating streams of 3D object properties such as geometry processor described with reference to .

In one example each of client system and client system may stream captured image frames to one or more 3D object detection services. In one example an behavior processing service provider server provides a service that includes both an object detector service such as 3D object detector for processing streamed images and other data and an behavior interpreter service such as behavior interpreter for predicting a type of behavior predicting a probability that the captured images represent the predicted type of behavior and predicting whether the behavior represents a behavioral signal which triggers control of a document and controlling output of the predicted behavior records to one or more other systems accessible via network .

As to behavior processing service provider server different entities may implement a behavior processing service and different entities may access the behavior processing service. In one example a user logged into one of client systems or client system may subscribe to the behavior processing service. In another example an image capture system or a particular application requesting behavior processing may automatically stream captured images and data to the behavior processing service. In yet another example a business or other entity may implement the behavior processing service in a communications network.

In another example each of client system and client system may stream captured frames to a 3D object detector server . 3D object detector server receives captured images and other data from image capture systems such as image capture system or image capture system and processes the images and other data to generate 3D object properties of detected behaviors for output to a behavior interpreter system such as behavior interpreter server or behavior processing service provider server . In additional or alternate embodiments an object detector service may be implemented within one or more other systems with one or more other services performed within those systems. In particular in additional or alternate embodiments a 3D object detector service may be implemented within a client system at which the images and other data are captured.

Each of the server systems described may be distributed across one or more systems. In addition each of the server systems may be distributed across systems with 3D image processing power including processors with the CellBE architecture programmed to perform efficient 3D data processing. In one example an entity such as a business or service provider may implement separate server systems for object detection and behavior interpretation wherein multiple behavior interpreter servers are implemented with each behavior interpreter server processing different types of 3D object properties.

Behavior processing service provider server behavior interpreter server and 3D object detector server may locally store a definition database such as definition database of raw images 3D behavior properties behavior definitions and other object definitions. In addition behavior processing service provider server behavior interpreter server and 3D object detector server may access a behavior database service server that facilitates definition database .

In addition behavior database service server includes a behavior learning controller . Behavior learning controller prompts users to provide samples of particular types of behaviors which should be recorded through a textual indicator in a document or which should represent behavioral signals for a particular function to be performed on a document. In addition behavior learning controller prompts users to indicate whether predicted behavior types are accurately predicted. In addition behavior learning controller gathers other information that enables behavior learning controller to learn and maintain behavior information in definition database that when accessed by behavior object detector services and behavior interpreter services increases the accuracy of generation of 3D object properties and accuracy of prediction of behaviors and whether behaviors represent behavioral signals from 3D object properties by these services.

Further behavior processing service provider server behavior interpreter server 3D object detector server or behavior database service server may access additional context information for a user specifying the types of behaviors that represent behavioral signals for a particular user from a client profile service server . I In one example client profile service server facilitates capturing and storing behavior definitions for particular types of behaviors which are then accessible to multiple services such as behavior processing service provider server and behavior interpreter server such that a user need not provide a separate gesture sample to each of the separate servers.

Behavior processing service provider server and behavior interpreter server stream predicted behavior records such as predicted behavior record to behavior enabled applications via network . In the example embodiment client system includes a document control system which is a behavior enabled document control system enabling client system to determine and control documents in a single application or across multiple applications based on behavior records and other data. Document control system at client system may receive predicted behavior records from client system as captured by image capture system or may receive predicted behavior records based on images and data detected by other image capture systems.

In addition in the example embodiment client service provider server includes a document control system which is a behavior enabled document control service for enabling client service provider server to control documents based on behavior records for multiple subscribers. In particular client service provider server represents a server which provides a document control service to one or more subscribing client systems or subscribing users. Document control system may receive behavior records associated with a particular user a particular image capture system a particular client system a particular environment or other associations and determine and output control signals for document control to one or more client systems. In addition document control system may receive behavior records determine required document control based on the behavior records and perform the required document control.

With reference now to a block diagram illustrates one example of an implementation of a behavior processing system communicating with a behavior enabled document control system. Document control system receives predicted behavior records such as predicted behavior record from behavior processing system . Based on the predicted behavior records document control system specifies a document control signal for controlling a specific document such as document or for controlling multiple documents. In one example the document control signal may specify an element to insert into a document or a function to apply to a document.

In the example a document element controller receives predicted behavior record and translates the behavior into document control system based on behavior translation database . In the example behavior translation database includes multiple entries for translating a behavior type and minimum probability from predicted behavior record into a document element or a function to be applied to the document.

In particular document element may represent a textual or graphical element. In addition document element may specify the behavior performed by a user or may specify a textual or graphical element translated from a behavior performed by a user. For example if a user holds up a pointer finger document element may include a textual statement of user holds up a pointer finger a graphical element of a hand with a pointer finger held up a textual statement of one or a textual statement of wait depending on whether the behavior is translated into a literal indicator of the behavior or translated into an indicator of the meaning of the behavior such as representing the number one or representing a request for someone to wait.

Function may represent a function to be applied to an entire document to a portion of a document or to a particular element within a document. Examples of function may include but are not limited to specifying text formatting such as changing text to bold italics or strikethrough tool application such as spell checking or tracking changes in a document or other specific functions of an application.

In addition it is important to note that that a behavior type may be translated into one or more document elements and function elements. In one example a user may point to multiple people during a meeting where pointing to the people triggers function element requesting that an email application access the email addresses of those persons pointed to and triggers document element to insert the email addresses as cc addressees of an electronic mail message. In another example a predicted behavior record may specify a behavior of pointing to an x ray or other displayed document and specify the position within the document pointed to. Function element may request a function for creating a comment box at the position in the document pointed to. Document element may specify the textual or graphical indicator to add to the comment box including but not limited to a translation of any speech into text detected in association with pointing at the document a textual indicator of the user pointing and a time stamp or other textual or graphical indicators of other behavior types.

In one example document control system is a functional component of a particular application which controls document . In another example document control system is implemented separately from a particular application and transmits document control signal as input to the application controlling document .

In addition document control system includes a document preference application programming interface API . In one example document preference API enables an interface through which a user may select whether predicted behavior types should be translated into document elements or functions. In another example document preference API enables an application to specify whether the application applies document control signals for inserting document elements or only applies document control signals for applying a function to a document. In addition document preference API enables an application to specify selections of types translate into document elements and selections of behavior types translate into functions.

Referring now to a block diagram illustrates one example of a document control system for managing document control signals for multiple applications. In the example document control system includes a translation preference learning controller . As will be further described translation preference learning controller facilitates an interface through which a user may select for each separate application rules for translating behavior types for storage in behavior translation database . In one example a translation rules translates a behavior type with a minimum probability of being correctly predicted for a particular application identifier into a particular document element or function of the identified application . In addition a factor in translating a behavior type may also include a user environment which may include a specific user environment or one or multiple application modes.

In one example if a user is viewing a slide presentation application and the user environment specifies that the user is viewing the slide presentation in a project creation mode for creating the frames of the presentation document then the behavior types for this environment specified for translation within behavior translation database separately than if a user is viewing a slide presentation in a user environment of the application s presentation mode. In one example a user may specify a particular behavior type such that when the user environment is presentation mode that particular behavior type translates into a function for triggering the application to advance to the next frame in the presentation document. In another example a user may specify a selection of behavior types that if made when the user environment is presentation mode should trigger the presentation application to insert textual or graphical indicators of the behavior types within the frames of the presentation or within a recording of the presentation. In one example a recording of a presentation may include a record of the frames including a record of the speech converted into text associated with the frames as displayed and the text or graphical indicators of a behavior type associated with the frames as displayed.

In addition document control system includes an application status detector that detects the status of the current active application accessible to the user. Application status detector records application status records such as application status records and in an application status time stamp log . In the example application status records and include an identifier ID for the application at least one document opened within the application and a time stamp when the application was selected as the active application. By recording a time stamp of when an application is active and the document or documents open within the application document element controller may more accurately map the time stamp range of predicted behavior record to the application or series of applications and document active when the behavior was detected.

In one example document element controller receives predicted behavior record with a time stamp matching application status record . Based on an entry in behavior translation database document element controller translates the predicted behavior type of fingers spread out with a probability of 80 and an environment of sitting into a function of format word series into bulleted list format . In the example document element controller generates document control signal with instructions to perform a function of format word series into bulleted list format for the application identifier word processor and the document presentation C . In the example the function element of document control signal includes a description of the function however the function element may also include a key command function call or other directions for specifically calling an application to perform a particular function.

With reference now to a block diagram illustrates examples of entries in a behavior translation database. In the example the entries displayed according to the application with which each entry is assigned. In particular entries and are applicable to all applications entries and are applicable to a web page editor application and entries and are applicable to an email application. It will be understood that behavior translation database may include entries applicable to one or more applications as depicted and may include entries applicable to additional or alternate applications from the applications depicted.

In the first column of entries applicable to all applications entry specifies that for a behavior type of clasp hands together with a minimum probability of 90 then a function of paragraph reflow should be triggered for one or more documents. In a next example entry specifies that for a behavior type of rotate hands in circle with a minimum probability of 95 then a function of spell check should be triggered for one or more documents. Entry specifies that for a behavior type of air quotes with a minimum probability of 95 a quotation mark should be inserted as a document element in a document. In addition entry specifies that for a behavior type of left hand curved swipe with a minimum probability of 90 a left parenthesis should be inserted as a document element in a document. It will be understood that additional or alternate types of entries may be applied to all applications. In addition it will be understood that while an entry may apply to more than one application the entry may include multiple separate function calls for actually triggering the function in each of the applications.

In the next column of entries applicable to a web page editor application entry specifies that for a series of behaviors of point to head and hold up number of fingers with a minimum probability of 95 a header class element numbered with the number of fingers should be added as a document element. For example if the user holds up three fingers then the class reference of would be added to the document. In a next example entry specifies that for a series of behaviors of circling finger and hold up number of fingers with a minimum probability of 70 a loop element with the basis of 1 to the number of fingers held up should be added as a document element. For example if the user flashes 10 fingers twice then the loop element of for 1 x

In a next selection of entries entry and are specified for separate applications and include the same behavior type but trigger different types of functions. In particular entry specifies that for a behavior of pound fist on desk once with a 3 5 inch bounce with a minimum probability of 95 a browser check function of the web page editor should be triggered. In the example the bounce range indicates the expected range of upward movement of a fist after hitting the desk. Entry specifies that for the same behavior but within the context of an email application the email application should be triggered to insert a document element of an electronic signature. In the example although the same behavior of a pounding fist triggers different functions in different applications both of the functions are representative of functions typically performed when a user is concluding a draft. For example a user triggers a browser check function when the user is ready to check how coded HTML will appear in a browser window and a user triggers an electronic signature function when the user is ready to sign off on an email composition. In other embodiments a user may select a same behavior type with slight variations such as a pounded fist with a smaller or greater bounce range may trigger different functions in a same application or different functions across different applications.

In addition in the example entries and illustrate a behavior type that triggers an application to perform functions and to insert document elements. In the example entry specifies that for a behavior of pointing another person with two fingers with a minimum probability of 75 a function of find email address for identified person is triggered and a document element of insert email address as primary addressee is triggered. Entry distinguishes that for a behavior of point at another person with one finger then the same function is triggered as in entry but the document element requires insert email address as a cc addressee where an email application allows a user to address the email to one or more primary recipients and one or more carbon copy cc recipients. It is important to note that when a predicted behavior record is translated into function and document elements document control system may send a document control signal with both the function and document elements or document control system may send a first function request and then based on the results of the first function send a document control signal for inserting the document element.

With reference now to a block diagram illustrates inserting a document element into a document where the document element is a textual indicator of a behavior type. In the example document control system receives predicted behavior record with a behavior type of holding up one finger a probability of 90 for a user identifier ID of Tate and a time range of 11 21 25 11 21 28. Document control system determines based on one or more entries in behavior translation database to translate predicted behavior record into a document control signal inserting a document element of holding up one finger for the user identifier as Tate at the time stamp 11 21 25.

In the example predicted behavior record is within the context of a web conference. A transcript controller receives web conference streams of video audio and text entered during the web conference. Behavior processing system may also access web conference streams and generate predicted behavior record from web conference streams . In another example behavior processing system may capture image streams of a user independent of web conference streams and generate predicted behavior record from the captured image streams.

In particular in the example transcript controller transcribes web conference streams from all the web conference participants into transcript . In addition transcript controller receives document control signals from one or more document control systems such as document control signal from document control system and inserts document elements specified in document control signals into transcript . In the example transcript controller inserts entry responsive to document control signal .

Referring now to a high level logic flowchart depicts a process and program for a behavior processing system to predict behavior types with a percentage probability. In the example the process starts at block and thereafter proceeds to block . Block depicts capturing via a stereoscopic image device multiple image streams and via sensors sensor data within a focus area. Next block illustrates tracking objects within the images and sensor data. Thereafter block depicts generating a stream of 3D object properties for tracked objects. Thereafter block depicts aggregating the 3D object properties for each of the tracked objects. In particular the aggregated 3D object properties represent one or more objects tracked in association with at least one monitored person representative of behaviors of the at least one monitored person. Next block illustrates predicting at least one type of behavior from the aggregated stream of 3D object properties from one or more behavior definitions that match the aggregated stream of 3D object properties with a percentage probability. Thereafter block depicts transmitting each predicted type of behavior and other metadata such as the percentage probability mapped locations user identifier and time stamp in a predicted behavior record to at least one behavior enabled document control system and the process ends.

With reference now to a high level logic flowchart depicts a process and program for a 3D object detector performing behavior detection by tracking objects within image streams and other sensed data and generating 3D object properties for the tracked objects representative of behaviors. As illustrated the process starts at block and thereafter proceeds to block . Block depicts an object detector system receiving multiple image streams via image capture devices and sensed data via one or more sensors. Next block illustrates the object detector system attaching metadata to the image frames and sensed data and the process passes to block . In one example metadata includes data such as but not limited to a camera identifier frame number timestamp and pixel count.

Block depicts the object detector system processing each image stream and sensed data to detect and track objects wherein objects may include physical objects and user movement indicative of a behavior. Next block illustrates generating streams of tracked object properties with metadata from each image stream. Thereafter block depicts combining the tracked object properties to generate 3D object properties with metadata. Next block illustrates transmitting the 3D tracked object properties to a behavior interpreter system and the process ends.

Referring now to a high level logic flowchart depicts a process and program for a behavior interpreter system performing behavior prediction from tracked 3D object properties. In the example the process starts at block and thereafter proceeds to block . Block depicts a determination whether the behavior interpreter system receives 3D object properties. When the behavior interpreter system receives 3D object properties then the process passes to block . Block depicts accessing a range of applicable behavior definitions which trigger control of a document and the process passes to block .

Block illustrates the behavior interpreter system comparing the 3D object properties for tracked objects with the applicable behavior definitions. Next block depicts the behavior interpreter system identifying at least one behavior definition with a closest match to one or more sets of 3D object properties. Thereafter block illustrates calculating a percentage probability that the 3D object properties match the identified behavior definitions. Next block depicts specifying at least one attribute of the identified behavioral signal type including but not limited to a user identifier an environment an application or a document. Next block depicts transmitting the predicted behavior record with predicted behavior type percentage probability behavioral signal type attribute time stamp and other metadata to one or more behavior enabled document control system and the process ends.

With reference now to a high level logic flowchart depicts a process and program for applying a predicted behavior record in a behavior enabled document control system. As illustrated the process starts at block and thereafter proceeds to block . Block depicts a determination whether a behavior enabled document control system receives a predicted behavior record. When the document control system receives a predicted behavior record then the process passes to block . Block illustrates accessing one or more entries within the behavior translation database applicable for the predicted behavior type associated application and other factors. Next block depicts translating the predicted behavior record into one or more document control signals for one or more of directing insertion of a document element into a document and triggering an application function to be performed on a document. Thereafter block illustrates controlling output of the document control signals to a selected application controller and the process ends.

While the invention has been particularly shown and described with reference to a preferred embodiment it will be understood by those skilled in the art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention.

