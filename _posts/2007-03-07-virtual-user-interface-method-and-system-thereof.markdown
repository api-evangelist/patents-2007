---

title: Virtual user interface method and system thereof
abstract: A virtual user interface (VUI) is provided. The VUI () can include a touchless sensing unit () for identifying and tracking at least one object in a touchless sensory field, a processor () communicatively coupled to the sensing unit for capturing a movement of the object within the touchless sensory field, and a driver () for converting the movement to a coordinate object (). In one aspect, the VUI can implement an applications program interface () for receiving the coordinate object and providing the coordinate object to the virtual user interface (VUI). An object movement within the sensory field of the VUI can activate user components in a User Interface ().
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08334841&OS=08334841&RS=08334841
owner: NaviSense
number: 08334841
owner_city: Plantation
owner_country: US
publication_date: 20070307
---
This application claims the priority benefit of U.S. Provisional Patent Application No. 60 781 179 entitled Sensory User Interface filed Mar. 13 2006.

This application also incorporates by reference the following Applications Ser. No. 11 683 410 entitled Method and Device for Three Dimensional Sensing Ser. No. 11 683 412 entitled Application Programming Interface API for Sensory Events Ser. No. 11 683 413 entitled Visual Toolkit for a Virtual User Interface and Ser. No. 11 683 416 entitled Touchless Tablet Method and Device Thereof all filed on the same day as this Application.

The present embodiments of the invention generally relates to the field of communications more particularly to user interfaces.

A graphical user interface GUI provides a visual interface whereby a user can interact with a program device or system through interaction with the GUI. A computer system generally includes a mouse and keyboard to navigate and control a cursor or data entry to the computer. Graphical components in the GUI can be activated by positioning the cursor over the component and physically performing a single or double click on the mouse to activate a component response. A user can also handle the mouse to move the cursor to other graphical components in accordance with the physical movement of the mouse. A user controlling the mouse is provided physical feedback through the physical actions of the handling and the positioning of the mouse. A touchpad stick or roller ball can also be used to control a cursor on the display. A touchscreen can also serve as an input device which allows a user to touch the screen for performing an input action or receiving a response. Menu buttons on a touchscreen are highlighted when depressed and the user can sense when the touchscreen is touched. Touchscreens can be used in applications where contact with the screen provides direct access to the GUI components. These various input devices allow a user to interact with a computer system to visually see and feel a response when interacting with the computer system. For example movement of a mouse corresponds with visual movement of a cursor object on a screen. Typing on a keyboard results in characters being displayed on a screen and interaction with a touchscreen can be seen as visual menu effects.

The mouse touchpad and stick generally require physical movement to assume control. A user can receive visual and physical feedback with the stick and touchpad. For example the user can see the cursor move in accordance with effort applied to the stick or touchpad. The user is also provided physical feedback when the user feels the slight resistance to sliding the finger along the touchpad. A user can also identify physical feedback when using a stick to control the cursor. A touchscreen or a tablet also provide physical and visual feedback. A user can physically touch the screen or use a stylus to activate a behavior. A keyboard can also serve as an input device which allows a user to enter in text to a computer program or application. The user can become accustomed to the level of resistance or the softness of keys when typing on a keyboard to become proficient. Also typing on a keyboard results in characters being displayed on a screen thereby providing visual feedback.

In one arrangement a motion sensing unit can be used to control a cursor on a screen. The motion sensing unit identifies and tracks finger movement similar to a mouse for controlling a cursor or for navigating a GUI similar to operation of a touchscreen. However a motion touchless sensing unit does not provide physical feedback. Consequently a user using a motion sensing unit to interface with a computer system does not receive any of the visual auditory or physical feedback commonly associated with standard input peripheral devices for controlling or assessing the recognition of their action. A need therefore exists for providing a touchless user interface when developing and using motion detection systems for input device pointing.

Embodiments of the invention are directed to a virtual user interface VUI . The VUI can include a touchless sensing unit for identifying and tracking at least one object in a touchless sensory field a processor communicatively coupled to the touchless sensing unit for capturing a movement of the object within the touchless sensory field and a driver for converting the movement to a coordinate object. In one aspect a communication device presenting the VUI can implement a sensory Applications Programming Interface API for receiving the coordinate object. The communication device can control one or more user components in a user interface UI or at least a portion of the user interface using the coordinate object through the sensory API. An object movement within the sensory field of the VUI can activate user components in the UI hosted by the communication device.

The VUI can include at least one virtual component wherein a movement in the sensory field at a location of the virtual component corresponds to an activation of a user component in the UI. A processor can determine at least one of a push action a release action a hold action and a sliding action of the object on a virtual component for controlling a behavior of a user component in the UI. The user component may be a graphical component or a non graphical component. The coordinate object can include one of an absolute location a relative difference a velocity and an acceleration of the object for identifying the behavior. The processor can also includes a timer for determining a length of time the object is at a position in a touchless sensory field. In one arrangement the sensing unit can be an array of motion detection sensors. The sensitivity of the sensory field can be adjusted by varying the strength of the touchless sensing unit.

Embodiments of the invention also concern a method for creating a coordinate object for use in a VUI. The method can include identifying and tracking at least one object in a touchless sensory field capturing a movement of the object within the touchless sensory field and converting the movement to a coordinate object. The coordinate object can be provided to the UI for processing a sensory event.

While the specification concludes with claims defining the features of the invention that are regarded as novel it is believed that the invention will be better understood from a consideration of the following description in conjunction with the drawing figures in which like reference numerals are carried forward.

As required detailed embodiments of the present invention are disclosed herein however it is to be understood that the disclosed embodiments are merely exemplary of the invention which can be embodied in various forms. Therefore specific structural and functional details disclosed herein are not to be interpreted as limiting but merely as a basis for the claims and as a representative basis for teaching one skilled in the art to variously employ the present invention in virtually any appropriately detailed structure. Further the terms and phrases used herein are not intended to be limiting but rather to provide an understandable description of the invention.

The terms a or an as used herein are defined as one or more than one. The term plurality as used herein is defined as two or more than two. The term another as used herein is defined as at least a second or more. The terms including and or having as used herein are defined as comprising i.e. open language . The term coupled as used herein is defined as connected although not necessarily directly and not necessarily mechanically. The terms program software application and the like as used herein are defined as a sequence of instructions designed for execution on a computer system. A program computer program or software application may include a subroutine a function a procedure an object method an object implementation an executable application an applet a servlet a source code an object code a shared library dynamic load library and or other sequence of instructions designed for execution on a computer system.

The term touchless sensing is defined as sensing movement without physically touching the object causing the movement. The term cursor can be defined as a cursor on a display and providing control to an underlying object. The cursor can be a handle to an object in the display or a physical object remote from the display but visually controlled using the cursor on the display. The term cursor object can be defined as an object that can receive coordinate information for positioning the object. The cursor can be a handle to the object wherein the object can be controlled via the cursor. In one example a cursor object can be the target of a game control for handling an object in the game. The term activating or activation is defined as enabling disabling or moderating a control. The term activation cue is defined as an action that imparts an operation on a control. The operation can be the initiating of the control the termination of the control the pausing of the control or the moderating of the control though is not limited to these. The activation cue can be a physical motion such as a finger movement hand gesture or a vocal motion such as a spoken utterance though is not limited to these. The term cue is defined as an act or behavior the act may be rehearsed or intentional but not limited to these.

The term User Interface UI can be defined as an interface providing access to one or more underlying controls of an application hosted by a computing device. As an example a UI can be a graphical user interface GUI that presents one or more graphical components that can perform a control function in response to an applied action. As another example a UI can be an Audible User Interface AUI that generates audio in response to a control of a user component. A user component can be an element in a UI that receives the action and performs a function in response to the action. A Virtual User Interface VUI can be defined as a touchless user interface having one or more virtual components that perform a function in response to an touchless sensory event applied in a sensing field. As an example a touchless sensory event can be a touchless pressing action of a finger on a virtual component in a touchless sensing field. A touchless sensory attribute can be broadly defined as a measurement of a touchless sensory event such as a magnitude of the touchless sensory event. A visual layout can be defined as a presentation of user components in a UI that are visible. A virtual layout can be defined as a presentation of virtual components in a VUI that are not directly visible.

Embodiments of the invention are directed to a system and method for a Virtual User Interface VUI . In one arrangement the VUI can include virtual components that correspond to user components in a User Interface. As an example a user interface may be a media console having user components for adjusting audio or visual controls. In another arrangement the VUI allows a user to control graphical components of a graphical user interface GUI . For example a GUI can contain a number of graphical components such as buttons sliders radio buttons and the like that can be manipulated to control an application a device or a computer system. Movement of an object such as a finger within a touchless sensory field of the VUI can acquire control of user components in the UI. A touchless sensing unit can produce the VUI such that a movement and location of an object within the virtual interface can be detected and identified. The movement and location can be used to activate behaviors of the user components within the UI. In one aspect a user can interact with a UI through a VUI in a manner similar to using a mouse but using touchless sensory events.

In a first embodiment a virtual user interface VUI is provided that maps virtual components in a touchless sensing field to user components in a user interface UI managed by a communication device and translates touchless finger actions applied to the virtual components to actions on the user components for controlling at least a portion of the UI.

In a second embodiment a method comprising a touchless sensing unit supplying a coordinate object to a communication device that receives the coordinate object for controlling at least a portion of a user interface UI managed by the communication device is provided. The coordinate object can be created by detecting a touchless finger action applied to at least one virtual component in a touchless sensory field an converting the finger action to a coordinate object for controlling at least a portion of the UI.

Referring to a VUI application is shown. The VUI application can include a touchless sensing unit a VUI a processor a driver a display and a UI . The touchless sensing unit can project a virtual layout of virtual components in the VUI that correspond to user components similarly arranged in the visual layout of the UI . The virtual layout of the VUI may be visible or non visible. For example if the sensing unit comprises an array of directional ultrasonic transducers then a non visible vertical planar sensory field can be generated. Also when the sensing unit comprises an array of omni directional ultrasonic transducers then a non visible region can be generated. In another arrangement holograms techniques can be employed to create a visible virtual layout or raster laser scanning to project a visible layout on a surface. In either mode virtual components can be arranged in the virtual layout of the VUI within the sensory field of the sensing unit . The sensory field is the region within which an object such as a finger can be detected and which corresponds to the virtual interface. In another arrangement the touchless sensing unit can be positioned on a side to generate the sensory field along the plane of a surface such as a desk. The sensing unit can project a visual representation of the VUI onto a form sheet.

The processor can identify a location of an object within the sensory field of the sensing unit and track movement of the object within the VUI . For example when the sensory field is an approximately vertical planar surface such as that shown in the processor can identify a location of an object such as a finger when the object enters the sensory field and obscures a continuity of a portion of the sensing field. The processor can convert the sensory information into coordinate information. For example the processor can determine an X Y and Z coordinate of an object within the sensory field. The driver can communicate the coordinate information as an object to the UI which can interpret the coordinate information and perform one or more control operations such as navigating a cursor along two dimensions in the display controlling a media console or navigating through a menu structure. The driver can generate a coordinate object that translates the coordinate information in a format provided by the processor to a format recognizable by a device exposing the UI . In various arrangements the driver can be a tablet driver a touchpad driver a touchscreen driver a stylus driver or a mouse driver

The UI can receive finger actions from the coordinate object which can identify locations of the user components in the UI corresponding to a location of the virtual components in the virtual layout of the VUI . When an object in the sensory field of the VUI moves over a virtual component the UI can determine which graphical component in the UI corresponds to the location of the object in the virtual layout . The UI can be presented on the display of a computer mobile device or any other system. The touchless sensing unit can receive power from a battery a USB line or an independent power supply. A user can interact with the UI by moving a finger to virtual components within the VUI . A user can perform touchless push actions on the virtual components of the VUI to activate a response in the UI . Notably the VUI is distinct and separate from the GUI in that it is touchless. A user can interact with the UI by moving a finger within the sensory field of the VUI .

In one embodiment but not herein limited the touchless sensing system can comprise an array of ultrasonic transducers that convert finger motion into coordinate locations. Alternatively a microphone array system a beamforming array a three dimensional imaging system a camera system a laser system an optical sensing unit a micro electromechanical MEMS system a laser system or an infrared unit can be employed for acquiring object movement such as a finger location for converting finger movement into a coordinate signal. In one application a user can move their finger within the VUI to navigate and control the cursor on the display . The coordinate information can also be used to navigate and control user components within the UI . For example a user can position a finger over component in the VUI and perform a touchless command. For example the user may push forward denoting a push action if the component is a virtual button. A user can move a finger within a sensing field of the VUI generated by touchless sensing unit and the processor and driver can provide control information to the UI in accordance with the finger movement. In one aspect the processor and the driver can be internal to the sensing device . In one configuration the sensing unit can provide coordinate information directly to the GUI through a USB port a Bluetooth Wireless Fidelity WiFi or a ZigBee connection.

Because the VUI is not generally visible it may be difficult to coordinate hand and eye movement to interact with visual components in the UI when moving the finger in the virtual layout of the VUI . Accordingly the UI identifies which buttons the finger is nearest in order to provide visual feedback to the user. For example the processor identifies a coordinate location of the finger in the VUI which is provided to the UI via the driver . The UI can translate the coordinate location to identify which user component is nearest in relative proximity to the virtual component closest to the finger in the VUI . The UI can adjust the color of the graphical component flash the graphical component or provide another visual or auditory indication to the user to indicate which graphical component is closest in association with the finger position. In particular the processor tracks finger movement and relays the coordinate information to the UI . The UI interprets the location and continually updates visual and or auditory aspects of the user components for providing the user feedback as to where the finger is in the VUI relative to a user component in the UI . Notably the UI can have more than the number of components shown with more function than an exemplary push button. The VUI can support most all graphical elements currently available to a mouse keyboard touchpad tablet or stylus input device.

The processor identifies and tracks relative motion such as a finger displacement in the VUI identifying differences in movement. Upon detecting a generally stationary motion the processor and driver signal a sensory event to the UI . For example a stationary movement can correspond to a user identifying a component with which to interact. The UI can provide increased visual or auditory feedback for acknowledging a recognition of the sensory event. Accordingly a user may push forward such as a button push or signal the finger up and down in the VUI to signify that the user is intending on depressing a virtual component. As another example the user may apply an up down left right clockwise or counterclockwise motion to a VUI component. Visual feedback can be provided by the UI for acknowledging the finger action. For example the user may position a finger above component for a short duration of time and then push forward to indirectly activate a depressing action on the graphical component .

The processor can discriminate between navigation commands and sensory events. For example a navigation command can be the moving of the finger in a generally forward vertical or horizontal direction. A sensory event can be a component action such as a button press a slide a release a hold or a combinational event such as a push and hold a push and slide a push slide and release. A finger event may be a pattern such as an up down left right or clockwise counter clockwise motion. As an example the processor detects the horizontal and vertical movement and relays the movement to the UI. The driver can provide pattern information absolute coordinates or relative coordinates to the UI. In the latter the processor identifies differential movement between current and previous movements and provides a relative motion to the driver. The processor can include a timer which determines a length of time at a position. It can include a storage for saving a history of previous events. For example the finger may be at Cartesian coordinate 1 20 and then move to coordinate 2 5 . The driver can send a differential coordinate 1 15 instead of the actual updated coordinate. The UI can determine if the differential movement corresponds to the location of another graphical component. For example the user s finger may be over Component and then slowly move to Component . The UI can determine using relative and differential coordinates that the finger has moved from graphical component to component in the GUI.

In one embodiment the touchless sensing unit can comprise ultrasonic sensors that emit and receive acoustic pulses. Accordingly the sensing field associated with the VUI corresponds to a region within which a reflected energy pulse can be detected. The sensing field can be a function of the emitted pulse strength and the range e.g. distance . For example an ultrasonic sensing unit may only need a range corresponding to the maximum extent of the hand or finger movement which may be under 12 inches. The ultrasonic sensing unit can be positioned above a communication device along a display or as a standalone unit. The user can move the finger within the sensing field for interacting with the VUI . The ultrasonic sensing unit can include at least one transmitter and receiver for transmitting and receiving ultrasonic signals. The transmitter and emitter can be the same for providing dual transmit and receive functions. In another arrangement the sensing element can be an array of micro acoustic microphones or micro speakers for transmitting and receiving audio signals. Principles of pulse echo detection can be employed to estimate a range and position of a finger within view of the sensing elements. For example a transmitter in the sensing unit emits a pulse shaped signal that reflects off the finger which is detected by a receiver element in the sensing unit. The receiver element is coupled with a detector that detects a reflected signal as part of the motion detection logic in the sensing unit. A time of flight can be determined and a position can be estimated from the time of flight. The detector can include additional processing logic such as thresholds comparators logic gates clocks and the like for detecting an object s motion. The sensing unit calculates a position of the object causing the reflection by solving a set of geometric equations.

In one exemplary embodiment a transmit and receive element pair in the touchless sensing unit calculates a first range e.g. distance of an object in the sensing field. A first transmit and receive pair on an x axis estimates a longitudinal range of the object e.g. finger . A second pair arranged separately from the first pair estimate a second range. The second pair estimates a latitudinal range of the object e.g. finger . Accordingly the two range measurements establish a position e.g. location of the object causing the signal reflection by mathematically combining the geometrically related range measurements. For example the first range measurement establishes a x coordinate and the second range measurement establishes a y coordinate. The location of the object is then determined to correspond to the point x y in a single plane. For example the plane will be oriented in the direction of the first and second paired ultrasonic elements. Accordingly a third pair can produce a range measurement in a third direction thereby establishing a three dimensional coordinate system x y z if the first second and third range measurement projections are orthogonal to one another.

Notably the sensing unit can contain multiple sensing elements positioned and arranged in various configurations for receiving range measurements in varying directions for calculating the position of the object causing the reflection using multi path signal processing techniques. The paired transmit and receive elements can be on a same principal axis or a different principal axis. The sensing unit can also employ beamforming techniques for estimating the objects location. The system can include a computer for receiving the coordinate signal from the sensing unit or detector for moving a cursor object in accordance with a detected movement. The sensing unit additionally produces differential coordinate signals for satisfying the input signal requirements of USB mouse input device or BlueTooth connection interface. Notably a computer mouse generally uses a USB or PS 2 device driver for receiving differential signals for moving a cursor along each principal axis of the computer coordinate system. The sensing unit can produce differential signal for each principal axis to comply with the requirements of the PS 2 and USB mouse device driver interface.

In one aspect the sensing unit determines a location of the finger using time of flight measurement for identifying navigational commands and sensory event commands. A navigational command is a movement along a horizontal and vertical plane in two dimensions. A sensory event command is an action along a third dimension orthogonal to the horizontal and vertical plane. For example the touchless sensing unit can produce a three dimensional field. Movement along two dimensions X Y corresponds to navigation and movement along the third dimension Z corresponds to a finger action such as a button push or slide. A detector identifies whether the location of the finger is associated with a virtual component for activating a control. When the finger is active within the sensing field of the VUI and navigating and controlling component within the UI the processor identifies sensory events the user initiates. The processor keeps track of navigational commands and sensory events when the finger is within the sensing field. The processor can extend the sensing field to a greater range by adjusting a sensitivity based on a position of the finger and a time length of the finger at a position. Adjusting the sensitivity changes the sensing field. In one example the coordinator increases the transmit pulse intensity to broaden the sensing field. Accordingly the coordinator decreases the transmit pulse intensity to lessen the sensing field. The coordinator can also change the sensitivity in software to expand or shrinking the sensing field. For example the detector adjusts a sensitivity by linearly increasing the time of flight value corresponding to the locus of points along the boundary for extending the boundaries. Notably the detector decreases the boundaries by decreasing the time of flight values associated with the boundary.

Referring to an exemplary VUI application is shown. In this example the VUI application serves as a media console. A user can adjust one or more media settings such as a volume level via touchless finger actions on virtual components in the VUI . Notably the processor can further includes a detector a timer and a coordinator . Briefly the VUI translates a coordinate space of the touchless sensory field to a coordinate space of the GUI . The touchless sensing unit can also include a wireless communication unit for conveying signals to another processor hosting the UI if the processor is external such as one in a computer or mobile device. In one aspect the timer can determine a time window for which an activation cue such as the positioning of a finger in the VUI is evaluated. For example when a user intentionally places a finger directly on a virtual component for second the sensing unit and timer together identify the position and timing of the finger as a signal to commence a control handled by the coordinator such as the enabling of a navigational control to move a cursor on the display . The detector can determine when a movement is detected and identify the location of an object. The timer determines a length of time an object such as a finger is at a particular location and the coordinator activates a virtual component control upon detection of an activation cue. An activation cue can be an intentional gesture or motioning of a finger to generate a sensory event within the sensory field. The communication unit can be a wireless communication unit such as a Bluetooth of ZigBee compliant unit.

A user can adjust sliders of the virtual media console within the VUI for changing sliders within the UI. The UI may be a graphical user interface GUI to control audio channels for mixing audio sounds or the sliders may be for equalization control or any other media control. The VUI can also include rotation knobs similar to volume knobs for changing a volume or level of corresponding controls in the GUI. A user can move a finger in a clockwise or counterclockwise direction to generate sensory events and gain control of the virtual components. Again as example the UI controls may connect to an audio mixer which can adjust audio parameters such as bass treble pan balance volume and the like in accordance with the touchless finger movements. is an exemplary illustration for an VUI application and various other applications are herein contemplated.

The coordinator can pass the coordinate object between the VUI and the UI for maintaining synchronization of actions detected in the VUI and events activated in the UI . In one arrangement the UI can implement a sensory API which provides method and function call control to the UI for handling the sensory events directly. The sensory API can expose methods and variables for accessing the coordinate object and for providing communication to and from the VUI and the UI . The sensory API provides portability across different platforms such as a computer mobile device or headset. The VUI application can be a C program a Java program a .NET program an HTML program and an XML program and the coordinate object can be a web component such as an HTML object an XML object a Java Object a C class structure a .NET object or a Java Servlet. In such regard a VUI application can interface to a website to interact with one or more components in the website via touchless controls.

Another embodiment is a method for creating a coordinate object for use with a VUI application. The method can include identifying and tracking at least one object in a touchless sensory field capturing a movement of the object within the touchless sensory field and converting the movement to a coordinate object . The coordinate object can be provided to the UI for processing the sensory event. An absolute location a relative difference a velocity and an acceleration of the object can be provided as parametric information to the UI from the sensing unit . A sensory API can expose the access to the underlying resources provided by the touchless sensing unit . The API can encapsulate the underlying methods functions and variables for interfacing the UI with the VUI . In one aspect the parametric information can be translated from the sensory space of the finger to the coordinate space of a VUI application. In one arrangement the VUI applications implements the sensory API to interpret sensory behaviors to control the VUI application. The parametric information can be used to determine at least one of a push action a release action a hold action and a sliding action of the object. An action can be performed on a virtual component in the VUI application and passed in the coordinate object to the UI . For example a finger movement or finger action can be detected as a sensory event in the VUI to navigate a menu within the UI or for controlling graphical components within a GUI application. The coordinate object can include a location and action of the finger detected in the sensing field of the VUI by the touchess sensing unit .

Processing a finger action generally entails handling at least one touchless sensory event. The handling of the touchless sensory event can include receiving a coordinate object containing at least one touchless navigation command and processing the touchless navigation command to produce an action on a component within the UI . For example the processing can include associating a touchless navigation command with a graphical component in a GUI. The VUI can perform similarly to a Graphical User Interface GUI though the activation behaviors are different. Whereas a GUI requires physical manipulation of an input device such as a mouse keyboard touchpad or touchscreen the VUI involves touchless sensing such as moving a finger to particular locations and performing particular actions in the virtual interface. The navigation commands can also include instructions for arranging at least one graphical component in a visual layout of the VUI application. For example a user can drag and drop components in the VUI from within the sensing field which correlates to dragging and dropping the corresponding graphical components in the UI . Accordingly graphical components can be arranged in accordance with the navigation commands detected. In practice a position of the finger can be associated with at least one graphical component in the VUI application. An action on a graphical component in the UI can be performed in response to at least one of a touchless finger depressing action a touchless finger release action a touchless finger hold action or a touchless finger sliding action in the VUI . The action can generate a response from the GUI application wherein the response can at least one of a visual change of the GUI application or an audible response to the action. In one aspect the VUI can include a protocol adapter for converting one programming format to another programming format. Various management layer programs can communicate with the protocol adapter which in turn implements the API to communicate with the native functions and methods of the sensing unit .

The coordinate object can identify at least one among an absolute location a relative difference a velocity a length of time and an acceleration of a finger producing the touchless finger movements for controlling at least a portion of the GUI. In one aspect the coordinate object can identify at least one among a positioning action a push action a release action a hold action and a sliding action for controlling at least a portion of the UI. In another aspect the coordinate object can identify at least one among an up movement down movement left movement right movement clockwise movement and counterclockwise movement of a finger producing the touchless finger movements for controlling at least a portion of the GUI.

During touchless interfacing the controlling element can generate at least one among a visual indicator or an audio indicator of a user component in the UI in response to a touchless finger action applied to a virtual component in the VUI . For example the communication device can change a color size orientation look and feel of a user component in the UI in response to a finger action applied to a virtual component in the VUI . The controlling element can correlate a position the finger in the VUI with at least one graphical component in a graphical user interface GUI . The UI can also exposes at least one property option for adjusting a sensitivity of the VUI. For example the communication device may present options for adjusting a size of the virtual components in the VUI or a sensitivity to touchless finger actions.

It should also be noted that the size of the VUI and the GUI may differ. In particular the VUI can be a magnified representation of the GUI . That is the sensing field may encompass a larger virtual area than the corresponding size of the display on the communication device . This allows a user through the VUI to interact with more components on the UI than may be available through a keypad touchpad or touchscreen of the communication device . Notably with physical touch based interfaces a user is generally limited to the physical buttons on the communication device which cannot support more components due to the size of the components and a user s ability to handle the small components. The VUI can effectively increase the size of the UI to generate a larger VUI thereby expanding the availability of interface components to the communication device .

Embodiments of the invention also concern a method for presenting a property option of a virtual pointer for use in a touchless user interface. The method can include identifying a touchless sensitivity within which an object acquires control a virtual pointer and exposing at least one property option for adjusting a behavior of the virtual pointer in the touchless sensitivity field. The sensitivity field can be represented in three dimensions. Property options can include selecting a virtual pointer speed for tracking the object such as a finger enhancing a virtual pointer precision for adjusting a resolution of tracking automatically moving a virtual pointer to a virtual component when the object is within a predetermined distance of the virtual component displaying virtual pointer trails as the object moves within the touchless sensory field showing the location of a virtual pointer when a virtual component is activated presenting a dialog box for displaying at least one virtual pointer property option.

The present invention may be realized in hardware software or a combination of hardware and software. The present invention may be realized in a centralized fashion in one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is VUIted. A typical combination of hardware and software may be a general purpose computer system with a computer program that when being loaded and executed controls the computer system such that it carries out the methods described herein.

The present invention also may be embedded in a computer program product which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

This invention may be embodied in other forms without departing from the spirit or essential attributes thereof. Accordingly reference should be made to the following claims rather than to the foregoing specification as indicating the scope of the invention.

