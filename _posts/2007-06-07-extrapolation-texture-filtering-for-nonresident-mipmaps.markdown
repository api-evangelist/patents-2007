---

title: Extrapolation texture filtering for nonresident mipmaps
abstract: A multi-threaded graphics processor is configured to use to extrapolate low resolution mipmaps stored in physical memory to produce extrapolated texture values while high resolution mipmaps are retrieved from a high latency storage resource. The extrapolated texture values provide an improved image that appears sharper compared with using the low resolution mipmap level texture data in place of the temporarily unavailable high resolution mipmap level texture data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07944453&OS=07944453&RS=07944453
owner: NVIDIA Corporation
number: 07944453
owner_city: Santa Clara
owner_country: US
publication_date: 20070607
---
The present invention generally relates to texture mapping and more specifically to using extrapolation to compute texture map values for mipmaps that are not available.

As the use of virtual memo Type your question here and then click Search ry has become more commonplace the number of texture maps that can be accessed during graphics processing is no longer limited by the amount physical memory local or system where the texture maps are conventionally stored. Texture data can be stored on other storage resources such as disk drives CD drives or even remote servers that have higher access latency than the physical memory. The texture data is retrieved as it is needed during processing. However unlike retrieving texture data from the physical memory the image quality is compromised during the time that the texture data is retrieved from the other storage resources.

It is particularly advantageous to store high resolution mipmaps of a texture on the other storage resources since those mipmaps are larger. Lower resolution mipmaps of the texture can be stored in the physical memory and used to produce images while the high resolution mipmaps are retrieved from the other storage resources. The result is that the texture map data appears blurry and then sharpens when the high resolution mipmaps become available in the physical memory.

Accordingly what is needed in the art are systems and methods for improving the appearance of low resolution texture map data that is used while high resolution mipmaps are retrieved from a high latency storage resource.

A multi threaded graphics processor is configured to extrapolate low resolution mipmaps stored in physical memory to produce extrapolated texture values while high resolution mipmaps are retrieved from a high latency storage resource. The extrapolated texture values provide an improved image that appears sharper compared with using the low resolution mipmap level texture data in place of the high resolution mipmap level texture data. Filtered texture values are produced using a mipmap filter that extrapolates two resident levels of detail to approximate the increased contrast and detail that would be produced from filtering if the nonresident level of detail mipmap was resident.

A deltaLOD level of detail is computed as the difference between the LOD of the ideal mipmap and the extrapolation threshold LOD a value greater than or equal to the LOD of the highest resolution resident mipmap . A resident mipmap is stored in low access latency physical memory in contrast with a nonresident mipmap that is stored in a high access latency storage resource. The deltaLOD is used to determine an extrapolation weight value that is used to produce the extrapolated texture values for use in place of the high resolution mipmap texture data.

Various embodiments of a method of the invention for extrapolating between resident mipmaps to produce texel values for a nonresident mipmap include computing an ideal level of detail LOD corresponding to an ideal mipmap computing a deltaLOD as a difference extrapolation threshold LOD a value greater than or equal to the LOD of the highest resolution resident mipmap and the ideal LOD determining an extrapolation weight based on the deltaLOD and determining coarse and fine lodweights based on the extrapolation weight which produce an extrapolated texture value. The extrapolation filtered result approximates the result that would be achieved with ideal mipmap and is computed using the weight texel values from the resident mipmap whose level of detail is equal to the truncated extrapolation threshold LOD and texel values from a lower resolution resident mipmap.

Various embodiments of the invention for extrapolating between resident mipmaps to produce texel values for a nonresident mipmap include a memory configured to store the resident mipmaps and a texture unit. The texture unit is coupled to the memory and configured to compute an ideal level of detail LOD corresponding to an ideal mipmap containing both those miplevels which are not resident in physical memory and those miplevels which are resident in physical memory compute a deltaLOD as a difference between extrapolation threshold LOD a value greater than or equal to the LOD of the highest resolution resident mipmap and the ideal LOD determine an extrapolation weight based on the deltaLOD and compute an extrapolated texel value. The extrapolated texel value approximates a texel value of the ideal mipmap and is computed using the weight texel values from the mipmap whose level of detail is equal to the truncated extrapolation threshold LOD and texel values from a lower resolution resident mipmap.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

When a software application requests texture filtering at a level of detail where all of the required mipmaps are resident in memory the texture unit fetches texels from the required mipmaps and applies a filter which produces a value interpolated between the texels fetched from the resident mipmap levels as is common in the state of the art. When a software application attempts to access a mipmap level that is paged out of physical memory i.e. a nonresident mipmap embodiments of the present invention fetch texels from the two nearest resident mipmaps and apply a filter which produces a value extrapolated from the texels fetched from the resident mipmap levels. Extrapolated texture values are computed and used to produce images until the nonresident mipmap is paged into physical memory becoming a resident mipmap.

For example the computed LOD for texture mapping is LOD and LOD mipmap is nonresident extrapolated texture values are computed as described in conjunction with using resident mipmaps LOD mipmap and LOD mipmap . The extrapolated texture values for a non resident LOD such as LOD are computed using an extrapolation minification filter. A minification filter is used when the ratio of texels to pixels is less than one. Once LOD is converted to a resident LOD mipmap the filter weights may be adjusted to phase in the LOD texels over a number of frames to produce a smooth visual transition rather than switching from an extrapolation filter to a conventional interpolation filter in a single frame. When the computed LOD is less than LOD i.e. higher resolution than LOD the extrapolated texture values are computed using an extrapolation magnification filter. A magnification filter is used when the ratio of texels to pixels is greater than one.

In conventional systems a technique known to those skilled in the art unsharp masking or sharpen texture is used to enhance the sharpness of texture lookups when the computed LOD is less than zero i.e. the desired texture resolution is higher than LOD by extrapolating between LOD and LOD to subtract out the contribution of the low frequency components from LOD. The present invention also uses extrapolation magnification extrapolation to produce texel values for computed LODs that are less than zero but uses new extrapolation filter types e.g. extrapolated mipmapped linear and extrapolated mipmapped nearest neighbor. Additionally minification extrapolation is performed to compute texture values for any nonresident textures not just for LOD values below LOD.

Conventionally the fine mipmap corresponds to the integer portion of the ideal LOD mipmap and the coarse mipmap corresponds to the integer portion 1 LOD mipmap. In step the method computes bilinearly filtered texel values for the fine and coarse mipmaps and then interpolates between the bilinearly filtered texel values using the fractional portion of the ideal LOD to produce a filtered texel value when the filter type is extrapolated mipmapped linear. When the filter type is extrapolated mipmapped nearest neighbor a nearest texel value is selected from the fine and coarse mipmaps to produce two point sampled texel values. The two point sampled texel values are then bilinearly interpolated using the fractional portion of the ideal LOD to produce the filtered texel value. Steps and are performed using conventional texture map filtering techniques.

If in step the method determines that the ideal LOD mipmap is a nonresident mipmap then in step the method notifies a device driver that one or more mipmaps for the texture should be paged into physical memory to convert those mipmaps from nonresident mipmaps to resident mipmaps for use in producing an image. In step the method may determine that the ideal LOD mipmap is nonresident when the ideal LOD is less than zero and the LOD mipmap is not resident indicating that the ratio of texels to pixels is greater than one and the extrapolated magnification filter should be used. The extrapolated minification filter should be used when the ideal LOD is greater than zero and the highest resident mipmap subtracted from the ideal LOD is less than zero indicating that the ratio of texels to pixels is less than one. When the ideal LOD equals LOD the extrapolated magnification may be used.

In step the method computes a deltaLOD as the difference between the ideal LOD and the extrapolation threshold LOD a value greater than or equal to the LOD of the highest resolution resident mipmap i.e. deltaLOD ideal LOD extrapolation threshold LOD. For example referring to when the ideal LOD is LOD corresponding to LOD mipmap the deltaLOD is 2 since extrapolation threshold LOD is LOD corresponding to LOD mipmap . Note that deltaLOD may also have a fractional component since the ideal LOD and the extrapolation threshold LOD can have a fractional component.

In step the method uses the deltaLOD to determine an extrapolation weight. A table stores extrapolation weight values corresponding to deltaLOD values. In some embodiments of the present invention the table may be programmed to specify the function used to determine the extrapolation weight values. Some embodiments of the present invention allow up to 64 LOD weight pairs to be loaded into the table in decreasing LOD order. By default this table contains six pairs 0 0 1 0.25 2 0.5 4 1.125 8 2.0 16 3.0 

Given a deltaLOD that is less than 16 the last entry in the table 0 then the extrapolation weight will be the weight of the last entry of the table 3.0 i.e. 3. If deltaLOD is less than zero but greater than the first entry in the table specified by the application the extrapolation weight will be the weight of the first entry in the table. Given a deltaLOD value of 5 that falls between two LOD values in the table a low value of LOD 4 weight 1.125 and a high value of LOD 8 weight 2.0 the extrapolation weight is linearly interpolated weight LOD deltaLOD LOD LOD weight deltaLOD LOD LOD LOD . eq. 1 The extrapolation weight is used in step to produce a filtered texel using texels read from the coarse LOD mipmap and the fine LOD mipmap.

In step the method reads four texels from the coarse LOD mipmap and four texels from the fine LOD mipmap when the specified filter type is extrapolated mipmapped linear. When the specified filter type is extrapolated mipmapped nearest neighbor the method reads a single texel from the fine LOD mipmap and a single texel from the coarse LOD mipmap. The fine LOD mipmap is the mipmap whose level of detail is equal to the truncated extrapolation threshold LOD the integer portion of the extrapolation threshold LOD and the coarse LOD mipmap is a lower resolution resident mipmap that has an LOD equal to the fine LOD plus one.

In step when the filter type is extrapolated mipmapped linear the method bilinearly interpolates texels read from the coarse and fine LOD mipmaps using the fractional portions of the texture map coordinates to produce texel values Tand T. When the filter type is extrapolated mipmapped nearest neighbor the method provides the texel read from the fine LOD mipmap and Tand the texel read from the coarse LOD mipmap as T. In step the method then computes the extrapolated texel value using T T and the extrapolation weight W using the following equation T 1.0 W T W. eq. 2 The extrapolated texel value can be combined with additional extrapolated texel values to produce filtered texel values for anisotropic texture mapping or other filtered texture functions. The extrapolated texel value is then used to produce a rendered image that is stored and or displayed.

If in step the method determines if the ideal LOD is not less than zero then in step the method determines if the ideal LOD is greater than or equal to the extrapolation threshold LOD. If in step the method determines that the ideal LOD mipmap is greater than or equal to the extrapolation threshold LOD then in step the filter type specified for the minification texture filter is used to compute the filtered texel value using interpolation in step . Otherwise in step the filter type specified for the extrapolated minification texture filter is used to compute the filtered texel value using extrapolation in step .

System memory also includes a device driver that is configured to provide an instruction stream that specifies the location of data such as mipmaps and program instructions to parallel processing subsystem . The program instructions and data are produced by a software application and may be stored in system memory or memory within other devices of system . Device driver is executed by CPU to translate instructions for execution by parallel processing subsystem based on the specific capabilities of parallel processing subsystem . The instructions may be specified by an application programming interface API which may be a conventional graphics API such as Direct3D or OpenGL.

Memory bridge which may be e.g. a Northbridge chip is connected via a bus or other communication path e.g. a HyperTransport link to an I O input output bridge . I O bridge which may be e.g. a Southbridge chip receives user input from one or more user input devices e.g. keyboard mouse and forwards the input to CPU via path and memory bridge . Parallel processing subsystem is coupled to memory bridge via a bus or other communication path e.g. a PCI Express Accelerated Graphics Port or HyperTransport link in one embodiment parallel processing subsystem is a graphics subsystem that delivers pixels to a display device e.g. a conventional CRT or LCD based monitor . A system disk is also connected to I O bridge . Some mipmaps particularly high resolution levels that require more storage space are stored in high latency storage such as disk or one a remote server CD drive DVD drive or the like. These mipmaps such as nonresident mipmaps are loaded into a lower latency memory storage as needed to become resident mipmaps that can be accessed by parallel processing subsystem during interactive rendering.

A switch provides connections between I O bridge and other components such as a network adapter and various add in cards and . Other components not explicitly shown including USB or other port connections CD drives DVD drives film recording devices and the like may also be connected to I O bridge . Communication paths interconnecting the various components in may be implemented using any suitable protocols such as PCI Peripheral Component Interconnect PCI Express PCI E AGP Accelerated Graphics Port HyperTransport or any other bus or point to point communication protocol s and connections between different devices may use different protocols as is known in the art.

An embodiment of parallel processing subsystem is shown in . Parallel processing subsystem includes one or more parallel processing units PPUs each of which is coupled to a local parallel processing PP memory . In general a parallel processing subsystem includes a number U of PPUs where U 1. Herein multiple instances of like objects are denoted with reference numbers identifying the object and parenthetical numbers identifying the instance where needed. PPUs and PP memories may be implemented e.g. using one or more integrated circuit devices such as programmable processors application specific integrated circuits ASICs and memory devices.

As shown in detail for PPU each PPU includes a host interface that communicates with the rest of system via communication path which connects to memory bridge or in one alternative embodiment directly to CPU . In one embodiment communication path is a PCI E link in which dedicated lanes are allocated to each PPU as is known in the art. Other communication paths may also be used. Host interface generates packets or other signals for transmission on communication path and also receives all incoming packets or other signals from communication path and directs them to appropriate components of PPU . For example commands related to processing tasks may be directed to a front end unit while commands related to memory operations e.g. reading from or writing to PP memory may be directed to a memory interface . Host interface front end unit and memory interface may be of generally conventional design and a detailed description is omitted as not being critical to the present invention.

Each PPU advantageously implements a highly parallel processor. As shown in detail for PPU a PPU includes a number C of cores where C 1. Each processing core is capable of executing a large number e.g. tens or hundreds of threads concurrently where each thread is an instance of a program one embodiment of a multithreaded processing core is described below. Cores receive processing tasks to be executed via a work distribution unit which receives commands defining processing tasks from a front end unit . Work distribution unit can implement a variety of algorithms for distributing work. For instance in one embodiment work distribution unit receives a ready signal from each core indicating whether that core has sufficient resources to accept a new processing task. When a new processing task arrives work distribution unit assigns the task to a core that is asserting the ready signal if no core is asserting the ready signal work distribution unit holds the new processing task until a ready signal is asserted by a core . Those skilled in the art will recognize that other algorithms may also be used and that the particular manner in which work distribution unit distributes incoming processing tasks is not critical to the present invention.

Cores communicate with memory interface to read from or write to various external memory devices. In one embodiment memory interface includes an interface adapted to communicate with local PP memory as well as a connection to host interface thereby enabling the cores to communicate with system memory or other memory that is not local to PPU including system disk . Memory interface can be of generally conventional design and a detailed description is omitted.

Cores can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying laws of physics to determine position velocity and other attributes of objects image rendering operations e.g. vertex shader geometry shader and or pixel shader programs and so on. PPUs may transfer data such as resident mipmap from system memory and or local PP memories into internal on chip memory process the data and write result data back to system memory and or local PP memories where such data can be accessed by other system components including e.g. CPU or another parallel processing subsystem .

Referring again to in some embodiments some or all of PPUs in parallel processing subsystem are graphics processors with rendering pipelines that can be configured to perform various tasks related to generating pixel data from graphics data supplied by CPU and or system memory via memory bridge and bus interacting with local PP memory which can be used as graphics memory including e.g. a conventional frame buffer and mipmaps to store and update pixel data delivering pixel data to display device and the like. In some embodiments PP subsystem may include one or more PPUs that operate as graphics processors and one or more other PPUs that are used for general purpose computations. The PPUs may be identical or different and each PPU may have its own dedicated PP memory device s or no dedicated PP memory device s .

In operation CPU is the master processor of system controlling and coordinating operations of other system components. In particular CPU issues commands that control the operation of PPUs . In some embodiments CPU writes a stream of commands for each PPU to a pushbuffer not explicitly shown in which may be located in system memory PP memory or another storage location accessible to both CPU and PPU . PPU reads the command stream from the pushbuffer and executes commands asynchronously with operation of CPU .

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip. The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

The connection of PPU to the rest of system may also be varied. In some embodiments PP system is implemented as an add in card that can be inserted into an expansion slot of system . In other embodiments a PPU can be integrated on a single chip with a bus bridge such as memory bridge or I O bridge . In still other embodiments some or all elements of PPU may be integrated on a single chip with CPU .

A PPU may be provided with any amount of local PP memory including no local memory and may use local memory and system memory in any combination. For instance a PPU can be a graphics processor in a unified memory architecture UMA embodiment in such embodiments little or no dedicated graphics PP memory is provided and PPU would use system memory exclusively or almost exclusively to store resident mipmaps . In UMA embodiments a PPU may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high speed link e.g. PCI E connecting the PPU to system memory e.g. via a bridge chip.

As noted above any number of PPUs can be included in a parallel processing subsystem. For instance multiple PPUs can be provided on a single add in card or multiple add in cards can be connected to communication path or one or more of the PPUs could be integrated into a bridge chip. The PPUs in a multi PPU system may be identical to or different from each other for instance different PPUs might have different numbers of cores different amounts of local PP memory and so on. Where multiple PPUs are present they may be operated in parallel to process data at higher throughput than is possible with a single PPU.

Systems incorporating one or more PPUs may be implemented in a variety of configurations and form factors including desktop laptop or handheld personal computers servers workstations game consoles embedded systems and so on.

In one embodiment each core includes an array of P e.g. 8 16 etc. parallel processing engines configured to receive SIMD instructions from a single instruction unit . Each processing engine advantageously includes an identical set of functional units e.g. arithmetic logic units etc. . The functional units may be pipelined allowing a new instruction to be issued before a previous instruction has finished as is known in the art. Any combination of functional units may be provided. In one embodiment the functional units support a variety of operations including integer and floating point arithmetic e.g. addition and multiplication comparison operations Boolean operations AND OR XOR bit shifting and computation of various algebraic functions e.g. planar interpolation trigonometric exponential and logarithmic functions etc. and the same functional unit hardware can be leveraged to perform different operations.

Each processing engine uses space in a local register file LRF for storing its local input data intermediate results and the like. In one embodiment local register file is physically or logically divided into P lanes each having some number of entries where each entry might store e.g. a 32 bit word . One lane is assigned to each processing engine and corresponding entries in different lanes can be populated with data for different threads executing the same program to facilitate SIMD execution. In some embodiments each processing engine can only access LRF entries in the lane assigned to it. The total number of entries in local register file is advantageously large enough to support multiple concurrent threads per processing engine .

Each processing engine also has access to an on chip shared memory that is shared among all of the processing engines in core . Shared memory may be as large as desired and in some embodiments any processing engine can read to or write from any location in shared memory with equally low latency e.g. comparable to accessing local register file . In some embodiments shared memory is implemented as a shared register file in other embodiments shared memory can be implemented using shared cache memory.

In addition to shared memory some embodiments also provide additional on chip parameter memory and or cache s which may be implemented e.g. as a conventional RAM or cache. Parameter memory cache can be used e.g. to hold state parameters and or other data e.g. various constants that may be needed by multiple threads. Processing engines also have access via memory interface to off chip global memory which can include e.g. PP memory and or system memory with system memory being accessible by memory interface via host interface as described above.

It is to be understood that any memory external to PPU may be used as global memory . As shown in global memory includes PP memory system memory and system disk . As previously described Texture data stored in global memory such as resident mipmaps and are considered resident texture data and other texture data stored in global memory such as nonresident mipmaps are considered nonresident texture data. As nonresident texture data is copied from system disk to system memory or PP memory the texture data becomes resident texture data. A driver program executing on CPU of can be used to specify which mipmaps are resident mipmaps and which mipmaps are nonresident. In other embodiments of the present invention whether a mipmap is resident or nonresident is determined based on at least a portion of the texel address. Processing engines can be coupled to memory interface via an interconnect not explicitly shown that allows any processing engine to access global memory .

In one embodiment each processing engine is multithreaded and can execute up to some number G e.g. 24 of threads concurrently e.g. by maintaining current state information associated with each thread in a different portion of its assigned lane in local register file . Processing engines are advantageously designed to switch rapidly from one thread to another so that instructions from different threads can be issued in any sequence without loss of efficiency.

Instruction unit is configured such that for any given processing cycle the same instruction INSTR is issued to all P processing engines . Thus at the level of a single clock cycle core implements a P way SIMD microarchitecture. Since each processing engine is also multithreaded supporting up to G threads concurrently core in this embodiment can have up to P G threads executing concurrently. For instance if P 16 and G 24 then core supports up to 584 concurrent threads.

Because instruction unit issues the same instruction to all P processing engines in parallel core is advantageously used to process threads in SIMD thread groups. As used herein a SIMD thread group refers to a group of up to P threads of execution of the same program on different input data with one thread of the group being assigned to each processing engine . A SIMD thread group may include fewer than P threads in which case some of processing engines will be idle during cycles when that SIMD thread group is being processed. A SIMD thread group may also include more than P threads in which case processing will take place over consecutive clock cycles. Since each processing engine can support up to G threads concurrently it follows that up to G SIMD thread groups can be executing in core at any given time.

On each clock cycle one instruction is issued to all P threads making up a selected one of the G SIMD thread groups. To indicate which thread is currently active an active mask for the associated thread may be included with the instruction. Processing engine uses the active mask as a context identifier e.g. to determine which portion of its assigned lane in local register file should be used when executing the instruction. Thus in a given cycle all processing engines in core are nominally executing the same instruction for different threads in the same SIMD thread group. In some instances some threads in a SIMD thread group may be temporarily idle e.g. due to conditional or predicated instructions divergence at branches in the program or the like. 

Operation of core is advantageously controlled via a core interface . In some embodiments core interface receives data to be processed e.g. primitive data vertex data and or pixel data as well as state parameters and commands defining how the data is to be processed e.g. what program is to be executed from work distribution unit . Threads or SIMD thread groups can be launched by other threads or by fixed function units such as triangle rasterizers. Core interface can load data to be processed into shared memory and parameters into parameter memory . Core interface also initializes each new thread or SIMD thread group in instruction unit then signals instruction unit to begin executing the threads. When execution of a thread or SIMD thread group is completed core advantageously notifies core interface . Core interface can then initiate other processes e.g. to retrieve output data from shared memory and or to prepare core for execution of additional threads or SIMD thread groups.

It will be appreciated that the core architecture described herein is illustrative and that variations and modifications are possible. Any number of processing engines may be included. In some embodiments each processing engine has its own local register file and the allocation of local register file entries per thread can be fixed or configurable as desired. Further while only one core is shown a PPU may include any number of cores which are advantageously of identical design to each other so that execution behavior does not depend on which core receives a particular processing task. Each core advantageously operates independently of other cores and has its own processing engines shared memory and so on.

In some embodiments multithreaded processing core of can execute general purpose computations using thread arrays. As used herein a thread array is a group consisting of a number n0 of threads that concurrently execute the same program on an input data set to produce an output data set. Each thread in the thread array is assigned a unique thread identifier thread ID that is accessible to the thread during its execution. The thread ID controls various aspects of the thread s processing behavior. For instance a thread ID may be used to determine which portion of the input data set a thread is to process and or to determine which portion of an output data set a thread is to produce or write.

In some embodiments the thread arrays are cooperative thread arrays or CTAs. As with other types of thread arrays a CTA is a group of multiple threads that concurrently execute the same program referred to herein as a CTA program on an input data set to produce an output data set. In a CTA the threads can cooperate by sharing data with each other in a manner that depends on thread ID. For instance in a CTA data can be produced by one thread and consumed by another. In some embodiments synchronization instructions can be inserted into the CTA program code at points where data is to be shared to ensure that the data has actually been produced by the producing thread before the consuming thread attempts to access it. The extent if any of data sharing among threads of a CTA is determined by the CTA program thus it is to be understood that in a particular application that uses CTAs the threads of a CTA might or might not actually share data with each other depending on the CTA program.

In some embodiments threads in a CTA share input data and or intermediate results with other threads in the same CTA using shared memory of . For example a CTA program might include an instruction to compute an address in shared memory to which particular data is to be written with the address being a function of thread ID. Each thread computes the function using its own thread ID and writes to the corresponding location. The address function is advantageously defined such that different threads write to different locations as long as the function is deterministic the location written to by any thread is predictable. The CTA program can also include an instruction to compute an address in shared memory from which data is to be read with the address being a function of thread ID. By defining suitable functions and providing synchronization techniques data can be written to a given location in shared memory by one thread of a CTA and read from that location by a different thread of the same CTA in a predictable manner. Consequently any desired pattern of data sharing among threads can be supported and any thread in a CTA can share data with any other thread in the same CTA.

CTAs or other types of thread arrays are advantageously employed to perform computations that lend themselves to a data parallel decomposition. As used herein a data parallel decomposition includes any situation in which a computational problem is solved by executing the same algorithm multiple times in parallel on input data to generate output data for instance one common instance of data parallel decomposition involves applying the same processing algorithm to different portions of an input data set in order to generate different portions an output data set. Examples of problems amenable to data parallel decomposition include matrix algebra linear and or nonlinear transforms in any number of dimensions e.g. Fast Fourier Transforms and various filtering algorithms including convolution filters in any number of dimensions separable filters in multiple dimensions and so on. The processing algorithm to be applied to each portion of the input data set is specified in the CTA program and each thread in a CTA executes the same CTA program on one portion of the input data set. A CTA program can implement algorithms using a wide range of mathematical and logical operations and the program can include conditional or branching execution paths and direct and or indirect memory access. As previously described in conjunction with a shader program used to process graphics data can be configured to include conditional execution paths using predicated or conditional instructions. For example based on a computed deltaLOD value a predicate or condition code is determined that causes the shader program to execute instructions for performing extrapolation to produce a filtered texel value. For another value of the predicate or condition code the shader program executes instructions for performing conventional interpolation to produce a filtered texel value.

In one embodiment a driver program executing on CPU of writes commands defining the CTA to a pushbuffer not explicitly shown in memory e.g. system memory from which the commands are read by a PPU . The commands advantageously are associated with state parameters such as the number of threads in the CTA the location in global memory of an input data set to be processed using the CTA which mipmaps are resident for a texture the location in global memory of the CTA program to be executed and the location in global memory where output data is to be written. The state parameters may be written to the pushbuffer together with the commands. In response to the commands core interface loads the state parameters into core e.g. into parameter memory then begins launching threads until the number of threads specified in the CTA parameters have been launched. In one embodiment core interface assigns thread IDs sequentially to threads as they are launched. More generally since all threads in a CTA execute the same program in the same core any thread can be assigned any thread ID as long as each valid thread ID is assigned to only one thread. Any unique identifier including but not limited to numeric identifiers can be used as a thread ID. In one embodiment if a CTA includes some number n of threads thread IDs are simply sequential one dimensional index values from 0 to n 1. In other embodiments multidimensional indexing schemes can be used. It should be noted that as long as data sharing is controlled by reference to thread IDs the particular assignment of threads to processing engines will not affect the result of the CTA execution. Thus a CTA program can be independent of the particular hardware on which it is to be executed.

Data assembler collects vertex data for high order surfaces primitives and the like and outputs the vertex data to vertex processing unit . Vertex processing unit is a programmable execution unit that is configured to execute vertex shader programs transforming vertex data as specified by the vertex shader programs. For example vertex processing unit may be programmed to transform the vertex data from an object based coordinate representation object space to an alternatively based coordinate system such as world space or normalized device coordinates NDC space. Vertex processing unit may read data that is stored in PP memory through memory interface for use in processing the vertex data.

Primitive assembler receives processed vertex data from vertex processing unit and constructs graphics primitives e.g. points lines triangles or the like for processing by geometry processing unit . Geometry processing unit is a programmable execution unit that is configured to execute geometry shader programs transforming graphics primitives received from primitive assembler as specified by the geometry shader programs. For example geometry processing unit may be programmed to subdivide the graphics primitives into one or more new graphics primitives and calculate parameters such as plane equation coefficients that are used to rasterize the new graphics primitives. Geometry processing unit outputs the parameters and new graphics primitives to rasterizer . Geometry processing unit may read data that is stored in PP memory through memory interface for use in processing the geometry data.

Rasterizer scan converts the new graphics primitives and outputs fragments and coverage data to fragment processing unit . Fragment processing unit is a programmable execution unit that is configured to execute fragment shader programs transforming fragments received from rasterizer as specified by the fragment shader programs. For example fragment processing unit and texture unit may be programmed to perform operations such as perspective correction texture mapping mipmap extrapolation shading blending and the like to produce shaded fragments that are output to raster operations unit . Fragment processing unit and texture unit may also be programmed to perform performs texture filtering operations e.g. bilinear trilinear anisotropic and the like.

Fragment processing unit may read data that is stored in PP memory through memory interface for use in processing the fragment data. Memory interface produces read requests for data stored in graphics memory and decompresses any compressed data. Raster operations unit is a fixed function unit that optionally performs near and far plane clipping and raster operations such as stencil z test and the like and outputs pixel data as processed graphics data for storage in graphics memory. The processed graphics data may be stored in graphics memory for display on display device .

The application specifies the filter type for each of the texture filters as members of the texture image state data structure. The texture filters include those common in the state of the art magnification and minification and two new texture filters extrapolated magnification and extrapolated minification. The filter types include those common in the state of the art nearest neighbor linear mipmapped nearest neighbor with nearest mipfiltering mipmapped nearest neighbor with linear mipfiltering mipmapped linear with nearest mipfiltering mipmapped linear with linear mipfiltering and transparent black . The transparent black filter type does no filtering and simply returns R G B A 0 which is useful when a predicate value or condition code causes a shader program to take alternate action. In the OpenGL graphics API applications programming interface these filter types are referred to as GL NEAREST GL LINEAR GL NEAREST MIPMAP NEAREST GL NEAREST MIPMAP LINEAR GL LINEAR MIPMAP NEAREST GL LINEAR MIPMAP LINEAR. Additional new filter types that may be specified to improve image quality for the extrapolated magnification and extrapolated minification filters are extrapolated mipmapped nearest neighbor and extrapolated mipmapped linear.

Device driver provides LOD unit with information needed to determine whether or not an LOD corresponds to a resident or nonresident mipmap. This information is the extrapolation threshold LOD that is stored in the texture image data structure and provided to LOD unit . In the preferred embodiment of the present invention the extrapolation threshold LOD is a real number stored in the texture image data structure representing the level of detail including fractional bits below which extrapolation magnification or extrapolation minification filtering is selected. The extrapolation threshold LOD allows driver to smoothly transition from extrapolated filtering to interpolated filtering instead of abruptly snapping to a new resident mipmap resolution when new mipmap levels are converted from nonresident mipmaps to resident mipmaps. In an alternative embodiment of the present invention the extrapolation threshold LOD is an integer value representing the lowest LOD resident mipmap level stored in the texture image data structure.

LOD unit performs step of B and C by comparing the ideal LOD with the extrapolation threshold LOD to select which texture filter e.g. magnification minification extrapolated magnification or extrapolated minification will be employed by texture sampler unit to sample and filter the texture image. LOD unit passes the texture filter selected texture filter type ideal LOD texture map coordinates and other sampling parameters common in the state of the art to texture sampler unit . Additionally LOD unit outputs the texture filter type and texture map identifier to address generation unit .

If LOD unit selects the texture filter type specified for the extrapolated minification or extrapolated magnification texture filter then LOD unit notifies device driver of via host interface of that the application has requested filtering of texels from a nonresident mipmap level and specifies the requested mipmap level. Device driver initiates a conversion of the nonresident mipmap to a resident mipmap as described in conjunction with .

If the selected texture filter is extrapolated magnification or extrapolated minification then texture sampler unit computes the deltaLOD step of by computing the difference between the idealLOD and extrapolation threshold LOD. Texture sampler unit then computes the extrapolation weight as a function of deltaLOD.

In one embodiment of the present invention texture sampler unit includes a weight table containing LOD weight pairs stored in order of decreasing LOD positive to negative . If deltaLOD is less than the lowest LOD value in weight table then the weight value from the entry in the table with the lowest LOD is selected as the extrapolation weight. If deltaLOD is greater than the largest LOD value in weight table then the weight value from the entry in the table with the highest LOD is selected as the extrapolation weight. If deltaLOD is equal to the LOD value of an entry in weight table then that entry s weight value is selected as the extrapolation weight. In some embodiments of the present invention if deltaLOD is between two entries in weight table then the extrapolation weight is computed via linear interpolation using the two nearest weight values. In other embodiments of the present invention if deltaLOD is between the LOD values of two adjacent entries in weight table then the extrapolatin weight is computed using a Catmull Rom cubic spline or any other interpolating spline known to those skilled in the state of the art.

In some embodiments of the present invention the contents of weight table are static. In other embodiments of the present invention device driver loads weight table according to an extrapolation filter specified by an application program. In other embodiments of the present invention device driver loads weight table or separate weight tables for extrapolated magnification and extrapolated minification into the texture sampler data structure. The texture sampler data structure may be stored in registers within PPU or the texture sample data structure may be stored in PP memory and cached within PPU .

If the ideal LOD corresponds to a resident mipmap level then texture sampler unit selects the fine and if necessary coarse mipmap level s and samples the pixel footprint on the selected mipmap level s using the selected texture filter type using techniques known to those skilled in the art to produce an interpolated texel value. The lodweight corresponds to the fractional portion of the ideal LOD. If the ideal LOD corresponds to a nonresident mipmap level then texture sampler unit truncates the extrapolation threshold value which is usually the lowest LOD resident mipmap level and highest resolution resident mipmap level to produce an integer portion of the extrapolation threshold value. The integer portion is the fine mipmap level LOD LODfine and the fine mipmap level 1 which is usually the second lowest LOD resident mipmap level and next highest resolution resident mipmap level is the coarse mipmap level LOD LODcoarse .

When the filter type is extrapolated mipmapped nearest neighbor texture sampler unit samples the pixel footprint in texture space outputting nearest neighbor samples on miplevels LODfine and LODcoarse corresponding to lowestResidentMipmap and lowestResidentMipmap 1 to address generation unit . When the filter type is extrapolated mipmapped linear texture sampler unit samples the pixel footprint in texture space outputting samples on miplevels LODfine and LODcoarse corresponding to lowestResidentMipmap and lowestResidentMipmap 1 to address generation unit . Texture sampler unit uses the 1 extrapolation weight as the lodweight for sampling the LODfine miplevel and extrapolation weight as the lodweight for sampling the LODcoarse miplevel when the filter type is extrapolated mipmapped linear or extrapolated mipmapped nearest neighbor.

The selected filter type lodweight anisotropic weight fine mipmap LOD level LODfine the coarse mipmap LOD level LODcoarse and samples corresponding to the texture map coordinates and selected filter type are output by texture sampler unit to address generation unit . Address generation unit generates uv weights bilinear or nearest neighbor for each texel according to the selected filter type using techniques known to those skilled in the art. If the filter type of the sample is extrapolated mipmapped linear address generation unit computes bilinear u v weights for the texels within each sample. If the filter type of the sample is extrapolated mipmapped nearest address generation unit computes nearest neighbor weights for the texels within each sample. Address generation unit uses the samples texture map identifier LODfine and LODcoarse to determine addresses to read texels from the resident mipmaps. When virtual memory addressing is used an additional address conversion may be performed by memory interface to determine the physical addresses needed to read the texels.

In one embodiment of the present invention each texel weight that is used to scale a texel read from a mipmap is the combination of the lodweight of the texel s miplevel the anisotropic filter weight for the footprint anisoweight and uv weights. Address generation unit computes a texel weight by multiplying the lodweight by the anisoweight by the texel uv weight and passes the result down to the Texture Filter unit . The texels are returned to filter unit and scaled by the texel weights computed by address generation unit .

In embodiments of the present invention with filter weights that sum to unity filter unit accumulates the scaled texel values into a texture color accumulator register. When the last texel of the last pixel has been weighted and accumulated texture unit returns the contents of the texture color accumulator register to the fragment Processing unit . In embodiments of the present invention with filter weights that do not sum to unity filter unit accumulates the scaled texel values into a texture color accumulator register and accumulates the texel weights into a texture weight accumulation register. When the last texel has been weighted and accumulated filter unit divides the contents of the color accumulator register by the contents of the weight accumulator register and returns resulting filtered texture value to fragment processing unit .

Texture unit can be configured to return per pixel status information in a manner that is advantageously accessible by the pixel shader program for selecting conditional execution paths. In one embodiment of the present invention texture unit can convey on a per pixel basis whether the act of texturing the pixel required texture unit to employ extrapolation filtering and the resulting values set predicates or condition codes which can be used to determine subsequent branch behavior in the pixel shader program. The shader program can conditionally handle texels of nonresident mipmaps with additional texture reads from the same texture to perform cubic filtering or issue texture reads from other textures to add synthetic detail or perform other operations.

Once the mipmap has been copied in step device driver updates the extrapolation threshold LOD. In some embodiments of the present invention the extrapolation threshold is updated to equal the value of the lowest resident mipmap level. In other embodiments of the present invention the extrapolation threshold LOD is reduced over several frames to smoothly transition from extrapolated filtering to interpolated filtering instead of abruptly snapping to a new resident mipmap resolution the nonresident mipmap level is converted to a resident mipmap level. For example an extrapolation threshold LOD of 3.0 may be successively reduced by 0.1 until a value of 2.0 is reached that equals the lowest resident mipmap level. In step device driver determines if the final value of the extrapolation threshold LOD has been reached and if not step is repeated. Otherwise in step the conversion of the nonresident mipmap level to a resident mipmap level is complete.

When extrapolation filtering is enabled and texels required for filtering come from nonresident mipmap levels texels produced using extrapolated filtering provide an improved image that appears sharper in proportion to the difference between the ideal miplevel and the resident miplevel. This conveys a more appropriate degree of detail compared with using the low resolution mipmap texture data in place of the high resolution mipmap texture data. Parallel processing subsystem is configured to extrapolate detail from resident mipmaps in parallel for processing multiple threads to produce extrapolated texture values while high resolution mipmaps e.g. nonresident mipmap and are retrieved from nonresident memory e.g. system disk system memory and the like.

One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored.

The invention has been described above with reference to specific embodiments. Persons skilled in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

