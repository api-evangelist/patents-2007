---

title: Adaptive data collection for root-cause analysis and intrusion detection
abstract: Endpoints in an enterprise security environment are configured to adaptively switch from their normal data collection mode to a long-term, detailed data collection mode where advanced analyses are applied to the collected detailed data. Such adaptive data collection and analysis is triggered upon the receipt of a security assessment of a particular type, where a security assessment is defined as a tentative assignment by an endpoint of broader contextual meaning to information (i.e., data in some context) that is collected about an object of interest. A specialized endpoint is coupled to the security assessment channel and performs as a centralized audit point by subscribing to all security assessments, logging the security assessments, and also logging the local actions taken by endpoints in response to detected security incidents in the environment. The specialized endpoint is arranged to perform various analyses and processes on historical security assessments.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08413247&OS=08413247&RS=08413247
owner: Microsoft Corporation
number: 08413247
owner_city: Redmond
owner_country: US
publication_date: 20070314
---
Post mortem analysis also known as root cause analysis is the process by which an administrator or a security analyst determines the steps that led to a security breach in an enterprise. The goal is to understand the incident so that similar incidents can be detected and prevented in the future. This type of analysis requires access to detailed data that is relevant to the incident under investigation. However it is often infeasible to collect all data at all times. Thus today s enterprise security products typically collect only a subset of the available data in the enterprise. Such products generally collect data based on a static policy defined by the administrator or analyst. Consequently the level of details that are collected is generally defined by the capacity of the collection system and not necessarily by the relevance of the data or the data source. Therefore in many cases the security analyst ends up with a huge amount of data most of which is almost completely irrelevant.

This Background is provided to introduce a brief context for the Summary and Detailed Description that follows. This Background is not intended to be an aid in determining the scope of the claimed subject matter nor be viewed as limiting the claimed subject matter to implementations that solve any or all of the disadvantages or problems presented above.

An enterprise wide sharing arrangement called ESAS Enterprise Security Assessment Sharing is provided in which a semantic abstraction called a security assessment is created to enable sharing of security related information between different security products called endpoints in an enterprise security environment. A security assessment is defined as a tentative assignment by an endpoint of broader contextual meaning to information i.e. data in some context that is collected about an object of interest in the environment such as a computer user service e.g. a website data or the enterprise as whole. The security assessment utilizes a concise vocabulary for an endpoint to declare that an object in the environment falls into a particular assessment category such as compromised or under attack along with the severity e.g. low medium high critical of the detected incident.

A security assessment is tentative because it is subject to some uncertainty and is valid for a limited period of time. The tentative nature of a security assessment is reflected in two of its components a fidelity field which expresses the level of confidence the endpoint has its assignment of contextual meaning and a time to live TTL field which reflects the endpoint s estimate of the time period for which the security assessment is expected to be valid. Thus for example a security assessment may be used by an endpoint to declare in light of that endpoint s current understanding of security incidents that a particular machine is compromised with a critical level of severity with medium fidelity and having a TTL of 30 minutes. A variety of types of security assessments may be used in any given enterprise security environment having varying combinations of assessment categories and severities.

In an illustrative example the present ESAS arrangement enables endpoints to be configured to adaptively switch from their normal data collection mode to a long term detailed data collection i.e. audit mode where advanced analyses may be applied to the collected detailed data. Such adaptive data collection and analysis is triggered at an endpoint upon the receipt of a security assessment of a particular type i.e. pertaining to an object and or assessment category that is of interest to the endpoint . For example a received low fidelity security assessment about an object can trigger a revision in a host s audit policy to thereby increase the data collected about the object. Advanced analyses or processes may also be applied to the larger set of detailed data that is adaptively collected about that specific object in response to the received assessment.

In another illustrative example a specialized endpoint called an ESAS central server is coupled to the security assessment channel that performs as a centralized audit point by subscribing to all security assessments logging the security assessments and also logging the local actions taken by endpoints in response to security incidents in the environment. The ESAS central server is arranged to perform various analyses and processes on historical security assessments i.e. those in which the TTL has expired in order to enhance the ability to detect potential security incidents.

A cyclic memory store is optionally used in various locations within the environment to collect recent detailed data i.e. detailed data collected over a short term about all objects in a monitored system. The recent detailed data may be persisted with the detailed data collected when an endpoint is adaptively switched to the long term detailed data collection mode. This methodology enables detailed data about the object of interest to be made available for review by a security analyst for events that occurred both before and after the security assessment which triggered the adaptive collection was received.

The present ESAS sharing arrangement provides a number of advantages. By employing a security assessment having a concise vocabulary overall data complexity in the enterprise is drastically reduced and only meaningful information is shared between endpoints. Use of the security assessment also eliminates the need to collect very large amounts of raw data in a central storage location and thereby enables highly scalable enterprise security solutions to be built on a very cost effective basis. The adaptive data collection triggers an endpoint to collect more data but such data collection is both effective and resource efficient because the endpoint is only collecting more data about a specific object. In addition the analytical techniques or processes may be applied to the larger set of detailed data which would be prohibitive if applied to all the data about all the objects in the environment. The adaptive switch to long term detailed data collection and or application of advanced analyses modes further advantageously enables a comprehensive amount of detailed and relevant data about an object of interest in the environment to be presented to a security analyst for further investigation.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Analysis of current enterprise security solutions indicates that there are still significant opportunities for addressing customer needs. For example each separate security product tends to have high rates of false positive and false negative detection of security incidents such as those produced through actions of malware or malicious users. Such low fidelity detection occurs because data from a single type of source i.e. a subset of the enterprise wide data does not normally provide the context needed to make an accurate assessment of the security incident.

The use of automatic actions or responses is very infrequent as a result of the low fidelity detection since confidence in the validity of the detected incident is low. In addition the typical response to a detected incident tends to be very harsh for example a user or machine may be disconnected from the network. Since such harsh actions generally impose significant costs to business activity in the enterprise automation of such actions based on low fidelity detection is not generally performed.

Upon detection of an incident of interest current security products typically perform investigation to determine the validity of the detection i.e. whether the incident is true or false and what action to take in response. Significant resources are expended on investigation to review the detailed data that is collected which may be relevant to the detected incident. Because it is not feasible to collect all data at all times a security product collects only a subset of the available data through application of policies defined by an administrator. Such policies are often static and are commonly defined based on the storage capacity of the collection system and not necessarily by the relevance of the incident data or the data source.

When an incident is detected application of the policies typically results in a review of the data which triggered the detection. When this data is deemed insufficient to generate a high fidelity response typically even more data is collected. For example all of the data traffic into and out of a suspected compromised machine may be monitored. In many cases a large amount of data is collected but is never used and has statistical significance only as noise. Consequently many present security products collect an often overwhelming amount of noise but not enough relevant data is collected.

Turning now to the figures where like reference numerals indicate like elements shows an illustrative enterprise security environment in which a variety of security products 1 2 . . . N called endpoints are deployed. It is emphasized that the number and type of endpoints shown in are merely illustrative and the specific number of endpoints can be scaled up or down and different types of security products endpoints can be utilized depending on the requirements of a specific application of enterprise security assessment sharing. For example in addition to those shown in and described below web application protection products SEM SIM Security Event Management Security Incident Management products operational heath monitoring and configuration management products e.g. Microsoft Windows Software Update Services Microsoft Operations Manager or identity management products e.g. Microsoft Active Directory are also usable in some applications.

In enterprise security environment a host security endpoint is deployed to protect assess and monitor a plurality of host computers in the enterprise . A commercial example of the host security endpoint is Microsoft Forefront Client Security which provides unified malware protection for the enterprise s desktops laptops and server operating systems.

An edge firewall is a security product that is arranged to protect the enterprise environment from Internet based threats while providing users with remote access to applications and data through a perimeter network . Edge firewall may be embodied by for example a Microsoft Internet Security and Acceleration ISA server.

A NAP security endpoint performs computer health policy validation by ensuring ongoing compliance with health policies defined by an administrator. Typically access is restricted for computers e.g. desktops and roaming laptops monitored by the NAP security endpoint that do not comply with system health requirements.

A NIDS security endpoint analyzes traffic inside the enterprise over an internal network . The NIDS security endpoint operates to detect malicious activity such as denial of service attacks port scans by monitoring network traffic on the internal network .

A line of business security endpoint protects various line of business applications . Line of business applications include for example an e mail application such as Microsoft Exchange that is used in the enterprise . Security endpoint typically monitors e mail to provide anti virus and anti spam protection.

Each of the security endpoints in the enterprise are normally arranged as individual islands as indicated by the dashed rectangles in . Accordingly each security endpoint is arranged for monitoring a subset of the available data pertaining to objects in the enterprise and for performing localized actions in response to a detected incident. In addition each endpoint typically includes a local management function 1 2 . . . N. As noted above the individual local management functions are not generally integrated to provide a single point of management.

The endpoints are enabled with functionality to publish security assessments onto the security assessment channel as well as subscribe to a subset of available security assessments published by other endpoints. Active security assessments in the environment function to provide a security context that gives these ESAS enabled endpoints with a new way to look at their own locally available information. Active security assessments are those having a time to live TTL field which indicates they are still valid as described in more detail below.

The security context enables the ESAS enabled endpoint to combine or correlate evidence from security assessments received from a variety of different sources and across object types in order to significantly enhance the quality of its detection of potential security incidents. The ESAS enabled endpoint then makes a decision as to what local action or response is appropriate using the security context in combination with its own security expertise i.e. correlation rules and locally available data. Such decision making is both efficient and cost effective because the security context enables distributed processing of enterprise wide information in the form of security assessments without the burden of sharing huge amounts of raw data throughout the enterprise most of which is completely irrelevant due to the lack of any context . ESAS enabled endpoints are arranged to roll back the local action upon expiration of the security assessment that prompted the local action i.e. when the security assessment exceeds the time to live specified in the TTL field .

The endpoints are isolated from the mechanics of the actual transport and management of the publish subscribe model through a semantic abstraction layer that is arranged to simplify interactions with the security assessment channel . The abstraction layer comprises tables describing the security assessment types to which the endpoints subscribe and tables describing the security assessment types that endpoints publish as described below not all endpoints generally subscribe to all security assessment types . In addition the abstraction layer provides an API application programming interface for reading received security assessments and an API for generating security assessments.

A specialized endpoint ESAS central server is coupled to the security assessment channel and performs as a centralized audit point for the ESAS arrangement . Accordingly the ESAS central server subscribes to all security assessments and permanently logs them. ESAS central server also receives and logs messages from the endpoints that indicate the local actions that are taken by an endpoint. The ESAS central server thus provides administrators with security assessment monitoring functionality that gives a comprehensive view of the history and current status of the enterprise as a whole and each ESAS enabled endpoint.

A security assessment may be performed on any object of interest in an enterprise security environment such as a user or a device. In this illustrative example assessments include four main object types 1 Host assessments about computers in an enterprise 2 User assessments about users or accounts in enterprise 3 Service assessments about a service provided to the enterprise such as a URL Uniform Resource Locator of a web site that has a reputation as being malicious 4 Enterprise assessments about the enterprise as a whole or a well defined subset of the enterprise such as a department subnet site or branch and 5 Data assessments about business related data e.g. as found in documents e mail business data in a database etc. that is present or accessed by objects in the enterprise.

It is emphasized that these object types are merely illustrative and other object types may be used as required by specific scenarios. In most applications of enterprise security assessment sharing endpoints only publish and subscribe to a subset of all of the available security assessment types since particular endpoints are generally going to have interest in particular objects in the enterprise environment. In addition while some endpoints will be both publishers and subscribers there is no requirement for every endpoint to support both functionalities. For these reasons the publish subscribe model used herein is said to be loosely coupled.

Table 1 below shows an illustrative set of assessment categories and their mapping to specific object types that may be contained in a typical security assessment 

In the present illustrative ESAS arrangement four levels of severity are typically utilized low medium high and critical. Three levels of fidelity are typically utilized low medium and high. Note that the number of levels for both severity and fidelity can be arranged to be different depending on the assessment category. For example it is possible to use the three severity levels for the assessment category of vulnerable machine while using four severity levels for the assessment category of compromised machine. The particular choice of number of levels to be utilized will depend on the requirements of a specific application of the present enterprise security assessment sharing.

A security assessment uses information that is available at the time the assessment is made and relies on the particular security expertise and knowledge that is resident in the endpoint that produces it. A security assessment is tentative because confidence in any particular event can never be absolute and also because the assessment is temporary in nature as it relies on information that is present at the time it was produced. At some future time other information will be available so the security assessment may change.

The tentative nature of a security assessment is reflected in two fields included in each assessment fidelity and time to live TTL . The fidelity field provides a way for endpoints to express their confidence level in an assignment of a broader contextual meaning to information being analyzed. The TTL field enables endpoints to reflect the best estimate of the time period for which the security assessment is expected to be valid. Or alternatively the TTL field provides the best estimate for a future security assessment update. When a TTL expires an endpoint that takes actions based on a security assessment to which it subscribes is expected to roll back such actions when the TTL of that assessment expires. Thus the TTL provides a safety valve functionality to prevent a user or a machine from getting inappropriately trapped with restricted access due to a false positive or the loss of a message somewhere in the enterprise. However if such restricted access is indeed appropriate then either a new security assessment may be generated to continue the restriction or the TTL extended.

The security assessment is designed to enable precise semantics i.e. the meaning imparted by the categorization used in the security assessment using a compact vocabulary. As shown in two of the endpoints in the enterprise log data about events that occur in their respective areas of interest. The host event logs and firewall event logs thus contain very large amounts of data. Typically the data is processed in the respective endpoints using correlation rules and in order to identify events of interest. The correlation rules which are often numerous define the localized sponsors or actions taken responsibly to a detected event.

By comparison the security assessments indicated by reference numeral contain only a relatively small amount of data. As security assessments are utilized to assign broad context to information they provide answers to the questions Who created the assessment When Why For how long And on which object does the assessment apply Thus in order to make use of a security assessment an endpoint need only understand the few assessment types of interest as compared with the unbounded number of information messages that result from application of the correlation rules. Accordingly the complexity of the data collected by each endpoint is reduced by mapping information into one or more of the assessment types. Using security assessments thus enables relevant information to be provided to subscribing endpoints without requiring that large amounts of data or information be shared across the enterprise.

Table 2 below provides an illustrative set of fields that may be included in a typical security assessment.

In this illustrative example of enterprise security assessment sharing each endpoint is arranged to perform at least some of the tasks noted below. In some arrangements each endpoint is enhanced with additional functionality as required to perform such tasks through use of a discrete ESAS agent. Alternatively the enhanced functionality may be more tightly integrated into the core functionality provided by the endpoint and a separate or discrete agent may not necessarily be embodied in the endpoint. Such tasks include 

A plurality of security assessments are available for each of the security assessment types i.e. hosts users reputations and enterprise . As indicated by reference numeral in this illustrative example ESAS agent subscribes to security assessments having a Host object type with an assessment category of Vulnerable. It is emphasized that a particular combination of object types and assessment categories that is of interest can be different for different endpoints. Again using the loosely coupled publish subscribe model there is no requirement that every endpoint subscribes to every security assessment.

At process block the endpoint processes the received security assessment using correlation rules and locally available data that may be of some relevance. The outputs of such assessment process include the generation of the new assessment and or an invocation of a local action . As noted above such local action is subject to roll back i.e. self recovery when the received assessment expires according to the TTL field contained therein.

It is important to note that the Rule 2 refers to the exclusivity of information and not data as these terms were defined above. Two endpoints can process the same or overlapping data sources provided that the information they extract from the data and later use to generate assessments is exclusive.

To illustrate the implications of Rule 3 consider the following example where a scheduled antivirus scan of a machine detects and removes a piece of known malware. Based on this detection other locally available information a received currently active assessment and on the endpoint s embedded knowledge about current security incidents the endpoint may conclude one of the following 1 a machine was infected in the past but is now clean and does not pose any additional future security risk 2 The machine was infected and while the particular malware was removed it is possible or likely that it still poses a security risk. According to Rule 3 an endpoint should generate a security assessment about the machine in the latter case and not generate one in the former.

As indicated by reference numeral the edge firewall first detects a short term spike in unusual traffic which may indicate a potentially compromised client. Second the edge firewall sends a security assessment that indicates the particular client is compromised with medium severity and low fidelity as indicated by reference numeral over the security channel to subscribing endpoints.

Third the subscribing endpoints host security and the ESAS central server upon receiving the security assessment typically apply a set of response policies to trigger an appropriate action. As collectively indicated by reference numeral in the host security endpoint issues a new local audit policy while the ESAS central server logs the security assessment and any local actions taken by endpoints in the enterprise.

Host security endpoint in this illustrative example is configured with a response policy that responsively to the receipt of low or medium fidelity security assessment increases the auditing of the object referenced in the security assessment note that response policy configuration is described below in the text accompanying . That is if host security endpoint normally collects data associated with certain events from the host s event logs when it receives a low or medium fidelity security assessment about an object it responds by taking a local action to collect detailed data about the object. The host security endpoint accomplishes this task through interaction with the host to change the audit policy in the host. Such audit policy instructs the host s operating system as to which events and how many events to write into the event log.

Thus in response to the received security assessment in this case either a low or medium fidelity assessment the host can write more data to its event logs and the host security endpoint can collect more data from the host s event logs. Such detailed logging and collection may be arranged to be performed on a long term basis if needed as defined by an endpoint s response policy for that particular security assessment type. It is emphasized that such adaptive data collection in which a short data segment as would typically support a low or medium fidelity assessment triggers a switch to a long term detailed data collection mode in regard to an object of interest may be arranged to be used with a variety of security assessment types having varying severity and fidelity according to the particular requirements of a specific application of security assessment sharing.

The fourth stage in the illustrative scenario shown in includes two different alternatives. In the first alternative indicated by reference numeral the host security endpoint applies an advanced analysis comprising an intrusion detection algorithm on the detailed data collected from the host. If the application of the algorithm results in an insufficient amount of evidence to support a conclusion that an incident occurred then the host security endpoint may adjust the host s local audit policy back to its previous state so that events are logged at a normal level.

If however the application of the algorithm results in a sufficient amount of evidence to support the conclusion that an incident occurred then alternatively the host security endpoint generates a new security assessment having high fidelity as indicated by reference numeral . The new security assessment is shared among the subscribing endpoints as shown in .

Upon the expiration of the TTL in security assessment the detailed data collection performed by the host is rolled back to its previous level according to the normal audit policy. However the collected detailed data is retained in accordance with a detailed data retention policy that may vary according to specific requirements.

The first illustrative scenario shown in highlights the ability of the present ESAS arrangement to differentiate between data that is relevant to an object of interest in the environment and data that is just statistical noise. Accordingly as described above an endpoint may be arranged to collect more data but such data collection is both effective and resource efficient because the endpoint is only collecting more data about a specific object. In addition as with the advanced intrusion detection algorithm described above additional analytical techniques or processes may be applied to the larger set of detailed data which would be prohibitive if applied to all the data about all the objects in the environment.

First host security endpoint performs a scan on a host which detects a virus as indicated by reference numeral . After the virus is detected it is cleaned from the host. Second although the event of interest is not particularly severe given that the virus was cleaned from the host the host security endpoint may nevertheless still generate a low fidelity assessment message which indicates that the host machine is potentially compromised with medium severity as indicated by reference numeral .

Security assessment is shared across the ESAS channel to the subscribing endpoints 2 3 N and ESAS central server . ESAS central server as noted above subscribes to all security assessments that are generated in the environment. As a result it maintains a history of all past i.e. expired security assessments from all of the endpoints. ESAS central server is arranged in this illustrative example to perform a variety of analyses on the historical security assessment data in order to detect potential security incidents. For example ESAS central server may be configured with a particular set of rules that indicates that three low fidelity security assessments being generated about a particular host in the environment in a month do not present a risk. However if that number of low fidelity assessments pertaining to the host i.e. its score were to increase to five or 10 then it could indicate that the particular host is indeed compromised despite the fact that at any individual moment in time there might only be a single low fidelity security assessment having applicability to the host.

At the third stage in the illustrative scenario shown in as indicated by reference numeral the ESAS central server performs an analysis of the history of security assessments pertaining to the identified host and calculates a new score in light of the security assessment received from the host security endpoint using its local rules. In this example the host s score exceeds a pre determined threshold and ESAS central server raises an alert for a security analyst e.g. administrator . At stage four as indicated by reference numeral responsively to its historical analysis the ESAS central server generates a new security assessment indicating the host is compromised with high severity and medium fidelity.

In this example at stage five as indicated by reference numeral responsively to the received security assessment from the ESAS central server each subscribing endpoint switches to an adaptive long term data collection mode. The host security endpoint changes the affected host s local auditing policy in a similar manner as that described above. The edge firewall switches from writing normal networking flow summaries to its event log to performing a detailed audit of traffic to and from the identified host on a packet basis. The line of business security endpoint also increases its auditing level by collecting copies of all outgoing e mail generated by the identified host. The local actions described here are rolled back to the previous state upon expiration of the TTL in the security assessment . However the collected detailed data is retained in accordance with a detailed data retention policy that may vary according to specific requirements.

In both of the illustrative scenarios shown in the adaptive switch to long term detailed data collection and or application of advanced analyses modes advantageously enables a comprehensive amount of detailed and relevant data about an object of interest in the environment to be presented to a security analyst for further investigation.

The persistent memory includes a cyclic store which is arranged to store detailed data about all objects in the system being monitored by the particular endpoint. Cyclic store is generally arranged as a relatively small memory that stores recent detailed data on a first in first out basis. Thus as security events occur in the monitored system they are collected in the cyclic store held for a time interval determined by the size of the cyclic store and then purged as more recent events are collected.

The persistent memory enables detailed data from the cyclic store to be retrieved and combined with detailed data collected when adaptive data collection is triggered as described above in the text accompanying by the receipt of a security assessment typically a low or medium fidelity assessment . For example if a particular object is identified in a security assessment detailed data about that object is retrieved from the cyclic store and combined with the detailed data that is collected during a long term detailed audit that is set for example by a change in a host s audit policies. Advantageously then detailed data about the object of interest is available for review by an analyst where such detailed data is associated with events that occurred both before and after the security assessment which triggered the adaptive collection was received.

In this illustrative example the hosts monitored by host security endpoint are optionally arranged with memories 1 2 . . . N for storing recent security events about all objects that exist in each of the host s local environments. Typically detailed data collected in the cyclic stores associated with the memories is not also collected by host security endpoint .

Screen is an illustrative example showing the configuration of enterprise wide response policies using fields 1 2 . . . N for a number of different endpoints for the case of an assessment category of a compromised machine with critical severity which defines a starting point i.e. anchor point for the response policy configuration as indicated by reference numeral . It is emphasized that other user interface screens would be utilized for other assessment categories object types severity levels etc. so that the user is enabled to define response policies for the plurality of different starting points that are likely to be used in a particular enterprise security environment. The response policies in this particular example are set depending upon the fidelity of a particular security assessment for a set severity level of Critical. Fields include a number of respective subfields that are arranged to reflect user defined input using for example text entry boxes drop down menus and the like that are employed in typical GUIs.

As indicated in subfield for a security assessment indicating a compromised machine with critical severity the edge firewall is configured to increase the amount of auditing i.e. moving to a deep audit level that increases the amount of data collected as compared with a normal level of auditing when a security assessment has low fidelity. Subfield shows that for an assessment having medium fidelity the edge firewall increases the auditing level and also restricts Internet access to the suspected compromised machine to only white list URLs which typically include sites that are known to not be malicious. When the fidelity is high as shown by subfield access to the Internet is blocked completely.

Field shows the response policy configuration for the host security endpoint . For a security assessment having low fidelity and indicating a compromised machine with critical severity the host security endpoint increases the amount of auditing to a deep audit level as indicated by subfield . Subfield indicates that for the cases of medium and high fidelity the host security endpoint increases its auditing and also increases performs a deep scan of it hosts where a deep scan may entail the computer to be rebooted one or more times .

Field shows the response policy configuration for the line of business security endpoint . For a security assessment having low fidelity and indicating a compromised machine with critical severity the line of business security endpoint increases the amount of auditing to a deep audit level as indicated by subfield . Subfield indicates that for a security assessment having medium fidelity the line of business security endpoint increases its data collection to a deep audit and also restricts file attachments to e mail. Subfield indicates that for a security assessment having high fidelity line of business security endpoint blocks all instant messaging IM traffic.

Field shows the response policy configuration for the ESAS central server in . For a security assessment having high fidelity the ESAS central server as indicated in subfield executes a port shutdown for the affected machine and generates an alert that the associated user account has been suspended. As with the subfields discussed above subfield is typically arranged to accept user defined input.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

