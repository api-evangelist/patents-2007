---

title: System and method for enabling a data container to appear in a plurality of locations in a super-namespace
abstract: A system and method to allow a volume to appear in multiple locations in a super-namespace. A super-namespace is a hierarchy of namespaces within a storage system environment, such as a storage system cluster. The volume can appear in multiple locations in the super-namespace and/or in multiple namespaces within the super-namespace.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08312046&OS=08312046&RS=08312046
owner: NetApp, Inc.
number: 08312046
owner_city: Sunnyvale
owner_country: US
publication_date: 20070228
---
The present invention is directed to storage systems and in particular to organizing data containers of a storage system into multiple related name spaces.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system generally provides its storage service through the execution of software modules such as processes. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive HDD or direct access storage device DASD .

The storage operating system of the storage system may implement a high level module such as a file system to logically organize the information as a hierarchical structure of data containers such as files and logical units stored on volumes. For example each on disk file may be implemented as set of data structures i.e. disk blocks configured to store information such as the actual data for the file. These data blocks are organized within a volume block number vbn space that is maintained by the file system. The file system may also assign each data block in the file a corresponding file offset or file block number fbn . The file system typically assigns sequences of fbns on a per file basis whereas vbns are assigned over a larger volume address space. The file system organizes the data blocks within the vbn space as a logical volume each logical volume hereinafter volume may be although is not necessarily associated with its own file system.

The storage system may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access data is containers stored on the system. In this model the storage system may be embodied as a file server executing an operating system such as the Microsoft Windows operating system hereinafter Windows operating system . Furthermore the client may comprise an application such as a database application executing on a computer that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing file based and block based protocol messages in the form of packets to the system over the network. By supporting a plurality of storage e.g. file based access protocols such as the conventional Common Internet File System CIFS and the Network File System NFS protocols the utility of the server is enhanced.

A plurality of storage systems may be interconnected to provide a storage system environment e.g. a storage system cluster configured to service many clients. Each storage system may be configured to service one or more volumes of the cluster wherein each volume comprises a collection of physical storage disks cooperating to define an overall logical arrangement of vbn space on the volume s . The disks within a volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID .

To facilitate client access to the information stored on the server the Windows operating system typically exports units of storage e.g. CIFS shares. As used herein a share is equivalent to a mount point or shared storage resource such as a folder or directory that stores information about files or other directories served by the file server. A Windows client may access information in the directory by mounting the share and issuing a CIFS protocol access request that specifies a uniform naming convention UNC path to the share. The UNC path or pathname is an aspect of a Windows networking environment that defines a way for a client to refer to a unit of storage on a server. The UNC pathname specifies resource names on a network. For example a UNC pathname may comprise a server name a share directory name and a path descriptor that collectively reference a unit of storage or share. Thus in order to access the share the client typically requires knowledge of the specific physical location i.e. the identity of the server exporting the share.

Instead of requiring the client to provide the specific identity of the file server exporting the unit of storage it is desirable to only require a logical pathname to that storage unit. That is it is desirable to provide the client with a globally unique pathname to the storage location without reference to the file server. The conventional Distributed File System DFS namespace service is well known to provide such a solution in a Windows environment through the creation of a namespace that removes the specificity of server identity. As used herein a namespace is a view of shared storage resources such as shares from the perspective of a client. The DFS namespace service is generally implemented using one or more DFS servers and distributed components in a network.

Using the DFS service it is possible to create a unique pathname in the form of a UNC pathname for a storage resource that a DFS server translates to an actual location of the resource in the network. However in addition to the DFS namespace provided by is the Windows operating system there are many other namespace services provided by various operating system platforms including the NFS namespace provided by the conventional Unix operating system. Each service constructs a namespace to facilitate management of information using a layer of indirection between a file server and client accessing a shared storage resource on the server. For example a storage resource may be connected or linked to a link point link in DFS terminology or a mount point in NFS terminology to hide the machine specific reference to the resource. By referencing the link point the client can automatically access information on the storage resource of the specific machine. This allows an administrator to store the information on any server in the network by merely providing a reference to the information.

The Virtual File Manager VFM developed by NuView Inc. and available from Network Appliance Inc. NetApp provides a namespace service that supports various protocols operating on various file server platforms such as NetApp filers and DFS servers. The VFM namespace service is well known and described in VFM Virtual File Manager Reference Guide Version 4.0 2001 2003 and VFM Virtual File Manager Getting Started Guide Version 4.0 2001 2003.

In a storage system cluster environment a clustered namespace may be implemented with multiple namespaces such that the clustered environment can be shared among multiple clients. When a request is made to access data stored in the cluster one or more unique identifiers such as volume identifiers etc. of the clustered namespace identify the storage locations originally used to store the data. The unique identifiers are organized within a storage location repository that is replicated throughout the cluster. The unique identifier contained in a data access request often may not correctly identify the storage location of the data for example if data has been moved by an administrator. In that case a redirection identifier is used to indicate that the requested data is not stored in the storage location identified by the unique identifier provided in the data access request. In response to encountering the redirection identifier during the data access request the storage location repository is examined to find the correct storage location of the data. Thus instead of explicitly managing a chain of identifiers to multiple storage locations a system administrator can use redirection identifiers to indicate that the replication storage location repository should be examined. This in turn enables the administrator to update the unique identifiers in a central yet replicated repository instead of employing the difficult and time consuming administration task of updating chains of identifiers.

A junction is an exemplary redirection identifier associated with a storage location that indicates that the data is not stored at the originally used location but is available at some other storage location. Junctions can be mounted during volume creation by the invocation of a management command from a command line interface CLI graphical user interface GUI or the like. For example the command may be create a volume and mount it on the namespace a b c wherein the namespace a b c comprises pathname components such as parent directory a and sub directory b followed by junction component c. Thus when searching for a file in the namespace a b c file the junction at the volume containing the component of the pathname c is a hint that the file is located on another volume potentially on a different storage system of the cluster. The new volume identifier can be recorded in the storage location repository.

Certain constraints have heretofore been applied to namespace architectures to ensure that a volume can be located unambiguously in a namespace hierarchy of e.g. a storage system cluster. For example if a parent volume in the cluster were to appear in multiple places in the namespace hierarchy a client could not perform a lookup operation to ascend from a child volume to the parent volume because it would be ambiguous as to which parent volume namespace it should ascend. In order to allow the client to unambiguously determine the parent volume of any child volume namespaces in the storage system cluster have heretofore been deliberately limited such that a volume can not appear in more than one storage system namespace. Namespaces have heretofore also been constrained such that a volume can not appear in more than one location in a namespace. These constraints can be disadvantageous in certain applications such as for example wherein volumes configured for multiple purposes could be more efficiently accessed if they are allowed to reside in a plurality of namespaces.

The disadvantages of the prior art are overcome by providing a system and method to allow a volume to appear in multiple locations in a super namespace. As used herein a super namespace is a hierarchy of namespaces within a storage system environment such as a storage system cluster. In illustrative embodiments the volume can appear in multiple locations in the super namespace and or in multiple namespaces within the super namespace.

The super namespace is illustratively organized as a tree structure having a trunk namespace with zero or more branch namespaces associated with the trunk namespace. Illustratively a volume is allowed to reside in the trunk namespace or in one or more of the branch namespaces associated with the trunk namespace. A volume does not appear in multiple places in the namespace which have an ancestor descendent relationship. In other words a volume is not its own ancestor or descendent but it can be otherwise related to itself within the hierarchy. This restriction obviates volume cycles i.e. it prevents a volume from providing a circular reference to itself.

When a request is made to access data stored in a volume a namespace identifier NSID identifies the trunk or branch namespace of the volume. The NSID that identifies the namespace of the volume is stored in an entry of a junction table of a volume location database VLDB when the volume is mounted in the namespace. In the illustrative embodiments a junction table comprises redirection information to the correct volume storing the data. Each junction table entry also includes a parent volume NSID and a child volume NSID along with a parent volume identifier a child volume identifier and a junction mode number generation number.

For an operation to access a volume in a super namespace by descending in the hierarchy i.e. from trunk namespace to branch namespace or from parent namespace to child namespace the operation can refer to the parent volume NSID in the VLDB to find the correct junction entry for the child volume and the child volume s namespace. If the parent namespace is the same as the child namespace it is implied that the volume being accessed resides in a trunk namespace.

For an operation to access a volume in a super namespace by ascending in the hierarchy i.e. from branch namespace to trunk namespace or from child namespace to parent namespace the operation can refer to its current namespace current volume and junction along with the parent NSID. The operation can thereby identify a correct parent for example wherein a junction may reference more than one higher level volumes.

The clients may be general purpose computers configured to interact with the nodes in accordance with a client server model of information delivery. For example interaction between the clients and nodes can enable the provision of storage services. That is each client may request the services of the node and the node may return the results of the services requested by the client by exchanging packets over the connection system which may be a wire based or wireless communication system embodied as a computer network. The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

Each node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of data containers such as volumes directories files and special types of files called virtual disks hereinafter generally blocks on the disks. However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor system. Illustratively one processor can execute the functions of the N module on the node while the other processor can execute the functions of the D module. It should also be appreciated that processors may include multiple processing cores thus improving the processing speed of the processors

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node .

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the connection system may be embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with the node over the connection system by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients . The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks of the disk array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

In accordance with an exemplary embodiment of the present invention storage of information on each disk array is preferably implemented as one or more volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each volume is generally although not necessarily associated with its own file system. The disks within a volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

To facilitate access to the disks the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by the disks . A file system logically organizes the information as a hierarchical structure of named directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. of Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such the storage operating system should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks . To that end the storage server includes a file system module a RAID system module and a disk driver system module . As described further herein the file system includes a redirection process adapted to manage data containers such as volumes using redirection identifiers associated with e.g. the volumes. The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces such as a user interface of a management framework of in response to a user system administrator issuing commands to the node . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks . That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes modes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store meta data describing the layout of its file system these meta data files include among others an mode file. A file handle i.e. an identifier that includes an mode number is used to retrieve an mode from disk.

Broadly stated all modes of the write anywhere file system are organized into the mode file. A file system fs info block specifies the layout of information in the file system and includes an mode of a file that includes all other modes of the file system. Each logical volume has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The mode of the mode file may directly reference point to data blocks of the mode file or may reference indirect blocks of the mode file that in turn reference data blocks of the mode file. Within each data block of the mode file are embedded modes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally a request from the client is forwarded as a packet over the connection system and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the mode file using the mode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the connection system .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by the client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node . It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In an illustrative embodiment the storage server is embodied as D module of the storage operating system to service one or more volumes of the disk array . Within a cluster having multiple D modules multiple volumes may be serviced by a single D module or multiple volumes may be serviced among multiple D modules. However any method of distributing multiple volumes among D modules or multiple volumes sharing a single D module are possible as long as the volumes have associated redirection identifiers that redirect data access requests when the data is relocated among the volumes. The redirection process manages the redirection identifiers or junctions by interfacing with management commands issued by an administrator via e.g. a graphical user interface GUI command line interface CLI or the like.

In addition the multi protocol engine is embodied as N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the connection system as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture of a clustered storage system. To that end each module includes a cluster fabric CF interface module adapted to implement intra cluster communication among the modules including D module to D module communication for data container e.g. a file access operations.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module . That is the N module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D modules of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D modules in the cluster . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster .

Further to the illustrative embodiment the N module and D module are implemented as separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as pieces of code within a single operating system process. Communication between an N module and D module is is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc.

In the illustrative embodiment a data container is represented in the write anywhere file system as an mode data structure adapted for storage on the disks . is a schematic block diagram of an mode which preferably includes a meta data section and a data section . The information stored in the meta data section of each mode describes a data container e.g. a file and as such includes the mode number type e.g. regular directory vdisk junction of file its size time stamps e.g. access and or modification time and ownership i.e. user identifier UID and group ID GID of the file. The meta data section also includes a generation number which illustratively is a monotonically increasing number adapted for use in a variety of embodiments. In one exemplary embodiment the mode number of a junction type mode is combined with the generation number to form a multi bit e.g. a 32 bit field that is used to access an entry of a junction table described further herein. However other exemplary embodiments may use any size for the bit field as long as the field is capable of storing information regarding the junction. Further the mode includes the data section . Specifically the data section may include file system data or pointers the latter referencing 4 kB data blocks on a disk used to store the file system data. Each pointer is preferably a logical vbn to facilitate efficiency among the file system and the RAID system when accessing the data on disks.

A file system layout is provided that apportions an underlying physical volume into one or more virtual volumes or flexible volume of a storage system such as node . In an exemplary embodiment the underlying physical volume is an aggregate comprising one or more groups of disks such as RAID groups of the node . The aggregate has its own physical volume block number pvbn space and maintains meta data such as block allocation structures within that pvbn space. Each flexible volume has its own virtual volume block number vvbn space and maintains meta data such as block allocation structures within that vvbn space. Each flexible volume is a file system that is associated with a container file the container file is a file in the aggregate that contains all blocks used by the flexible volume. Moreover each flexible volume comprises data blocks and indirect blocks that contain block pointers that point at either other indirect blocks or data blocks.

In one embodiment pvbns are used as block pointers within buffer trees of files stored in a flexible volume. This hybrid flexible volume embodiment involves the insertion of only the pvbn in the parent indirect block e.g. Mode or indirect block . On a read path of a logical volume a logical volume vol info block has one or more pointers that reference one or more fsinfo blocks each of which in turn points to an Mode file and its corresponding Mode buffer tree. The read path on a flexible volume is generally the same following pvbns instead of vvbns to find appropriate locations of blocks in this context the read path and corresponding read performance of a flexible volume is substantially similar to that of a physical volume. Translation from pvbn to disk dbn occurs at the file system RAID system boundary of the storage operating system .

In an illustrative dual vbn hybrid flexible volume embodiment both a pvbn and its corresponding vvbn are inserted in the parent indirect blocks in the buffer tree of a file. That is the pvbn and vvbn are stored as a pair for each block pointer in most buffer tree structures that have pointers to other blocks e.g. level 1 L1 indirect blocks Mode is file level 0 L0 blocks. For example in a root top level Mode such as an embedded Mode references indirect e.g. level 1 blocks . Note that there may be additional levels of indirect blocks e.g. level 2 level 3 depending upon the size of the file. The indirect blocks and Mode contain pvbn vvbn pointer pair structures that ultimately reference data blocks used to store the actual data of the file.

The pvbns reference locations on disks of the aggregate whereas the vvbns reference locations within files of the flexible volume. The use of pvbns as block pointers in the indirect blocks provides efficiencies in the read paths while the use of vvbn block pointers provides efficient access to required meta data. That is when freeing a block of a file the parent indirect block in the file contains readily available vvbn block pointers which avoids the latency associated with accessing an owner map to perform pvbn to vvbn translations yet on the read path the pvbn is available.

In an exemplary embodiment luns blocks directories qtrees and files may be contained within flexible volumes such as dual vbn flexible volumes that in turn are contained within the aggregate . The aggregate is illustratively layered on top of the RAID system which is represented by at least one RAID plex depending upon whether the storage configuration is mirrored wherein each plex comprises at least one RAID group . Each RAID group further comprises a plurality of disks e.g. one or more data D disks and at least one P parity disk. Whereas the aggregate is analogous to a physical volume of a conventional storage system a flexible volume is analogous to a file within that physical volume. That is the aggregate may include one or more files wherein each file contains a flexible volume and wherein the sum of the storage space consumed by the flexible volumes is physically smaller than or equal to the size of the overall physical volume.

The management processes have interfaces to are closely coupled to RDB . The RDB comprises a library that provides a persistent object store storing of objects for the management data processed by the management processes. Notably the RDB replicates and synchronizes the management data object store access across all nodes of the cluster to thereby ensure that the RDB database image is identical on all of the nodes . At system startup each node records the status state of its interfaces and IP addresses those IP addresses it owns into the RDB database. Specifically the VLDB process and the RDB cooperate to provide a storage location repository that includes e.g. a junction table . It should be appreciated that separation of the VLDB process and the RDB is purely exemplary. Accordingly in another exemplary embodiment the storage location repository may be implemented as a VLDB having an integrated database functioning as a persistent object store. Via the administrator interface which may be operated on a node or client management tools may be used to create modify and check inconsistencies of the entries of the storage location repository . Such exemplary tools are useful for maintaining the replicated database within the cluster thus avoiding or preventing data inconsistencies within the storage system repository.

In an exemplary embodiment the N module of each node accesses configuration table to obtain information that maps volume identifiers to a D module that owns services a data container within the cluster. The configuration table illustratively functions as a cache to store frequently accessed VLDB information. The VLDB is capable of tracking the locations of volumes and aggregates of nodes . Specifically the VLDB includes a plurality of entries which in turn provides the contents the configuration table . Among other things the VLDB entries keep track of the locations of the flexible volumes hereinafter generally volumes and aggregates within the cluster.

The VLDB illustratively implements a RPC interface e.g. an ONC RPC interface which allows an N Module to query the VLDB . When encountering is contents of a data container handle that are not stored in its configuration table the N Module sends an RPC to a VLDB process. In response the VLDB process returns to the N Module the appropriate mapping information including an ID of the D Module that owns the data container. The N Module caches the information in its configuration table and uses the D Module ID to forward the incoming request to the appropriate D module servicing the requested data container. All functions and communication between the N Module and D Module are coordinated on a cluster wide basis through the collection of management processes and the RDB library user mode applications.

Specifically upon receipt of the data access request the N module examines the VLDB to determine the location of a volume that contains the pathname z1 using e.g. the VLDB entries described with respect to . Illustratively the VLDB identifies volume C as the volume that contains z1 and in turn identifies volume C s location as D module . The N module then forwards the access request to the appropriate D module e.g. D module and the file system executing on D module retrieves the modes for z1 and z2 from volume C . Upon examining the mode for z2 the file system executing on D module encounters a junction

As noted the junction is a type of mode that functions as a redirection identifier associated with a storage location to indicate that the data is not stored at the originally used location but is available at some other storage location. Junctions are generally created during volume creation and or administrative reorganization by the invocation of a management command from a CLI GUI or the like issued by the administrator. The command in turn may create a junction type mode and initiate the storage of volume identification and namespace information relating to the junction in the storage location repository .

Since the junction is an indication that data is located elsewhere the file system i.e. the redirection process of the file system examines the VLDB to determine the new location of the data. Specifically the redirection process combines the mode number of the junction type mode with the generation number from the mode to form an index into the junction table of the storage location repository . is a schematic block diagram of the junction table that may be advantageously used with the present invention.

In an exemplary embodiment the junction table is a data structure comprising a plurality of entries . Each junction table entry is identified by an index e.g. the inode number generation number of the corresponding junction type inode. Each junction table entry includes a parent e.g. trunk master set identifier MSID a parent namespace identifier NSID a child e.g. branch MSID and a branch NSID . The MSIDs are volume identifiers that identify a master or active volume and its identically replicated volume in the cluster. An example of MSIDs that may be advantageously used is described in U.S. patent application Ser. No. 11 648 161 which application is hereby incorporated by reference in its entirety. In the illustrative embodiment the NSIDs identify the namespaces of volumes and their associated parent or child volumes and are stored in junction table entry when the volumes are mounted in the namespaces.

Upon examining the junction table of the VLDB the redirection process essentially redirects the access request via the cluster switching fabric to the D module servicing volume B . Again the file system executing on the D module retrieves and examines the mode for the requested file. The file info.txt is found under the pathname z1 z2. By using the junction table individual volumes need not store volume identification information or maintain complex chains of redirection information.

The storage operating system thereby provides a redirection processes that cooperates with a local configuration table or commonly accessible VLDB to keep track of data as it is written to and moved among disks of a cluster in RAID groups and within flexible volumes and or aggregates. However a client is is generally not concerned with physical location of data within the storage system. Rather it is desirable for a client to have access to data containers e.g. files by reference to the data containers location in a namespace such as a super namespace using a pathname for example without requiring the client to have information about the physical location of the data containers data.

The present invention is directed to a system and method for allowing a volume to appear in multiple locations in a super namespace. As used herein a super namespace is a hierarchy of namespaces within a storage system environment such as a storage system cluster. In illustrative embodiments the volume can appear in multiple locations in the super namespace and or in multiple namespaces within the super namespace.

The super namespace is illustratively organized as a tree structure having a trunk namespace with zero or more branch namespaces associated with the trunk namespace. Illustratively a volume is allowed to reside in the trunk namespace or in one or more of the branch namespaces associated with the trunk namespace. A volume generally does not appear in the namespace in a parent child relationship. In other words a volume generally is not its own parent or child but it can be otherwise related to itself within the hierarchy. This restriction obviates volume cycles i.e. it prevents a volume from providing a circular reference to itself.

A super namespace is created and accessed by maintaining a database which associates MSIDs with NSIDs. Embodiments of the present invention utilize junction table in the VLDB of the storage location repository to associate MSIDs of each volume with a corresponding NSID. The VLDB can include additional information is such as parent MSID child MSID parent NSID and child NSID to define parent child relationships among volumes in the super namespace.

The structure of the super namespace can be enforced by imposing restrictions during the assignment of parent NSIDs and child NSIDs for particular volumes i.e. for corresponding MSIDs. Embodiments of the invention allow a volume to be stored in a plurality of branch namespaces. A volume stored in the trunk namespace cannot be stored in branch namespace. Conversely a volume stored in a branch namespaces cannot be stored in a trunk namespace. These restrictions can be enforced by requiring a volume in a trunk namespace to always have its parent and child NSIDs be equal to the NSID of the trunk namespace and by requiring a volume with its child NSID equal to that of branch namespace always have its parent NSID be equal to the NSID of the trunk namespace or equal to the volume s child NSID. The use of NSIDs associated with each volume allows a client operation to uniquely identify a parent volume when ascending through the super namespace to access data beyond a child volume s root directory for example.

For example when a request is made to access data stored in a volume that is in a branch namespace the NSID identifies the branch namespace of the volume. The child NSID and associated child MSID identify the branch namespace and the volume associated with the junction type mode as referenced by the mode number generation number . The parent MSID and parent NSID identify a parent volume associated with the child volume and a possibly different i.e. trunk namespace. If a volume referred to by a junction type mode resides in a trunk namespace the corresponding junction table entry includes a parent NSID and child NSID that are equal.

If a junction table entry has parent and child values with the same NSID it is permissible for this NSID to be that of a branch namespace.

If a junction table entry has parent and child values with different NSIDs then the parent NSID must be that of a trunk namespace and the child NSID must be that of a branch namespace.

Associations between certain MSIDs and NSIDs in a junction table entry can be limited in order to restrict the location of volumes within a super namespace according to certain rules. When a volume s MSID is paired with a corresponding NSID in the child MSID and child NSID fields of a junction table entry the volume identified by the child MSID is effectively mounted to i.e. appears in the namespace identified by the corresponding child NSID.

Embodiments of the present invention provide a super namespace comprising one trunk namespace at the root of the super namespace and zero or more branch namespaces associated with the trunk namespace. Volumes can appear in just one place in the trunk namespace. The trunk namespace can be used to build a top tier of the super namespace. Although the illustrative embodiments typically include only a single volume in a trunk namespace alternative embodiments could have multiple volumes mounted in a trunk namespace within the scope of the present disclosure.

A volume can also only appear in one place within a given branch namespace. However a volume that appears in a branch namespace can appear in other branch namespaces and can appear in different places within the different branch namespaces.

Although the illustrative embodiments typically include only a single volume in a branch namespace alternative embodiments could have multiple volumes mounted in a branch namespace within the scope of the present disclosure.

A volume that appears in one or more branch namespaces cannot also reside in a trunk namespace. Thus volumes can be allowed to appear in multiple places in the super namespace as long as the places are limited to branch namespaces. This constraint can prevent volume cycles that could otherwise occur if a volume could be referenced as its own ancestor or descendent.

A storage system can unambiguously determine a higher level directory or volume such as . . . relative to the current directory or volume of a file system when at the root of a current directory or volume in a super namespace by referring to the parent NSID and parent MSID in the junction table entry that also defines the namespace of the current directory or volume. is a schematic block diagram illustrating a super namespace having a trunk namespace and a plurality of branch namespaces associated with the trunk namespace . Each namespace comprises a plurality of locations as represented by e.g. pathnames. Any number of branch namespaces may be associated with a trunk namespace according to illustrative embodiments of the invention. The super namespace can be transparent to a client such that from the client s perspective the existence of trunk and branch namespaces is not apparent.

In an illustrative embodiment a first volume Volume A can be mounted in one location of the trunk namespace . The trunk namespace has certain properties of a traditional namespace whereby it can only store one volume and can only store the volume in a single location e.g. location . Although Volume A is shown in a particular location it should be apparent that the illustrated location is an arbitrary location of the trunk namespace and that the first volume could alternatively be mounted in any one of the other locations of the trunk namespace . In alternative embodiments of the invention additional distinct volumes not shown can be also be mounted to other locations of the trunk namespace . Any volume that is mounted to a location of the trunk namespace can not also be mounted to another location such as a branch namespace described below.

Another volume Volume B can be mounted in more than one branch namespace and can be mounted in a different location within different branch namespaces . Volume B is illustrative of a volume that can be stored in multiple name spaces branch namespaces according to the illustrative embodiments of the invention. Although Volume B is shown in particular locations of particular branch namespaces it should be apparent that the illustrated locations are arbitrary locations of the branch namespaces and that the Volume B could alternatively be mounted in any one location of any branch namespace . However a volume cannot be mounted to more than one location of a particular branch namespace.

A junction in parent volume P is illustratively referenced by an mode having mode number X and generation number Y. The junction table entry is indexed by mode number X generation number Y volume P s MSID and a parent NSID that identifies namespace in which volume P is mounted . The junction table entry includes a child MSID that identifies volume C and a child NSID that identifies the namespace in which volume C is mounted. Because volume P is mounted in a trunk namespace and volume C is mounted in a branch namespace the parent MSID and parent NSID in the junction table entry have different values from the child MSID and child NSID.

While illustrates a child volume in a branch namespace having as its immediate ancestor parent a volume in a trunk namespace there is no requirement that this be always the case. A second junction table entry could have its parent MSID be equal to 0100 i.e. that of volume C and its parent NSID be 1111 i.e. also that of volume C . The second junction table entry would have child NSID also be 1111 and its child MSID be that of volume B as referenced in .

A data access request generally references a correct volume by using a file handle such as an NFS file handle included in the request. NFS file handles are well known to persons having ordinary skill in the art. In a traditional namespace the file handle includes the volume s MSID which is sufficient to identify the correct volume. However in the super namespace provided by the present invention because a volume can appear in multiple places the volume s MSID alone provides insufficient information for locating the volume. Therefore in an illustrative embodiment the present invention provides a modified NFS file handle which also includes the volume s NSID.

A client that requires access to data in a super namespace can ascend from the root of a volume to a parent volume or can descend from a parent volume to a child volume. For example when an NFS client needs to ascend from the root of a child volume to its parent volume the junction table is searched to identify junction table entries is wherein the Child NSID field and Child MSID field matches the NSID and MSID provided within the NFS file handle. Once the correct junction table entry is located the parent MSID and parent NSID found therein is used to identify the parent volume and locate its namespace. When an NFS client needs to descend from a parent volume to a child volume the junction table is searched to identify junction table entries wherein the Parent NSID field and Parent MSID field matches the NSID and MSID provided within the NFS file handle. Once the correct junction table entry is located the child MSID and child NSID found therein is used to identify the child volume and its locate its namespace.

Additional steps may be taken to enforce rules of the super namespace when mounting volumes to a namespace within the super namespace. For example before allowing a volume to be mounted to a trunk namespace the file system must confirm that the volume is not already mounted to the trunk namespace and that it is not already mounted to a branch of the trunk namespace. This check can be performed for example by searching the junction table for entries having the proposed MSID in the Child MSID field. If an existing junction table entry includes the proposed MSID in its Child MSID field then the procedure to mount the volume into the trunk namespace can be halted and an error message can be issued to indicate that the volume is already mounted to the trunk namespace.

In order to enforce the restriction whereby a volume can be stored only once in a branch namespace the junction table can be searched for entries having the proposed MSID NSID pair in the Child MSID and Child NSID fields. If an existing junction table entry includes the proposed MSID NSID pair in its Child MSID and Child NSID fields then the procedure can be halted and an error message can be issued to indicate that the volume is already in the proposed branch namespace.

In order to enforce the restriction whereby a volume proposed to be mounted onto a branch namespace cannot also be stored in a trunk namespace the junction table can be searched for entries having the proposed MSID in the Child MSID field. If an existing junction table entry includes the proposed MSID pair in its Child MSID field and the junction table entry s Child NSID field is that of trunk namespace then the is procedure can be halted and an error message can be issued to indicate that the volume is already in a proposed trunk namespace.

According to an illustrative embodiment the use of a super namespace permits a volume to appear in multiple places in the same namespace of the same storage system. The extensions to a traditional namespace disclosed herein are achieved while still allowing an instruction i.e. operation to unambiguously determine the parent volume of any child volume that appears in multiple places in a namespace or cluster for example.

The operations herein described are purely exemplary and imply no particular order. Further the operations can be used in any sequence when appropriate and can be partially used. With the above embodiments in mind it should be understood that the invention can employ various computer implemented operations involving data stored in computer systems. These operations are those requiring physical manipulation of physical quantities. Usually though not necessarily these quantities take the form of electrical magnetic or optical signals capable of being stored transferred combined compared and otherwise manipulated.

Any of the operations described herein that form part of the invention are useful machine operations. The invention also relates to a device or an apparatus for performing these operations. The apparatus can be specially constructed for the required purpose or the apparatus can be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines can be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The invention can also be embodied as computer readable code on a computer readable medium. The computer readable medium is any data storage device that can store data which can be thereafter be read by a computer system. Examples of the computer readable medium include hard drives accessible via network attached storage NAS Storage Area Networks SAN read only memory random access memory CD ROMs CD Rs CD RWs magnetic tapes and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion. The computer readable medium can also be distributed using a switching fabric such as used in compute farms.

The foregoing description has been directed to particular embodiments of this invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. Specifically it should be noted that the principles of the present invention may be implemented in non distributed file systems. Furthermore while this description has been written in terms of N and D modules the teachings of the present invention are equally suitable to systems where the functionality of the N and D modules are implemented in a single system. Alternately the functions of the N and D modules may be distributed among any number of separate systems wherein each system performs one or more of the functions. Additionally the procedures processes and or modules described herein may be implemented in hardware software embodied as a computer readable medium having program instructions firmware or a combination thereof. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention

