---

title: Interprocessor direct cache writes
abstract: In a multiprocessor system level 2 caches are positioned on the memory side of a routing crossbar rather than on the processor side of the routing crossbar. This configuration permits the processors to store messages directly into each other's caches rather than into system memory or their own coherent caches. Therefore, inter-processor communication latency is reduced.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08327071&OS=08327071&RS=08327071
owner: NVIDIA Corporation
number: 08327071
owner_city: Santa Clara
owner_country: US
publication_date: 20071113
---
The present invention generally relates to caching and more specifically to a multiprocessor system with caching on the memory side of a routing crossbar.

Current multiprocessor systems include level 2 L2 caches that are directly coupled to each processor. This configuration allows each processor to snoop writes to the caches of the other processors in the system and to access the caches with low latency compared with the number of clock cycles needed to retrieve data from system memory. System memory is typically accessed through a bridge device e.g. a Northbridge chip and is shared with other devices in the system that are also coupled to the bridge device.

As the number of processors in multiprocessor systems increases the complexity of the snooping and accessing of caches that are coupled to other processors increases. Therefore the complexity and access latency increases as the parallelism increases.

Accordingly what is needed in the art is a system and method for configuring caches in a multiprocessor system that allows for increased parallelism without increasing complexity and cache access latency.

In a multiprocessor system L2 caches are positioned on the memory side of a routing crossbar rather than on the processor side of the routing crossbar. This configuration permits the processors to store messages directly into each other s caches rather than into system memory or their own coherent caches. Therefore inter processor communication latency is reduced. Processor parallelism may be increased by adding processors and corresponding L2 caches to the routing crossbar to improve overall system processing throughput without increasing the complexity of accessing the L2 caches. Each processor may access a cache line in any one of the L2 caches by determining the correct memory controller based on the physical cache line address and then issuing a request directly to that memory controller using the available routing fabric. Therefore cache misses are not broadcast to all of the memory controllers. Additionally a central processing unit CPU may write data to the L2 caches of the multiprocessor system and one or more of the processors in the multiprocessor system can read the data from the L2 caches. The number of clock cycles need to transfer data from the CPU to the processors in the multiprocessor system is reduced compared with other transfer mechanisms such as having the processors read from the CPU system memory or an L2 cache coupled to the CPU or having the CPU write to the multiprocessor system memory.

Various embodiments of a method of the invention for transmitting messages in a multiprocessor system include outputting a message produced by a central processing unit CPU to a level 2 cache of a processing core translating a virtual address corresponding to the message into a physical address corresponding to a location in a processing core memory that is coupled to the level 2 cache storing the message and at least a portion of the physical address in the level 2 cache updating a ready value to indicate that the message is available in the level 2 cache and reading the message from the level 2 cache by the processing core.

Various embodiments of the invention for a multiprocessor system that includes a parallel processing unit and a central processing unit CPU . The parallel processing unit includes a plurality of processing cores configured to execute instructions to process data a parallel processing memory configured to store messages including the instructions and the data a plurality of level 2 caches that are coupled to the parallel processing memory and configured to store the messages and a memory crossbar that is coupled between the plurality of processing cores and the plurality of level 2 caches and configured to route the messages between each one of the processing cores and each one of the level 2 caches. The CPU is configured to produce the messages and write the messages to the plurality of level 2 caches.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

System memory includes a device driver that is configured to provide one or more messages that specify the data and program instructions for processing by parallel processing subsystem . The messages may be stored in system memory caches or memory within other devices of system . Device driver is executed by CPUs to translate instructions for execution by parallel processing subsystem based on the specific capabilities of parallel processing subsystem . The instructions may be specified by an application programming interface API which may be a conventional graphics API such as Direct3D or OpenGL.

Memory bridge which may be e.g. a Northbridge chip is connected via a bus or other communication path e.g. a HyperTransport link to an I O input output bridge . I O bridge which may be e.g. a Southbridge chip receives user input from one or more user input devices e.g. keyboard mouse and forwards the input to CPUs via communication path and memory bridge . A parallel processing subsystem is coupled to memory bridge via a bus or other communication path e.g. a PCI Express Accelerated Graphics Port or HyperTransport link . In one embodiment parallel processing subsystem is a graphics subsystem that delivers pixels to a display device e.g. a conventional CRT or LCD based monitor .

A system disk is also connected to I O bridge . A switch provides connections between I O bridge and other components such as a network adapter and various add in cards and . Other components not explicitly shown including USB or other port connections CD drives DVD drives film recording devices and the like may also be connected to I O bridge . Communication paths interconnecting the various components in may be implemented using any suitable protocols such as PCI Peripheral Component Interconnect PCI Express PCI E AGP Accelerated Graphics Port HyperTransport or any other bus or point to point communication protocol s and connections between different devices may use different protocols as is known in the art.

An embodiment of parallel processing subsystem is shown in . Although only a single parallel processing unit PPU is shown in parallel processing subsystem may include one or more PPUs each of which is coupled to a local parallel processing PP memory . One or more messages that specify the location of data and program instructions for execution by each PPU may be stored in each PP memory . In general a parallel processing subsystem includes a number U of PPUs where U 1. PPU and PP memory may be implemented e.g. using one or more integrated circuit devices such as programmable processors application specific integrated circuits ASICs and memory devices.

As shown in detail for PPU each PPU includes a host interface that communicates with the rest of system via communication path which connects to memory bridge or in one alternative embodiment directly to CPUs or caches . In one embodiment communication path is a PCI E link in which dedicated lanes are allocated to each PPU as is known in the art. Other communication paths may also be used. Host interface outputs messages or other signals for transmission on communication path and also receives all incoming messages or other signals from communication path and directs them to appropriate components of PPU . For example messages related to processing tasks may be directed to a work distribution unit while messages related to memory operations e.g. reading from or writing to PP memory may be directed to a memory crossbar . Host interface work distribution unit and memory crossbar may be of generally conventional design and a detailed description is omitted as not being critical to the present invention.

Each PPU advantageously implements a highly parallel processor. As shown in detail for PPU a PPU includes a number C of cores and corresponding caches where C 1. Each cache is coupled to a corresponding portion of PP memory shown as a memory and through C 1 . The cache memory pairs e.g. cache and memory do not communicate with each other. Each processing core is capable of executing a large number e.g. tens or hundreds of threads concurrently where each thread is an instance of a program one embodiment of a multithreaded processing core is described in conjunction with . A processing context encompasses a complete set of state through PPU while a thread may encompass only the state required to shade a single pixel. Threads run inside processing contexts one processing context might contain thousands of running threads. Cores receive processing tasks to be executed via a work distribution unit . Work distribution unit can implement a variety of algorithms for distributing work. For instance in one embodiment work distribution unit receives a ready signal from each core indicating whether that core has sufficient resources to accept a new processing task. When a new processing task arrives work distribution unit assigns the task to a core that is asserting the ready signal if no core is asserting the ready signal work distribution unit holds the new processing task until a ready signal is asserted by a core .

In some embodiments of the present invention a message implements a remote procedure call RPC that includes a bundle of data that is included with the handle of a routine for processing the data. When the data has been processed by cores host interface sends a return message that includes data to CPU .

Cores communicate with memory crossbar to read from or write to caches and PP memory . In one embodiment memory crossbar includes an interface adapted to communicate with local PP memory as well as a connection to host interface thereby enabling cores to communicate with system memory or other memory that is not local to PPU . Similarly one or more of CPUs can read from or write to caches and or PP memory through memory crossbar . Importantly read and write accesses from CPUs pass through host interface which performs memory address translation to convert virtual addresses provided by CPUs into physical memory addresses for caches and PP memory .

Cores can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying laws of physics to determine position velocity and other attributes of objects image rendering operations e.g. vertex shader geometry shader and or pixel shader programs and so on. PPU may transfer data from system memory local PP memory and or caches into internal on chip memory process the data and write result messages back to system memory local PP memory and or caches where such messages can be accessed by other system components including e.g. CPUs or another parallel processing subsystem .

In some embodiments some or all of cores in PPU are graphics processors with rendering pipelines that can be configured to perform various tasks related to generating pixel data from graphics data read from a message via memory bridge and bus interacting with caches and local PP memory which can be used as graphics memory including e.g. a conventional frame buffer messages texture maps and the like to store and update pixel data deliver pixel data to display device and the like. In some embodiments PP subsystem may include one or more PPU that operate as graphics processors and one or more other PPU that are used for general purpose computations. The PPUs may be identical or different and each PPU may have its own dedicated PP memory device s or no dedicated PP memory device s .

Referring back to in operation one or more CPUs are the master processors of system controlling and coordinating operations of other system components. In particular CPUs issue commands that control the operation of PPU . In some embodiments CPUs write a message for each PPU to a buffer not explicitly shown in and which may be located in system memory cache PP memory or another storage location accessible to both CPUs and PPU . PPU reads the instructions and data from the message and executes commands asynchronously with operation of CPUs .

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges may be modified as desired. For instance in some embodiments system memory is connected to CPUs directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPUs . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPUs rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip. The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

The connection of PPU to the rest of system may also be varied. In some embodiments PP system is implemented as an add in card that can be inserted into an expansion slot of system . In other embodiments a PPU can be integrated on a single chip with a bus bridge such as memory bridge or I O bridge . In still other embodiments some or all elements of PPU may be integrated on a single chip with CPU .

A PPU may be provided with any amount of local PP memory including no local memory and may use local memory and system memory in any combination. For instance a PPU can be a graphics processor in a unified memory architecture UMA embodiment in such embodiments little or no dedicated graphics PP memory is provided and PPU would use system memory exclusively or almost exclusively. In UMA embodiments a PPU may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high speed link e.g. PCI E connecting the PPU to system memory e.g. via a bridge chip.

As noted above any number of PPU can be included in a parallel processing subsystem. For instance multiple PPU can be provided on a single add in card or multiple add in cards can be connected to communication path or one or more of the PPU could be integrated into a bridge chip. The PPU in a multi PPU system may be identical to or different from each other for instance different PPU might have different numbers of cores different amounts of local PP memory and so on. Where multiple PPU are present they may be operated in parallel to process data at higher throughput than is possible with a single PPU . Systems incorporating one or more PPU may be implemented in a variety of configurations and form factors including desktop laptop or handheld personal computers servers workstations game consoles embedded systems and so on.

Various techniques may be used to transmit messages produced by CPUs to consumers of the messages such as PPU . The messages may contain instructions command and or data. Conventional systems have been configured to have the producer of the data write the messages to system memory or to the consumer s memory e.g. system memory or PP memory and have the consumer read the messages. Alternatively the produce writes the messages to cache and the consumer reads the messages from cache . In order to ensure that the consumer reads valid messages the producer writes a value indicating that a message is ready and the consumer polls the ready value only reading the message when the value indicates that the message is valid.

Assuming that it takes T clock cycles for the producer to write to system memory i.e. from the initiate of the write until the result is posted in system memory various relative times can be determined for other memory accesses. T clock cycles are also needed for a processor producer or consumer to read from system memory. A typical value for T is 250 clock cycles. 2T clock cycles are needed for a processor to read from or write to another processor s memory i.e. for CPU to read from or write to PP memory . T 5 clock cycles are needed for a processor to read from or write to its own L2 cache i.e. CPU to read from or write to cache and core to read from or write to cache . 6T 5 clock cycles are needed for a processor to read from or write to another processor s cache i.e. CPU to read from or write to cache and core to read from or write to cache . Furthermore because writes are pipelined each additional write in a group that is unbroken by a read adds a cost of 1 clock cycle. Reads are not pipelined.

Given the number of clock cycles that are needed for the various read and write operations a minimum time needed to pass a message of length K may be computed in terms of T for each message transmission technique. TABLE 1 lists the message transmit time for four different configurations. The first column describes the system configuration. The second column specifies the number of clock cycles for the producer of the message to write the message to memory and set the ready value. The third column specifies the number of clock cycles needed for the consumer of the message to read the ready value. The fourth column specifies the number of clock cycles needed for the consumer to read the message from the memory and the fifth column specifies the total number of clock cycles needed for communication of the message between the producer and consumer.

In a first system configuration read write system memory the message producer CPU writes to system memory and a core reads the message from system memory . Since the producer writes its own memory and writes are pipelined the number of clock cycles needed to write memory and set the ready value is T K 1. Since the consumer needs to read the ready value from the producer s memory the read takes 2T clock cycles. Likewise since the consumer reads the message from the producer s memory the message read takes 2KT clock cycles.

In a second system configuration read write PPU memory the message producer CPU writes to PPU memory and a core reads the message from PPU memory . Since the producer writes the consumer s memory and writes are pipelined the number of clock cycles needed to write memory and set the ready value is 2T K 1. Since the consumer needs to read the ready value from its own memory the read takes T clock cycles. Likewise since the consumer reads the message from its own memory the message read takes KT clock cycles. This configuration of communicating between CPU and a core reduces the number of clock cycles by 1KT compared with the first system configuration.

In a third system configuration read write CPU cache the message producer CPU writes to cache and a core reads the message from cache . Since the producer writes its own memory and writes are pipelined the number of clock cycles needed to write memory and set the ready value is T 5 K 1. Since the consumer needs to read the ready value from the producer s cache the read takes 6T 5 clock cycles. Likewise since the consumer reads the message from the producer s cache the message read takes 2KT 6 5T 12KT 5 clock cycles. The difference between using the third system configuration for communicating between CPU and a core and the second system configuration is 7KT 8T 5 so when K 8 7 or K 1 since K is an integer the third configuration is faster.

In a fourth system configuration read write core cache the message producer CPU writes to cache and a core reads the message from cache . Since the producer writes the consumer s cache and writes are pipelined the number of clock cycles needed to write memory and set the ready value is 6T 5 K 1. Since the consumer needs to read the ready value from its own cache the read takes T 5 clock cycles. Likewise since the consumer reads the message from its own cache the message read takes KT 5 clock cycles. This configuration of communicating between CPU and a core reduces the number of clock cycles by 11KT 5 compared with the third system configuration. Specifically when T is 250 and K is 16 the third system configuration requires 1012 clock cycles to transfer the message while the fourth system configuration requires only 132 clock cycles. In order to benefit from the quicker transfers system is configured to transfer messages from CPUs to cores by writing the messages to caches as described in conjunction with . Conversely system is configured to transfer messages from cores to CPUs by writing the messages to caches .

In one embodiment each core includes an array of P e.g. 8 16 etc. parallel processing engines configured to receive SIMD instructions from a single instruction unit . Each processing engine advantageously includes an identical set of functional units e.g. arithmetic logic units etc. . The functional units may be pipelined allowing a new instruction to be issued before a previous instruction has finished as is known in the art. Any combination of functional units may be provided. In one embodiment the functional units support a variety of operations including integer and floating point arithmetic e.g. addition and multiplication comparison operations Boolean operations AND OR XOR bit shifting and computation of various algebraic functions e.g. planar interpolation trigonometric exponential and logarithmic functions etc. and the same functional unit hardware can be leveraged to perform different operations.

Each processing engine uses space in a local register file LRF for storing its local input data intermediate results and the like. In one embodiment local register file is physically or logically divided into P lanes each having some number of entries where each entry might store e.g. a 32 bit word . One lane is assigned to each processing engine and corresponding entries in different lanes can be populated with data for different threads executing the same program to facilitate SIMD execution. In some embodiments each processing engine can only access LRF entries in the lane assigned to it. The total number of entries in local register file is advantageously large enough to support multiple concurrent threads per processing engine .

Each processing engine also has access to an on chip shared memory that is shared among all of the processing engines in core . Shared memory may be as large as desired and in some embodiments any processing engine can read to or write from any location in shared memory with equally low latency e.g. comparable to accessing local register file . In some embodiments shared memory is implemented as a shared register file in other embodiments shared memory can be implemented using shared cache memory.

In addition to shared memory some embodiments also provide additional on chip parameter memory and or cache s which may be implemented e.g. as a conventional RAM or cache. Parameter memory cache can be used e.g. to hold state parameters and or other data e.g. various constants that may be needed by multiple threads. Processing engines also have access via memory crossbar to off chip global memory which can include e.g. PP memory caches and or system memory with system memory being accessible by memory crossbar via host interface as previously described. It is to be understood that any memory external to PPU may be used as global memory. Processing engines can be coupled to memory crossbar via an interconnect not explicitly shown that allows any processing engine to access global memory.

In one embodiment each processing engine is multithreaded and can execute up to some number G e.g. 24 of threads concurrently e.g. by maintaining current state information associated with each thread in a different portion of its assigned lane in local register file . Processing engines are advantageously designed to switch rapidly from one thread to another so that instructions from different threads can be issued in any sequence without loss of efficiency. Since each thread may correspond to a different context multiple contexts may be processed over multiple cycles as different threads are issued for each cycle.

Instruction unit is configured such that for any given processing cycle an instruction INSTR is issued to each P processing engines . Each processing engine may receive a different instruction for any given processing cycle when multiple contexts are being processed simultaneously. When all P processing engines process a single context core implements a P way SIMD microarchitecture. Since each processing engine is also multithreaded supporting up to G threads concurrently core in this embodiment can have up to P G threads executing concurrently. For instance if P 16 and G 24 then core supports up to 384 concurrent threads for a single context or N 24 concurrent threads for each context where N is the number of processing engines allocated to the context.

Operation of core is advantageously controlled via a work distribution unit . In some embodiments work distribution unit receives pointers to data to be processed e.g. primitive data vertex data and or pixel data as well as locations of messages containing data or instructions defining how the data is to be processed e.g. what program is to be executed . Work distribution unit can load data to be processed into shared memory and parameters into parameter memory . Work distribution unit also initializes each new context in instruction unit then signals instruction unit to begin executing the context. Instruction unit reads instructions contained in messages and executes the instructions to produce processed data. When execution of a context is completed core advantageously notifies work distribution unit . Work distribution unit can then initiate other processes e.g. to retrieve output data from shared memory and or to prepare core for execution of additional contexts.

It will be appreciated that the core architecture described herein is illustrative and that variations and modifications are possible. Any number of processing engines may be included. In some embodiments each processing engine has its own local register file and the allocation of local register file entries per thread can be fixed or configurable as desired. In particular entries of local register file may be allocated for processing each context. Further while only one core is shown a PPU may include any number of cores which are advantageously of identical design to each other so that execution behavior does not depend on which core receives a particular processing task. Each core advantageously operates independently of other cores and has its own processing engines shared memory and so on.

Data assembler is a fixed function unit that collects vertex data for high order surfaces primitives and the like and outputs the vertex data to vertex processing unit . Vertex processing unit is a programmable execution unit that is configured to execute vertex shader programs transforming vertex data as specified by the vertex shader programs. For example vertex processing unit may be programmed to transform the vertex data from an object based coordinate representation object space to an alternatively based coordinate system such as world space or normalized device coordinates NDC space. Vertex processing unit may read data that is stored in caches or PP memory through memory crossbar for use in processing the vertex data.

Primitive assembler receives processed vertex data from vertex processing unit and constructs graphics primitives e.g. points lines triangles or the like for processing by geometry processing unit . Geometry processing unit is a programmable execution unit that is configured to execute geometry shader programs transforming graphics primitives received from primitive assembler as specified by the geometry shader programs. For example geometry processing unit may be programmed to subdivide the graphics primitives into one or more new graphics primitives and calculate parameters such as plane equation coefficients that are used to rasterize the new graphics primitives. Geometry processing unit outputs the parameters and new graphics primitives to rasterizer . Geometry processing unit may read data that is stored in caches or PP memory through memory crossbar for use in processing the geometry data.

Rasterizer scan converts the new graphics primitives and outputs fragments and coverage data to fragment processing unit . Fragment processing unit is a programmable execution unit that is configured to execute fragment shader programs transforming fragments received from rasterizer as specified by the fragment shader programs. For example fragment processing unit may be programmed to perform operations such as perspective correction texture mapping shading blending and the like to produce shaded fragments that are output to raster operations unit . Fragment processing unit may read data that is stored in caches or PP memory through memory crossbar for use in processing the fragment data. Memory crossbar produces read requests for data stored in graphics memory decompresses any compressed data and performs texture filtering operations e.g. bilinear trilinear anisotropic and the like. Raster operations unit is a fixed function unit that optionally performs near and far plane clipping and raster operations such as stencil z test and the like and outputs pixel data as processed graphics data for storage in graphics memory. The processed graphics data may be stored in graphics memory e.g. caches PP memory and or system memory for display on display device .

By writing the message into the consumer memory particularly into caches the transfer is performed in fewer clock cycles compared with writing the message into cache or system memory and having cores read the message from cache or system memory . Furthermore since caches are positioned on the memory side of memory crossbar any core can access any cache to read a message. This configuration advantageously permits the addition of cores and corresponding L2 caches by adding ports to memory crossbar . Therefore processor parallelism may be increased to improve overall system processing throughput without increasing the complexity of accessing caches .

The invention has been described above with reference to specific embodiments. Persons skilled in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

