---

title: System and method for displaying a digital video sequence modified to compensate for perceived blur
abstract: A system for displaying a digital video sequence includes a graphics processing unit (GPU) and a display device. The GPU receives and modifies the digital video sequence to compensate for perceived blur based on motion between frames of the digital video sequence. The display device displays the modified digital video sequence. A method and computer readable medium having computer readable code is also provided.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07843462&OS=07843462&RS=07843462
owner: Seiko Epson Corporation
number: 07843462
owner_city: Tokyo
owner_country: JP
publication_date: 20070907
---
The present invention relates generally to image processing and more particularly to a method and system for displaying a digital video sequence modified to compensate for perceived blur.

Moving objects in digital video displayed on a hold type display device such as a liquid crystal display LCD device can appear blurry to an observer. The perceived blur is known to be caused in part by the relatively slow LC response of the liquid crystal cells. When compared with an impulse type device such as a cathode ray tube CRT device for example an LCD device has a much slower brightness transition response time. The perceived blur is also caused in part by prolonged light emission inherent in the sample and hold driving technique commonly employed by LCD devices which results in formation of after images on the human retina. These after images produce a blurred visual perception as the video sequence is being observed.

Various methods have been proposed to compensate for perceived blur. These include methods that modify the hold type device itself for example by black frame insertion or backlight blinking and those that pre process frames of the digital video sequence prior to display on the hold type device in order to compensate for motion blur such as low pass filtering or inverse filtering .

U.S. Patent Application Publication No. 2005 0265454 to Muthukrishnan et al. discloses a motion estimation algorithm for predictive coding of a digital video stream. A best match of a given block in a current frame is found by identifying a set of predictor search points in a reference frame based on a median vector of an adjacent already coded macro block a zero motion vector a temporally co located macroblock surrounding macroblock motion vectors and a global motion vector. A Sum of Absolute Difference SAD between the given block and each reference block in the reference frame that is centered on the respective predictor points is then calculated. The predictor point corresponding to the lowest SAD is then used as the center of a further diamond search within a given pixel range to identify a better match i.e. one that yields a lower SAD . If a better match is found a diamond search within a reduced pixel range is conducted about the center of the better match. The process continues with progressively reduced pixel ranges for a predetermined number of iterations or until the minimum SAD stays at the center of a current diamond search. The motion vector for the given block is then deemed to correspond to the center of the last diamond search.

U.S. Patent Application Publication Nos. 2006 0056513 and 2006 0056708 to Shen et al. disclose a method for accelerating video encoding using both a central processing unit CPU to encode the video and a graphics processing unit GPU to perform motion estimation for use during the encoding. A particular video frame is identified by the CPU and provided to the GPU for processing. The GPU conducts motion estimation using block matching during which reference pixel blocks within a search window are compared with a current pixel block in the current frame to find a reference pixel block yielding a minimum SAD. The motion vector representing motion estimation of a pixel block is provided by the GPU to the CPU in order to encode the current frame into a digital video data stream. A depth buffer is employed by the GPU to accelerate motion estimation.

U.S. Pat. No. 5 764 787 to Nickerson discloses a method of estimating motion between successive video frames during encoding of the video frames into a video stream. During the method a current pixel block in a current frame is compared using SAD or Sum of Squares of Differences SSD with a plurality of reference pixel blocks within a search window in a reference frame to determine a best match. Only half of the pixels in the current pixel block distributed in a checkerboard pattern are compared thereby to reduce computational load.

U.S. Pat. No. 6 496 538 to Drysdale discloses a method and apparatus for estimating motion between video frames during encoding of a set of video frames. During the method a first macroblock from a video frame in the set is compared to a second macroblock in a reference video frame to determine a differential value. The differential value is then compared to a comparison value. If the differential value is no smaller than the comparison value the differential value is compared to a minimal differential value. If the differential value is less than or equal to the minimal differential value the differential value is stored as the new minimal differential value thereby to establish a better macroblock match.

U.S. Pat. No. 6 549 576 to Moriyoshi discloses a method for detecting motion vectors between blocks of pixels in frames for compression coding a digital video sequence. During the method a current image is divided into pixel blocks and the difference in position between each pixel block in the current image and its best match in a search range of a reference image is determined. A motion vector for a pixel block residing in the same position in the reference frame is set as the predictive vector and the end position of the predictive vector is set to a scan start position. Scanning during searching is spirally performed from the scan start position toward the vicinity of the outside of the search range so as to locate high probability matches early in the search.

U.S. Pat. No. 6 707 853 to Cook et al. discloses a method for compensating for motion between frames in a digital video sequence for the purpose of digitally encoding the video sequence. During the method a picture in a sequence is reconstructed by predicting the colors of pixels in pixel macroblocks using motion vectors that have previously been obtained for forward and or backward reference pictures in the sequence.

U.S. Pat. No. 6 778 607 to Zaccarin et al. discloses a method for multi rate encoding of video sequences. During the method motion information relating a frame to previously encoded frames is calculated using both spatial and frequency domain representations of the frame and a previous frame. Motion compensation prior to encoding is performed in the frequency domain.

U.S. Pat. No. 6 842 483 to Au at al. discloses a method for estimating motion between successive images in a digital video sequence using block matching. During the method a search is performed for a pixel block of a previous frame that is similar to a current pixel block of a current frame. A search area based on points in the previous frame is arranged in successively larger diamond shaped zones. The diamond shaped zones may be centered on the corresponding position of the pixel block in the previous frame or centered on a point that is based on a previous prediction of motion. The search in successive zones for the best matching pixel block in the previous frame continues until a threshold number of diamond shaped zones have been searched.

U.S. Patent Application Publication No. 2004 0227763 to Wichman et al. discloses a coprocessor for conducting motion estimation between frames in order to encode or decode a digital video stream. During motion estimation one 1 motion vector over a 16 16 pixel macroblock and four 4 motion vectors over four 4 8 8 pixel blocks in a frame are computed. The coprocessor and a processor cooperate to perform both single motion vector searches for the full macroblock and multiple motion vector searches for the four 4 8 8 blocks.

U.S. Patent Application Publication No. 2004 0247029 to Zhong et al. discloses a method for estimating motion between frames in a digital video sequence. During the method a plurality of predicted start motion vectors are selected. Coarse block motion searches using the plurality of predicted start motion vectors are performed to obtain a SAD value and an associated vector. A fine block motion search is then conducted using as a starting position the motion vector of the best match resulting from the coarse search. The predicted start motion vectors are preferably vectors corresponding to macroblocks both above and to the left of the current macroblock. The coarse searches are preferably 16 16 diamond searches and the fine searches include both an 8 8 search and a half pixel search.

U.S. Patent Application Publication No. 2004 0264570 to Kondo et al. discloses a method for encoding and decoding pictures in a series of moving pictures. Storage of motion vectors used for predictive coding of pictures is controlled such that fewer motion vectors than the number of reference pictures is stored. If a required motion vector is stored coding is conducted using the required motion vector. Otherwise coding is performed using a motion vector corresponding to a neighboring block.

U.S. Patent Application Publication No. 2005 0179814 to Pau et al. discloses a method for de interlacing digital images formatted according to the Phase Alternate Line PAL display system in order to display the images on non PAL devices.

U.S. Patent Application Publication No. 2005 0190844 to Kadono et al. discloses a method for estimating motion between frames in a digital video sequence in order to perform compression coding of the video sequence. During the method a reference block in a reference picture is defined as a search center. An error between a current block in a current picture and the reference block and an error between the current block and each of neighboring reference blocks of the reference block are then calculated. The reference block having the minimum error is identified. Based on the minimum error it is determined whether or not motion estimation should be terminated. Based on the position of the reference block having the minimum error a reference block removed by two pixels or more from the search center is set as the next search center and the calculation is repeated based on the next search center. The method proceeds iteratively until the value of the newest minimum reaches a threshold value at which point the search is terminated.

U.S. Patent Application Publication No. 2006 0002474 to Au et al. discloses a method for estimating motion of multiple frames during compression of digital video. During the method macroblocks and their respective locations in a current frame are defined. Both a search region for each macroblock in the reference frame and a search point for each relative displacement of a macroblock within the search region are defined. A hierarchy of modes or levels of possible subdivision of each macroblock into smaller non overlapping regions is then constructed. An elaborated search i.e. a pixel precision search for each macroblock for the highest level of subdivision of the macroblock is then conducted to find a macroblock match. Then a small diamond search around the motion vector obtained from the highest level elaborated search is conducted. The best motion vector for the macroblock is the motion vector corresponding to the subdivision of the macroblock in the reference frame that has the smallest mismatch measure i.e. SAD .

U.S. Patent Application Publication No. 2006 0067406 to Kitada et al. discloses an apparatus for decoding a compression encoded motion video stream. The apparatus implements a motion compensation procedure that generates an inter frame prediction signal corresponding to an undecoded picture using previously decoded pictures. Motion vector information for generating the inter frame prediction signal is separated from the motion video stream by an entropy decoding unit.

U.S. Patent Application Publication No. 2006 0109910 to Nagarajan et al. discloses a method for interpolating motion vectors with sub pixel accuracy during compression coding of digital video. A block matching process for calculating a full pixel motion vector for each block comprises comparing pixel blocks in a current frame with reference pixel blocks in a search range in a reference frame. The method by which the motion vector is interpolated is based on the orientation of the calculated full pixel motion i.e. horizontal vertical or diagonal .

U.S. Patent Application Publication No. 2006 0120612 to Manjunath et al. discloses a method for estimating motion between frames in a digital video sequence during video sequence encoding. During the method a motion vector predictor is calculated based on motion vectors previously calculated for a frame s video blocks that are proximal to the current video block. The motion vector predictor is used as a basis from which to search for a prediction video block for encoding the current video block. A difference block indicative of differences between the current video block and the prediction video block is then calculated and used to encode the current video block.

U.S. Patent Application Publication No. 2006 0126739 to Stoner et al. discloses a method for optimizing motion estimation during encoding of a digital video sequence. During the method a SAD value is calculated between a current macroblock in a current frame and each of a plurality of reference macroblocks within a search range in a reference frame. SAD values are then calculated for all microblocks of a smallest block size within the macroblock. The SAD values of the smallest microblocks are used to calculate the SAD values for microblocks of other sizes within the macroblock i.e. by summing the SAD values for microblocks in different combinations . The motion vectors corresponding to the lowest of the SAD values from the various sized microblocks in each macroblock are then deemed to be the macroblock motion vectors.

While it is well known to estimate motion between frames in a digital video sequence for encoding digital video improved techniques for pre compensating for perceived blur in a digital video sequence displayed on a hold type device are desired.

It is therefore an object to provide a novel system and method for displaying a digital video sequence modified to compensate for perceived blur.

According to one aspect there is provided a system for displaying a digital video sequence comprising 

a graphics processing unit GPU receiving and modifying the digital video sequence to compensate for perceived blur based on motion between frames of the digital video sequence and

According to an embodiment the GPU comprises a programmable fragment processor texture memory storing frames of the digital video sequence program memory storing a computer program executable by the programmable fragment processor. The computer program comprises program code estimating motion between pixels in a current frame and a previous frame and program code filtering pixels in the current frame based on the estimated motion to compensate for perceived blur.

modifying an input digital video sequence using a graphics processing unit GPU to compensate for perceived blur based on motion between frames and

According to yet another aspect there is provided a computer readable medium having a computer program thereon that is executable by a graphics processing unit GPU for displaying digital video the computer program comprising 

computer program code modifying the digital video sequence to compensate for perceived blur based on motion between frames and

The methods and systems described herein increase the perception of sharpness in digital video displayed on a hold type display and do not suffer from excessive noise amplification as is common in many known inverse filtering methods. Furthermore use of the GPU for motion estimation and compensation results in a significant performance increase over methods using only a central processing unit CPU .

For ease of understanding perceived blur in a digital video image caused by a hold type display such as an LCD device that uses a sample and hold display format will firstly be discussed. As a digital video sequence is input to an LCD device each digital video image or frame of the digital video sequence is displayed and sustained on the LCD device for one frame interval. While viewing a scene in motion the human eyes actively track the scene with smooth pursuit eye movement so as to generate a stabilized image on the human retina as described by M. J. Hawken and K. R. Gegenfurtner in the publication entitled Pursuit Eye Movements to Second Order Motion Targets Journal of the Optical Society of America A 18 9 pp 2292 2296 2001 . The human visual system then undertakes visual temporal low pass filtering in order to perceive a flicker free image.

The tracking behavior of the human eye causes integration of frame data over at least one frame interval resulting in perceived blur. The combination of the LCD device and the tracking behavior of the human visual system therefore results in a spatial low pass filtering effect. Methods for pre compensating for motion blur in a digital image captured with a digital camera using estimates of motion direction and motion extent of the image are described in United States Patent Application Publication No. 2005 0231603 the content of which is incorporated herein by reference. Methods for pre compensating for perceived motion blur in a digital video sequence are described in United States Patent Application Publication No. 2006 0280249 the content of which is incorporated herein by reference.

In the following description a system and method for displaying a digital video sequence modified to compensate for perceived blur is provided. The system comprises a graphics processing unit GPU that receives and modifies the input digital video sequence to compensate for perceived blur based on motion between frames of the digital video sequence and a display device displaying the modified digital video sequence.

In this embodiment the CPU is a Pentium4 2.26 GHz system with a Windows 2000 Professional English Version operating system. The GPU is a NVIDIA GeForce 6000 Series or higher with ForceWare Graphics Driver Version 81 or higher.

Memory also stores a framebuffer object for holding intermediate frame results during processing. The variable definition of the framebuffer object is set out in Table 2 below 

As will be understood use of the framebuffer object permits direct rendering to textures. This permits increased performance because data does not have to be copied from the default frame buffer. The framebuffer object itself stores a pointer array with a base level and a block level each with respective texture attachments. The texture attachments have 16 bit float type precision and the RGBA color format. The RGBA color format of the texture attachments provides access to four single channel luminance images from each texture permitting efficient memory access.

The base level includes four 4 texture attachments for storing previous frame data pre processed frame results a rendering buffer and a frame motion vector map. Each of the base level texture attachments are the same size as a current frame. The variable definitions of the framebuffer object texture attachments are set out in Table 3 below 

The block level of the pointer array includes three 3 texture attachments each of which are one eighth the size of the current frame for block motion estimation.

Memory also includes two off screen framebuffers. The off screen framebuffers are used for multi pass rendering of textures in the framebuffer objects as will be described. Two off screen framebuffers are required because textures in framebuffer objects cannot be read from and written to simultaneously.

Memory also includes program memory for storing an OpenGL Open Graphics Library application written using the GLUT OpenGL Utility Toolkit library. OpenGL is an industry standard graphics application programming interface API for two dimensional 2D and three dimensional 3D graphics applications. In general the OpenGL API processes graphics data representing objects to be rendered that is received from a host application and renders graphical objects on a display device for viewing by the user.

The OpenGL application includes vertex shader and fragment shader programs. General descriptions of the vertex shader programs are shown in Table 4 below 

The vertex shader programs serve to reduce the workload of the fragment shader programs by pre computing texture coordinates of the convolution kernels. The fragment shader programs are compiled at run time using the known NV fragment program fp40 profile.

The vertex shader and fragment shader programs are written in C and Cg with the C compiler being Microsoft Visual Studio C 6.0. The Cg compiler is Cg Compiler 1.4. OpenGL Version 2.0 with OpenGL Framebuffer Object Extension is employed.

The Cg development environment is set up with header files cg.h and cgGL.h libraries cg.lib and cgGL.lib and DLLs cg.dll and cgGL.dll. The standard GLUT display and callback functions are used to create a video playback window. Video playback is synchronized with the VSYNC signal of display device if the video synchronization option is on and otherwise may proceed as quickly as possible.

When it is desired to display a digital video sequence on the LCD device the CPU retrieves the digital video stream from memory . Once retrieved the CPU decodes the digital video stream and outputs each frame Fof the resultant digital video sequence DVS to the GPU . The GPU processes the frames of the DVS resulting in output frames Othat are modified to compensate for perceived motion blur step . The modified output frames Oform a processed DVS that is displayed by the LCD device step .

First an MPEG decoding thread is initiated on the CPU for decoding the MPEG digital video stream retrieved from memory into a digital video sequence . After each frame is decoded the CPU sets the m FrameIsReadyEvent event flag and waits for a m FrameWantedEvent event flag from the GPU before continuing to decode the next frame in the digital video sequence.

The GPU upon receipt of a current frame binds the current frame to texture mpegTexID and then sets the m FrameWantedEvent flag so as to signal the CPU that another frame may be provided to the GPU . Using fragment shader program fragment01 jgb2gray the GPU converts the current frame from the RGB colorspace to the YIQ colorspace using a simple linear transformation. The luminance channel of the current frame is then extracted and stored at m nDestTexID see Table 3 and . Motion between pixels in the current frame stored at m nDestTexID and those in a previous frame stored at m nSrcTexID is then estimated.

During estimation of the motion between pixels in the current frame and the previous frame the current frame having a height h and width w is divided into b b pixel blocks wherein b 8 such that there are sixty four 64 pixel blocks. A motion vector field Vhaving dimensions of h 8 w 8 is then initialized. A search is then conducted for each pixel block in the current frame to find its best match within the previous frame.

During the search a search window having a radius r 16 pixels and its center at the position in the previous frame corresponding to the pixel block is defined. Twelve 12 candidate motion vectors including a zero vector are then identified based on the search window in various directions and distances from its center. In particular five 5 candidate motion vectors extending from the search window center in the shape of a are defined at a distance of d r 4 four 4 candidate motion vectors extending from the search window center in the shape of an x are defined at a distance of and four 4 candidate motion vectors extending from the search window center in the shape of a are defined at a distance of

Using fragment shader programs fragment02 absdiff0 fragment03 sumdiff0 fragment04 absdiff1 and fragment04 sumdiff1 to calculate the Sum of Absolute Differences SAD for the zero vector and the first level block motion searches candidate matching pixel blocks having centers defined by respective candidate motion vectors are then compared with the current pixel block in the current frame. The SAD is calculated according to Equation 1 below 

The best match is then identified as the candidate matching pixel block that yields the lowest SAD. If the best match is the candidate matching pixel block that corresponds to the zero vector searching is complete and motion of pixels in the pixel block is deemed to be zero. Otherwise the search is further refined. During refinement of the search eight 8 additional candidate motion vectors extending from the center of the best match by a distance d in respective directions are defined. Using fragment shader programs fragment06 absdiff2 and fragment07 sumdiff2 additional candidate matching pixel blocks having centers defined by respective ones of the additional candidate motion vectors are then compared with the pixel block in the current frame. If none of the additional candidate matching pixel blocks yields a SAD that is lower than that of the current best match the current best match is considered to be the final match and motion of the block is deemed to correspond to the candidate motion vector of the final match. Otherwise the additional candidate matching pixel block yielding the lower SAD is deemed to be the new best match. The search is then further refined based on eight 8 additional candidate motion vectors extending from the center of the new best match by a distance d 2 in respective directions using fragment shader programs fragment06 absdiff2 and fragment07 sumdiff2.

The process described above continues with progressive refinements using fragment shader programs fragment06 absdiff2 and fragment07 sumdiff2 and additional motion vectors extending progressively smaller distances from the centers of the previous best matches. The search is terminated either when the lengths of additional candidate motion vectors are less than one 1 pixel or when further refinement fails to yield an additional candidate matching pixel block with a lower SAD than that of the current best match.

The position in the vector field corresponding to the block position in the current frame is then updated with the motion vector corresponding to its best match in the previous frame. The process described above is conducted for each of the blocks in the current frame.

With a motion vector having been obtained for each pixel block the motion vector map is complete. The fragment shader program fragment09 vecmap then converts the motion vector map from one eighth the size of the current frame to the size of the current frame using bilinear interpolation of the motion vectors. Each pixel in the current frame is thereby allocated a respective motion vector having a direction and an extent.

In the event that there are no motion vectors then no motion pre compensation is required. In this case the output frame Ois equal to the current frame F.

Where there has been motion estimated between the current frame Fand the previous frame F however motion blur pre compensation is conducted in order to generate an output frame Ofor the current frame FThe pre compensating used by the system is a modification of the technique disclosed in above mentioned U.S. Patent Application Publication No. 2005 0231603 as will be described.

Given the current frame Fand the previous frame F fragment shader program fragment10 imggrad calculates both an intra difference frame and an inter frame differences at each pixel location based on its respective motion vector. The differences are stored as weights for used by fragment shader program fragment11 setweight to set the amount of correction to be performed on each pixel.

A filter bank is constructed and for each motion vector a linear blurring filter fis created with size sand direction corresponding to the respective motion vector and added to the filter bank. For example where K 2 a first of the two 2 blurring filters fbased on a motion vector with direction 0 and extent s 5 pixels would be as follows 

The second of the two 2 blurring filters fbased on a motion vector with direction 90 and extent s 3 pixels would be as follows 

An initial guess frame is established by setting the current frame Fas the initial guess image for output frame O. A guess pixel is selected from the guess image and a blur filter corresponding to the guess pixel is retrieved from the filter bank. If the blur filter does not have an extent that is at least one 1 pixel then the next pixel is selected. Otherwise the edge magnitude of the guess pixel is estimated in the motion direction of the blur filter using a Sobel edge filter and stored as a weighting factor in W x y . The Sobel edge filter operation and weighting is conducted according to Equations 2 3 and 4 below 

The guess pixel is then blurred using the blur filter by convolving the output frame pixel O x y with the filter fto obtain blurred pixel B x y . An error between the pixel in the frame Fand the blurred guess pixel is determined by subtracting the blurred pixel B x y from the current frame pixel F x y to obtain error E x y . Error E x y is then blurred using the blur filter f and weighted using the edge magnitude that had been obtained as a weighting factor. The guess pixel is then updated with the weighted and blurred error according to Equation 5 below 5 where 

If there are more pixels to select in the guess image then the next pixel is selected for pre processing as described above. Otherwise the total adjustment to output frame Oacross all pixels is calculated to determine the overall difference E between the guess image and the current frame Fas calculated across all pixels according to Equation 6 below 

If the overall difference E is below a pre defined error threshold then pre compensation is complete for the current frame F. The resultant output frame Ois sharpened with fragment shader program fragment14 sharpen using a directional sharpening filter tuned to the motion direction at each pixel location and stored in rendering buffer m nBufferTexID. Otherwise a pixel in the updated guess image is selected and the process described above is re iterated until complete.

Once the sharpening operation on output frame Ohas been completed an overdrive operation is performed using fragment shader program fragment15 overdrive. The results of the overdrive operation are stored as pre processed frame results in m nDestTexID. Overdrive provides additional compensation for the relatively slow response time of a hold type device. During overdrive the driving voltage applied to each device pixel is adjusted by an amount proportional to a gray level transition. This technique is described by H. Okurama M. Akiyama K. Takotoh and Y. Uematsu in the publication entitled A New Low Image Lag Drive Method For Large Size LCTVs SID 02 Digest pp. 1284 1287 2002 .

Once overdrive has been completed on output frame O the fragment shader program fragment16 gray2rgb inserts the output frame Ointo the luminance channel Y of the YIQ frame stored at mpegTexID converts the frame into RGB color space and provides the modified RGB frame data to the LCD device .

The two off screen framebuffers are used to achieve multi pass rendering by swapping the source and target texture IDs at the end of each rendering pass. In this way the target texture from the first pass is used as the source for the second pass.

Performance of the system has been found to be better than that provided by systems employing only a CPU for motion pre compensating. The test environment used to evaluate the performance of the system included a Pentium4 2.26 GHz CPU with a Windows 2000 Professional English Version operating system 256 MB RAM and a bus speed of 266 MHz. GPU was a NVIDIA GeForce 6600 Series GPU with 256 MB DDR RAM. Metrics were obtained using the performance counter function provided by the kernel32.lib library and results were verified using the gDEBugger OpenGL profiler version 2.3. For each evaluated implementation option the time taken to process 100 frames of a test video sequence was measured. Each video sequence was measured five times and the average frame processing time was captured.

1. A Foreman real video sequence of a static background with a subject moving in the foreground and having a resolution of 176 144 pixels 

2. A Vision Chart animated video sequence of a vision chart in various font sizes scrolling from left to right under constant motion and having a resolution of 256 192 pixels 

3. A Road real video sequence of a camera panning from left to right capturing the motion of a fast moving vehicle and having a resolution of 320 240 pixels 

4. A Football real video sequence of multiple fast moving subjects in the foreground and having a resolution of 352 240 pixels and

5. A TwoDogs real video sequence of two dogs moving in different directions and having a resolution of 640 480 pixels.

All real video sequences were saved in MPEG format while the animated sequence was created by causing a BMP image to move using DirectX and saving the result to an MPEG file. A frame rate of 30 fps frames per second was used in all of the test video sequences.

The total individual processing times taken by the GPU and the CPU to complete all operations in the algorithm were compared. These operations included colorspace conversion motion estimation motion blur compensation motion sharpening and overdrive. is a graph illustrating the differences in overall GPU and CPU processing times during modification of frames to compensate for perceived blur for several frame sizes. is a table correlating the overall GPU and CPU processing times of with processing times per frame and frame rates. Overall the GPU implementation yielded a performance speed up factor of about 2.5 times over the counterpart CPU implementation. Performance speed up became more pronounced as the frame size increased.

It was observed that in most cases it took longer to complete the actual motion blur compensation on the GPU than on the CPU. This could be due to additional data copying time in the GPU implementation during use of the off screen render targets which are either read only or write only. Other reasons may include the overhead imposed on the iterative rendering process by having to swap between different frame buffers. Furthermore in the GPU implementation the motion vector is stored separately for each pixel which while yielding smoother results inherently increases the time required for texture fetch.

Although a specific embodiment has been described above with reference to the Figures it will be appreciated that alternatives are possible. For example while pixel block sizes of 8 8 for block motion estimation were described larger or smaller block sizes may be selected having an according effect on the processing time vs. performance trade off.

Alternatives to the criterion described above for determining that iterative pre compensation is complete whereby it is determined whether the sum of pixel error has changed by more than a threshold amount may be employed. For example the pixel blurring comparing error pixel blurring and weighting and combining may be performed iteratively a predetermined number of times as required to suit the particular implementation.

Although embodiments have been described those of skill in the art will appreciate that variations and modifications may be made without departing from the spirit and scope of the invention defined by the appended claims.

