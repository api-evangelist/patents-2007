---

title: Force feedback system including multi-tasking graphical host environment and interface device
abstract: A force feedback system provides components for use in a force feedback system including a host computer and a force feedback interface device. An architecture for a host computer allows multi-tasking application programs to interface with the force feedback device without conflicts. One embodiment of a force feedback device provides both relative position reporting and absolute position reporting to allow great flexibility. A different device embodiment provides relative position reporting device allowing maximum compatibility with existing software. Information such as ballistic parameters and screen size sent from the host to the force feedback device allow accurate mouse positions and cursor positions to be determined in the force feedback environment. Force feedback effects and structures are further described, such as events and enclosures.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07969288&OS=07969288&RS=07969288
owner: Immersion Corporation
number: 07969288
owner_city: San Jose
owner_country: US
publication_date: 20071102
---
This application is a continuation of pending U.S. application Ser. No. 11 504 131 filed Aug. 14 2006 which is a continuation of U.S. application Ser. No. 09 974 197 filed on Oct. 9 2001 now U.S. Pat. No. 7 168 042 which is a continuation of U.S. application Ser. No. 08 970 953 filed Nov. 14 1997 now U.S. Pat. No. 6 300 936 issued on Oct. 9 2001 and entitled Force Feedback System Including Multi Tasking Graphical Host Environment and Interface Device in the names of the same inventors and commonly owned herewith.

One or more embodiments were made with Government support under Contract Number F41624 96 C 6029 awarded by the Department of Defense. The Government has certain rights in one or more of these embodiments.

The present system and method relates generally to interface devices for allowing humans to interface with computer systems and more particularly to computer interface devices that allow the user to provide input to computer systems and provide force feedback to the user.

Computer systems are used extensively to implement many applications such as word processing data management simulations games and other tasks. A computer system typically displays a visual environment to a user on a display screen or other visual output device. Users can interact with the displayed environment to perform functions on the computer play a game experience a simulated environment use a computer aided design CAD system etc. One visual environment that is particularly common is a graphical user interface GUI . GUI s present visual images which describe various graphical metaphors of a program or operating system implemented on the computer. Common operating systems using GUI s include the Windows operating system from Microsoft Corporation and the MacOS operating system from Apple Computer Inc. The user typically moves a displayed user controlled graphical object such as a cursor or pointer across a computer screen and onto other displayed graphical objects or predefined screen regions and then inputs a command to execute a given selection or operation. The objects or regions targets can include for example icons windows pull down menus buttons and scroll bars. Most GUI s are currently 2 dimensional as displayed on a computer screen however three dimensional 3 D GUI s that present simulated 3 D environments on a 2 D screen can also be provided. Other programs or environments that may provide user controlled graphical objects such as a cursor or a view controlled by the user include graphical web pages or other environments offered on the World Wide Web of the Internet CAD programs video games virtual reality simulations etc.

The user interaction with and manipulation of the computer environment is achieved using any of a variety of types of human computer interface devices that are connected to the computer system controlling the displayed environment. In most systems the computer updates the environment in response to the user s manipulation of a user manipulatable physical object user object that is included in the interface device such as a mouse joystick etc. The computer provides feedback to the user utilizing the display screen. A computer mouse is a common user object used to interact with a GUI or other graphical environment. A mouse and other mouse type devices such as a track ball is typically used as a position control device in which displacement of the mouse in a planar workspace e.g. on a mouse pad is directly correlated to displacement of the user controlled graphical object such as a cursor displayed on the screen.

Force feedback interface devices allow a user to experience forces on the manipulated user object based on interactions and events within the displayed graphical environment. Typically computer controlled actuators are used to output forces on the user object in provided degrees of freedom to simulate various sensations such as an obstruction force when moving the cursor into a wall a damping force to resist motion of the cursor and a spring force to bias the cursor to move back toward a starting position of the spring. Force feedback devices can be implemented in many forms such as a joystick mouse steering wheel etc.

When implementing force feedback sensations in a GUI of an operating system several problems can arise. One problem is the use of force feedback when multiple application programs are simultaneously running in a multi tasking environment on the host computer. Most operating systems allow such multi tasking for example to allow a user to interact with one application while one or more applications are also running receiving data outputting data or performing other tasks. For example in the Windows operating system one application is the active application that typically displays an active window in the GUI. The user can manipulate the functions of the active application using the cursor. Other inactive applications are also running and may have inactive windows displayed in the GUI. The user can switch to a different application by clicking the cursor in an inactive window for example which causes the new application to be the active application and the formerly active application to become inactive.

Each application run by an operating system may have its own set of force sensations that it needs to command to the force feedback device. Thus one application may need to command spring force vibration and texture force sensations while a different application may need to command spring damper and jolt force sensations. The force feedback device typically cannot store all possible force sensations for each application running in the operating system so there is a problem of which force sensations the force feedback device should store and implement at any one time. In addition if two of the multi tasking applications command conflicting force sensations the force feedback device needs to choose one of the force sensations to output and there currently is no system or method of doing so.

A different problem occurs when using a force feedback device with a GUI. Traditional mouse controllers used with GUI s are relative position reporting devices i.e. they report only changes in position of the mouse to the host computer which the host computer uses to calculate a new position for the cursor on the screen. Many force feedback devices in contrast are typically absolute position reporting devices which report an absolute position of the cursor such as screen coordinates to the host computer. This is because the force feedback device needs to know the cursor position to accurately determine when forces are to be applied and to accurately calculate the forces. However it would be desirable in some instances to have a relative position reporting force feedback device since the host computer is standardized to receive and interpret relative positions at the most basic level. Furthermore such a relative device would permit the host computer to perform needed adjustments to cursor position such as ballistics calculations which modify cursor position based on mouse velocity to provide enhanced control. If the host computer performs such adjustments the force feedback device processors are relieved of computational burden. In addition some types of interface devices such as trackballs are better suited to relative position reporting since an absolute limited workspace is not easily defined for these devices.

Another problem occurs when force feedback is implemented with a GUI or other graphical environment and the graphical environment changes resolution or aspect ratio. For example if a resolution of 640 480 is being displayed by the host computer on a screen the force feedback device assumes that graphical objects in the GUI have a size proportional to screen dimensions and outputs forces accordingly. However when the resolution is changed the objects displayed on the screen change size in proportion to the screen dimensions. The force feedback device continues to check for conditions and generate forces as if the old resolution were active resulting in forces that do not correlate with displayed interactions on the screen. The aspect ratio of a display screen can also change e.g. when two screens are used to provide double the amount of displayed area in the GUI the aspect ratio doubles in one dimension. Using prior art force feedback devices forces can become distorted from such an aspect ratio change. For example a circle object displayed on the screen may have forces at its borders that feel like an ellipse to the user of the interface device since the aspect ratios of the screen and the mouse workspace are different.

An embodiment is directed to a force feedback system and architecture which allow control of a force feedback device in a multi tasking graphical host environment. In addition force feedback device embodiments are disclosed which provide for relative position reporting and absolute position reporting to the host computer.

More specifically a method interfaces a multi tasking graphical environment implemented on a host computer with a force feedback interface device coupled to the host computer where multiple application programs may run in the multi tasking environment. A context in created for association with each application program running in the multi tasking graphical environment. Force effect commands are received from the application programs where the force effect commands command the force feedback interface device to output a force effect specified by the command. The force effect commands are stored into the contexts. Each context is associated with one of the application programs running on the host computer and each force effect command is stored in a context associated with the application program that sent the force effect command. The force effect commands in the context of a particular application program are sent to the force feedback device when that particular application program is active in the multi tasking environment. Preferably the force effect commands in contexts of inactive application programs are not sent to the force feedback device. Thus only the active application program may command forces on the force feedback device.

When the application program becomes inactive and a new application program becomes active new force effect commands are sent to the force feedback device to replace the force effect commands of the formerly active application. Preferably a background application is provided which also provides force effects to the force feedback device and may output forces on the device even when not active. Events are also provided which allow a graphical action such as an interaction of the cursor in the graphical environment with another object to cause an event notification to be sent to the application program. Preferably only the active and background application programs may receive events.

In another aspect a force feedback device such as a force feedback mouse provides relative positions to a host when force feedback is not enabled and provides absolute positions to the host when force feedback is enabled. Sensor data is read and a position of a user manipulandum in a device frame is determined. A device delta position of the manipulandum includes the change in position of the manipulandum from a previous position and is reported to the host computer when the host computer is not enabled to receive an absolute screen position of the cursor from the force feedback device. When the host computer is enabled to receive the cursor screen position the screen position is determined from the device delta position and is reported to the host computer to allow said host computer to display the cursor in the screen frame at the screen position. A scaled position or ballistic position of the manipulandum related to the screen position of the cursor is used in determining a force to be output by actuators of the force feedback device. Preferably the scaled position is determined from the delta position and the screen position of said cursor is determined based on the scaled position. Ballistic parameters and screen size of the host display screen can be received from the host computer to determine the scaled position and screen position.

In another aspect a force feedback device such as a force feedback mouse provides only relative positions to a host computer. A local microprocessor on the force feedback device determines a delta position of the manipulandum that includes the change in position of the manipulandum from a previous position and the delta position is reported to the host. A screen position of the cursor is modeled from the delta position and the modeled screen position is used in the determination of a force output by actuators of the device. An actual screen position of the cursor is received from the host computer and the modeled screen position is corrected based on the actual screen position. Preferably the modeled screen position is corrected incrementally over time to smooth out the correction and reduce a perception of the error correction by the user. Preferably both the host and the device adjust the screen position using a ballistics algorithm. The host also preferably sends ballistic parameters and screen size to the device to allow accurate modeling of the screen position.

In another aspect a force feedback device reads sensor data and determines a position of a user manipulandum in a workspace of the device. A position of the manipulandum is reported to the host computer so that the host can display the cursor in a graphical environment. Information is received from the host computer describing a current screen size of the display screen of the host computer where the current screen size is included in a determination of the position reported to the host.

Also described herein are several force feedback sensations and structures including enclosures textures and grids. For example an enclosure is a rectangular or elliptical shape having walls that provide forces to an associated graphical object such as an icon or window. The enclosure includes walls where each wall may be associated with a force. In one embodiment a particular type of enclosure is provided to modify the forces of a different enclosure to prevent conflicts between the forces of overlapping or closely positioned enclosures.

The specification provides several embodiments of components in a force feedback system. The architecture on the host computer allows multi tasking application programs to interface with the force feedback device without conflicts. The force feedback device provides both relative position reporting and absolute position reporting to allow great flexibility. A relative position reporting device allows maximum compatibility with existing software. Information such as ballistic parameters and screen size sent from the host to the force feedback device allow accurate mouse positions and cursor positions to be determined in the force feedback environment.

These and other advantages will become apparent to those skilled in the art upon a reading of the following specification and a study of the several figures of the drawing.

User object or manipulandum is an physical object that is preferably grasped or gripped and manipulated by a user. By grasp it is meant that users may releasably engage a portion of the object in some fashion such as by hand with their fingertips etc. For example images are displayed and or modified on a display screen of the computer system in response to such manipulations. In the described embodiment user object is a mouse that is shaped so that a user s fingers or hand may comfortably grasp the object and move it in the provided degrees of freedom in physical space. For example a user can move mouse to correspondingly move a computer generated graphical object such as a cursor or other image in a graphical environment provided by computer . The available degrees of freedom in which mouse can be moved are determined from the interface described below. In addition mouse preferably includes one or more buttons to allow the user to provide additional commands to the computer system.

It will be appreciated that a great number of other types of user manipulable objects can be used with the method and apparatus in place of or in addition to mouse . For example such objects may include a sphere a puck a joystick cubical or other shaped hand grips a fingertip receptacle for receiving a finger or a stylus a flat planar surface like a plastic card having a rubberized contoured and or bumpy surface a handheld remote control used for controlling web pages or other devices or other objects. In one embodiment a small fingertip joystick can be provided where a small stick is moved in a small planar region with a user s fingertips. Spring forces can be provided by the actuators of the device to bias the stick or any type of joystick toward the center of the planar region to simulate a spring return on the joystick. Since fingertips are used output forces need not be as high a magnitude as in other embodiments. Also mouse can be provided with such a centering spring bias e.g. when used like a joystick in gaming applications.

Interface interfaces mechanical and electrical input and output between the mouse and host computer implementing the application program such as a GUI simulation or game environment. Interface provides multiple degrees of freedom to mouse in the preferred embodiment two linear planar degrees of freedom are provided to the mouse as shown by arrows . In other embodiments greater or fewer degrees of freedom can be provided as well as rotary degrees of freedom. For many applications mouse need only be moved in a very small workspace area.

In a preferred embodiment the user manipulates mouse in a planar workspace much like a traditional mouse and the position of mouse is translated into a form suitable for interpretation by position sensors of the interface . The sensors track the movement of the mouse in planar space and provide suitable electronic signals to an electronic portion of interface . The interface provides position information to host computer . In addition host computer and or interface provide force feedback signals to actuators coupled to interface and the actuators generate forces on members of the mechanical portion of the interface to provide forces on mouse in provided or desired degrees of freedom. The user experiences the forces generated on the mouse as realistic simulations of force sensations such as jolts springs textures enclosures circles ellipses grids vibrations barrier forces and the like.

The electronic portion of interface may couple the mechanical portion of the interface to the host computer . The electronic portion is preferably included within the housing of the interface or alternatively the electronic portion may be included in host computer or as a separate unit with its own housing. More particularly interface includes a local microprocessor distinct and separate from any microprocessors in the host computer to control force feedback on mouse independently of the host computer as well as sensor and actuator interfaces that convert electrical signals to appropriate forms usable by the mechanical portion of interface and host computer .

For example a rigid surface is generated on computer screen and a computer object e.g. cursor controlled by the user collides with the surface. In a preferred embodiment high level host commands can be used to provide the various forces associated with the rigid surface. The local control mode using a local microprocessor in interface can be helpful in increasing the response time for forces applied to the user object which is essential in creating realistic and accurate force feedback. For example it is preferable that host computer send a spatial representation to the local microprocessor which is data describing the locations of some or all the graphical objects displayed in a GUI or other graphical environment which are associated with forces and the types characteristics of these graphical objects. The microprocessor can store such a spatial representation in local memory and thus will be able to determine interactions between the user object and graphical objects such as the rigid surface independently of the host computer. In addition the microprocessor can be provided with the necessary instructions or data to check sensor readings determine cursor and target positions and determine output forces independently of host computer . The host could implement program functions such as displaying images when appropriate and synchronization commands can be communicated between the microprocessor and host to correlate the microprocessor and host processes. Such commands and related functionality is discussed in greater detail below. Alternatively the computer can directly send force feedback signals to the interface to generate forces on mouse . A suitable embodiment of the electrical portion of interface is described in detail with reference to .

The interface can be coupled to the computer by a bus which communicates signals between interface and computer and also in the preferred embodiment provides power to the interface e.g. when bus includes a USB interface . In other embodiments signals can be sent between interface and computer by wireless transmission reception. In preferred embodiments the interface serves as an input output I O device for the computer . The interface can also receive inputs from other input devices or controls that are associated with mouse system and can relay those inputs to computer . For example commands sent by the user activating a button on mouse can be relayed to computer by interface to implement a command or cause the computer to output a command to the interface .

Host computer is preferably a personal computer or workstation such as an IBM PC compatible computer or Macintosh personal computer or a SUN or Silicon Graphics workstation. For example the computer can operate under the Windows or MS DOS operating system in conformance with an IBM PC AT standard. Alternatively host computer system can be one of a variety of home video game systems commonly connected to a television set such as systems available from Nintendo Sega or Sony. In other embodiments host computer system can be a set top box which can be used for example to provide interactive television functions to users or a network or internet computer which allows users to interact with a local or global network using standard connections and protocols such as used for the Internet and World Wide Web. Host computer preferably includes a host microprocessor random access memory RAM read only memory ROM input output I O circuitry and other components of computers well known to those skilled in the art.

Host computer preferably implements one or more application programs applications with which a user is interacting via mouse and other peripherals if appropriate and which can include force feedback functionality. For example the host application programs can include a simulation video game Web page or browser that implements HTML or VRML instructions word processor drawing program spreadsheet scientific analysis program or other application program that utilizes input of mouse and outputs force feedback commands to the mouse . Typically an operating systems such as Windows MS DOS MacOS Unix is also running on the host computer and preferably includes its own force feedback functionality. In one preferred embodiment the operating system and application programs utilize a graphical user interface GUI to present options to a user display data and images and receive input from the user. In the preferred embodiment multiple applications can run simultaneously in a multitasking environment of the host computer as is detailed below. Herein computer may be referred as displaying graphical objects or computer objects. These objects are not physical objects but are logical software unit collections of data and or procedures that may be displayed as images by computer on display screen as is well known to those skilled in the art. A displayed cursor or a simulated cockpit of an aircraft might be considered a graphical object. The host application program checks for input signals received from the electronics and sensors of interface and outputs force values and or commands to be converted into forces on mouse . Suitable software drivers which interface such simulation software with computer input output I O devices are available from Immersion Human Interface Corporation of San Jose Calif.

Display device can be included in host computer and can be a standard display screen LCD CRT etc. 3 D goggles or any other visual output device. Typically the host application provides images to be displayed on display device and or other feedback such as auditory signals. For example display screen can display images from a GUI. Images describing a moving first person point of view can be displayed as in a virtual reality game. Or images describing a third person perspective of objects backgrounds etc. can be displayed. Alternatively images from a simulation such as a medical simulation can be displayed e.g. images of tissue and a representation of a manipulated user object moving through the tissue etc.

There are two primary control paradigms of operation for mouse system position control and rate control. Position control is the more typical control paradigm for mouse and similar controllers and refers to a mapping of mouse in which displacement of the mouse in physical space directly dictates displacement of a graphical object. Under a position control mapping the computer object does not move unless the user object is in motion. Also ballistics or other non linear adjustments to cursor position can be used in which for example slow motions of the mouse have a different scaling factor for cursor movement than fast motions of the mouse to allow more control of short cursor movements. Several different ways of implementing ballistics and other control adjustments in a force feedback device are described in U.S. Pat. No. 6 252 579 and these adjustments can be used in mouse system if desired.

As shown in the host computer may have its own screen frame or host frame which is displayed on the display screen . In contrast the mouse has its own device frame or local frame in which the mouse is moved. In a position control paradigm the position or change in position of a user controlled graphical object such as a cursor in host frame corresponds to a position or change in position of the mouse in the local frame .

Rate control is also used as a control paradigm. This refers to a mapping in which the displacement of the mouse is abstractly mapped to motion of a computer object under control. There is not a direct physical mapping between physical object mouse motion and computer object motion. Thus most rate control paradigms are fundamentally different from position control in that the user object can be held steady at a given position but the controlled computer object is in motion at a commanded or given velocity while the position control paradigm only allows the controlled computer object to be in motion if the user object is in motion.

The mouse interface system is useful for both position control isotonic tasks and rate control isometric tasks. For example as a traditional mouse the position of mouse in its local frame workspace can be directly mapped to a position of a cursor in host frame on display screen in a position control paradigm. Alternatively the displacement of mouse in a particular direction against an opposing output force can command rate control tasks in an isometric mode. An implementation that provides both isotonic and isometric functionality for a force feedback controller and which is very suitable for the interface device is described in U.S. Pat. No. 5 825 308.

Mouse is preferably supported and suspended above a grounded pad by the mechanical portion of interface described below. Pad or similar surface is supported by grounded surface . Pad or alternatively grounded surface provides additional support for the mouse and relieve stress on the mechanical portion of interface . In addition a wheel roller Teflon pad or other device can be used to support the mouse.

Mouse can be used for example to control a computer generated graphical object such as a cursor displayed in a graphical computer environment such as a GUI. The user can move the mouse in 2D planar workspace to move the cursor to graphical objects in the GUI or perform other tasks. In other graphical environments such as a virtual reality video game a user can be controlling a computer player or vehicle in the virtual environment by manipulating the mouse . The computer system tracks the position of the mouse with sensors as the user moves it. The computer system may also provide force feedback commands to the mouse for example when the user moves the graphical object against a generated surface such as an edge of a window a virtual wall etc. It thus appears and feels to the user that the mouse and the graphical object are contacting real surfaces.

The mouse system may also include an indexing function or indexing mode which allows the user to redefine the offset between the positions of the mouse in the local frame and a user controlled graphical object such as a cursor in the host frame displayed by host computer . In one implementation the user may reposition the mouse without providing any input to the host computer thus allowing the user to redefine the offset between the object s position and the cursor s position. Such indexing is achieved through an input device such as a button switches sensors or other input devices. As long as the indexing device is activated the mouse is in indexing mode when the button is released or indexing mode otherwise exited the position of the cursor is again controlled by the mouse . A hand weight switch can also be provided which inherently causes indexing when the user removes hand or finger weight from mouse . In one embodiment the functionality of a safety switch and the indexing mode are integrated into one input device as described in greater detail in U.S. Pat. Nos. 5 825 308 and 6 100 874.

A different way to allow indexing is to provide a combined position control and rate control device which allows different forms of control of the cursor depending on the position of the mouse in its workspace. This embodiment is described in greater detail below and in U.S. Pat. No. 6 252 579.

Ground member of the linkage is a base for the support of the linkage and is coupled to or resting on a ground surface . The ground member in is shown as a plate or base that extends under mouse . The members of linkage are rotatably coupled to one another through the use of rotatable pivots or bearing assemblies all referred to as bearings herein. Base member is rotatably coupled to ground member by a grounded bearing and can rotate about an axis A. Link member is rotatably coupled to base member by bearing and can rotate about a floating axis B and base member is rotatably coupled to ground member by bearing and can rotate about axis A. Link member is rotatably coupled to base member by bearing and can rotate about floating axis C and link member is also rotatably coupled to link member by bearing such that link member and link member may rotate relative to each other about floating axis D.

Linkage is formed as a five member closed loop chain. Each member in the chain is rotatably coupled to two other members of the chain to provide mouse with two degrees of freedom i.e. mouse can be moved within a planar x y workspace. Mouse is coupled to link members and by rotary bearing . and may rotate at least partially about axis D. A pad or other support can be provided under mouse to help support the mouse. Transducer system is used to sense the position of mouse in its workspace and to generate forces on the mouse . Transducer system preferably includes sensors and actuators . The sensors collectively sense the movement of the mouse in the provided degrees of freedom and send appropriate signals to the electronic portion of interface . Sensor senses movement of link member about axis A and sensor senses movement of base member about axis A. These sensed positions about axis A allow the determination of the position of mouse using known constants such as the lengths of the members of linkage and using well known coordinate transformations.

Sensors are in the described embodiment grounded optical encoders that sense the intermittent blockage of an emitted beam. A grounded emitter detector portion includes an emitter that emits a beam which is detected by a grounded detector. A moving encoder disk portion or arc is provided at the end of members and which each block the beam for the respective sensor in predetermined spatial increments and allows a processor to determine the position of the arc and thus the members and by counting the spatial increments. Also a velocity of members and based on the speed of passing encoder marks can also be determined. In one embodiment dedicated electronics such as a haptic accelerator may determine velocity and or acceleration as disclosed in U.S. Pat. No. 5 999 168. Sensors are described in greater detail in U.S. Pat. Nos. 6 100 874 and 6 166 723.

Transducer system also preferably includes actuators to transmit forces to mouse in space i.e. in two or more degrees of freedom of the user object. The bottom housing plate of actuator is rigidly coupled to ground member or grounded surface and a moving portion of actuator preferably a coil is integrated into the base member . The actuator transmits rotational forces to base member about axis A. The housing of the grounded portion of actuator is rigidly coupled to ground member or ground surface through the grounded housing of actuator and a moving portion preferably a coil of actuator is integrated into base member . Actuator transmits rotational forces to link member about axis A. The combination of these rotational forces about axis A allows forces to be transmitted to mouse in all directions in the planar workspace provided by linkage through the rotational interaction of the members of linkage . The integration of the coils into the base members and is advantageous and is discussed below. The operation of the electromagnetic actuators is described in greater detail is U.S. Pat. Nos. 6 100 874 and 6 166 723. In other embodiments other types of actuators can be used.

As explained with reference to computer is preferably a personal computer workstation video game console or other computing or display device. Host computer system commonly includes a host microprocessor random access memory RAM read only memory ROM input output I O electronics a clock a display device and an audio output device . Host microprocessor can include a variety of available microprocessors from Intel AMD Motorola or other manufacturers. Microprocessor can be single microprocessor chip or can include multiple primary and or co processors. Microprocessor preferably retrieves and stores instructions and other necessary data from RAM and ROM as is well known to those skilled in the art. In the described embodiment host computer system can receive sensor data or a sensor signal via a bus from sensors of system and other information. Microprocessor can receive data from bus using I O electronics and can use I O electronics to control other peripheral devices. Host computer system can also output commands to interface device via bus to cause force feedback for the interface system .

Clock is a standard clock crystal or equivalent component used by host computer to provide timing to electrical signals used by host microprocessor and other components of the computer system . Display device is described with reference to . Audio output device such as speakers can be coupled to host microprocessor via amplifiers filters and other circuitry well known to those skilled in the art. Host processor outputs signals to speakers to provide sound output to the user when an audio event occurs during the implementation of the host application program. Other types of peripherals can also be coupled to host processor such as storage devices hard disk drive CD ROM drive floppy disk drive etc. printers and other input and output devices.

Electronic interface is coupled to host computer system by a bi directional bus . The bi directional bus sends signals in either direction between host computer system and the interface device. Bus can be a serial interface bus providing data according to a serial communication protocol a parallel bus using a parallel protocol or other types of buses. An interface port of host computer system such as an RS232 serial interface port connects bus to host computer system . In another embodiment an additional bus can be included to communicate between host computer system and interface device . One preferred serial interface bus used is the Universal Serial Bus USB . The USB standard provides a relatively high speed serial interface that can provide force feedback signals with a high degree of realism. USB can also source power to drive actuators and other devices. In addition the USB standard includes timing data that is encoded along with differential data. Alternatively Firewire also called can be used as bus or the bus can be between an interface card in the host computer where the interface card holds components of device such as microprocessor .

Electronic interface includes a local microprocessor local clock local memory sensor interface and actuator interface . Interface may also include additional electronic components for communicating via standard protocols on bus . In various embodiments electronic interface can be included in mechanical portion in host computer or in its own separate housing. Different components of interface can be included in portion or host computer if desired.

Local microprocessor preferably coupled to bus and may be closely linked to mechanical portion to allow quick communication with other components of the interface device. Processor is considered local to interface device where local herein refers to processor being a separate microprocessor from any processors in host computer . Local also preferably refers to processor being dedicated to force feedback and sensor I O of the interface system and being closely coupled to sensors and actuators of the mechanical portion such as within the housing of or in a housing coupled closely to portion . Microprocessor can be provided with software instructions to wait for commands or requests from computer host parse decode the command or request and handle control input and output signals according to the command or request. In addition processor preferably operates independently of host computer by reading sensor signals and calculating appropriate forces from those sensor signals time signals and force processes selected in accordance with a host command and output appropriate control signals to the actuators. A suitable microprocessor for use as local microprocessor includes the 8 930AX by Intel or alternatively the MC68HC711E9 by Motorola or the PIC 16C74 by Microchip for example. Microprocessor can include one microprocessor chip or multiple processors and or co processor chips. In other embodiments microprocessor can include digital signal processor DSP functionality.

For example in one host controlled embodiment that utilizes microprocessor host computer can provide low level force commands over bus which microprocessor directly transmits to the actuators. In a different local control embodiment host computer system provides high level supervisory commands to microprocessor over bus and microprocessor manages low level force control loops to sensors and actuators in accordance with the high level commands and independently of the host computer . In the local control embodiment the microprocessor can process inputted sensor signals to determine appropriate output actuator signals by following the instructions of a force process that may be stored in local memory and includes calculation instructions formulas force magnitudes or other data. The force process can command distinct force sensations such as vibrations textures jolts or even simulated interactions between displayed objects. An enclosure host command can also be provided which causes the microprocessor to define a box like enclosure in a graphical environment where the enclosure has sides characterized by wall and texture forces as described in U.S. Pat. No. 6 100 874. The host can send the local processor a spatial layout of objects in the graphical environment so that the microprocessor has a mapping of locations of graphical objects like enclosures and can determine interactions with the cursor locally. Force feedback used in graphical environments is described in greater detail in U.S. Pat. Nos. 6 219 032 5 825 308 and 6 252 579.

Sensor signals used by microprocessor are also reported to host computer system which updates a host application program and outputs force control signals as appropriate. For example if the user moves mouse the computer system receives position and or other signals indicating this movement and can move a displayed cursor in response. These embodiments are described in greater detail in U.S. Pat. No. 5 734 373. In an alternate embodiment no local microprocessor is included in interface system and host computer directly controls and processes all signals to and from the interface and mechanical portion .

A local clock can be coupled to the microprocessor to provide timing data similar to system clock of host computer the timing data might be required for example to compute forces output by actuators e.g. forces dependent on calculated velocities or other time dependent factors . In alternate embodiments using the USB communication interface timing data for microprocessor can be retrieved from the USB interface. Local memory such as RAM and or ROM is preferably coupled to microprocessor in interface to store instructions for microprocessor and store temporary and other data. Microprocessor may also store calibration parameters in a local memory such as an EEPROM. As described above link or member lengths or manufacturing variations and or variations in coil winding or magnet strength can be stored. Memory may be used to store the state of the force feedback device including a reference position current control mode or configuration etc.

Sensor interface may optionally be included in electronic interface to convert sensor signals to signals that can be interpreted by the microprocessor and or host computer system . For example sensor interface can receive signals from a digital sensor such as an encoder and convert the signals into a digital binary number representing the position of a member of mechanical apparatus . An analog to digital converter ADC in sensor interface can convert a received analog signal to a digital signal for microprocessor and or host computer . Such circuits or equivalent circuits are well known to those skilled in the art. Alternately microprocessor or host can perform these interface functions. Other types of interface circuitry can also be used e.g. the electronic interface described in U.S. Pat. No. 5 576 727.

Actuator interface can be optionally connected between the actuators and microprocessor . Interface converts signals from microprocessor into signals appropriate to drive the actuators. Interface can include power amplifiers switches digital to analog controllers DACs and other components. Such interfaces are well known to those skilled in the art. In alternate embodiments interface circuitry can be provided within microprocessor or in the actuators.

In the described embodiment power is supplied to the actuators and any other components as required by the USB. Since the electromagnetic actuators of the described embodiment have a limited physical range and need only output for example about 3 ounces of force to create realistic force sensations on the user very little power is needed. For example one way to draw additional power from the USB is to configure device II to appear as more than one peripheral to host computer for example each provided degree of freedom of mouse can be configured as a different peripheral and receive its own allocation of power. Alternatively power from the USB can be stored and regulated by device and thus used when needed to drive actuators . For example power can be stored over time and then immediately dissipated to provide a jolt force to the user object . A battery or a capacitor circuit for example can store energy and discharge or dissipate the energy when power is required by the system and or when enough power has been stored. Alternatively a power supply can optionally be coupled to actuator interface and or actuators to provide electrical power. The power storage embodiment described above using a battery or capacitor circuit can also be used in non USB embodiments to allow a smaller power supply to be used.

Mechanical portion is coupled to electronic portion and preferably includes sensors actuators and linkage . These components are described in detail above. Sensors sense the position motion and or other characteristics of mouse along one or more degrees of freedom and provide signals to microprocessor including information representative of those characteristics. Typically a sensor is provided for each degree of freedom along which mouse can be moved or a single compound sensor can be used for multiple degrees of freedom. Example of sensors suitable for embodiments described herein are rotary optical encoders as described above linear optical encoders analog sensors such as potentiometers or non contact sensors such as Hall effect magnetic sensors or an optical sensor such as a lateral effect photo diode having an emitter detector pair. In addition velocity sensors e.g. tachometers and or acceleration sensors e.g. accelerometers for measuring acceleration can be used. Furthermore either relative or absolute sensors can be employed.

Actuators transmit forces to mouse in one or more directions along one or more degrees of freedom in response to signals output by microprocessor and or host computer i.e. they are computer controlled. Typically an actuator is provided for each degree of freedom along which forces are desired to be transmitted. Actuators can be the electromagnetic actuators described above or can be other active actuators such as linear current control motors stepper motors pneumatic hydraulic active actuators a torquer motor with limited angular range or passive actuators such as magnetic particle brakes friction brakes or pneumatic hydraulic passive actuators passive damper elements an electrorheological fluid actuator a magnetorheological fluid actuator. In addition in voice coil embodiments multiple wire coils can be provided where some of the coils can be used to provide back EMF and damping forces. In some embodiments all or some of sensors and actuators can be included together as a sensor actuator pair transducer.

Mechanism is preferably the five member linkage described above but can also be one of several types of mechanisms. For example mechanisms disclosed in U.S. Pat. Nos. 5 731 804 5 767 839 5 721 566 5 805 140 5 691 898 6 028 593 6 024 576 and 5 828 197 can be included. Mouse can alternatively be a puck joystick or other device or article coupled to linkage as described above.

Other input devices can optionally be included in system and send input signals to microprocessor and or host computer . Such input devices can include buttons such as buttons on mouse used to supplement the input from the user to a GUI game simulation etc. Also dials switches voice recognition hardware with software implemented by host or other input mechanisms can be used.

Safety or deadman switch is preferably included in interface device to provide a mechanism to allow a user to deactivate actuators for safety reasons. In the preferred embodiment the user must continually close safety switch during manipulation of mouse to activate the actuators . If at any time the safety switch is deactivated opened the actuators are deactivated while the safety switch is open. For example one embodiment of safety switch is a capacitive contact sensor that detects when the user is contacting mouse . Alternatively a mechanical switch electrostatic contact switch or optical switch located on mouse or on a convenient surface of a housing can be used. A z axis force sensor piezo electric sensors force sensitive resistors or strain gauges can also be used. The state of the safety switch can be sent to the microprocessor or host . In one embodiment mouse includes a hand weight safety switch. The safety switch preferably deactivates any generated forces on the mouse when the mouse is not in use and or when the user desires to deactivate output forces.

In some embodiments of interface system multiple mechanical apparatuses and or electronic interfaces can be coupled to a single host computer system through bus or multiple buses so that multiple users can simultaneously interface with the host application program in a multi player game or simulation for example . In addition multiple players can interact in the host application program with multiple interface systems using networked host computers as is well known to those skilled in the art. Also the interface device can be coupled to multiple host computers for example a local host computer can display images based on data received from a remote host computer coupled to the local host through a network.

The host computer interacts with interface device using a number of different levels of controllers. These controllers are preferably implemented in software e.g. program instructions or code and such is the embodiment described herein however all or part of the controllers may also be implemented in hardware where the conversion of functionality of the software to hardware is well known to those skilled in the art.

The architecture described herein is oriented towards providing force feedback functionality to a host computer connected to a force feedback interface device where multiple applications may be running simultaneously on the host computer. Each application program may have its own set of force sensations to output to the device since the device cannot implement all force sensations at any one time due to cost and hardware constraints the forces commanded by each application must be organized by the architecture to take these limitations into account.

A master application also is running on host computer and is also known as the background force feedback application. This application is preferably a general purpose program that always runs inactively in the operating system and whose set of commanded forces are always available to be output and controlled on the interface device and or other devices. For example a preferred embodiment of the master application is a desktop control panel for force feedback. An example of such a program is illustrated in . A mouse properties dialog box can be displayed which allows the user to specify force sensations that are assigned to specified object types in the displayed graphical environment e.g. a graphical user interface GUI . For example the assigned force sensation can be a vibration force having a specified frequency magnitude duration etc. a single pop or jolt at a specified time and duration a tick that is a small jolt that is output based on movement of the user object cursor or at regular intervals during an activity such as scrolling a damping force condition that causes a resistance to mouse motion depending on the velocity of the user object in its degrees of freedom or the cursor in the host frame on screen or a spring that resists motion of the user object based on distance to an origin of the spring. Other types of forces are possible as well and are described in greater detail in U.S. Pat. Nos. 5 734 373 and 6 020 876.

The force sensations specified by the user for a user interface object type in dialog box will be output by the force feedback device by default unless a different foreground application program deactivates the force sensations or replaces a force sensation with its own. For example the user can specify that menu objects in a GUI have a snap force associated with them by moving the slider to the right where the slider scale specifies the magnitude of the snap force. Thus when the user moves the cursor in the GUI over a menu item in a menu during normal operation the user will feel a snap force i.e. a force that draws the cursor user object toward the center of the menu item to allow easier menu selection. This snap force is preferably applied to all menus of all running application programs in the GUI since it is a background force effect. If the foreground application program has its own force sensations which define that application s menus to have a jolt instead of a snap then the jolt will be superimposed on the snap unless the active application program deactivates the background force s . In general a particular active application program can only command forces to objects in its own windows e.g. that application s own menus scrollbars icons window borders etc.

A user can specify multiple background force sensations for each user interface object . This allows the multiple force sensations to be superimposed on each other unless the application program overrides one or more of the superimposed forces. Thus a user can assign a damper force sensation and a ticks force sensation to scrollbars in box and all scrollbars will have these forces associated with them unless overriden by the foreground application program.

Other controls in box include a device gain to set the percentage of maximum magnitude for all the forces for items and selections to allow force feedback functionality on the host computer. Advanced button when selected preferably displays an additional window of options with which the user can customize the force sensations . For example a user can specify positive or negative damping where negative damping feels like moving on ice or unidirectional or bidirectional dampers. The user can specify the spacing or orientation of snap points on a grid the envelope parameters for a vibration or the parameters for a snap force e.g. the snap force is defined as an enclosure and the user can customize the enclosure to turn off forces on one wall turn on inner walls etc. Such options can be similar to those described in U.S. Pat. Nos. 6 147 674 and 6 169 540 or those features provided in the force designing application I Force Studio from Immersion Human Interface Corporation. Other application programs can also be used as the background or master application program .

Referring back to application programs and communicate with an force feedback Application Programming Interface API which is resident in the host computer s memory and which allows a given application program to communicate with lower level drivers and other force feedback functions. For example in the Windows operating system API s are commonly used to allow a developer of an application program to call functions at a high level for use with the application program and not worry about the low level details of actually implementing the function.

The API includes a set of software objects that can be called to command a force feedback interface device . Objects are a set of instructions and or data which can be called by a pointer and or an identifier and parameters to implement a specified function. For example three types of objects are provided in one preferred API implementation interface objects device objects and effect objects. Each of these objects makes use of one or more force feedback device drivers which are implemented as objects in the API .

Interface objects represent the API at the highest level. An application program which is referred to as a client of the API can create an interface object to access lower level objects and code that implement force feedback device functionality. For example the interface object allows an application program to enumerate and receive information about individual devices and create lower level objects for each device used by the application program.

Device objects each represent a physical force feedback device or other compatible device or peripheral in communication with the host computer . For example the force feedback mouse device would be represented as a single device object. The device objects access the set of force feedback routines to receive information about an associated physical device set the properties of the physical device register event handlers if events are implemented on the host and to create effect objects.

Effect objects each represent a force feedback effect defined by the application program to be output one or more times on a force feedback device. The effect objects access the set of force feedback routines to download force effects to the force feedback device start and stop the output of effects by the force feedback device and modify the parameters of the effects. Event objects can also be created to define events as described in greater detail below.

A force effect as referred to herein is a single defined force or series of forces that may be commanded within the API. The effect typically has a name to identify it and one or more parameters to modify or customize the force as necessary. For example several types of force effects have been defined including vibrations enclosures grids textures walls dampers snap sensations vibrations circles ellipses etc. For example a vibration force effect may have parameters of duration frequency magnitude and direction. The force sensations defined in dialog box can be force effects however more generally force sensations can be made up of one or more force effects. Force effects in turn are made up of one or more basic force prototypes such as springs dampers and vector forces.

In the preferred embodiment an application program interacts with the API by first receiving a pointer to a resident force feedback interface for example a preferred interface includes procedures provided in the Component Object Model COM interface of Microsoft Windows an object oriented interface. Using the force feedback interface the application program enumerates the force feedback devices on the computer system . The application program selects a desired one of the force feedback devices and creates a device object associated with that device. Using the force feedback interface the application then acquires the device sets the device up with setup and default parameters and creates effect objects and event objects at times designated by the developer of the application. At appropriate times the application program will command the start stop or pause of the playback of force effects by accessing the appropriate interface instructions associated with the desired effect. The API indicates to the context driver to create a context i.e. effect set for an application program and sends effects and events to be associated with that context. A context is a group or set of effects and events that are associated with a particular application program.

Context driver receives effect set and device manipulation data from the API . The context driver is provided at a level below the API to organize contexts for applications and running on the host computer. In the preferred embodiment the effects and events in a context are not known to the application program itself rather the context driver creates a context internally for an application. Thus an application commands that a particular force sensation be output in response to different interactions or occurrences e.g. an interaction of a cursor with an object or region or the output of a force based on other criteria time received data random event etc. . The API sends the commanded effect s to the context driver and the context driver stores the effects in the created context for that application program.

Since each application may have a completely different set of force effects to output each context is associated with its particular application program. In the preferred embodiment there are two types of contexts foreground contexts and background or primary contexts. A foreground context is associated with the application program or that is active in the operating system. A background context includes the effects and events for the master application program e.g. the effects and events necessary to implement those force sensations selected by the user in the dialog box of to be associated with particular GUI objects. In addition a global context can be provided which includes common effects almost always used by application programs e.g. a pop force and which can automatically be downloaded to the force feedback device. Effects in the global context need not be stored in individual contexts for particular application programs and need not be sent to the force feedback device thus saving memory on the device.

When an application program is first executed by the host computer and loaded into memory if the application is able to command a force feedback device the application will query for the API . Once communication is established the API will contact the context driver to create an entry location for a context set for the initiated application program.

The context driver receives individual effects and events as they are created by the application program using API and stores the effects and events in a context list storing each context in a different storage location in the host s memory or on some other type of storage device. An active or inactive application program can create a context and have it stored but only the active application s context will be sent to the force feedback device. The context driver can examine an identifier in a created effect or event to determine which application program is associated with it and thus store the effect or event in the proper memory location. The context driver sets pointers to the contexts and to the effects and events in the contexts to access them. An effect can be created initially before any forces are commanded or the effect can be created and then immediately commanded to be output to the force feedback device. Each context also preferably includes an entry into a device state structure. This entry governs the gain or force level for all effects in the context. For example all the forces in the context can be cut to 50 of full magnitude by storing a value of 50 in the device state structure. One of the contexts stored in list is a primary context which is the list of effects and events used by the master application or background application.

At a later time after the context driver has stored the contexts in list an application program may send a command to the API to output or start a particular force sensation. The API checks whether the application program is active or in the background primary if not the API ignores the command. Alternatively the commands from an inactive foreground application can be stored and then sent to the device when the application becomes active. If the application is active or background the API sends the start information to the context driver indicating the application program that commanded the force and the particular force effects to be commanded. The context driver then associates the commanding application program with a context in list and sends the effects from the context to the force feedback device if not previously sent . For example if a context for a particular application program includes a spring effect a damper effect and a vibration effect and the application program commands the vibration to be output then the context driver selects the vibration effects to be output to the device. The data describing this effect is then output by the context driver . Similarly the application program can send a command to stop particular force effects to pause the effects to get the status information of an effect or to destroy an effect.

A context is therefore only allowed to exert forces with the force feedback device when that context is active i.e. is associated with an active application program or the background application. In the described embodiment only one foreground context can be active at any given time. Any number of background contexts may be simultaneously active however there may be a device limit on the number of background contexts that may be created. For example the mouse device may only allow one background context to be created at any one time which is the preferred embodiment. Thus if an inactive not in focus foreground application program commands a force to be output the API will ignore the command after determining that the commanding application is not active or the command will only be sent to the device when the application becomes active .

If the active application program becomes inactive i.e. loses foreground or is removed from the host s memory and a different application becomes active then the API indicates this to the context driver which then deactivates the context associated with that application program and loads the effects from the new active context to the force feedback device . Likewise when the original application program again becomes active the API tells the context driver to activate the associated context and load the appropriate effects to the force feedback device.

Device manipulation data is also received by context driver . Data is used to set a global device state on the force feedback device as described below and this information is passed unmodified to the translation layer.

Translation layer manages the sending of effects to the device receives information from the device to the host and maintains a representation of the memory of device . Layer receives an individual effect for the active or background application program and device manipulation data sent by the context driver . The translation layer receives the effect from an context list of individual effects list represents a context . A different context list is provided in each context of list . Each effect in list is a force that is to be output on the mouse by the force feedback device . Then the effects are to be sent to the device when commanded by the application they are selected and copies are output to the device. Preferably each effect is output by the translation layer as soon as possible in the order of receiving the effects. Each effect stored in list as examined by the translation layer is available on force feedback device i.e. the local microprocessor should recognize the effect and be able to output the effect either immediately or when conditions dictate. The size of the list should be the same or smaller than the available memory for such a list in the force feedback device so that the translation layer knows when the memory of the force feedback device is full. If full the translation layer can delay the output of additional effects until enough memory space is available.

When an active application becomes inactive the translation layer is instructed by the context driver to unload the effects of the context of the previous active application from the force feedback device receives the effects from the now active application and loads those effects to the force feedback device the effects for the background primary application are preferably never unloaded .

The translation layer also preferably handles events. For example when a screen position is received from the device the translation layer can check whether any of the conditions triggers of the active events are met. If so a message is sent which eventually reaches the associated active or background application program as described in greater detail below. In alternate embodiments the microprocessor on device can check for events and send event notifications through translation layer up to the application program. However this can be undesirable in some embodiments since the event notification is provided at a much slower rate than the force control loop of the microprocessor .

The translation layer also can store a device state in memory. Device manipulation data from the active application determines the device state. This is the state that the active application program wishes to impose on the device if any. For example an overall condition can be stored such as an enable or disable for all forces so that if all forces are disabled no forces will be output by the device. An overall gain can also be set to limit all output force magnitudes to a desired level or percentage of maximum output.

The translation layer outputs device messages to the device by way of the next layer. Messages may include force effects to be output and or any other information such as device identification numbers or instructions from the context driver for an effect start stop pause reset etc. The translation layer outputs messages in a form the device can understand.

Device communication driver communicates directly with the force feedback device. Driver receives the device messages from translation layer and directly transmits the messages to the force feedback device over bus e.g. a USB. The driver is implemented in the preferred embodiment as a standard driver to communicate over such a serial port of host computer . Other types of drivers and communication interfaces can be used in other embodiments.

The host computer and API handle a variety of processes controlling force feedback device . The preferred functionality of much of the application programs and API is described in greater detail in U.S. Pat. Nos. 6 147 674 6 078 308 and 6 252 579. Different effects and events are used in force feedback systems are described below.

Effects are standardized forces that are determined according to a predefined characterization which may include force algorithms stored force magnitudes functions of space and or time a history of stored motion values of the mouse or other steps or parameters. An effect may be based on time and be independent of the position of the mouse or may be spatially determined based on mouse position velocity acceleration or any combination of these. Preferably the device includes a number of effects standardized in its memory which it can implement if the effects are within the active or background context. When the device is determining output forces based on effects the microprocessor checks if the effect is active calculates the raw contribution to the output force from the effect applies an envelope scaling detailed in U.S. Pat. No. 5 959 613 and sums the scaled contribution to the total sum of forces contributed to by all effects currently being output. In determining the total sum the device preferably combines all constant forces e.g. conditions and time based forces and limits the constant force sum to a predetermined magnitude then combines all dynamic forces and limits the dynamic force sum to a predetermined magnitude. Dynamic forces are detailed in U.S. Pat. No. 6 147 674. The two sums are then added together and the total force sum is output by the actuators of the device . Alternatively all forces can be treated the same and summed together.

The enclosure object also preferably has a number of other parameters that further define the enclosure. For example parameters may be specified separately for each wall to define such characteristics as the magnitude of the resistive or attractive forces which of the eight walls are assigned forces which walls provide clipping the saturation level of the force applied in association with a wall and the interior force which can be specified as any force effect such as a damping texture or vibration force that is applied while the cursor is inside the enclosure.

Enclosures are quite useful for adding forces to web links on a web page viewed in a web browser such as Netscape Communicator by Netscape Communications or Internet Explorer by Microsoft. The inner wall forces of can be used on a web link displayed on a web page to draw the cursor user object onto the link thus allowing a user to more easily determine which images are links on the page. Other types of forces can also serve this function. Also an inside condition such as a texture can be provided for the web link to cause some links to feel rough some smooth some bumpy etc.

In another example of an enclosure a circular or elliptical shape is defined. For example an elliptical enclosure can include only one wall that is curved all the way around the ellipse. The inner and outer walls can still be provided as well as the rest of the other parameters. The dimensions of the ellipse circle can be provided as X and Y widths. Alternatively an eccentricity parameter can define the eccentricity of the ellipse.

One problem with enclosures is that often the forces of the enclosure are desirable at one point in time but not at a different point in time. For example a button having an enclosure has a attractive force associated with the enclosure to assist the user in acquiring the button with the cursor. However once the cursor is positioned inside the enclosure the user may want to be able to freely move the cursor again without enclosure wall forces since the target button has already been acquired. Thus in one embodiment the forces associated with the enclosure are turned off once the cursor is moved inside the enclosure. Alternatively the force magnitudes can be weakened or adjusted to a lower level e.g. the normal magnitude to allow the cursor to easily exit the enclosure. Once the cursor has exited the forces can be turned back on. In an alternate embodiment the forces associated with the enclosure can be turned off or weakened for a predetermined period of time to allow the user to move the cursor out of the enclosure without hindrance and then the forces can be automatically turned back on once the time has expired. I yet a different embodiment the forces are turned off only when the cursor has not moved for a predetermined period of time this indicates that the user has acquired the desired target. In still a further embodiment the forces can be turned off when a command gesture such as a button activation is detected by the host or microprocessor . These various embodiments can also be combined in other embodiments as desired. The host or microprocessor can determine when the enclosure has been entered and exited by the cursor through the use of events explained below.

Enclosures can also be defined to compensate for user controlling peculiarities. For example it has been found that up down movement of the mouse for most users is much easier that left to right movement. An enclosure having equal strength forces on all walls will feel as if the left and right walls have stronger forces. Thus the upper and lower walls can be defined with stronger forces e.g. two times the magnitude of the left and right wall forces to compensate for this effect. Such an enclosure typically feels as if forces of all the walls are of equal strength.

Preferably an enclosure can be defined to control the effects of other enclosures. For example the enclosure occupies a region around buttons and is preferably not visible to the user. Enclosure turns off the forces of enclosure inside its borders and leaves the snap forces of the buttons on. This allows the user to feel the snap forces of the buttons without conflict from the forces of the window enclosure . Alternatively the forces of enclosure can be reduced inside enclosure to an extent so that the user is not encumbered by competing forces. A similar type of enclosure can be user on sliders where the moveable thumb of the slider can be provided with an enclosure similar to enclosure surrounding it. The enclosure surrounding the slider turns off or reduces forces from other enclosures and allows the snap attractive force of the thumb to attract the user object cursor to the thumb. Events can be used to determine when the cursor enters enclosure .

Positive Coefficient Negative Coefficient Height magnitude of bumps when travelling along the axis in the positive or negative direction respectively. Positive Saturation Negative Saturation Period in pixels of bumps bump width plus deadband width when travelling along the axis in the positive or negative direction respectively. DeadBand Percentage of period which is not occupied by a bump.

Offset O If the grid is inside of an enclosure defines the horizontal and vertical distance in pixels from the upper left corner of the enclosure s deadband to the first snap point. If the grid is stand alone defines the horizontal and vertical distance in pixels from the upper left corner of screen to the first snap point.

Grids can also be implemented as one dimensional forces e.g. as tick marks on a slider or any other graphical object when moved. The tick marks would only be felt when the mouse was moving in the defined dimension. For example a slider can be defined with a one dimensional grid so that as the knob of the slider is moved a snap point is applied as the knob moves over each tick of the grid. The ticks thus have a spatial correlation with the distance that the knob has moved and inform the user of such.

One structure used by the host in force feedback implementation is called an event. Events are notifications from the force feedback device to application programs and of one or more actions or interactions that have occurred in the graphical environment which has been sensed by the force feedback device . For example an event can be any interaction of the cursor in the graphical environment with another object. Thus an event can be defined when the cursor enters into an enclosure object described below through a particular border. Alternatively an event can be triggered when the cursor moves over a close box of a window or the cursor moves over a file icon and a button is pressed. An event can also be defined when the cursor moves within a specified range of a particular graphical object or region. Also an event may be defined with a temporal component e.g. an event is triggered when the cursor remains in a particular region for a specified period of time. Events are similar to button press triggers in that a condition is detected on the force feedback device independently of the host however the trigger for an event is a graphical interaction rather than a physical button press.

An application program initially defines what the event is by creating an event definition with the API . The event definition preferably includes an event identifier an application program or window identifier information specifying actions that trigger an event notification and force effects if any that the event is associated with. The application program assigns a separate identification value to each event at the time of creating the event and keeps track of the created identification values in its own list. The API can enable disable or delete an event at the request of an application program.

The translation layer receives and stores events when the application that created the event is the active or background application program. Thus events are preferably included in the active context. The translation layer preferably checks for events based on the position of the cursor or mouse received from the force feedback device when the associated application program is or becomes active. The translation layer then sends the event notification up to the context driver or API which sends an event notification to the application program e.g. sends a message to a window handler which the application program checks. The event notification includes the event identifier so that the application program can check its own list and identify the event. Once identified the application program initiates the appropriate response as defined by the developer e.g. running another application program outputting a sound displaying a new image etc. Alternatively the force feedback device is loaded with the defined event similar to being loaded with effects. The local microprocessor can check for events for active application programs and send information back through the translation layer to the context driver to notify the application program.

For an application program to receive an event notification the application s context must be active i.e. the application must be active or background. Thus inactive applications will not receive event notifications. Some events can be defined to have priority over other events. For example if a trigger is associated with an event in a foreground context as well as an event in a background context the foreground context can be defined to have priority so that only the application program in the foreground receives an event notification. Alternatively only the background application or both the foreground and background applications can receive the event notification.

Preferably both one time event triggers and periodic event triggers are available. One time event triggers are triggers associated with discrete actions that have no significant duration such as breaking through an enclosure. Periodic event triggers are associated with conditions that have a finite duration such as being inside the wall of an enclosure or holding down a scroll button. During the duration of the condition which triggers the event e.g. while the cursor is inside the enclosure the API repeatedly sends event notifications to the application program according to a predetermined periodicity that can be set by the application program.

As an example an application program creates an event definition for an enclosure event. This is an event that occurs when a user moves the cursor into an enclosure. The event definition includes a unique event identifier an identifier for the application program that created it actions that trigger the event notification entering the enclosure and any effects such as the particular enclosure which the event is associated with.

ONENTERTOP ONENTERLEFT etc. The mouse broke into the associated enclosure by breaking through the top bottom left or right wall as specified.

ONEXITTOP ONEXITLEFT etc. The mouse broke out of the associated enclosure by breaking through the top bottom left or right wall as specified.

INBOUNDTOPWALL INBOUNDLEFTWALL etc. The mouse is in the top bottom left or right wall as specified and attempting to break into the associated enclosure. OUTBOUNDTOPWAL OUTBOUNDLEFTWALL etc. The mouse is in the top bottom left or right wall as specified and attempting to break out of the associated enclosure. INBOUNDANYWALL OUTBOUNDANYWALL The mouse is in a wall and attempting to break into or out of the associated enclosure. ANYWALL The mouse is in a wall of the associated enclosure. ONSCROLL The user is holding down the scroll button of the mouse. ONEFFECTCOMPLETION The associated effect has completed.

The host computer or force feedback device may also command the mouse to act as a different type of control device for some embodiments. The force feedback device is primarily intended in the embodiment described in to be used in a graphical environment such as a GUI in which multi tasking applications are provided. When the force feedback device is used as such the API and other layers described above are preferably used. However the force feedback device can also be used as a different type of controller and may use other standard API s. For example the mouse force feedback device as shown in may also be desired to be used in conjunction with game application programs simulations or the like in which a joystick or other type of controller is typically used. Device can be configured to work with such game applications in a joystick mode as if it were a standard force feedback joystick or as if it were a non force feedback joystick. The user can be provided with a physical switch on the device to switch from mouse to joystick mode or a software switch provided in master application or other program on the host computer can be used to switch modes. For example when the device is in the joystick mode the host computer can use the DirectX API from Microsoft Corp. which includes force feedback functionality for standard joystick controllers. The device becomes an absolute position reporting device in joystick mode.

Mouse device as shown in is not easily applicable to many applications intended for use with a joystick since the mouse is a planar device having a free workspace while a joystick typically has rotary degrees of freedom and springs to bias the joystick to a center position. Thus forces are preferably used to cause the mouse to be more similar to a joystick. When the mouse device is used in the joystick mode in the preferred embodiment spring forces are applied to bias the mouse toward the center of its workspace. These forces are applied by actuators and controlled by the local microprocessor as background forces that are always in effect unless the device is switched out of joystick mode.

The force feedback interface device as described above preferably includes a local microprocessor that implements various processes to provide the functionality and features of the force feedback implementation described herein. Various aspects of the force feedback device implementation are described below.

The force feedback mouse device as described herein needs the ability to know the location of the cursor on the display device of the host computer. This is because the device must monitor the interaction of the cursor with other objects in the graphical environment to determine when associated forces should be output. The force feedback device uses different reference frames to determine where the cursor is positioned. There are three primary reference frames that are described herein the device or local frame the ballistic frame and the screen or host frame .

The device frame is the frame of the physical workspace of the force feedback device. In a preferred embodiment the device frame is defined with the origin at the top left the positive x direction to the right and the positive y direction down. This frame is used to describe the absolute position of the end point of the mechanism in its workspace. The scale of movement units for the device frame is arbitrary and fixed although it should be a higher resolution than the resolution of sensors to guarantee no loss of sensor data. Furthermore in the preferred embodiment the device position is scaled to reflect a 4 3 aspect ratio which matches the preferred mechanical design guide opening as well as the typical computer monitor size. However in designs having different sized mechanical workspace the device frame is scaled to the provided mechanical workspace.

In the preferred embodiment rate control indexing is provided. This allows the mouse to reach the limit to its workspace and still control the cursor in the same direction. In brief the central portion of the device workspace is designated as the position control region where a change in position of the mouse in its workspace directly correlates to a change in position of the cursor on the screen. The change in device position is translated to a change in ballistic position based on the velocity of the device according to a ballistics algorithm described below .

An edge region of the device frame is designated as the rate control region in which the mode of moving the cursor changes from position control to rate control where the cursor continues to move on the screen in the same direction while the mouse is in the rate control region. The rate control region boundary is accompanied by a repelling force that pushes the user object towards the center of the device workspace. As the user pushes against this force the cursor is moved in a speed proportional to the distance the device is displaced into the rate control region. Therefore the cursor moves relative to device position in the rate control region not relative to device motion. This mode may be thought of as moving the position control central area of the device frame around the screen using rate control. This type of indexing is described in greater detail in co pending U.S. patent application Ser. No. 08 924 462. The size of the rate control region is specified by a percentage. For example a 400 300 device workspace has a rate control region of 10 . The cursor position would be determined according to a position control paradigm when the user object position is read between the values of x y 20 15 and 380 285 of the device frame and according to a rate control paradigm if the user object position is outside of that range.

The screen frame is used to describe the position of the cursor or other user controlled graphical object on the screen. It is defined with the origin at the top left the positive x direction to the right and the positive y direction down. This frame is scaled corresponding to the resolution of the display device or devices . For example if a computer monitor resolution is set to 1024 768 pixels the x screen position ranges from 0 to 1023 as the cursor moves to the right and from 0 to 767 as the cursor moves down. All coordinate communication between the force feedback device and the host computer are in terms of screen coordinates normalized for device standards .

The ballistic frame or scaled frame is a frame created by the force feedback device to determine cursor position. By applying a ballistics algorithm to the change in position of the user object the cursor position is determined. The ballistic frame is defined as a multiple of the screen frame i.e. the ballistic frame preferably has a higher resolution than the screen frame but has the same aspect ratio as the screen frame. The ballistic frame is created by the force feedback device so that pixel changes smaller than one screen pixel can be made to the cursor position which is crucial when implementing the fast control loop rates demanded by force feedback. For example if the control frequency is 1 kHz and a monitor provides 1000 pixels across a change of 1 pixel per control cycle would cause the cursor to travel the entire screen width in one second. In rate control mode such adjustments in cursor position may have to be made every control cycle which is far too fast to allow control of the cursor. Thus a frame with higher resolution the ballistic frame needs to be defined to allow the force feedback device to change cursor position at its own control frequency but which can be divided down to the screen frame size to slow down the cursor as it is moved across the screen. In addition the aspect ratio of the ballistic frame is the same as that of the screen frame. This allows force effects on the device to spatially correlate with graphical objects displayed on the display device. This is described in greater detail below.

The microprocessor must make transformations between frames to convert a device position to an eventual screen coordinate. When transforming from the device frame to the ballistic frame the microprocessor may use sensitivity. Sensitivity is the ratio of the size of the device frame to the size of the ballistic frame. A higher value of sensitivity allows the cursor to traverse the screen with less physical motion of the mouse thus causing the cursor to be harder to control. A lower value of sensitivity allows more motion on the device to cause less motion of the cursor on the screen and is suitable for fine cursor control. At low sensitivity values the cursor may only be able to reach the entire screen area by using rate control mode.

In the preferred embodiment the cursor can exactly move across the entire width of the screen as the mouse moves across the extent of the position control region of the device workspace as long as the velocity of the mouse remains in a middle range to provide a one to two ballistic mapping a one to one ballistic mapping is provided with velocities in the lower range causing less cursor movement and a one to four ballistic mapping is provided with velocities in the high range causing more cursor movement . As sensitivity is increased the screen can be traversed in less than the entire position control region of the device workspace assuming a middle range velocity . At lower sensitivities the entire screen cannot be traversed assuming a middle range velocity without using the rate control mode.

Since the aspect ratios of the device frame and the ballistic frame may not match updating position changes in the ballistic frame may require that a change in device position or a simulated change when in rate control mode be added to the ballistic frame. If changes in mouse position are always used problems with different aspect ratios are eliminated. This is described in greater detail below.

Transforming from the ballistic frame to the screen frame is simpler. Once the ballistic cursor position is known the cursor position in the screen frame can be found by dividing the ballistic cursor position by the known multiple. Thus if the ballistic frame is defined to be 10 times the resolution of the screen frame the ballistic position is divided by 10 to obtain the screen position.

Standard mouse controllers are open loop devices which report position changes of the mouse in its workspace to the host computer and have no ability to know the current cursor position. The force feedback mouse device in contrast needs to know the location of the cursor on the display device of the host computer since the mouse device must monitor the interaction of the cursor with other objects in the graphical environment to determine when associated forces should be output.

There are two primary means for a force feedback mouse to report the position of the mouse to the host and to know the screen position of the cursor. The first preferred method for the mouse to know cursor location is for the mouse to report normalized absolute coordinates to the host computer to dictate the position of the cursor. For example the actual screen coordinates of cursor can be determined by the mouse and reported to the host and the host simply displays the cursor at the reported coordinates. This allows the mouse to know exactly where the cursor is at all times and to therefore have an accurate model of the screen for use with outputting forces. However any ballistic behavior of the cursor must be calculated on the mouse device itself creating more calculation overhead for the local microprocessor . In addition an absolute reporting scheme is not compatible with traditional non force feedback mouse devices thus excluding a default mode in case the force feedback functionality is not operational.

A second method provides a force feedback mouse device that reports position changes of the mouse in its workspace as a standard mouse does today rely on the host computer to send back the cursor position. This approach allows the host software to maintain the job of doing cursor ballistics but requires twice the communication as well as a need for the mouse to predict intermediate values of cursor position while waiting for the next host cursor position update since the cursor reporting occurs at a slower rate e.g. about 50 60 Hz than the haptic control rate e.g. about 1000 Hz . This embodiment is described in greater detail with respect to .

The processes described below are preferably implemented as firmware in integrated circuits and memory provided on the force feedback device. Alternatively the processes can be implemented as hardware or software or a mixture of these. The processes themselves can be implemented using program instructions or code stored on or transferred through a computer readable medium such as a memory chip circuit or other device magnetic media such as hard disk floppy disk or tape or other media such as CD ROM DVD PCMCIA cards etc. The program instructions may also be transmitted through a channel to device from a different source.

One problem with absolute position reporting is that the host computer cannot detect the force feedback device as a standard input controller such as a mouse. If relative position reporting were provided the host computer can detect the force feedback mouse as a traditional non force feedback mouse. This is desirable when force feedback functionality is not currently active to allow the user to select options within the operating system such as during startup of the device before force feedback drivers have been loaded or during emergencies when force feedback is not enabled.

The present embodiment provides a relative positioning mode when the force feedback device is powered up or when the host computer first detects the force feedback device. In this mode the device provides a device delta position to the host as shown in . The host is expecting to receive a delta value upon startup and thus simply processes the delta position normally. This allows the user to use the device for normal positioning of the cursor in a GUI or graphical environment before force functionality has been initialized. The microprocessor does not need to model the cursor position in this stage of the process because no force feedback functionality has yet been enabled on the device . However once the force feedback functionality is enabled e.g. the force feedback driver software has been enabled on the host computer then an absolute position is reported to the host in place of the delta position relative position reporting. In addition ballistic parameters and the host screen size are sent to the device to allow the device to determined the absolute cursor position. Force feedback commands are also sent to the device in this stage when appropriate.

The normalized joint angles resulting from step are then processed by applying kinematics in step . Kinematic equations as is well known to those skilled in the art are used to derive a position of a desired point on a mechanical linkage or other apparatus from known lengths of links in the linkage and the current angles between those links. For example a Cartesian position of a point on the linkage in a Cartesian or other type of coordinate system can be determined using geometrical equations. A current device position is determined from the kinematics of step which is the position of the mouse in its workspace.

The current device position is used with a previous device position to determine a device delta position . The previous device position was the current device position in a previous iteration of process this is indicated by the dashed line between current and previous device positions. The device delta position is simply the difference between the current device position and the previous device position . In step the microprocessor sends the determined delta position to the host computer over bus . This is a relative position reporting step and is only performed if the system does not yet have force feedback functionality e.g. at startup when the force feedback drivers are being loaded. The host computer uses the delta position to position the cursor on the screen of display device . For example the host computer adjusts the position of a cursor by the delta amount to achieve a new position of the cursor in the screen frame . During relative position reporting mode the delta position is sent to the host at a slower rate than the rate of process therefore in some iterations of process the delta need not be reported to the host. This is because the host only needs to receive a cursor delta position at the rate of displaying the cursor on the screen which typically is on the order of 60 Hz or every 20 ms. The microprocessor on the other hand must control forces at a much higher rate 1000 Hz or above and thus needs to execute process much faster.

If the device is in relative position reporting mode the process iteration is complete after the delta position is reported to the host. The process then returns to measure joint angles and start the next iteration when triggered to do so. The microprocessor does not need to model the cursor position in this mode because no force feedback functionality has yet been enabled on the device .

Relative position reporting mode is exited and absolute position reporting mode is entered when the appropriate software drivers or other programs are loaded into the memory of host computer to allow the host to recognize interface with and command the force feedback of device . In absolute position reporting mode the delta position continues to be determined as described above. However the delta position is no longer reported directly to the host computer but is instead used in a ballistics process to determine a ballistic delta described below.

In absolute position reporting mode the force feedback device also determines a velocity of the user object for use in the present process in addition to the joint angle processing described in steps and . A current joint velocity is first measured. In the preferred embodiment this is accomplished by a dedicated circuit or device that determines a velocity from the sensed positions reported by sensors e.g. the sensed positions of members and . For example a haptic accelerator device can include a circuit dedicated to calculating velocity from multiple sensed positions and time periods between the sensed positions. Such an embodiment is described in greater detail in U.S. Pat. No. 5 999 168. Alternatively velocity sensors can be used to directly measure velocity of joints or members of the device.

The current joint velocity is used in a process step which uses kinematics in determining device velocity. As described above kinematics are well known equations for determining position and also are used to determine a velocity of a desired point of a linkage. Thus the kinematics can be used to determine the velocity of the mouse which is referred to as the device velocity herein. The current device velocity resulting from step indicates the current velocity of mouse .

Ballistics step uses a ballistic algorithm or similar method to model or predict a position of the cursor in the screen frame based on the position and velocity of the mouse . This modelling is accomplished to allow the local microprocessor to determine where the cursor is positioned in the ballistic frame. Using the spatial layout of graphical objects that is stored locally in memory and which is continually updated by the host the local microprocessor determines when the cursor is interacting with an object and when to output forces associated with the interaction. The local determination of the cursor position allows the microprocessor to know the position of the cursor for the determination of forces.

In the ballistics step the local microprocessor uses a ballistics algorithm to determine a ballistic delta for the cursor i.e. a change in position of the cursor measured in the ballistic frame. A ballistics algorithm is a method of changing the position of the cursor so as to break the direct mapping between the mouse position and the cursor position to allow the user greater control over the cursor position. For example most ballistics algorithms vary the position of the cursor based on the velocity of a mouse in its workspace. The faster the velocity of the mouse the greater the distance that the cursor is moved on the screen for a given distance the mouse is moved. This allows slow motions of the mouse to finely control the cursor while fast motions coarsely control the cursor and allow the cursor to be moved quickly across the screen. Other algorithms can also be used which are similar to ballistics in that cursor position is altered to allow fine cursor control in situations when needed such as for targeting graphical objects with the cursor and to allow coarse cursor control when needed. Ballistics and other methods of such enhanced cursor control are described in greater detail in co pending application Ser. No. 08 924 462 and can be used.

The microprocessor preferably uses a ballistic algorithm that provides two velocity thresholds to determine how far the cursor is to be moved. This is described in greater detail below in the method of . Other ballistics algorithms may be used in other embodiments e.g. a continuous algorithm that adjusts cursor position continuously along a range of mouse velocity.

The microprocessor uses several types of data to determine the ballistic delta of the cursor including the device delta position current device velocity and host ballistic parameters . The host ballistic parameters are received by the microprocessor from the host at some point before the process starts or when the ballistic model changes and are stored in local memory to be accessed when required. Such parameters can include the distance to move the cursor for each threshold range of device velocity the velocity values of the two thresholds and any conditions concerning when to apply the ballistics algorithm. If the host changes ballistics algorithms notification can be sent to the microprocessor and new parameters stored in local memory . The microprocessor examines the current device velocity and the parameters to determine in which threshold range the mouse velocity currently is placed and then multiplies the device delta position by the appropriate amount e.g. 1 2 or 4 depending on the velocity . This results in the ballistic delta which is provided in terms of the ballistic frame. Changes in mouse position i.e. delta positions are used in the absolute position reporting mode because this allows problems with changes to screen aspect ratios and resolutions to be alleviated. In addition an indexing function of the device is preferably implemented which may cause the ballistic delta to be calculated differently in a rate control mode as detailed below in .

The microprocessor then proceeds to a process step in which a current ballistic location or scaled position of the cursor is determined in the ballistic frame. To accomplish this the microprocessor uses the ballistic delta and the previous ballistic location . The previous ballistic location was the current ballistic location in an earlier iteration of process as indicated by dashed line . Previous location can be stored in local memory and then discarded after being used to determine the current ballistic location. At its simplest level the current ballistic location is determined in step by taking the previous ballistic location and adjusting that location by the ballistic delta .

The current ballistic location however cannot be directly transformed to a screen pixel position in the screen frame. Step uses the current ballistic location and the host screen size to determine the screen pixel location.

The host screen size is the current pixel resolution and or aspect ratio being displayed on the display device of the host computer . For example the host may be displaying a screen resolution of 800 600 pixels 1024 768 pixels or 1280 960 pixels. Also in some embodiments the host may be displaying a different aspect ratio. For example a typical computer monitor aspect ratio is 4 3 however if two monitors are connected to provide a continuous screen space the aspect ratio becomes 8 3. i.e. double the size of a single screen in one dimension. The host screen size can be received by the device when the ballistic parameters are received or at any other time and or when sensitivity information is received if applicable . Different resolutions or aspect ratios have different numbers of pixels in respective dimensions of the screen and thus cause displayed objects to appear in different areas of the screen. For example if a graphical object is centered in the screen at 640 480 resolution the object will appear in the upper left quadrant of the screen when the resolution changes to 1280 960.

The change in resolution or aspect ratio can create a problem with force sensations created by the force feedback device. The device has been sent a spatial layout of objects from the host that indicates the coordinates of an object on the screen. When the screen size is changed and the device is not informed the device will output forces corresponding to the object as it appeared in the old screen size. This can cause large discrepancies between where an object is displayed and where the forces of an object are encountered in the device workspace. For example if an icon is at coordinates 64 48 on a 640 480 resolution screen the device knows that the icon is positioned 10 of the entire screen dimension away from the top and left borders of the screen workspace. The device accordingly outputs associated forces when the mouse is moved into the region of the device workspace corresponding to the icon i.e. 10 of the workspace distance from the top and left borders of the workspace. However after a resolution change to 1280 960 the icon is displayed 5 of the screen dimension away from the top and left borders. The device however would output forces based on the old position of the icon which no longer corresponds with the position of the icon. Thus the device needs to be told the new screen resolution so it may provide forces at the appropriate region of the device workspace.

Similarly a change in aspect ratio can cause distortion in the force feedback sensations. For example a circle object is displayed on a 4 3 aspect ratio screen and has a force wall surrounding its shape. The circle will feel like a circle on the device if the ballistic frame also has an aspect ratio of 4 3. However if another monitor is activated so that the screen has an 8 3 aspect ratio the circle object will feel like an oval due if the screen is directly mapped to the device frame i.e. if the device frame is stretched to fit the screen frame. The use of delta positions prevents this distortion. Since only changes in the mouse position are used to determine cursor position and not an absolute mouse position in the device workspace the cursor is positioned without distortion. However the new aspect ratio still needs to be communicated to the force feedback device since there is a larger area in which the cursor may be positioned. For example in a 4 3 aspect ratio the microprocessor might clip a pixel location to the edge of the screen if that location were beyond the edge of the displayed range. However on an 8 3 aspect ratio the location should not be clipped since the displayed range is larger.

To prevent distortion caused by a change in screen size the ballistic location of the cursor is divided by a divisor that takes the host screen size into account. For example if the ballistic frame is 10 times the resolution of the screen frame then the divisor is 10 the ballistic location would be divided by 10 to achieve a screen pixel position of the cursor if the host screen size had not changed. If the host screen size did change then the ballistic frame is resized to take the new screen size into account. Preferably the screen pixel position is equal to the screen size times the ballistic location divided by the ballistic frame size i.e. the divisor . Thus if screen resolution on the host has doubled the divisor would halve and the screen pixel location would double. The host can also send sensitivity information to the device. If a new sensitivity value is received the ballistic frame is resized accordingly as sensitivity is increased the ballistic frame size is decreased.

Once the screen pixel location is translated from the ballistic position the screen pixel location is then typically normalized to a standardized range of numbers since communication software and devices have standard values to receive. The normalized screen pixel position is then sent to the host computer as an absolute position in step e.g. as coordinates for display on the screen. The host receives the screen pixel location and displays the cursor appropriately.

In step the local processor uses the preferably non normalized screen pixel location and other data to determine whether any forces should be output and outputs those forces. For example the local microprocessor checks the spatial layout of graphical objects which is stored in local memory and determines whether the screen pixel location intersects or contacts any of those objects. If so there may be a force associated with the interaction depending on the graphical object intersected. Such a force would be calculated based on a variety of data. Such data may include the device velocity the device position the elapsed time from an event the distance from a different graphical object in the graphical environment a history of past mouse motion etc. Some examples of forces that can be output in a graphical environment are described in U.S. Pat. Nos. 6 219 032 and 5 825 308.

Once step is complete and any other desired steps in the provision of forces which is not detailed herein the process when needed returns to the measuring of joint angles and joint velocity to begin another iteration of the absolute position reporting process.

No error correction is required in the present embodiment unlike the embodiment below because the force feedback device is operating in absolute mode when the pixel position is determined. The host is no longer receiving simply a change in position delta of the user object but is receiving an absolute coordinate in a coordinate system and need perform no other significant processing such as ballistics to display the cursor i.e. the local microprocessor solely performs the necessary adjustments such as ballistics and reports the adjusted position to the host computer. Thus the host and local microprocessor have the same position value for the cursor and no opportunity exists for the two positions to diverge.

The method is similar to the indexing methods described above in that a position control region and isometric edge region are provided in the mouse workspace. Method is intended to be used on the preferred embodiment of and accounts for the use of a ballistic frame on the mouse and for changes in screen size.

Method begins at in which raw sensor data is read using the sensors . In step the joint positions of linkage are calculated from the raw sensor data. In step position kinematics are used to determined the device position position of the mouse or other user object in the device frame as explained above with reference to . The last device position is stored in local memory in step and a delta device position is determined in step using the current device position resulting from step and the last device position stored in step . From step step is initiated which checks whether the device position is in the position control region of the device frame. If not the mouse is in the isometric region and is in rate control mode where a resistive force is preferably output on the mouse opposing movement into the region. In step the rate control delta is calculated to be the difference between the device position and the rate control edge this provides the distance of penetration into the isometric region. In step the rate of cursor movement is equal to the maximum rate times the rate control delta divided by the rate control edge width i.e. the width of the isometric regionj. The rate control delta divided by the rate control edge width provides the percentage of penetration of the mouse into the isometric region which is designated to be the same percentage of the maximum rate of the mouse. The maximum rate can be set by the developer for example 1000 ballistic pixels per second or 8000 ballistic pixels per second. In step the ballistic position i.e. the position of the cursor in the ballistic frame is set equal to the old ballistic position plus the rate determined in step . The process then continues to step detailed below.

If the device position is in the position control region in step then a position control method is followed. Steps implement a preferred ballistics algorithm to provide enhanced cursor control to the user of device . This algorithm is generally as follows. If the current device velocity is below the first threshold the cursor is moved by the smallest amount e.g. a one to one mapping between device movement units and ballistic pixels. If the current device velocity is between the first and second thresholds the cursor is moved double the amount of pixels as it is moved when mouse velocity is under the first threshold e.g. a one to two mapping between device and ballistic frames. If the user object velocity is above the second threshold the cursor is moved double the amount of pixels as between the first and second thresholds a one to four mapping between device and ballistic frames . In other embodiments more or less thresholds or a continuous function can be used. Also the host may change the ballistics algorithm in a variety of ways and indicate the changes to the device using ballistic parameters as described for .

Thus step checks the device velocity so that a ballistic position may be determined. The device velocity is determined in steps and where step determines joint velocities from the sensor data read in step and device velocity is determined from the joint velocities and velocity kinematics in step as described above in . In step this device velocity is compared to the ballistic thresholds used in the ballistics algorithm. If the device velocity is within the first ballistic threshold in step then step determines the new ballistic position to be the old ballistic position plus the delta device position from step . The process then continues to step detailed below. If the device velocity is greater than the first ballistic threshold then in step the device velocity is compared to the second ballistic threshold. If the device velocity is between the first and second thresholds then in step the new ballistic position is set equal to the old ballistic position plus twice the delta device position. This causes the cursor to move at a faster rate based on mouse velocity. The process then continues to step detailed below. If the device velocity is greater than the second ballistic threshold in step then in step the new ballistic position is set equal to the old ballistic position plus 4 times the delta device position. The process then continues to step .

In step the screen position of the cursor is set equal to the ballistic position as determined in one of steps and divided by a divisor. The divisor is known from the resolution of the ballistic frame and the resolution of the screen frame as follows. In step the screen resolution or size is obtained from the host. In step the divisor is set equal to the ballistic range or resolution divided by the screen resolution as detailed above.

After the screen position is determined in step it is sent to the host computer in step and used in the display of the cursor in the screen frame. The device II also determines and outputs forces as detailed in . It should be noted that the forces on mouse may be determined or calculated differently in rate control isometric mode than in position control mode as described in greater detail in U.S. Pat. No. 6 252 579.

Embodiment provides a device delta position to the host computer from which the host computer determines a cursor screen pixel position. The host computer sends cursor position updates to the force feedback device to provide the cursor position which the device needs to determine when forces are output and to determine the force characteristics. However since the host cursor position updates occur at a much slower rate the device must model the cursor position on its own to be able to output forces between host cursor updates. Thus the host sends ballistic parameters and screen size information so the device can model the cursor position. When the device receives an update it corrects for any error between its modelled position and the actual cursor position as detailed below. The host also sends force feedback commands to the device indicate which forces are to be output.

Process includes many similar steps to process . The current joint angles are measured and normalized in step and the normalized joint angles are processed with kinematics in step to provide a current device position . The current device position and previous device position are used to determine a delta position which is reported to the host computer in step over bus . As in process this portion of the process functions like a traditional relative mouse where only a delta value is reported to the host computer. However unlike the process the delta position is always reported to the host computer i.e. there is no absolute position to report at any stage of the process. The host computer uses the delta position to position the cursor or other user controlled graphical object on the display device . For example the host computer adjusts the position of a cursor using the delta amount and its own ballistics algorithm to achieve a new position of the cursor in the screen frame . The delta position is sent to the host at a slower rate than the rate of process therefore in some iterations of process the delta need not be reported to the host. This is because the host only needs to receive a cursor delta position at the rate of displaying the cursor on the screen which typically is on the order of 60 Hz or every 20 ms. The microprocessor on the other hand must compute forces at a much higher rate 1000 Hz or above and thus needs to execute process much faster.

The current joint velocity also is read in process both the joint angles and the joint velocity are always measured in the current process . Kinematics are applied in step to obtain the current device velocity and ballistics are applied in step using the device velocity ballistic parameters and delta position to obtain a modeled pixel delta . The pixel delta is a modeled or predicted value in this embodiment because the host has determined the actual screen position using its own ballistics algorithm and using the delta position provided by the device. The microprocessor preferably uses in step the same ballistics algorithm used by the host computer in its determination of cursor position.

In step a current pixel location is modeled using the modeled pixel delta a previous pixel location and the host screen size resulting in a current pixel location . Step is similar to step of process in that pixel delta and the previous pixel location are used substantially similarly. The host screen size information can be used to determine whether the determined pixel location should be clipped or not. For example if the aspect ratio has changed from 4 3 to 8 3 double the amount of area is provided. If the pixel location is one that would normally extend beyond the edge of a 4 3 screen area it would be clipped to the border of the 4 3 screen but may not be clipped if it is still within the 8 3 area. In one embodiment if the change in screen size is to a different resolution the current pixel location can also be adjusted depending on the change in resolution. For example if screen resolution has been adjusted from 640 480 to 1280 960 then the pixel delta can be doubled before it is added to the previous pixel location.

Step is different from step in in that a current pixel error is included in the determination of the current pixel location . The pixel error is the error between the cursor position modeled by the local microprocessor and the actual cursor position as determined and displayed by the host computer. Although the microprocessor is typically applying the same ballistics algorithm and screen size adjustments to the cursor pixel location as the host computer errors can be introduced to cause the two cursor location calculations to diverge. For example in some embodiments the host computer can clip the received delta position i.e. ignore the delta position so that the cursor stays at a constant position. Such a technique is used for example to provide realistic obstruction force sensations when forces are limited in magnitude as described in greater detail in co pending application Ser. No. 08 664 086. Or error can be caused by an application program on the host computer moving the cursor based on the program s own criteria and completely independently of the force feedback device. Or error can be caused by the host computer switching to a different ballistics algorithm where a delay exists between the host using the new ballistic algorithm and the force feedback device receiving the parameters for the new algorithm.

The current pixel error is determined by examining the current pixel location at the time of reporting the delta position and a current screen position of the cursor that has been received from the host computer. The host computer periodically reports a current screen position of the cursor to the microprocessor where the screen position is based on the delta position reported to the host in step . The reporting of the screen position by the host is triggered by receiving the delta position from the device and the microprocessor recalculates pixel error when it receives the host screen position .

The microprocessor compares the received screen position with the modeled current pixel location at the time of report it should be noted that the current pixel location may have a previously determined error correction value included in its determination as explained below . If the two locations are the same no error exists between the two cursor positions and the microprocessor uses the current pixel location as the location of the cursor in its force calculations and force outputs. If the two locations are not the same error exists between the two cursor positions and the current pixel location must therefore be corrected. An error correction value is determined as the difference between the current pixel location and the host screen position . This value when added to or subtracted from the pixel location will cause the two cursor locations to be the same. Alternatively if the microprocessor knows the error then the current pixel location can be adjusted to provide no error.

The screen position from the host is reported to the microprocessor at a slower rate than the rate of process since the force feedback process must be much faster than the displaying processes of the host computer. There may therefore be several iterations of process before the screen position is reported by the host to the microprocessor . Therefore once the error correction value is determined after one report of screen position that same value is continually used in iterations providing the determination of current pixel location until the next screen position is received from the host.

In the preferred embodiment it is desirable to smooth out the correction of the error in pixel location by only incrementally changing the pixel location in each iteration of process . For example if the current pixel location were adjusted with the entire error correction value at one time this might cause a large change in the pixel location . If forces output on the user object are based on the current pixel location of the cursor then the user may notice the correction of the error by feeling a small jump or discontinuity in the forces applied to mouse after the error is corrected. This is because the forces were determined based on a pixel location having a large error at one point in time and then the forces are based on the corrected pixel location at the next point in time. Thus the difference in pixel locations in each iteration is preferably smoothed by only partially correcting the current pixel location . With such smoothing the user does not feel as large a discontinuity in forces when the error in pixel position is corrected. The error correction rate is preferably based on the desired system modeling. The error can be corrected at a rate so that there is no error between the pixel location and the host screen position by the time a new host screen position is received which may provide a new error of course . Or the error can be corrected at a rate that spans multiple received host screen positions . Typically the correction of errors is desired to be as slow as possible to maintain high fidelity of forces to the user yet the error must be corrected fast enough so as not to create a discontinuity between what the host is displaying on the display device and the forces the user feels on the force feedback device.

It should be noted that in the example of the error value determined at the sixth iteration continues to be added indefinitely. Thus if further errors are indicated at later host reports the new error value must be added to the old error value. is a diagrammatic illustration of the error correction value increasing over time. Initially the error correction value is 0 then 1. Over the next host report of the screen position the current error is 2 pixels which is added to the previous error correction value of 1 so that the total error correction value is 3. Likewise in the next host report the current error is 3 pixels so that the total error correction value is 6. In one embodiment if a very large error accumulates the microprocessor can make the error zero in one iteration regardless of the effect on the user.

Referring back to once the current pixel location is determined the local processor uses that location in step to determine whether any forces should be output and outputs those forces. It should be noted that most forces are determined preferably based on the velocity of the user object in its workspace not the velocity of the cursor in the graphical environment. Thus these forces will always feel the same to the user regardless of any error between the modeled pixel location and the host screen position since they is independent of screen velocity. However some forces are dependent on interactions in the graphical environment and will be affected by a cursor position error. Likewise the ability to initiate or discontinue forces based on interactions in the graphical environment is also affected by cursor position error.

Once step is complete and any other desired steps in the provision of forces which is not detailed herein the process preferably returns to the measuring of joint angles and joint velocity to begin another iteration of the process.

In the above described embodiments the host computer performs such tasks as the implementation and display of the graphical or other environment and cursor the implementation of multi tasking application programs the sending of high level force feedback commands to the force feedback device and the reception of position or other data from the force feedback device indicating the user s desired interaction with the computer implemented environment. The force feedback device in contrast performs such tasks as reading the position of the user object receiving high level force feedback commands from the host computer determining whether a force should be output based on conditions sent from the host computer and implementing and outputting the required forces when necessary.

Some tasks however may be implemented in either the host computer or the force feedback device and there are different tradeoffs to choosing one implementation over the other. Events for example are one such feature. Another force feedback feature that can be implemented in either the host or the force feedback device is clipping. Clipping is the selective omission of reported position data when displaying the cursor or other user controlled graphical object to cause a sensory effect on the user in certain circumstances. Such circumstances include those where displaying the cursor in correlation with the precise user object position would not provide a desired effect. The most common case for clipping is when the cursor encounters a surface or object in the graphical environment which is desired for the user to experience as a hard obstruction or wall. The wall is desired to prohibit further movement into the surface or object however due to limitations in hardware a strong enough force cannot be output on the user object to actually prevent motion into the wall. Thus clipping is performed the user object e.g. mouse is allowed to be moved by the user into the wall against a resistive force but the reported position data indicating this movement is clipped or ignored so that the cursor is displayed positioned against the wall surface where it first encountered the wall. The user sees the cursor prevented from moving into the wall and feels a resistive force and believes that it is a hard obstruction even though the user object can actually be moved into the wall this is because the user heavily relies on the visual perception of the interaction. Clipping is described in greater detail in U.S. Pat. Nos. 6 028 593 and 5 825 308.

Clipping can be easily performed by the force feedback device to ease computational burden on the host. For example the local microprocessor can check whether the user object is being moved into a wall in the graphical environment if so the microprocessor can simply discard the position data and report a constant position or delta to the host computer that would cause the host to display the cursor against the wall surface. This allows the host computer to be completely ignorant of the clipping process and simply display the cursor at the reported position thus freeing the host to perform other tasks. However problems occur in this implementation due to extra burden on the local microprocessor. Since the microprocessor is implementing a very fast force feedback loop in which the user object position is read and forces are output it is very disruptive to this loop when the microprocessor has to check whether to clip the read position of the user object and may cause a small distortion in output forces.

The host computer can also perform clipping in an alternate embodiment. Since it is desirable to keep the application programs and the operating system and other high level programs ignorant of the clipping process a lower level program preferably handles the clipping. For example the translation layer as shown in or alternatively the context driver or API can check for the conditions that cause clipping to be applied. If clipping is to be performed the translation layer alters the input position data appropriately before it is sent on to the context driver API operating system and any application programs. Problems with this implementation include increased load on the host with the overhead of intercepting and translating incoming messages.

A different force feedback feature that can be implemented either by the force feedback device or the host computer is pressure clicks or click surfaces. As described above these are surfaces objects or regions in a graphical environment which have additional functionality based on the position of the cursor in relation to the object. Thus a border of a window can be designated a click surface such that if the user overcomes a resistive force and moves the cursor or user object over a threshold distance into the border a function is activated. The function can be scrolling a document in the window closing the window expanding the window size etc. A similar click surface can be used on an icon or button to select or activate the icon or button.

The force feedback device can implement click surfaces to ease computational burden on the host computer. The local microprocessor can check whether the cursor has moved into an click surface or region and output the resistive force as appropriate. When the cursor is moved past the threshold distance the microprocessor reports to the host computer that the action associated with the click surface has been made for example the microprocessor reports that a button press or double click has been performed by the user. The host receives a signal that a button has been pressed and acts accordingly. Or the microprocessor reports that a particular click surface has been specifically selected where the host can interpret the selection and implement an associated function.

The host may also implement at least a portion of the functionality of click surfaces. The microprocessor can output the resistive or other type force associated with the click surface but only reports the user object position to the host. When the host determines that the cursor or user object has moved beyond the threshold distance the host implements the function associated with the click surface scrolling text closing a window etc. As with clipping it is preferred that the translation layer handle this functionality by modifying data accordingly before passing it to the context driver and API.

While the subject matter has been described in terms of several preferred embodiments it is contemplated that alterations permutations and equivalents thereof will become apparent to those skilled in the art upon a reading of the specification and study of the drawings. For example although examples in a GUI are described the embodiments herein are also very well suited for other two dimensional graphical environments and especially three dimensional graphical environments where a user would like fine positioning in manipulating 3 D objects and moving in a 3 D space. For example the isometric limits are quite helpful to move a cursor or controlled object in a 3 D environment further than physical limits of the interface device allow.

In addition many different types of forces can be applied to the user object in accordance with different graphical objects or regions appearing on the computer s display screen and which may be mouse based force sensations or cursor based force sensations. Also the various features of the embodiments herein can be combined in various ways to provide additional embodiments. In addition many types of user objects and mechanisms can be provided to transmit the forces to the user such as a mouse trackball joystick stylus or other objects. Furthermore certain terminology has been used for the purposes of descriptive clarity and not to be limiting. It is therefore intended that the following appended claims include all such alterations permutations and equivalents as fall within the true spirit and scope of the inventive subject matter.

