---

title: Accelerated video encoding
abstract: A video encoding acceleration service to increase one or more of the speed and quality of video encoding is described. The service acts as an intermediary between an arbitrary video encoder computer program application and arbitrary video acceleration hardware. The service receives one or more queries from the video encoder to identify implementation specifics of the video acceleration hardware. The service interfaces with the video acceleration hardware to obtain the implementation specifics. The service communicates the implementation specifics to the video encoder. The implementation specifics enable the video encoder to: (a) determine whether one or more of speed and quality of software encoding operations associated with the video encoder can be increased with implementation of a pipeline of one or more supported encoding pipeline configurations and capabilities, and (b) implement the pipeline by interfacing with the service.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08654842&OS=08654842&RS=08654842
owner: Microsoft Corporation
number: 08654842
owner_city: Redmond
owner_country: US
publication_date: 20070209
---
This application is a continuation in part of co pending U.S. patent application Ser. No. 11 276 336 filed on Feb. 24 2006 titled Accelerated Video Encoding and hereby incorporated by reference.

Multimedia content production and distribution operations typically include video encoding. Video encoding processes are typically very data and computationally intensive. As a result video encoding processes can be very time consuming. For example it may take several tens of hours for a software encoder to encode a high quality high definition movie. Since quality and speed of video encoding processes are significant factors for successful multimedia content production and distribution pipelines systems and techniques to increase the speed at which high quality video content can be encoded would be useful.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

In view of the above a video encoding acceleration service to increase one or more of the speed and quality of video encoding is described. The service acts as an intermediary between an arbitrary video encoder computer program application and arbitrary video acceleration hardware. The service receives one or more queries from the video encoder to identify implementation specifics of the video acceleration hardware. The service interfaces with the video acceleration hardware to obtain the implementation specifics. The service communicates the implementation specifics to the video encoder. The implementation specifics enable the video encoder to a determine whether one or more of speed and quality of software encoding operations associated with the video encoder can be increased with implementation of a pipeline of one or more supported encoding pipeline configurations and capabilities and b implement the pipeline by interfacing with the service.

Systems and methods for accelerated video encoding provide a video encoding acceleration service. This service allows an arbitrary video encoder application to interface in a device independent manner with arbitrary video acceleration hardware to define and implement a substantially optimal video encoding pipeline. To accomplish this the service exposes video acceleration VA application program interfaces APIs . These APIs encapsulate a model of the video encoding process. To define an encoding pipeline the video encoder application uses the VA APIs to query implementation specifics e.g. capabilities etc. of available video graphics acceleration hardware. The video encoder evaluates these specifics in view of the application s particular video encoding architecture software implemented to identify any encoding operations that could benefit e.g. speed and or quality benefits from being accelerated in hardware. Such operations include for example motion estimation transform and quantization operations and inverse operations such as Motion compensation inverse transforms and inverse quantization. The API also allows the video encoder to design an encoding pipeline that substantially minimizes dataflow transitions across buses and processors associated with the host computing device and the acceleration hardware and thereby further increase encoding speeds. The API also allows the acceleration hardware to influence the location of the data to improve local caching e.g. the video acceleration hardware may functional more efficiently on memory local to the video hardware .

Based on these evaluations the video encoder designs a customized video encoding pipeline that performs some number of encoding operations in software and some number of encoding operations using the acceleration hardware i.e. at least a subset of the operations that could benefit from being hardware accelerated . The encoder application then uses the API to create the pipeline and encode video content. This customized pipeline is substantially optimized as compared to a completely software implemented pipeline because certain encoding operations are accelerated and data transitions between the host and the acceleration hardware are minimized. Additionally processing time freed up by accelerating certain aspects of the encoding process and minimizing data transitions allow the host processor s to perform higher quality encoding operations with freed up processing cycles. The API is also designed to allow components to operate in parallel so that computational resource usage can be maximized.

These and other aspects of the systems and methods for accelerated video encoding are now described in greater detail.

Although not required the systems and methods for accelerated video encoding are described in the general context of computer executable instructions program modules being executed by a computing device such as a personal computer and graphics video encoding acceleration hardware. Program modules generally include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types.

In this implementation video encoder is an arbitrary video encoder. This means that the particular architecture operation data formats etc implemented and or utilized by video encoder are arbitrary. For example video encoder may be distributed by a third party an OEM etc. Additionally although shows video encoding acceleration service independent of the operating system portion of other program modules in one implementation video encoding acceleration service is part of the operating system.

Video processing modules receive compressed or uncompressed input video data . When input video data is compressed already encoded video processing modules decode the input video data to produce decoded source video data. Such decoding operations are performs by a decoder module. In another implementation partially decoded data could also be retained to farther assist the encoding process. For purposes of exemplary illustration such a decoder module is shown as a respective portion of other video processing modules . Thus decoded source video data is represented either by input video data that was received in a decoded state or represented with results of decoding input video data that was received in an encoded state. Decoded source video data is shown as a respective portion of other program data .

To design and implement a customized video encoding pipeline that can be used to encode decoded source video data into encoded video data video encoder interfaces with video encoding acceleration service via video acceleration VA APIs . One exemplary implementation of multiple possible implementations of VA APIs is described in the Appendix. To define an encoding pipeline the video encoder application uses respective ones of the VA API e.g. please see the Appendix 3.4 IVideoEncoderService to obtain implementation specifics of available acceleration hardware . Such implementation specifics include for example 

Responsive to receiving such requests from the video encoder video encoding acceleration service queries the video acceleration hardware for the requested implementation specifics and returns information associated with the corresponding responses from the acceleration hardware to the video encoder . Video encoding acceleration service interfaces with the video acceleration hardware using a corresponding device driver. Such a device driver is shown as respective portion of other program modules .

Video encoder evaluates the implementation specifics supported by acceleration hardware in view of the application s particular video encoding architecture software implemented to identify any encoding operations that could benefit e.g. speed and or quality benefits from being accelerated in hardware select a search profile to encapsulate a trade off between video encoding quality and speed minimize data transitions across buses and between processors etc. Exemplary operations that may benefit from hardware acceleration include for example motion estimation transform and quantization. For example one reason to perform quantization in hardware is to minimize dataflow between pipeline stages.

In this example implementation video encoder takes as input some form of compressed or uncompressed video data please also see input video data of . Please note that the exemplary pipeline configuration of does not copy input source video raw video source to the host computing device if the source is not originating from the host and if the host decision making engine e.g. video encoder does not use the source video. For example if quantization decisions do not require the host to touch the video data the data will not be transferred. In this example pipeline is configured to convert the input data to another compressed form using the respective operations of blocks and through .

Such operations may include converting uncompressed YUV video data to compressed MPEG 2 or it may include transcoding video data from MPEG 2 data format to WMV data format. For purposes of exemplary illustration assume that the transcoding operations include a full or partial decompression stage followed by an encode stage there are more efficient models which by pass decompression and work purely in the transform DCT space . A number of video compression formats make use of motion estimation transform and quantization to achieve compression. Of the compression stages motion estimation is typically the slowest step including a massive search operation where an encoder e.g. video encoder attempts to find the closest matching reference macroblock for macroblocks in a given image.

Once the optimal motion vectors are determined e.g. via block for each of the macroblocks the encoder computes the differential residues e.g. via block based on the previously coded image and the optimal motion vector. The motion vector along with the differential residue is a compact representation of the current image. The motion vector data is further represented differentially. The host encoder can optionally request the re evaluation of motion vectors by the video acceleration hardware to find a macroblock with a smaller combined motion vector and or residual. The resulting differential motion vector data and the residual data are compacted e.g. via block for example using techniques like run length encoding RLE and differential coding e.g. Huffman and Arithmetic coding to generate the final coded stream of bits encoded video data to communicate to a destination block for presentation to a user. In this example the operations of blocks and through e.g. operations such as motion estimation mode decision motion vector MV selection and rate control prediction formation transform and quantization operations quantizer inversion and transform and version and entropy coding are well known in the art and are thus not described further herein.

Referring again to in one implementation video encoder is a multi threaded application providing for full utilization of acceleration hardware . In this implementation when determining which video encoding operations are to be accelerated in hardware video encoder may structure the particular pipeline configuration such that both processor and acceleration hardware is fully utilized. For example when video encoding pipeline motion estimation operations are being performed by hardware for a particular frame of video data the pipeline may be configured to perform entropy or arithmetic or Huffman coding operations in software by the host on a different frame of video data. An exemplary single motion vector pipeline representing the particular pipeline configuration selected structured is described below in the Appendix in section 5.1.1. Exemplary multiple motion vector relatively complex pipelines wherein video encoder requests multiple motion vectors from acceleration hardware and selects one motion vector pipeline based on various parameters is described below in the Appendix in section 5.1.2.

With respect to selecting a search profile the quality of motion vectors refers to a bitrate of a stream generated by the use of the motion vectors. High quality motion vectors are associated with low bitrate streams. The quality is determined by the completeness of the block search the quality of the algorithm the distance metric used etc. High quality motion vectors should be used to perform high quality video encode operations. To address this video encoding acceleration service provides a generic construct called a search profile to encapsulate a trade off between quality and time. The search profile also includes meta data to identify the search algorithm used by the acceleration hardware etc. Video encoder chooses a particular search profile based on the particular requirements of the encoder s implementation.

With respect to minimizing data transitions across buses and between processors an encode process implemented by a video encoding pipeline configuration will typically include several processing stages each of which may or may not be accelerated via acceleration hardware . In cases where video encoder determines to utilize hardware acceleration in successive stages of the encode pipeline it may not be necessary to move data from acceleration hardware based memory to the system memory associated with the host computing device and then back to acceleration hardware based memory for the next stage and so on.

More particularly while pointers to various types of video and motion vector data may be transferred back and forth between the host computing device and the acceleration hardware in one implementation actual data is copied to system memory only when the data pointer a D3D9 Surface pointer is explicitly locked using for example IDirect3DSurface9 LockRect. Exemplary interfaces for locking a surface are known e.g. the well known IDirect3DSurface9 LockRect.interface . Thus in cases where two encoding pipeline stages follow one another and host computing device does not need to do perform any intermediate processing host computing device can decide not to Lock the allocated buffer between the processing stages. This will prevent a redundant memory copy of data and thereby avoid unnecessary data movement transfers. In this manner video encoder design a video encoding pipeline that substantially minimizes data transfers across buses and between processors and thereby further increase video encoding speeds.

At this point video encoder has evaluated the implementation specifics supported by acceleration hardware in view of the application s particular video encoding architecture software implemented to identify any encoding operations that could benefit from being accelerated in hardware selected a search profile minimized data transitions across buses and between processors and or so on. Based on these determinations video encoder selects a particular pipeline configuration to encode decoded source video data and thereby generate encoded video data . Next video encoder interfaces with video encoding acceleration service to create an encoder object to implement the selected pipeline please see the Appendix CreateVideoEncoder API 3.4.6 . In this implementation an encoder object e.g. a regular COM object is created by identifying the selected pipeline configuration and one or more of the following a format for the output encoded bitstream the number of input and output data streams associated with the pipeline configuration static configuration properties a suggested number of buffers surfaces for association with the different I O streams based on the selected pipeline configuration and a driver specified allocator queue size based on resources a graphics device driver is able to gather and other parameters. Queue size and the number of data buffers are essentially referring to the same thing one is suggested the other is actual .

Next video encoder uses the created encoder object to interface with the video encoding acceleration service to encode the decoded source video data. To this end the encoder object submits execute requests to acceleration hardware please see the Appendix IVideoEncode Execute API 3.2.3 .

In view of the above system allows arbitrary implementations of video encoder applications to define and create video encoding pipeline configurations during runtime to take full advantage of available video encoding acceleration hardware to increase encoding speed and quality. As part of these runtime configuration operations the video encoder can use VA APIs to specify that the encoding pipeline is to implement iterative directed searching multiple search passes of increasing refinement define and use generically selectable search strategies e.g. selecting a search algorithm based on quality metrics independent of any knowledge of details about the actual algorithm been employed utilize format independent methodologies e.g. where a video encoder is unaware of the particular image format of input video data and the acceleration hardware is unaware of the compressed output format for the encoded video data to control searching adapt data sizes e.g. where the video encoder selects a macro block size based on a search algorithm and so on.

At block video encoder receives input video data . If the input video data is not compressed the input video data represents decoded source video data. At block if the input video data is compressed video encoder decompresses the input video data to generate decoded source video data. At block video encoder interfaces with VA API to query acceleration hardware for capabilities and video encoding pipeline configuration implementation specifics. At block video encoder evaluates the supported capabilities and implementation specifics within the context of the implementation of the video encoder to identify video encoding operations associated with the particular implementation of the video encoder that may benefit from hardware acceleration make encoding speed and or quality decisions minimize data transitions across busses and between processors and or so on.

At block video encoder creates an encoding object that implements an encoding pipeline configured to execute the identified video encoding operations that may benefit from hardware acceleration in acceleration hardware implement the speed quality tradeoffs e.g. via a selected search profile and minimize data flow transitions. At block video encoder uses the created encoder object to encode the decoded source video data according to the sequence of operations and encoding architecture delineated by the customized video encoding pipeline generated at block . These encoding operations of block generate encoded video data .

Although the systems and methods for accelerated video encoding have been described in language specific to structural features and or methodological operations or actions it is understood that the implementations defined in the appended claims are not necessarily limited to the specific features or actions described.

For example although API s of have been described within the context of encoding video data APIs can be used outside of the encoding context for hardware acceleration of other functions such as edge detection motion vector based noise reduction image stabilization sharpening frame rate conversion velocity computation for computer vision applications etc. For instance with respect to noise reduction in one implementation video encoder computes motion vectors for all macroblocks of decoded source image data. Then video encoder utilizes motion magnitude direction and correlation to motion vectors of surrounding macroblocks to determine whether there is a local object motion in the input image. In this implementation the video encoder of then utilizes the magnitude of the vector to direct object tracking filtering aggressiveness or average shifts of a particular object to reduce statically random noise.

In another example with respect to image stabilization in one implementation video encoder computes motion vectors for all macroblocks and decoded source data. Video encoder then determines whether there is global motion in the image. This is accomplished by correlating all motion vector values and determining whether the correlated values are similar. If so then video encoder concludes that there is global motion. Alternatively the video encoder utilizes a large macroblock size and determines if there is overall motion of the large macroblock. After determining whether global motion is present if video encoder also finds that the global motion vector tends to be jerky across frames video encoder concludes that there is camera jerkiness and compensates for this before starting noise filtering and encoding operations.

Accordingly the specific features and operations of system are disclosed as exemplary forms of implementing the claimed subject matter.

This Appendix describes various exemplary aspects of an exemplary implementation of the video encoding acceleration APIs for accelerated video encoding also referred to as Video Acceleration VA Encode. This is only one example of API and other implementations can be implemented in view of this detailed description. In this implementation and as already described in the detailed description APIs allow video encoder module to leverage acceleration hardware e.g. a GPU support for accelerating Motion Estimation Residue Computation Motion Compensation Transform and or other encoding operations.

Block represents a number n image surfaces of input data. In one implementation each image surface represents a respective D3D surface. Operations of block implement noise filtering operations. In this implementation the noise filtering operations are implemented to by the known IDirectXVideoProcessor interface although other interfaces could also be used. Block represents noise filtered image surface data. At block motion estimation operations are implemented possibly via acceleration hardware as described above. The motion vectors are represented in buffer . Operations of block implement motion compensation. The resulting coded image surfaces are represented at block . Residual data computation operations of block operate on image data from buffers and resulting in residual data in buffer . In view of the residual data operations of function perform DCT and quantization processes to generate the residual data of buffer . Entropy coding operations of function and inverse quantization and IDCT operations of function operate on this residual data transformed and quantized data . Function generates residual data that is quantized and inverse quantized as shown in buffer . In view of the data in buffer function implements motion compensation operations.

Significantly the various data in respective ones of the buffer and are well known in the art. Additionally operations associated with respective ones of the functions and are also well known in the art. Thus it is not the respective data and details of the operations of the video encoder that are novel. In contrast to conventional encoders encoder interfaces with accelerated video encoding service and corresponding exposed video acceleration APIs to create a customized encoding pipeline e.g. a pipeline of configuration that interfaces in a device independent manner with arbitrary video acceleration hardware . The customized pipeline may implement at least a subset of the functions illustrated in and possibly more or different encoding functions. Exemplary such APIs a are described below although different implementations could also be put into practice.

Acceleration hardware is viewed as a pipeline. In this implementation a pipeline GUID is used to describe the most basic configuration elements of the pipeline. A goal of encode speed up may be thought of being related to a goal of pipeline efficiency.

The design allows for split or multi stage pipelines where data goes back and forth between the host PC and the hardware before the final output is obtained. The following described pipeline configurations represent non split single stage pipelines.

In this implementation the number of streams NumStreams is five although other numbers of streams could also be used. The actual StreamIds are shown in the diagram in parentheses.

 This is an example of a single stage non split pipeline and hence the Stage parameter of Execute is not used .

NumStreams for this pipeline configuration is three. The StreamIds for the various streams are show in the diagram in paranthesis.

This function returns buffers encode surfaces for use in the exemplary Execute call described below. The buffers are released promptly after use by calling ReleaseBuffer to avoid stalling the pipeline.

Normally this function returns control very soon as the buffers are already present in the allocator queue. The only conditions under which this function should block or return E NOTAVAILABLE are when all buffers from the allocator queue have been submitted to the device or being consumed by the application and hence not released.

This function is used to submit requests to acceleration hardware . It provides input and output data buffers obtained via GetBuffer as well as some configuration information. The function is asynchronous and its completion is indicated by the event being signaled. The completion status is indicated using the pStatus parameter which is allocated on the heap and checked only after the event has been signaled.

The buffers supplied as parameters to this function are not be read from eg via LockRect or written to by the encoding application until the function has truly completed. In this implementation true completion is implied by an error value being returned by the function or if this function returns success then by the signaling of hEvent parameter to this function . When the same buffer is input to several instances of the Execute call it is not be accessed until all associated Execute calls have completed. The pointer to a surface in use by Execute may still be supplied as a parameter to VA functions like Execute since this doesn t require the data to be locked. This last rule explains how the same input image may be used in multiple Execute calls at the same time.

The buffers supplied to this call obey the allocator semantics negotiated at creation time. If an external allocator is used when GetBuffer is expected to be used this function will return E FAIL.

If the event handle gets signaled it means that LockRect should complete instantly when called on any of the output surfaces since they are ready. In particular the LockRect call is expected to not block for any length of time by waiting on any event handles. Nor is it allowed to waste CPU time through busy spins.

The Execute call has data parameters and configuration parameters. Specific data parameters can be thought of as deriving from VA2 Encode ExecuteDataParameter base class or structure and specific configuration parameters can be thought of as deriving from VA2 Encode ExecuteConfigurationParameter base class or structure .

This structure acts as a base type for more specialized configuration information. The base type is typecast to a more specialized type based on the ConfigurationType parameter. The mapping between ConfigurationType and the specialized structures is described in the table below.

In computing sub pixel motion vector values the encoder estimates luma and chroma values using interpolation. The specific interpolation scheme is format dependent and the following GUIDs part of static configuration control the interpolation scheme.

There is no universally accepted definition of absolute quality so we are settling for a relative measure. The values indicated against TimeTaken should follow a strict proportion rule. If profile 1 takes 10 ms and profile 2 takes 20 ms the TimeTaken values should be in the ratio 20 10 2.

The methods in this interface allow an application to query the hardware for its capabilities and create an encoder object with a given configuration.

This base structure is typecast to a derived type on the StreamType field. The typecasts are described in the documentation for VA2 Encode StreamType.

Motion Vectors Surfaces and Residue Surfaces are associated with the above new D3D Format types which indicate the size of individual Motion Vectors and Residues. This size information is used by the driver when the application creates surfaces using one of the surface or resource creation APIs provided by. The resource flag associated with encode surfaces is VA2 EncodeBuffer.

This structure is effectively derived from IDirect3DSurface9 and carries state information that allows one to interpret the contents of the embedded D3D surface.

This enumeration value is used to decode the contents of the Motion Vector D3D9 Surface. Depending on the type of Motion Vector one of several different Motion Vector structures is used to interpret the contents of the surface.

In this implementation a residue surface is an array of signed integer values that are two bytes long e.g. of type INT16. This scheme is practical. For example MPEG 2 deals with 9 bit residue values and H.264 deals with 12 bit residues. Also if the original data was YUY2 the luma values occupy one byte each and hence the residues use 9 bits 0 255 255 . Further applying a DCT type transform increases the data requirement to 11 bits per residue value. All of these cases are adequately addressed by using 2 byte long signed residue values.

The width of a residue surface is the number of residue values in a line. For example a 640 480 progressive image with 4 2 2 sampling has 640 luma values and 320 chroma values per line. The size of associated the luma surface is 640 480 2 and that of the chroma surface is 320 480 2 bytes.

Residue Surfaces are created using the D3DFMT RESIDUE16 format flag and VA2 EncodeBuffer resource type.

Extension Devices are a pass through mechanism provided by the VA Interfaces to add new functionality in addition to Video Decoder and Video Processor functions. For example in one implementation such a mechanism is used to support a new Video Encoder function.

Extension Devices are analogous to an untyped funnel through which the application can send receive data to from the driver. The meaning of the data is unknown to the VA stack and is interpreted by the driver based on the pGuid parameter of the CreateExtensionDevice call and the Function parameter of ExtensionExecute.

Extension Devices are enumerated using the FND3DDDI GETCAPS with the type parameter being set to GETEXTENSIONGUIDCOUNT or GETEXTENSIONGUIDS. The codec application looks for VA Encoder Extension in the list of extension guids returned by GETEXTENSIONGUIDS to determine whether VA Encode support is available.

When querying for capabilities of the extension device the Encoder device the GETEXTENSIONCAPS is used with the following structure as pInfo in the D3DDDIARG GETCAPS structure.

The output of GETEXTENSIONCAPS is encapsulated in the pData parameter of D3DDDIARG GETCAPS. The pData parameter is interpreted as follows 

The actual creation happens via a D3DDDI CREATEEXTENSIONDEVICE call whose primary argument is shown below 

The actual extension unit functions are invoked via a D3DDDI EXTENSIONEXECUTE call. The instance of the Extension Unit is already associated with a GUID so the type of the extension unit is already known when the execute call is made. The only additional parameter is Function which indicates the particular operation to perform. For example an Extension Device of type Encoder may support MotionEstimation as one of its functions. Typically the Extension Device will have a GetCaps function of its own that enumerates the capabilities of the Extension Device.

The pBuffers parameter is not used by VA Encode and should be considered a reserved parameter. The Function parameter takes the following values for VA Encode 

The pPrivateInput and pPrivateOutput parameters of D3DDDIARG EXTENSIONEXECUTE are used to encapsulate the parameters of the Execute API call.

The following sections describe various structures and function callbacks associated with the VA Extension mechanism.

The following D3D structures and callback represent a generic D3D mechanism to obtain the capabilities of an extension device.

In one implementation to achieve maximum efficiency the encoder application is structured to full utilize processor s and graphics hardware . In one example while Motion Estimation is in progress for a certain frame the Quantization Step may be executed on a different frame.

The following 2 threaded application in pseudo code illustrates one way for the encoder to implement a 2 stage software pipeline and offers some examples of how to use the VA Encode interfaces effectively. This particular implementation enforces a buffering of k AllocatorSize as seen in the software thread. This accounts that there is asynchrony in the submission of a hardware request the hardware thread submits requests while the software thread picks up the results after a while and processes them.

ProcessInput above may be considered a wrapper around Execute and GetBuffer while ProcessOutput may be considered a wrapper around a Wait on the execute event followed up with appropriate ReleaseBuffer calls.

Parameter k represents the buffer between the pipeline stages. It denotes the allocator size and as a starting point we could use the same value used in the allocator negotiation between the Codec and the VA Encoder object the queue length . If k is larger than the allocator size then the ProcessInput call is likely to block anyway even before the k buffers get used.

The goal of the application should be to maximize time spent in SoftwareThread without blocking on ProcessOutput. In other words the application should be working on the VLE and Bitstream functions most of the time. If the hardware is very slow then ProcessOutput will block despite the allocator size of k . Software will always be ahead . The above pipeline is efficient only to the extent that the hardware takes about as much time to process a buffer as software takes to run VLE and Bitstream. All that the buffering of k achieves is to pad for jitters.

The following code fragment shows an exemplary pseudocode implementation of GetBuffer and ReleaseBuffer.

An exemplary complex pipeline is now described where the encoder requests multiple motion vectors from hardware and chooses one based on various parameters and resubmits them for processing. The following code naively continues to use a 2 stage pipeline as before requests multiple motion vectors and resubmits the best one. There is inherent serialization involved in this.

In the above example software operatios are blocked on ProcessOutput and ProcessOutput2 half of the time negatively effecting pipeline efficiency. On the other hand CPU utilization will be quite low and the overall throughput is still higher than non accelerated encode.

Since there are 3 pipeline stages additional buffer is added to pad between the two hardware stages. Hence the two values k1 and k2.

