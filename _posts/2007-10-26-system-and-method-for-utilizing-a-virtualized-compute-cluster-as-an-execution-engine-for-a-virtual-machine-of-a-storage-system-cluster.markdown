---

title: System and method for utilizing a virtualized compute cluster as an execution engine for a virtual machine of a storage system cluster
abstract: A system and method employs one or more clients of a virtualized compute cluster as an execution engine for a portion of a storage operating system implemented as a virtual machine on a storage system node of a storage system cluster. If there is processing bandwidth of a client that is not fully utilized and the load on the storage system node is high, the portion of the storage operating system is ported to the client of the compute cluster in a manner that externally distributes the storage architecture from the storage system cluster. Advantageously, the processing performance of the storage system cluster is improved by, among other things, offloading some of the network processing load from the storage system node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08468521&OS=08468521&RS=08468521
owner: NetApp, Inc.
number: 08468521
owner_city: Sunnyvale
owner_country: US
publication_date: 20071026
---
The invention relates to virtual machine environments and more specifically to utilization of a virtualized computer cluster as an execution engine for a virtual machine of a storage system cluster.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a host computer or client. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive HDD or direct access storage device DASD .

The storage system may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access data containers stored on the system. The clients may be interconnected in a tightly coupled configuration and organized as a compute cluster to facilitate load balancing and availability of services. In this model each client may comprise an application executing on a computer e.g. an application server of the cluster that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. The client may request the services of the storage system by issuing file based and block based protocol data access requests in the form of packets to the system over the network.

A virtual machine environment illustratively includes a computer such as a client and or storage system executing a virtual machine operating system as well as one or more guest operating systems to essentially implement virtual machines on the client and or storage system. Each guest operating system may comprise a conventional operating system such as the Linux operating system or a specialized operating system such as a storage operating system. The virtual machine environment may also include a plurality of guest operating systems or portions of a guest operating system executing on each client within a virtual machine cluster environment such as a virtualized compute cluster. In this latter environment each client of the virtualized compute cluster may request the services of the storage system by accessing data containers stored on the system.

Over time storage processing performance of the storage system may degrade as the data access request load originating from the compute cluster increases. A common solution to this problem is to interconnect a plurality of storage systems to provide a storage system cluster configured to service the clients of the compute cluster. Each storage system or node may be configured to service one or more volumes of disks wherein each volume stores one or more data containers such as files and logical units. Alternatively the volumes serviced by the particular storage system node may be distributed among all of the nodes of the storage system cluster. This configuration distributes the data access requests along with the processing resources needed to service such requests among all of the storage system nodes thereby reducing the individual processing load on each node.

Another solution is to provide a proxy caching system that includes a front end proxy device having local storage i.e. a network cache coupled to a back end storage system or node having remote storage. The network cache is configured to locally store cache certain data that may be used to service certain data access requests from the clients. In particular data access requests directed to the cached data are serviced by the network cache thereby offloading servicing of those requests from the storage system node while allowing the node to perform other useful storage processing functions.

The invention relates to system and method for employing one or more clients of a virtualized compute cluster as an execution engine for a portion of a storage operating system implemented as a virtual machine on a storage system node of a storage system cluster. Each storage system node and client executes a virtual machine operating system comprising a hypervisor module configured to mask low level hardware operations from one or more guest operating systems executing on the virtual machine operating system. A plurality of domains is illustratively disposed over the hypervisor module wherein each domain is representative of a virtual machine within which a guest operating system or a portion thereof executes.

In an illustrative embodiment each node of the storage system cluster is organized as a network module N module and a disk module D module . The D module is embodied as a high availability data layout portion of the storage operating system that services one or more storage devices such as disks whereas the N module is embodied as a network facing portion of the storage operating system that terminates a network connection for block or file protocols from a client of the virtualized compute cluster. The N and D modules of the storage system node cooperate with N and D modules of other storage system nodes to provide a distributed storage architecture of the storage system cluster. Notably each N module and D module operates in a virtual machine of each storage system node.

According to the invention an N module may be ported as a virtual machine from a node of the storage system cluster to a client to absorb utilize available processing bandwidth of the virtualized compute cluster. That is if there is processing bandwidth of the client that is not fully utilized and the load on the storage system node is high the invention enables porting of the N module to the client of the compute cluster to thereby utilize that processing bandwidth of the client as an execution engine in a manner that externally distributes the storage architecture from the storage system cluster. Thereafter instances of the ported N module may be spawned any number of times to execute over the hypervisor module within any number of clients to absorb available processing bandwidth of the compute cluster. Advantageously the invention increases the processing performance of the storage system cluster by among other things offloading some of the network processing load from the storage system node.

In an illustrative embodiment each client may be a general purpose computer having a multi processor architecture comprising processing elements and or logic circuitry configured to execute software code and manipulate data structures. In addition the client may be configured to interact with the storage system in accordance with a client server model of information delivery. That is the client may request the services of the storage system node and the node may return the results of the services requested by the client by exchanging packets over the network . The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of data containers such as files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of data containers such as blocks.

The storage system is illustratively a computer comprising one or more processors a memory a network adapter and a storage adapter interconnected by a system bus . The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with an illustrative embodiment described herein. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The network adapter illustratively comprises a plurality of ports adapted to couple the storage system to the clients over network embodied as point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the storage system to the network . Illustratively the computer network may be further embodied as an Ethernet network or a Fibre Channel FC network.

The storage adapter cooperates with the virtual machine operating system executing on the storage system to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on storage devices such as disks . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

As multi processor clients and or storage system nodes increase in processing power using e.g. a multi processor architecture it may make economic sense to execute a number of guest operating systems or instantiations of a guest operating system of the virtual machine operating system instead of acquiring a number of physical hardware systems. For example each client of the virtualized compute cluster may execute instances e.g. two of the same guest operating system such as the conventional Linux operating system disposed over the virtual machine operating system to use the available processing bandwidth provided by the clients of the cluster. Alternatively the client may execute a heterogeneous pairing of guest operating systems e.g. a Microsoft Windows based guest operating system paired with a Linux based guest operating system. Similarly and as described further herein each storage system may execute an instance of a guest operating system such as a storage operating system. Alternatively the storage system node may execute a plurality of guest storage operating systems within a storage system cluster configuration to inter alia efficiently service the clients of the virtualized compute cluster .

Illustratively disposed over the hypervisor module is a plurality of domains for example Domain etc. Each domain is representative of a virtual machine within which a guest operating system or a portion thereof executes. In the illustrative embodiment of the VMware virtual machine operating system Domain provides administrator functionality and as such may execute a guest operating system based kernel and or one or more administrative modules such as management applications described further herein. Domain may also include for example a plurality of software drivers adapted to interface with various hardware components including for example in the case of storage system node network adapter and storage adapter . The drivers illustratively provide an interface for I O operations issued by the guest operating system.

Each Domain and illustratively executes a guest operating system or a portion of a guest operating system. For example in the case of a client each Domain and executes an instance of a guest operating system such as the Linux operating system whereas in the case of storage system node each Domain and executes a portion of one or more guest operating systems such as a storage operating system . However it should be noted that in accordance with the principles of the present invention other types of guest operating systems may be used. As such the description of a storage operating system being utilized as the guest operating system of storage system should be taken as exemplary only.

The storage operating system implements a data layout engine such as an illustrative write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by storage devices such as disks. The file system logically organizes the information as a hierarchical structure of named data containers such as directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of named data containers such as blocks on the disks that are exported as named logical unit numbers luns .

In an illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term Data ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks of the storage system node . To that end the storage server includes a file system module in cooperating relation with a RAID system module and a disk driver system module . The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system implements a virtualization system of the storage guest operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces such as a user interface of a management framework see in response to a user system administrator issuing commands to the storage operating system . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store meta data describing the layout of its file system these meta data files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Broadly stated all inodes of the write anywhere file system are organized into the inode file. A file system fs info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode file may directly reference point to data blocks of the inode file or may reference indirect blocks of the inode file that in turn reference data blocks of the inode file. Within each data block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally a request from the client is forwarded as a packet over the network and onto storage system node where it is received at the network adapter . An appropriate network driver of the virtual machine operating system processes the packet and forwards it to the appropriate guest operating system such as storage operating system . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the storage operating system returns a reply to the driver which forwards the reply over the network adapter to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage system may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a guest operating system implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein. It should be noted that while this description is written in terms of a write anywhere file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In an illustrative embodiment the storage server is embodied as a disk module D module of the storage operating system to service one or more disks . In addition the multi protocol engine is embodied as a network module N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the storage operating system e.g. guest operating system within the storage system cluster. Accordingly one portion of the storage operating system executing in Domain is illustratively the N module while another portion of the storage operating system executing in Domain is illustratively the D module .

Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture. To that end each module includes a cluster fabric CF interface module adapted to implement communication among the modules including D module to D module communication. The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module . That is the N module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D module of the storage system cluster. Notably the CF interface modules cooperate to provide a single file system image across all D modules . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module .

Further to the illustrative embodiment the N module and D module are implemented as separately scheduled processes of guest operating system however in an alternate embodiment the modules may be implemented as pieces of code within a single operating system process. Communication between an N module and D module is thus illustratively effected through the use of message passing between the modules. A known message passing mechanism provided by the operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc. The SpinFS protocol is described in U.S. Patent Application Publication No. US 2002 0116593.

The CF interface module implements the CF protocol for communicating file system commands among the N D modules of the virtual machine operating system . Communication is illustratively effected by the D module exposing the CF API to which an N module or another D module issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N module encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same domain or ii a remote procedure call RPC when communicating the command to a D module residing on a different domain of the virtual machine operating system . In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

The VLDB is a database process that implements a namespace for among other things tracking the locations of various storage components e.g. flexible volumes aggregates etc. among various storage operating systems . Flexible volumes hereinafter volumes and aggregates are further described in U.S. Publication No. 2005 0246401 now issued as U.S. Pat. No. 7 409 494 on Aug. 5 2008 entitled Extension of Write Anywhere File System Layout by John K. Edwards et al. the contents of which are hereby incorporated by reference. Illustratively the VLDB includes a plurality of entries used to keep track of the locations of the volumes and aggregates within the storage system cluster. Examples of such VLDB entries include a VLDB volume entry and a VLDB aggregate entry .

The VLDB illustratively implements a RPC interface e.g. a Sun RPC interface which allows the N module to query the VLDB . When encountering contents of a data container handle the N module sends an RPC to the VLDB process. In response the VLDB returns to the N module the appropriate mapping information including an ID of the D module that owns the data container. The N module caches the information in e.g. a configuration table and uses the D module ID to forward the incoming request to the appropriate data container. All functions and interactions between the N module and D module are coordinated on a virtual machine operating system wide basis through the collection of management processes and the RDB library applications.

To that end the management processes have interfaces to are closely coupled to RDB . The RDB comprises a library that provides a persistent object store storing of objects for the management data processed by the management processes. Notably the RDB replicates and synchronizes the management data object store access across all nodes of the storage system cluster to thereby ensure that the RDB database image is identical on all of the domains of the virtual machine operating systems executing on those nodes. At system startup each guest operating system records the status state of its interfaces and IP addresses those IP addresses it owns into the RDB database.

The present invention is directed to a system and method for employing one or more clients of virtualized compute cluster as an execution engine for a portion of storage operating system implemented as a virtual machine on storage system node of a storage system cluster. As noted each storage system node and client executes a virtual machine operating system comprising a hypervisor module configured to mask low level hardware operations from one or more guest operating systems executing on the virtual machine operating system. A plurality of domains is illustratively disposed over the hypervisor module wherein each domain is representative of a virtual machine within which a guest operating system or a portion thereof executes.

According to the invention an N module may be ported as a virtual machine from a node of the storage system cluster to a client to absorb utilize available processing bandwidth of the virtualized compute cluster. That is if there is processing bandwidth of the client that is not fully utilized e.g. at least one processor of the multi processor architecture of the client is under utilized and if the load on the storage system node is high the invention enables porting of the N module to the client of the compute cluster to thereby utilize the under utilized processor as an execution engine in a manner that externally distributes the storage architecture from the storage system cluster. Thereafter instances of the ported N module may be spawned any number of times to execute over the hypervisor module within any number of clients of the compute cluster to absorb available processing bandwidth of the cluster. Advantageously the invention increases the processing performance of the storage system cluster by among other things offloading some of the network processing load from the storage system node.

As used herein the term port denotes a change in location of a software module e.g. executing on a virtual machine from an operating system environment e.g. a virtual operating system environment in which it was developed to another operating system environment at which it can be run. For example use of the term port may include creation of an entirely new instance of an N module and execution of that N module instance on a processor of the client of the virtualized compute cluster instead of on the guest storage operating system of a storage system node .

In addition the term port may include migration of the N module from the storage system node to the client. Migration in this context may be effected through use of hypervisor properties that allow propagation of a virtual machine running an existing software component such as an N module to another physical platform. Broadly stated data and program state of the N module is captured and saved i.e. checkpointed and the N module is shut down. The checkpointed state of the N module is then propagated over to the client so that the N module may be restarted to resume processing where it left off. In an illustrative embodiment a virtual machine running an N module may be migrated from a storage system node to a client of the virtualized compute cluster using a conventional migration system such as the VMotion migration system available from VMware Inc. The VMotion migration system is well known and described in a paper titled Fast Transparent Migration for Virtual Machines by Michael Nelson et al. Proceedings of USENIX 05 General Track USENIX Association April 2005 which paper is hereby incorporated by reference as though fully set forth herein.

However if the service load is determined to be high then a determination is made Step as to whether there is processing bandwidth on the clients that is not fully utilized and thus available for use by the storage system node. In the illustrative embodiment determination of available processing bandwidth on the client is made using conventional policy based load management tools. If there is no available processing bandwidth on the client an error message is generated and provided to the administrator via e.g. the administrator user interface of the management framework at Step and the procedure ends at Step . Yet if there is available processing bandwidth on the client the portion of the storage operating system implemented as a virtual machine e.g. an N module on the storage system node is ported to the client in Step . In Step one or more instances of the ported portion of the storage operating system is spawned to execute over the hypervisor module of the virtual operating system executing on the client to absorb available processing bandwidth of the compute cluster. In Step the portion and or the spawned instance of the storage operating system services a subsequent data access request issued by the client by inter alia forwarding the request to another portion of the storage operating system implemented as a virtual machine e.g. a D module on the storage system node. Note that the ported portion of the storage operating system e.g. N module forwards the request to the other portion of the storage operating system e.g. D module by encapsulating one or more CF messages over conventional network messaging such as TCP IP to thereby enable e.g. N module to D module communication as described herein. The procedure then ends at Step .

While there have been shown and described illustrative embodiments for employing one or more clients of a virtualized compute cluster as an execution engine for a portion of a storage operating system implemented as a virtual machine on a storage system node of a storage system cluster it is to be understood that various other adaptations and modifications may be made within the spirit and scope of the present invention. For example in other embodiments of the invention the ported N module may be configured to cache certain data used to service certain data access requests from one or more clients of the virtualized compute cluster. The storage system environment of these embodiments may incorporate a vertical scaling network configuration wherein the ported N module is configured to provide a form of network caching that exploits the location of the module closer to the consumers i.e. clients that actually access the data. Illustratively the ported N module may be configured to provide caching of data using a flexible number of cache control policies and protocols such as NFS NRV SpinNP etc.

Illustratively in one such embodiment the ported N module may be configured with non persistent volatile cache storage i.e. of a virtualized compute cluster client implemented with a write through cache coherency policy to prevent data loss. In another embodiment the ported N module may be configured with persistent cache storage utilizing non volatile memory of the client and implemented with write through write back or other similar policies to ensure cache coherency in the externally distributed storage system cluster architecture. Here the non volatile memory is illustratively embodied as a large volume solid state random access memory array having either a back up battery or other built in last state retention capabilities e.g. FLASH memory . In still yet another embodiment the VLDB namespace may be apportioned among a plurality of ported and or spawned N modules configured to cooperatively interact to implement shared cache coherency policies in the externally distributed architecture. These embodiments of the invention advantageously accelerate I O performance of the storage system services by allowing the N module s to operate closer to the consumers i.e. clients of those services.

The foregoing description has been directed to specific embodiments of this invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. For instance it is expressly contemplated that the components elements modules and or structures described herein can be implemented as software including a computer readable medium having program instructions executing on a computer hardware firmware or a combination thereof. Also electromagnetic signals may be generated to carry computer executable instructions that implement the present invention over e.g. a wireless data link or a data network such as the Internet. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the invention. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention.

