---

title: Automatic reading tutoring with parallel polarized language modeling
abstract: A novel system for automatic reading tutoring provides effective error detection and reduced false alarms combined with low processing time burdens and response times short enough to maintain a natural, engaging flow of interaction. According to one illustrative embodiment, an automatic reading tutoring method includes displaying a text output and receiving an acoustic input. The acoustic input is modeled with a domain-specific target language model specific to the text output, and with a general-domain garbage language model, both of which may be efficiently constructed as context-free grammars. The domain-specific target language model may be built dynamically or “on-the-fly” based on the currently displayed text (e.g. the story to be read by the user), while the general-domain garbage language model is shared among all different text outputs. User-perceptible tutoring feedback is provided based on the target language model and the garbage language model.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08433576&OS=08433576&RS=08433576
owner: Microsoft Corporation
number: 08433576
owner_city: Redmond
owner_country: US
publication_date: 20070119
---
Automatic reading tutoring has been a growing application for natural language processing and automatic speech recognition tools. An automatic reading tutoring system can provide a story or other text for a student to read out loud and track the student s reading and any errors the student makes. It can diagnose particular kinds of systematic errors the student makes respond to errors by providing assistance to the student and evaluate d student s reading aptitude.

Automatic reading tutoring systems typically involve building a language model for a given story or other text prior to presenting the text to the student to begin a reading tutoring episode. Building the language model for the story or other text typically involves preparing to accommodate all possible words in the text being used as well as all possible mistaken words the student might utter to the best that these can be foreseen. This is particularly difficult for reading tutoring systems since one of their main audiences is children learning to read their native language and children tend not only to make many unpredictable mistakes in reading but also to get distracted and make frequent utterances that have nothing to do with the displayed text.

Building the language model for the text also typically involves accessing a large corpus and requires a significant amount of time to prepare. It also presents a large processing burden during runtime which tends to translate into processing delays between when the student reads a line and when the computer is able to respond. Such delays tend to strain the student s patience and interrupt the student s attention. Additionally the reading tutoring system cannot flag all possible reading errors and may erroneously indicate the student has made an error when the student reads a portion of text correctly. Trying to improve the system s ability to catch errors and not indicate false alarms typically involves raising the time spent processing and further stretching out the delays in the system s responsiveness while trying to reduce the system s lag time in responding conversely tends to degrade performance in error detection and false alarms.

The discussion above is merely provided for general background information and is not intended to be used as an aid in determining the scope of the claimed subject matter.

A novel system for automatic reading tutoring is disclosed herein that naturally provides effective error detection and reduced false alarms combined with low processing time burdens and response times short enough to maintain a natural engaging flow of interaction. According to one illustrative embodiment an automatic reading tutoring method includes displaying a text output and receiving an acoustic input. The acoustic input is modeled with a domain specific target language model specific to the text output and with a general domain garbage language model. User perceptible feedback is provided based on the target language model and the garbage language model.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The claimed subject matter is not limited to implementations that solve any or all disadvantages noted in the background.

Automatic reading tutoring method includes step of displaying a text output such as a sentence of a paragraph from within a larger story or other text. Method next includes step of receiving an acoustic input such as a spoken word utterance from a student reading aloud the text output from step . Method further includes step of modeling the acoustic input with a domain specific target language model specific to the text output and step of further modeling the acoustic input with a general domain garbage language model.

The target language model and the garbage language model may be used for example to detect elements of the acoustic model that do not correspond properly to the text output and to identify these elements as miscues. The target language modeling and garbage language modeling are done in parallel and used together although in an asymmetrical fashion since the target language model is domain specific while the garbage language model is general domain providing unique advantages. Because the speech inputs are compared between the parallel target language model and garbage language model and assigned to the one or the other the language modeling can be thought of as polarizing the speech inputs. The steps of modeling the language models are further elaborated below.

Method also includes step of providing user perceptible feedback based on the target language model and the garbage language model. Such user perceptible feedback may provide confirmation and or encouragement when the student reads the texts correctly and may include information on any miscues such as by providing suggestions or other assistance when the student makes a mistake in reading the text. Such assistance may take the form of audible corrections to the miscues provided by the computing device in spoken word for example. These spoken word corrections may be pre recorded or may be generated by a text to speech tool for example. The assistance may also take the form of displaying a phonetic representation of the portion of the text output corresponding to a miscue on a monitor or other visual output for example. Such user perceptible feedback thereby provides automatic reading tutoring for the student.

The user perceptible feedback may also take the form of an evaluation of how well the acoustic input corresponds to the text output such as a score representing how much of the acoustic input is free of miscues. Such an evaluation can constitute a score on which the student is graded for a test or it can be used by the student or the student s teacher or parent to keep track of the student s progress over time and set goals for further learning.

In an application in which the reading tutoring system is provided for students of a second language as opposed to children learning to read their native language the user perceptible feedback may include displaying a translation into a different language such as the student s native language of a portion of the text output for which the acoustic input includes a miscue.

Such user perceptible feedback may also be provided when the system evaluates the acoustic input to correctly correspond to the text output indicating that the student has correctly read the text output. The system here and throughout the remaining disclosure may be used as a shorthand reference to a computing system executing an illustrative embodiment such as method . For example the system may provide a low key unobtrusive running indicator of each of the student s spoken word inputs that represents a correct reading of the corresponding text output. This might be for example a green light that lights up in a corner of a screen every time the student correctly reads a text output in one illustrative embodiment.

Method further includes decision node in which the system evaluates whether it is finished with a larger reading tutoring episode of which the text output of step is part.

If the system is finished with a reading tutoring episode the method may proceed to endpoint . If it is not yet finished the system may return to the beginning and iteratively repeat the process of displaying additional text outputs as in step receiving corresponding acoustic inputs as in step assembling additional domain specific target language models respectively based on each of the additional text outputs as in step modeling the acoustic input with the general domain garbage language model as in step and provide new user perceptible feedback for that iteration as in step .

Step of modeling the acoustic input with a domain specific target language model specific to the text input may be performed within a restricted period of time relative to when its respective text output is displayed as in step . That is the system may calculate a language model score for the target words of the displayed text only once the target words are called up for display which may be in a sentence or a paragraph at a time for example. In this illustrative embodiment therefore a small language model is built just for an individual sentence or paragraph at the time that short text sample is brought up for display for the student to read.

This provides a number of advantages. Because the text sample is so small the system can process it in a very short time short enough that the student will not experience any noticeable delay. This also allows the system to begin functioning or to begin a tutoring episode based on a text or a position within a text just selected by the student without having to then stop and process the entire text prior to allowing the student to continue.

A reading tutoring system such as this is illustratively depicted as language modeling system in . Language modeling system involves a combination of general domain garbage modeling and domain specific target modeling implemented in this illustrative embodiment with target language model a domain specific statistical language model implemented as a Context Free Grammar CFG in this embodiment and garbage language model a general domain N gram statistical language model implemented as a Context Free Grammar CFG in this embodiment. Target language model is engaged through grammar entry node and leads to grammar exit node in this illustrative embodiment.

Ordinarily a general domain N gram statistical language model has a low processing burden but poor modeling performance while a domain specific statistical language model ordinarily has high modeling performance but imposes a high processing burden involving significant delays.

Language modeling system combines the best of both worlds as an on line interpolated language model with the domain specific target language model at its core which is trained on the fly from a limited set of training sentences such as a then current sentence or paragraph in a story or other text output. At the same time the general domain garbage language model which may be implemented as an N Gram Filler such as a restricted version of a dictation grammar is attached to target language model through a unigram back off node comprised in target language model . Garbage language model thereby provides robustness in that it is able to siphon off general miscues without having to have them defined in advance. The interpolation between target language model and garbage language model may be promoted by reserving some unigram counts for unforeseen garbage words to get swept aside from target language model by garbage language model .

Beginning from the path from grammar entry node to target language model target language model then has a weight wthat controls the possibility of moving from within target language model to unigram back off node . A second weight w controls the possibility of moving from unigram back off node to garbage language model . The target language model is relatively small such as enough to occupy a few kilobytes of memory in one embodiment due to being based on only the text sample currently on display at a given time such as a paragraph or a sentence. The garbage language model is significantly larger in the same embodiment it may be enough to occupy several megabytes of memory but this does not pose any significant processing burden or reaction time delay because the one single garbage language model may be shared for the purposes of all the text samples that are successively modeled with the target language model . So the only new language model that is built within the timeframe of providing the display text output is the few kilobytes worth or so of the local scale on the fly target language model .

In addition to the special nodes consisting of the grammar entry node grammar exit node and unigram back off node target language model includes three nodes corresponding to bigram states with one word each bigram state node for the word Giants bigram state node for the word are and bigram state node for the word huge . Running between the nodes are possible paths that may be taken depending on the acoustic input. Each of grammar entry node and bigram state nodes and have possible paths leading to unigram back off node and thence to garbage language model and back if any one of the nodes is followed by a miscue. Target language model may also include more complex N gram nodes to provide stronger robustness such as trigram nodes Giants Giants Are Are Huge Huge not shown in in the example sentence Giants are huge . The domain specific target language model may therefore be constructed with different complexity ranging from simple unigrams to more complex bigrams and trigrams to more complicated higher order N grams in order to provide different levels of robustness in the system. Using a relatively simpler unigram or bigram garbage language model may provide significant advantages in efficiency. The complexity of the domain specific target language model may be user selectable and may be selected according to the nature of the applications being implemented and the user s reading ability.

Grammar entry node bigram state nodes and and grammar exit node also have possible paths running in sequence between them allowing for the potential match of the acoustic input with bigrams composed of the bigram state nodes if the student correctly reads aloud as embodied in the acoustic input the two word sequence Giants are as well as the potential match of the acoustic input with bigrams composed of the bigram state nodes if the student correctly reads aloud the two word sequence are huge .

The garbage language model is used to detect reading miscues whether or not unforeseen and without any need to predict in advance what the miscues will be like to assemble and comb through a miscue database or to try to decode miscues phonetically. This provides a particular advantage in a reading tutoring system for children who are liable to say any variety of things or make any variety of sounds that have nothing to do with the displayed text output they are supposed to be reading. It is also advantageous for adult learners of a second language as it detects the frequent miscues they may make such as by mispronouncing the words in the text output of the non native language they are studying.

Garbage language model may be obtained from a general domain N gram model but restricted or trimmed down to a smaller selection from a dictation grammar such as only the 1 600 most common words for example. This is one example of a small selection that will reduce the processing burden on the system that is nevertheless very effective. A small set of lexical features may be used by the garbage language model to save further on the processing burden. It has been found for example that basing the garbage language model only on unigrams and bigrams provided very effective garbage language modeling that was not substantially improved by trying to add additional more burdensome lexical features to the language modeling. Garbage language model with different complexity may be used ranging from a simple unigram to more a more complex bigram or trigram or higher order N gram although in some embodiments higher orders of N grams may provide diminishing returns in robustness while increasing the computational burden such that building the garbage language model in a unigram or bigram form may provide the best efficiency for the goals for that embodiment.

Garbage language model can thereby be built on the fly during usage of the reading tutoring system garbage language model does not impose any burden to change decoding engines and it can interface with any automatic speech recognition system and it provides adjustable tunable weighting parameters w wthat allow the sensitivity of the garbage language modeling in terms of its Receiver Operating Characteristic ROC curve to be freely adjusted based on preference the level of the student s knowledge and so forth. depicts just such an ROC curve for garbage language model based on further discussion on the weighting parameters provided below in connection with .

Given a spoken word acoustic input X a target word T and a garbage word G a hypothesis testing scenario can be obtained as follows 

where P X T and P X G are the acoustic score for the target and garbage words respectively and P T and P G are language model scores for the target and garbage words respectively. The above decision rule is equivalent to the following decision rule 

where is a threshold as an explicit function of the weighting parameter w. This detection scenario is equivalent to regular hypothesis testing in an utterance verification problem.

When the garbage model weight is increased therefore the miscue detection rate also increases along with some increase in the rate of false alarms. The relationship between the prevalence of the two factors according to one illustrative implementation can be seen in where the curve represents the rates of both detection and false alarm corresponding to a series of selected weighting parameters wwith a set of pre trained acoustic model and language models. In general it may often be desirable to train better acoustic and language models to obtain a curve towards the upper left corner of the graph to work a good compromise between relatively high detection and relatively low false alarm rate.

For a fixed set of acoustic and language models it may also be desired to adjust the weighting parameter to be more lenient and occupy a spot on the curve more toward the lower left such as for beginning students. This is to specifically avoid false alarms that might discourage these beginning students at the expense of performance in the absolute detection rate. For more advanced students such as adult learners of a second language and still assuming a fixed set of acoustic and language models it may be preferable to adjust the weighting parameter wto make the system more strict i.e. to move the operating point toward the upper right portion of the curve in when the students might be expected to understand the false alarms as such or to have more patience with them but be more interested in addressing as many errors in reading as possible.

The miscues may be identified with one or more miscue categories and the user perceptible feedback may be based in part on one of the miscue categories with which a miscue in the acoustic input is identified so that it will correct a mispronunciation for example but simply continue to prompt for an acoustic input if the miscue is an interjection or background noise. The miscue categories may include for example word repetition breath partial word pause hesitation or elongation wrong word mispronunciation background noise interjection or insertion non speech sound and hyperarticulation.

Computing system environment as depicted in is only one example of a suitable computing environment for implementing various embodiments and is not intended to suggest any limitation as to the scope of use or functionality of the claimed subject matter. Neither should the computing environment be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary operating environment .

Embodiments are operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with various embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers telephony systems distributed computing environments that include any of the above systems or devices and the like.

Embodiments may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. Some embodiments are designed to be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules are located in both local and remote computer storage media including memory storage devices. As described herein such executable instructions may be stored on a medium such that they are capable of being read and executed by one or more components of a computing system thereby configuring the computing system with new capabilities.

With reference to an exemplary system for implementing some embodiments includes a general purpose computing device in the form of a computer . Components of computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media include both volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media include but are not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computer .

Communication media typically embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and include any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media include wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example and not of limitation illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they may be different copies.

A user may enter commands and information into the computer through input devices such as a keyboard a microphone and a pointing device such as a mouse trackball or touch pad. Other input devices not shown may include a joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer is operated in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a hand held device a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

Memory is implemented as non volatile electronic memory such as random access memory RAM with a battery back up module not shown such that information stored in memory is not lost when the general power to mobile computing device is shut down. A portion of memory is illustratively allocated as addressable memory for program execution while another portion of memory is illustratively used for storage such as to simulate storage on a disk drive.

Memory includes an operating system application programs as well as an object store . During operation operating system is illustratively executed by processor from memory . Operating system in one illustrative embodiment is a WINDOWS CE brand operating system commercially available from Microsoft Corporation. Operating system is illustratively designed for mobile devices and implements database features that can be utilized by applications through a set of exposed application programming interfaces and methods. The objects in object store are maintained by applications and operating system at least partially in response to calls to the exposed application programming interfaces and methods.

Communication interface represents numerous devices and technologies that allow mobile computing device to send and receive information. The devices include wired and wireless modems satellite receivers and broadcast tuners to name a few. Mobile computing device can also be directly connected to a computer to exchange data therewith. In such cases communication interface can be an infrared transceiver or a serial or parallel communication connection all of which are capable of transmitting streaming information.

Input output components include a variety of input devices such as a touch sensitive screen buttons rollers and a microphone as well as a variety of output devices including an audio generator a vibrating device and a display. The devices listed above are by way of example and need not all be present on mobile computing device . In addition other input output devices may be attached to or found with mobile computing device .

Mobile computing environment also includes network . Mobile computing device is illustratively in wireless communication with network which may be the Internet a wide area network or a local area network for example by sending and receiving electromagnetic signals of a suitable protocol between communication interface and wireless interface . Wireless interface may be a wireless hub or cellular antenna for example or any other signal interface. Wireless interface in turn provides access via network to a wide array of additional computing resources illustratively represented by computing resources and . Naturally any number of computing devices in any locations may be in communicative connection with network . Mobile computing device is enabled to make use of executable instructions stored on the media of memory component such as executable instructions that enable mobile computing device to implement various functions of automatic reading tutoring with parallel polarized language modeling in an illustrative embodiment.

Although the subject matter has been described in language specific to certain illustrative structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as illustrative examples of ways in which the claims may be implemented. As a particular example while the terms computer computing device or computing system may herein sometimes be used alone for convenience it is well understood that each of these could refer to any computing device computing system computing environment mobile device or other information processing component or context and is not limited to any individual interpretation. As another particular example while many embodiments are presented with illustrative elements that are widely familiar at the time of filing the patent application it is envisioned that many new innovations in computing technology will affect elements of different embodiments in such aspects as user interfaces user input methods computing environments and computing methods and that the elements defined by the claims may be embodied according to these and other innovative advances in accordance with the developing understanding of those skilled in the art while still remaining consistent with and encompassed by the subject matter defined by the claims herein.

