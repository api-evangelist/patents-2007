---

title: Vertex cache map mode for per-vertex state changes
abstract: A vertex cache within a graphics processor is configured to operate as a conventional round-robin streaming cache when per-vertex state changes are not used and is configured to operate as a random access storage buffer when per-vertex state changes are used. Batches of vertices that define primitives and state changes are output to parallel processing units for processing according to vertex shader program. In addition to allowing per-vertex state changes, the vertex cache is configured to store vertices for primitive topologies that use anchor points, such as triangle strips, line loops, and polygons.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08237725&OS=08237725&RS=08237725
owner: NVIDA Corporation
number: 08237725
owner_city: Santa Clara
owner_country: US
publication_date: 20071105
---
The present invention generally relates to graphics technology for generating vertex and primitive information in a graphics system. More particularly the present invention is directed towards generating vertex and primitive information in a graphics system when per vertex state changes exist.

Current graphics processors use a vertex cache to store recently used vertices that define primitives to be rendered. Since a single vertex is oftentimes reused to define more than one primitive the cache is an efficient storage mechanism. The vertex cache is typically a round robin streaming cache that does not accommodate changes in vertex processing state. Since conventional vertex processing programs do not change the vertex processing state on a per vertex level i.e. within a primitive a round robin streaming cache is an effective storage mechanism with the oldest vertex being overwritten by the newest vertex.

More recently the OpenGL graphics API allows per vertex state changes. Conventional vertex caches are not configured to support per vertex state changes. Accordingly what is needed in the art is a system and method for providing the efficient storage benefits of vertex caching while allowing per vertex state changes.

A vertex cache within a graphics processor is configured to operate as a conventional round robin streaming cache when per vertex state changes are not used and is configured to operate as a random access storage buffer when per vertex state changes are used. Batches of vertices that define primitives and state changes are output to parallel processing units for processing according to vertex shader program. In addition to allowing per vertex state changes the vertex cache is configured to store vertices for primitive topologies that use anchor points such as triangle strips line loops and polygons.

Various embodiments of a method of the invention for loading vertex slots with vertex information include receiving first vertex information for a first vertex of a primitive storing the first vertex information in a primitive buffer and receiving second vertex information for a second vertex of a primitive. When a vertex state change has occurred within the primitive and after the first vertex information was stored in the primitive buffer vertex slots are flushed to initiate processing of a first batch of vertices by a vertex shader and the vertex state change is output to configure the vertex shader for processing subsequent vertices.

Various embodiments of the invention for a computing system configured to assemble vertex information include a multithreaded processing core and a data assembler. The multithreaded processing core includes vertex slots and a vertex shader that is configured to process vertex information stored in the vertex slots according to a vertex shader program. The data assembler includes a primitive buffer and is coupled to the multithreaded processing core. The data assembler is configured to receive the vertex information for a vertices of a primitive store the vertex information in the primitive buffer determine when a vertex state change occurs within the primitive flush the vertex slots to initiate processing of a batch of vertices specified by the vertex information by the vertex shader and output the vertex state change to configure the vertex shader for processing subsequent vertices.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

System memory includes a device driver that is configured to provide one or more instruction stream buffers that specify the location of data and program instructions to parallel processing subsystem . The data and program instructions may be stored in system memory or memory within other devices of system . Device driver is executed by CPU to translate instructions for execution by parallel processing subsystem based on the specific capabilities of parallel processing subsystem . The instructions may be specified by an application programming interface API which may be a conventional graphics API such as Direct3D or OpenGL.

Memory bridge which may be e.g. a Northbridge chip is connected via a bus or other communication path e.g. a HyperTransport link to an I O input output bridge . I O bridge which may be e.g. a Southbridge chip receives user input from one or more user input devices e.g. keyboard mouse and forwards the input to CPU via communication path and memory bridge . A parallel processing subsystem is coupled to memory bridge via a bus or other communication path e.g. a PCI Express Accelerated Graphics Port or HyperTransport link . In one embodiment parallel processing subsystem is a graphics subsystem that delivers pixels to a display device e.g. a conventional CRT or LCD based monitor .

A system disk is also connected to I O bridge . A switch provides connections between I O bridge and other components such as a network adapter and various add in cards and . Other components not explicitly shown including USB or other port connections CD drives DVD drives film recording devices and the like may also be connected to I O bridge . Communication paths interconnecting the various components in may be implemented using any suitable protocols such as PCI Peripheral Component Interconnect PCI Express PCI E AGP Accelerated Graphics Port HyperTransport or any other bus or point to point communication protocol s and connections between different devices may use different protocols as is known in the art.

An embodiment of parallel processing subsystem is shown in . Parallel processing subsystem includes one or more parallel processing units PPUs each of which is coupled to a local parallel processing PP memory . A pushbuffer shown as instruction stream buffer that specifies the location of data and program instructions for execution by each PPU may be stored in each PP memory . In general a parallel processing subsystem includes a number U of PPUs where U 1. Herein multiple instances of like objects are denoted with reference numbers identifying the object and parenthetical numbers identifying the instance where needed. PPUs and PP memories may be implemented e.g. using one or more integrated circuit devices such as programmable processors application specific integrated circuits ASICs and memory devices.

As shown in detail for PPU each PPU includes a host interface that communicates with the rest of system via communication path which connects to memory bridge or in one alternative embodiment directly to CPU . In one embodiment communication path is a PCI E link in which dedicated lanes are allocated to each PPU as is known in the art. Other communication paths may also be used.

Host interface may perform any necessary translations of abstract graphics commands into a format required by graphics hardware. For example host interface may receive conventional graphics API calls from CPU such as graphics API calls in accordance with conventional graphics languages for pipelined graphics architectures such as the OpenGL API. Host interface generates packets or other signals for transmission on communication path and also receives all incoming packets or other signals from communication path and directs them to appropriate components of PPU . For example commands related to processing tasks may be directed to a front end unit while commands related to memory operations e.g. reading from or writing to PP memory may be directed to a memory interface . Host interface may be of generally conventional design and a detailed description is omitted as not being critical to the present invention.

Each PPU advantageously implements a highly parallel processor. As shown in detail for PPU a PPU includes a number C of cores where C 1. Each processing core is capable of executing a large number e.g. tens hundreds or thousands of threads concurrently where each thread is an instance of a program one embodiment of a multithreaded processing core is described below. A processing context encompasses a complete set of state through PPU while a thread may encompass only the state required to shade a single pixel. Threads run inside processing contexts one processing context might contain thousands of running threads. Cores receive processing tasks to be executed via a work distribution unit which receives commands defining processing tasks from a front end unit . Work distribution unit can implement a variety of algorithms for distributing work. For instance in one embodiment work distribution unit receives a ready signal from each core indicating whether that core has sufficient resources to accept a new processing task. When a new processing task arrives work distribution unit assigns the task to a core that is asserting the ready signal if no core is asserting the ready signal work distribution unit holds the new processing task until a ready signal is asserted by a core .

Cores communicate with memory interface to read from or write to various external memory devices. In one embodiment memory interface includes an interface adapted to communicate with local PP memory as well as a connection to host interface thereby enabling the cores to communicate with system memory or other memory that is not local to PPU . Memory interface can be of generally conventional design and a detailed description is omitted.

Cores can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying laws of physics to determine position velocity and other attributes of objects image rendering operations e.g. vertex shader geometry shader and or pixel shader programs and so on. PPUs may transfer data from system memory and or local PP memories into internal on chip memory process the data and write result data back to system memory and or local PP memories where such data can be accessed by other system components including e.g. CPU or another parallel processing subsystem .

Referring again to in some embodiments some or all of PPUs in parallel processing subsystem are graphics processors with rendering pipelines that can be configured to perform various tasks related to generating pixel data from graphics data supplied by the pushbuffer via memory bridge and bus interacting with local PP memory which can be used as graphics memory including e.g. a conventional frame buffer instruction stream buffer texture maps and the like to store and update pixel data delivering pixel data to display device and the like. In some embodiments PP subsystem may include one or more PPUs that operate as graphics processors and one or more other PPUs that are used for general purpose computations. The PPUs may be identical or different and each PPU may have its own dedicated PP memory device s or no dedicated PP memory device s .

In operation CPU is the master processor of system controlling and coordinating operations of other system components. In particular CPU issues commands that control the operation of PPUs . In some embodiments CPU writes a stream of commands for each PPU to instruction stream buffer and which may be located in system memory PP memory or another storage location accessible to both CPU and PPU . PPU reads the command stream from instruction stream buffer and executes commands asynchronously with operation of CPU .

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip. The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

The connection of PPU to the rest of system may also be varied. In some embodiments PP system is implemented as an add in card that can be inserted into an expansion slot of system . In other embodiments a PPU can be integrated on a single chip with a bus bridge such as memory bridge or I O bridge . In still other embodiments some or all elements of PPU may be integrated on a single chip with CPU .

A PPU may be provided with any amount of local PP memory including no local memory and may use local memory and system memory in any combination. For instance a PPU can be a graphics processor in a unified memory architecture UMA embodiment in such embodiments little or no dedicated graphics PP memory is provided and PPU would use system memory exclusively or almost exclusively. In UMA embodiments a PPU may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high speed link e.g. PCI E connecting the PPU to system memory e.g. via a bridge chip.

As noted above any number of PPUs can be included in a parallel processing subsystem. For instance multiple PPUs can be provided on a single add in card or multiple add in cards can be connected to communication path or one or more of the PPUs could be integrated into a bridge chip. The PPUs in a multi PPU system may be identical to or different from each other for instance different PPUs might have different numbers of cores different amounts of local PP memory and so on. Where multiple PPUs are present they may be operated in parallel to process data at higher throughput than is possible with a single PPU . Systems incorporating one or more PPUs may be implemented in a variety of configurations and form factors including desktop laptop or handheld personal computers servers workstations game consoles embedded systems and the like.

Data assembler is a fixed function unit that collects indices corresponding to vertex data for high order surfaces primitives and the like and outputs the vertex indices in batches to vertex processing unit . In conventional systems a round robin streaming cache is used to store the batches of vertex indices. In order to allow per vertex state changes randomly accessible vertex slots are included within each core that is configured to function as a vertex processing unit to store the vertex indices as described in conjunction with .

Vertex processing unit is a programmable execution unit that is configured to execute vertex shader programs transforming vertex data as specified by the vertex shader programs. For example vertex processing unit may be programmed to transform the vertex data specified by the vertex indices from an object based coordinate representation object space to an alternatively based coordinate system such as world space or normalized device coordinates NDC space. Vertex processing unit may read data that is stored in PP memory through memory interface for use in processing the vertex data.

Primitive assembler receives processed vertex data from vertex processing unit and constructs graphics primitives e.g. points lines triangles or the like for processing by geometry processing unit . Geometry processing unit is a programmable execution unit that is configured to execute geometry shader programs transforming graphics primitives received from primitive assembler as specified by the geometry shader programs. For example geometry processing unit may be programmed to subdivide the graphics primitives into one or more new graphics primitives and calculate parameters such as plane equation coefficients that are used to rasterize the new graphics primitives. Geometry processing unit outputs the parameters and new graphics primitives to rasterizer . Geometry processing unit may read data that is stored in PP memory through memory interface for use in processing the geometry data.

Rasterizer scan converts the new graphics primitives and outputs fragments and coverage data to fragment processing unit . Fragment processing unit is a programmable execution unit that is configured to execute fragment shader programs transforming fragments received from rasterizer as specified by the fragment shader programs. For example fragment processing unit may be programmed to perform operations such as perspective correction texture mapping shading blending and the like to produce shaded fragments that are output to raster operations unit . Fragment processing unit may read data that is stored in PP memory through memory interface for use in processing the fragment data. Memory interface produces read requests for data stored in graphics memory decompresses any compressed data and performs texture filtering operations e.g. bilinear trilinear anisotropic and the like. Raster operations unit is a fixed function unit that optionally performs near and far plane clipping and raster operations such as stencil z test and the like and outputs pixel data as processed graphics data for storage in graphics memory. The processed graphics data may be stored in graphics memory e.g. PP memory and or system memory for display on display device .

A data assembler unit receives a serial stream of vertices and commands from host interface . As an illustrative example the serial stream may include a to do list for a set of vertices and a command indicating operations to be performed on the vertices such as a triangle strip command a point command a line command a triangle command a line strip command a line loop command a quad command or a polygon command. The to do commands thus provide instructions for generating primitives from vertices.

Data assembler unit includes a primitive generator unit to perform at least an initial stage of primitive assembly in which primitives are identified based on input vertices and commands. An attribute fetch unit is provided to fetch information for identified vertices. For example attribute fetch unit may access vertex attributes from a vertex cache or other resource. As described below in more detail data assembler unit accumulates vertex indices in a primitive buffer and writes batches of work to cores based on an initial identification of primitives belonging to a set of vertices. The batches of work are processed independently by cores and each batch is dispatched to a single core via path or .

As previously described PPU is implemented with a highly scalable architecture for performing vertex processing and geometry processing operations required to generate a representation of primitive in screen space rasterize primitives and perform other steps required to render pixels for display. In one embodiment cores through C are configured to perform vertex and geometry processing operations where C is a whole number at least equal to three. Each core includes a vertex buffer with vertex slots for buffering a set of vertex indices and a vertex shader for performing vertex processing. Note that vertex shader may be configured to perform geometry shading and or fragment shading operations to execute vertex shader programs geometry shader programs and or fragment shader programs. In one embodiment there are 32 slots per vertex slots although more generally there may be an arbitrary number depending on the implementation. Vertex shader may be configured to process multiple threads in parallel such as for example 8 threads.

Each core operates independently of the other cores and does not share vertex information from its vertex slots . Thus for example core utilizes indices in vertex slots to perform vertex processing operations on a set of primitives i.e. batch of work. However in this example core has no knowledge of the vertices in the vertex slots C of core C . This partitioning of functionality facilitates a highly scalable architecture. However one consequence of the lack of vertex sharing between parallel vertex geometry processors is that work should be efficiently allocated to each individual core as independent units of work.

As illustrated in data assembler unit converts a serial stream of vertices and commands into batches of work assigned to an available core . Each batch includes a list of primitives assigned to the batch along with a set of vertex indices for the primitives. That is data assembler performs at least an initial stage of primitive assembly in terms of identifying primitives e.g. triangles polygons and a corresponding set of vertex indices such that vertex information can be sent in batches. To perform this task data assembler preferably incorporates several rules. One rule is that a batch has a pre selected maximum number of vertex indices. For example the maximum number of vertex indices per match may be determined by the size of each vertex slots . Another rule is that primitives can t span batches. In other words a batch includes an identification of all of the vertex indices necessary for the set of primitives associated with the batch. Other optional rules may also be included such as a rule defining a limit on the number of primitives per batch. A result of the rules is that an available core that receives a set of vertex indices assigned to its vertex slots can perform vertex geometry processing without having to communicate with other cores for vertex data.

Vertex indices are accumulated in primitive buffer and flushed to output a batch to a core . In one embodiment data assembler implements the batch rules to make flush no flush decisions as primitives are collected by primitive generator . For example the primitive assembly may be performed sequentially based on the input commands i.e. from a set of input vertex indices one primitive is identified then another and another and so on. For each primitive that is identified a lookup operation is performed on the vertex indices of the primitive and then a batch decision is made consistent with the batch rules. For example at some initial starting time a new batch is started. As each additional primitive is collected a decision is made for the additional primitive whether adding it to the batch is consistent with the batch rules such as a limit on a total number of vertex indices per batch and primitives not spanning different batches. When additional primitives can no longer be added to a batch the batch is flushed i.e. the old batch is written to an available core and a new batch begun . In one implementation all of the lookups for a primitive e.g. three lookups for the case the primitive is a triangle are performed in a single clock cycle.

As another illustrative example consider an example in which the primitives are triangles and there is a triangle list. In this example assume a batch size of 8 vertex indices indices and and a triangle list for drawing primitives and . A first primitive primitive with vertex indices and is added to the new batch. Vertex indices and are thus already cached in primitive buffer . As a second primitive arrives primitive with vertex indices and it can be added to the batch by writing vertex indices and i.e. with a batch size of eight caching five vertex indices and as a batch provides the necessary information for processing primitives and . Consider now that a batch has previously had six vertex indices added to the batch. Since there are six entries and a batch size of eight only two new vertex indices may be written for the batch. Suppose then that the next primitive identified by data assembler is a new primitive including none of the vertex indices in the six entries. In this example there is insufficient room in the batch to add the new primitive. As a result the old batch is flushed and then a new batch is started beginning with the vertex indices for the new primitive.

Importantly an initial stage of primitive stage is performed before slot assignment of vertex information for vertex geometry processing. That is batch generation is primitive oriented in order to produce an efficient allocation of work to the different cores . The primitive oriented approach results in batches that are as full as they can possibly be without reordering the original input stream. In contrast if a conventional vertex oriented scheme was used in which vertex slots were assigned prior to primitive assembly batch assignment would have to be designed with a large margin to account for the worst case number of missed for a primitive resulting in large inefficiencies.

As previously described a conventional vertex buffer does not allow for state changes within a primitive since the vertex information is stored in a streaming buffer with the oldest vertex information being replaced with the newest vertex information. When the state information for processing a primitive changes within the primitive i.e. between vertices the loading of vertex slots changes from a normal streaming buffer mode to a map mode where data assembler specifies the slot in vertex slots that each vertex index is written into. Vertex slots is flushed each time that the vertex processing state changes in order to process the pre state change vertices with the previous state information. Therefore the batch sizes are smaller and processing throughput is reduced when the map mode is used to write vertex slots . Consequently it is desirable to enter map mode only when it is needed to process state changes and exit map mode to return to normal mode when there are no state changes.

If in step data assembler determines that a vertex state change has not occurred then in step data assembler determines if the map mode is the current processing mode for vertex indices. If the map mode is not the current mode i.e. the normal mode is the current mode then in step the vertex index is stored in primitive buffer . In step data assembler determines if the vertex corresponding to the vertex index completes a segment of a primitive where a segment is defined as a line connecting two vertices of a primitive. Otherwise if the map mode is the current mode data assembler proceeds to step . If in step data assembler determines that the vertex index does not complete a segment of a primitive then the assembly of the vertex index is complete and data assembler proceeds to step .

If in step data assembler determines that the vertex index does complete a segment of a primitive then in step primitive buffer emits the vertex index to a vertex slot within one of vertex slots . A batch is assembled in vertex slots using vertex indices that are emitted from primitive buffer and processing of the vertex indices stored in the vertex slots the batch is initiated when vertex slots are flushed by data assembler . When the normal mode is used emitted vertex indices are stored in the oldest slot so it is not necessary to specify a slot with the emitted vertex index.

In step data assembler determines if the vertex index is the last vertex index in a program or if all of the slots in vertex slots are full and if so in step data assembler flushes vertex slots to output the batch of vertex indices for processing and proceeds to step . If in step data assembler determines that the vertex index is not the last vertex index in a program and all of the slots in vertex slots are not full then data assembler proceeds directly to step .

If in step data assembler determines that a vertex state change has occurred then in step data assembler sets the current mode to map mode. In step data assembler flushes vertex slots to initiate processing of the vertex indices in the current batch using the current state. In step data assembler outputs the state change to the next core that will receive a batch of vertex indices. In step data assembler copies the vertex indices in primitive buffer to vertex slots in the next core . The vertex indices that are copied are those that may be reused to define a new primitive such as a segment of a triangle in a triangle strip a segment of a polygon or a closing segment of a line loop as described in conjunction with .

In step the vertex index is stored in primitive buffer . In step data assembler determines if the vertex index completes a segment of a primitive and if not then the assembly of the vertex index is complete and data assembler proceeds to step . Otherwise in step data assembler determines which slot in vertex slots should be overwritten with the vertex index. One or more of the copied vertex indices are not needed to define a new primitive and slots storing those vertex indices should be overwritten with new vertex indices. In step primitive buffer writes the vertex index to an available vertex slot within one of vertex slots . When the map mode is used each vertex index output by primitive buffer is written to a particular slot in vertex slots so it is necessary to specify a slot with the vertex index. After writing the slot data assembler proceeds to step to determine whether or not to flush vertex slots .

A third vertex index index is received and stored in primitive buffer . Since index completes another segment of primitive it is emitted to vertex slots . A fourth vertex index index is received and stored in primitive buffer overwriting index since that index will not be reused to form another primitive. Because index completes another segment of primitive it is emitted to vertex slots . For the purposes of this example vertex slots includes four slots so after index is emitted vertex slots is full and data assembler flushes vertex slots . The contents of the slots are shown as X X X X since the current batch is completed and a new batch will be started with the next vertex index. Since the workload is distributed between the different cores a different vertex slots may receive the new batch.

A vertex state change occurs following the fourth vertex index. Then a fifth vertex index index is received. Since a state change occurred vertex slots should be flushed. However since vertex slots was already flushed that action is not repeated. The current state is changed from normal to map and the contents of primitive buffer are copied into slots of vertex slots . Specifically indices and are copied from primitive buffer to vertex slots . Note that index is not needed in vertex slots since the segment defined by index and was included in the last batch. Therefore index should be overwritten and is determined to be available as such by data assembler . Index is stored in primitive buffer and since it completes a segment of primitive primitive buffer writes index to the slot occupied by index . Finally the contents of vertex slots are indices and . Since primitive is complete vertex slots may be flushed.

After vertex slots is flushed data assembler may set the mode to normal if a vertex index is received for a new primitive. In embodiments of the present invention with more slots in vertex slots the penalty for flushing vertex slots without filling all of the slots increases. Therefore it is desirable to resume processing in the normal mode whenever possible. Rather than flushing vertex slots for each segment vertex slots are flushed for a state change or when a new incoming vertex will write into the slot of an as yet unflushed vertex. However the map mode is needed to allow per vertex state changes by configuring vertex slots as a random access storage buffer or a conventional round robin streaming cache for the different modes.

The invention has been described above with reference to specific embodiments. Persons skilled in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

