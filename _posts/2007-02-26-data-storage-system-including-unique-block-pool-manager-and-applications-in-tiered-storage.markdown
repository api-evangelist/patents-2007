---

title: Data storage system including unique block pool manager and applications in tiered storage
abstract: A data storage system () that receives a data set from a software module (A-D) includes a first tier storage device (), a second tier storage device (), a redundancy reducer () and a migration engine (). The first tier storage device () has a first effective storage capacity and the second tier storage device () can have a second effective storage capacity that is greater than the first effective storage capacity. The redundancy reducer () subdivides the data set into a plurality of data blocks () and reduces the redundancy of the data blocks (). The migration engine () moves one or more of the data blocks () between the first tier storage device () and the second tier storage device () based on a migration parameter of the data block (). The first tier storage device () can store data in a random access manner and the second tier storage device () can store data in a random or sequential access manner. The first tier storage device () has a first I/O bandwidth, and the second tier storage device () can have a second I/O bandwidth that is less than the first I/O bandwidth. The first tier storage device () has a first access time to data, and the second tier storage device () can have a second access time to data that is lengthier than the first access time to data. The data storage system () can also include a third tier storage device () used for retired data blocks ().
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07831793&OS=07831793&RS=07831793
owner: Quantum Corporation
number: 07831793
owner_city: San Jose
owner_country: US
publication_date: 20070226
---
This Application claims the benefit on U.S. Provisional Application Ser. No. 60 778 639 filed on Mar. 1 2006. The contents of U.S. Provisional Application Ser. No. 60 778 639 are incorporated herein by reference.

Conventional data backup systems can be relatively inefficient in the manner in which data is copied. Using such systems can in many cases result in a large amount of duplicate data or blocks of data being copied. It is typical for computers to back up many versions of the same document with only slight variations between the versions. For example if data from a computer system is routinely copied on a weekly basis to backup media from week to week only a relatively small amount of data may be new. Unfortunately in conventional backup systems both the unchanged data and the new data are copied which results in a significant amount of unnecessary redundant data on the backup media.

To illustrate after one week all of the data that is backed up is considered unique. If 10 of the data changes from week to week then after week two only 55 of the total data will be unique. After week three only 40 of the total data that is backed up is unique and so on. After one year less than 12 of the total data that is backed up is unique. As a result greater storage capacity is required resulting in increased storage costs. Further this redundancy of data being stored can cause an increase in data retrieval time from backup decreasing the overall efficiency of the system.

The present invention is directed toward a data storage system that receives a data set from a software module. In one embodiment the data storage system includes a first tier storage device a second tier storage device a redundancy reducer and a migration engine. In certain embodiments the first tier storage device has a first effective storage capacity. The second tier storage device has a second effective storage capacity that is greater than the first effective storage capacity. The redundancy reducer subdivides the data set into a plurality of data blocks and reduces the redundancy of the data blocks. The migration engine moves one or more of the data blocks between the first tier storage device and the second tier storage device based on a migration parameter of the data block.

In one embodiment the redundancy reducer includes a hash engine and or a lookup engine. In further embodiments the migration parameter can include one or more of a reference count a last access time an access rate and or a user defined access policy.

In certain embodiments the storage system also includes a metadata index that is stored one of the storage devices. In these embodiments the migration engine can update the metadata index in conjunction with movement of one of the data blocks by the migration engine. In some embodiments the first tier storage device includes a disk drive that stores at least a portion of the data set prior to reduction of the redundancy of the data blocks by the redundancy reducer. Further the second tier storage device can include a disk drive that stores at least a portion of the data set following reduction of the redundancy of the data blocks by the redundancy reducer. In another embodiment the storage system includes a third tier storage device having a third effective storage capacity that is greater than the second effective storage capacity. In this embodiment the migration engine moves one of the data blocks between the second tier storage device and the third tier storage device based on a migration parameter of the data block. The third tier storage device can include a tape library a MAID or another type of archival storage device fixed or removable.

In some embodiments the software module is selected from the group consisting of virtual tape network attached storage content addressable storage and a cluster file system. In one embodiment the first tier storage device stores data in a random access manner and the second tier storage device stores data in a random access or sequential access manner. In certain embodiments the first tier storage device has a first I O bandwidth and the second tier storage device has a second I O bandwidth that is less than the first I O bandwidth. In some embodiments the first tier storage device has a first access time to data and the second tier storage device has a second access time to data that is lengthier than the first access time to data.

Specific embodiments of the invention will now be described in some further detail with reference to and as illustrated in the accompanying figures. These embodiments are illustrative and are not meant to be restrictive of the scope of the invention. Suggestions and descriptions of other embodiments may be included within the scope of the invention but they may not be illustrated in the accompanying figures or alternatively features of the invention may be shown in the figures but not described in the specification.

The API allows users of the storage system to store and retrieve a plurality of data blocks using a ticket that is returned by the API on a store and is presented to the API to retrieve a data block . The API can also facilitate the creation merging movement migration statistical analysis etc. of one or more of the block pools A themselves.

The design of the manager can vary to suit the design requirements of the storage system . In one embodiment the manager can include one or more of i a redundancy reducer having a hash engine and or a lookup engine ii an access engine and iii a migration engine . The redundancy reducer reduces the redundancy of a given data set. The specific method employed by the redundancy reducer can vary depending upon the design requirements of the storage system . Various methods of reducing the redundancy of a data set including various embodiments of the hash engine are disclosed in U.S. Pat. No. 5 990 810 issued to Ross Neil Williams on Nov. 23 1999 and in United States Patent Application Publication US 2006 0282457 A1 filed by Ross Neil Williams and published on Dec. 14 2006. To the extent permitted the disclosures of U.S. Pat. No. 5 990 810 and United States Patent Application Publication US 2006 0282457 A1 are incorporated herein by reference.

The redundancy reducer utilizes the hash engine and the lookup engine to store only unique data blocks in the unique data block pools A . By storing only the unique data blocks a substantial savings of storage space can be realized. For example assuming a backup of data occurs on a given system on a weekly basis and assuming only 5 of the data saved week over week is new data ultimately nearly a 20 fold reduction in storage can be achieved. Stated another way the initial set of data is stored and only the new unique data is stored thereafter. Over time the stored unique data approaches 1 20 of the total raw data that would otherwise have been stored which would include redundancies. It is recognized that the extent of the reduction in storage space can vary from the preceding example based on the percentage of unique data blocks that are included in the data set from one backup session to the next.

The hash engine can be implemented in software or on an application specific integrated circuit ASIC in hardware as non exclusive examples. The hash engine includes a hash algorithm that is a reproducible method of associating a different digital fingerprint with each unique data block . The hash engine is responsible for generating a hash on a data set that is fed into the storage system from one of the software modules A D via the API . Hash engines are generally known in the art.

The type of hash engine that can be incorporated into the manager can vary depending upon the design requirements of the storage system . As used herein the term hash can be a fixed length sequence of bytes or bits that is generated by the hash engine . Generally the hash engine accepts one or more data blocks that each includes a finite input sequence of bytes or bits of data and generates the hash that includes a finite sequence of bytes or bits that is highly dependent upon the input sequence of each data block . Each fixed length hash corresponds to a data block which may have varying lengths relative to one another. The hash engine can be used to test whether two data blocks may be identical of different without having to compare the data blocks directly. In one embodiment the hash is computed over the entire data set which is input into the manager . The entire data set is organized within the manager into one or more block pools A . The hash can also be returned to the user of the API as a handle for retrieval of a portion or all of the data set. In one embodiment once the hash is computed it is passed on along with the entire data set to the lookup engine .

The lookup engine maintains one or more metadata indices A of the data blocks . As used herein metadata is information about a data block . A block s metadata can include without limitation a length of the data block a hash of the data block an expiry date of the data block a reference count of the data block and or any other relevant identifier information about the data block . The expiry date defines the earliest date when the data block is guaranteed not to be required by a user of the storage system as described in greater detail below. Further the reference count is associated with the current number of references to the data block over a predetermined period of time. The system utilizes the reference count to determine when the data block is no longer required and or when the data block should be migrated to another location as set forth in greater detail below.

In one embodiment on a write request by a user of the storage system the lookup engine can look up the hash value generated by the hash engine against one or more of the metadata indices e.g. metadata index A for a given block pool e.g. block pool A. If the hash already exists the lookup engine can determine that the data block already exists in the block pool A and can increase the reference count for that data block . If the hash does not exist an entry can be made in the metadata index A and the data block can be submitted to the access engine for storage.

On a read request by a user of the storage system the lookup engine can determine if the hash exists in the metadata index A. If it does the read request can be submitted to the access engine for retrieval. Upon retrieval the lookup engine can mark the last access time for the data block before returning the data block to the user.

In certain embodiments the access engine is responsible for storage and retrieval of the unique data blocks from one or more of the block pools e.g. block pool A. In one embodiment the access engine is given the current location by the lookup engine . The access engine can then retrieve or store the data blocks to one or more different storage media A B which can include fixed or removable disk A tape B or optical disk not illustrated in as non exclusive examples.

The migration engine is responsible for migrating data blocks based on one or more migration parameters that can include reference count last access time number of accesses etc. As described in greater detail below in one embodiment the migration engine moves data blocks between different storage devices also referred to herein as tiers as described in greater detail below such as disk A and or removable media B and updates the location s of the data blocks in the relevant metadata indices A once the data blocks are moved.

The following examples of applications and or operations explain some of the functions of the manager and the storage system as a whole. These non exclusive examples are provided for purposes of illustration only and are not intended to limit the scope of the present invention in any manner.

Store Dataset This operation is the basic store operation. In one embodiment upon receiving a data set the hash engine computes an overall data set hash which is used as a handle for the data set. The data set is split into blocks that are checked for uniqueness against one or more of the block pools A . At the end the entire data set can be replaced with pointers to unique data blocks forming a pointer map. When a new unique data block is linked in the relevant migration parameters for that data block are tracked such as a reference count a last access time the number of accesses per unit time or any other suitable user defined access policy. The user defined access policy can include any policy that suits the needs of the user such as the user determining a specified time period that the data block or data set should be located in a particular storage device or tier before the data block or data set is migrated or deleted as one non exclusive example. Any other suitable user defined access policies can be included as a migration parameter.

Retrieve Dataset In this operation the user can input a hash handle. The hash handle is used to retrieve the pointer map. The pointer map is then used to recreate the data set by assembling the data blocks and then returning the data set to the user. When a data block is retrieved the relevant migration parameters such as reference count last time accessed etc. are updated.

Migrate Unmigrate Block Data blocks in the block pool A can contain the reference count the last time accessed and or an access rate number of accesses per unit time for example. Any or all of these migration parameters can be used to set policies to move the data blocks between different storage devices A B also referred to as tiers as illustrated in for example . As one non exclusive example a data block that has a last time accessed that is greater than three weeks can be migrated from disk A to tape B. This time frame is provided as one example only and it is recognized that any length of time can be set which determines when a data block is migrated from one tier to another such as from disk to tape. The data blocks can be migrated back e.g. from tape B to disk A if required to retrieve a data set. Additionally the location of data blocks can be routinely updated as necessary and maintained in the relevant metadata index A 

Retire Block A Retire Block is an internal cleanup operation where a data block can be deleted from the relevant block pool A as the reference count drops to zero or some other predetermined number over a predetermined period of time for example.

Merge Pool As the system allows multiple block pools A to exist each with its own unique metadata index A under certain circumstances it may be necessary or advantageous to merge two or more independent block pools A . For example this can be required when two block pools that were initially thought to contain non overlapping data blocks but actually do contain overlapping data blocks . This operation can be performed in a dry run mode so the user can determine the results before determining whether to actually execute the operation. Once the user is satisfied with the dry run results the actual merge pool operation can be executed.

It is recognized that the effective storage capacity in these storage devices can be varied to accommodate the design requirements of the storage system . It is further understood that in reference to the storage devices the terms first second and third are provided for ease of discussion only and that any of the storage devices can be the first second or third storage device. It is also understood that alternative embodiments of the storage system can include less than three storage devices or greater than three storage devices. Further the third tier storage device can be any storage device that is consistent with the teachings herein. For example the third tier storage device can be a massive array of idle disks MAID or any other type of archival storage device.

In this embodiment the first tier storage device includes a front end cache for fast data transfer between the host and storage appliance . The storage appliance includes a migration engine illustrated in for data movement between first tier storage device the second tier storage device and the third tier storage device under the control of unique block pool manager illustrated in .

The storage appliance can also include a secure replication engine not shown for data movement between block pools A for more efficient data transfer between a plurality of storage appliances for remote branch office consolidation.

The backup data can be transferred natively to the third tier storage device in case the storage appliance fails then the third tier storage device can be directly attached for fast restore in enterprise applications. As used herein the term native means data in its raw form such that it is not in a deduplicated form e.g. has not been reduced by the redundancy reducer . The unique data block pool A can also be transferred to the third tier storage device for redundancy at very little storage premium. The overall solution means that with a very small amount of disk space as little as 2 of size of the third tier storage device in one non exclusive example a very large amount of storage can be virtualized to the host at roughly the cost point of tape. Further one or more high disaster recovery features i.e. mirroring replication policy driven storage provisioning etc. can be utilized. The ratio of the effective storage capacity of the storage appliance to the effective storage capacity of the third tier storage device is dependent on unique data content and data storage retention policies which can be defined and or determined by the user as necessary.

Moreover in certain embodiments the data blocks can be compressed prior to or following movement from one storage device to another. Compression engines known to those skilled in the art such as gzip or bzip2 as non exclusive examples can be used for this purpose.

The storage system can also include the migration engine illustrated in for data movement between the first tier storage device the second storage device and or the third tier storage device . The storage system can also include a secure replication engine not shown for data movement between block pools for more efficient data transfer between a plurality of storage appliances for remote branch office consolidation.

In operation the backup data can be transferred as unique block pool data to the third tier storage device . The overall solution means that a very large amount of storage can be virtualized relative to the size of the tape cartridge effectively up to 20 times the density or 20 effective cartridges on a single tape cartridge. Also many types of backup targets and file formats can be exposed with disaster recovery features i.e. mirroring replication policy driven storage provisioning etc. . The ratio of storage on the storage appliance to storage on the third tier storage device is dependant on unique data content and data storage retention policies.

For example in the embodiment illustrated in the storage system includes a first tier storage device a second tier storage device a third tier storage device and a fourth tier storage device . The unique data blocks illustrated in are moved between the storage devices by the migration engine illustrated in based on the migration parameters as described previously. The specific types of storage devices that are included in the storage system have different access parameters and are therefore tailored to accommodate data blocks with different migration parameters.

In this non exclusive embodiment unique data blocks can reside on native disk redundancy reduced disk native tape and or redundancy reduced tape . The first tier storage device has a relatively high I O bandwidth a relatively small effective storage capacity and uses random access. The second tier storage device has less I O bandwidth than the first tier storage device larger effective storage capacity than the first tier storage device and also uses random access. The third tier storage device has less I O bandwidth than the second tier storage device has a relatively large effective storage capacity particularly when multiple tapes are used and uses sequential access. The fourth tier storage device has less I O bandwidth than the third tier storage device greater effective storage capacity than the second tier storage device and the third tier storage device and uses sequential access. Therefore each data block can reside on a preferred or most desired storage device in order to satisfy the goals and objectives of the user and the storage system . The benefits of this type of system can include one or more of increased performance increased effective storage capacity and or a decreased storage cost per GB. Importantly it is recognized that the storage capacities illustrated in are provided as representative examples and are not in any way intended to limit the scope of the present invention.

It is further recognized that the storage system in accordance with the present invention need not have all four storage devices but can alternatively utilize any two or three of the storage devices illustrated or any other suitable type of storage device consistent with the teachings herein. It is further understood that two or more of the storage devices can be combined into a single storage device. As one non exclusive example a single disk drive can serve the functions of the first tier storage device cache for example and the second tier storage device redundancy reduced disk .

While the particular storage system and unique block pool manager as shown and disclosed herein are fully capable of obtaining the objects and providing the advantages herein before stated it is to be understood that they are merely illustrative of the presently preferred embodiments of the invention and that no limitations are intended to the details of the methods construction or design herein shown and described.

