---

title: System and method for automating and scheduling remote data transfer and computation for high performance computing
abstract: The invention pertains to a system and method for a set of middleware components for supporting the execution of computational applications on high-performance computing platform. A specific embodiment of this invention was used to deploy a financial risk application on Blue Gene/L parallel supercomputer. The invention is relevant to any application where the input and output data are stored in external sources, such as SQL databases, where the automatic pre-staging and post-staging of the data between the external data sources and the computational platform is desirable. This middleware provides a number of core features to support these applications including for example, an automated data extraction and staging gateway, a standardized high-level job specification schema, a well-defined web services (SOAP) API for interoperability with other applications, and a secure HTML/JSP web-based interface suitable for non-expert and non-privileged users.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09104483&OS=09104483&RS=09104483
owner: International Business Machines Corporation
number: 09104483
owner_city: Armonk
owner_country: US
publication_date: 20070118
---
The present invention generally relates to data processing and more particularly to a system and method for a middleware for automating the synchronization and scheduling of a sequence of the steps of remote data movement and computational processing in an individual high performance computing application.

The present invention is concerned with a system and method for a middleware capability preferably suitable for automating the entire workflow comprising separate data transfer and computational processing steps of an individual application on high performance computing HPC platforms. This middleware approach is very suitable for a large class of HPC applications that need to exchange data with a variety of external data sources using different storage formats and access protocols.

One interesting application of the middleware has been in the deployment of certain financial applications for risk modeling on HPC systems in a production setting. These are important applications in the financial industry which in recent years has been impacted in recent years by a number of issues such as the increasing competitive pressures for profits the emergence of new financial products and the tighter regulatory requirements being imposed for capital risk management. A number of quantitative applications for financial risk analytics have been developed and deployed by banks insurance companies and corporations. These applications are computationally intensive and require careful attention to the discovery and aggregation of the relevant financial data used in the analysis.

In general the requirements of such financial risk applications differ from other traditional scientific engineering applications that usually run on HPC platforms in a number of ways. For example financial risk applications may require external data sources that include SQL databases remote files spreadsheets and web services or streaming data feeds in addition to the usual pre staged or pre existing flat files on the file system of the computing platform. These applications often interact with larger intra or inter company business workflows such as trading desk activites portfolio tracking and optimization and business regulatory monitoring applications. High level services specifications for these applications must be separated from low level service and resource provisioning since there is a frequently a need to provide dynamic provision resources based on of quality of service or time to completion requirements. Finally the computationally intensive parts of these applications are usually quite easy to parallelize since they are often independent or embarrassingly parallel and can be easily deployed to a variety of parallel computing platforms. On many of these parallel computing platforms after an initial broadcast distribution of financial data to the compute nodes each node just performs independent floating point intensive computations with very little inter processor communication and synchronization.

A good example is a specific proprietary application which does a Value at Risk computation D. Duffie and J. Pan An overview of value at risk Journal of Derivatives Vol. 4 1997 p. 7 as defined below which has may of the characteristics given above. The relevant input data for that application consisted of historic market data for the risk factors simulation data and asset portfolio details and was initially extracted from an SQL database. The relevant output data consisted of empirical profit loss distributions was also stored in an SQL database for post processing and for archival value.

Although the data management and computing requirements of financial risk applications appear straightforward we have discerned that there are some inhibitors to porting and deploying these applications to HPC platforms. A major difficulty is that many HPC platforms are not well suited for accessing data stored in high latency external data sources outside of their local area network. In common desktop computing platforms the network and transaction latencies for remote data access can be overlapped with other useful work by using multithreaded programming in the application. However in many scalable distributed memory HPC platforms the individual compute nodes do not support multi threaded user execution since this will have a negative impact on the machine inter processor communication performance and therefore it is not possible to hide the long and unreliable latencies of remote data access. Another issue on distributed memory HPC platforms is that applications run in a space sharing rather than a time slicing mode i.e. each application uses a distinct physically partitioned set of nodes which is reserved for the entire duration of the application. Lastly for performance reasons in many HPC systems it is desirable and in some cases mandatory to have the program data staged to and from a specialized parallel file system such as GPFS that is tightly coupled to the HPC platform. These data staging requirements lead to the application deployment on HPC platforms being quite ad hoc in nature with specialized scripts to deal with data specification data migration and job scheduling for each individual application or problem instance.

The present invention relates to a novel system and method for a set of middleware components that starting from a client job specification completely automates the tasks of marshaling the data between a set of distributed storage repositories and the specialized parallel file system. In addition the middleware also handles the synchronization and scheduling of the individual data transfer and computing tasks on the HPC platform.

The design of the middleware components preparably is motivated by requirements that are likely to be encountered across a broad range of financial computing applications. First this layer simplifies and automates the application workflow and provides a framework for application code organization. Second it can separate the data extraction and data migration from the computational steps on the HPC platform so that the overall performance in a multi application environment can be optimized by co scheduling these different steps in conjunction with the relevant platform specific resource schedulers reservation systems and administrative policies used on the data and computing platforms. Third it can provide support for accessing a rich variety of data sources including databases spreadsheets flat files and potentially web services data which give clients the flexibility to customize the application to the different data requirements needed for risk computations with specific trading profiles or for faster response to changing market conditions maintained in heterogeneous data sources. Fourth the use of the fast parallel file systems for intermediate data staging ensures the best I O performance during the computational phase so that valuable computer time is not tied up in high latency unreliable I O operations to remote servers. Fifth it can provide a capability to invoke the application via a web service interface thereby allowing BG L to participate in external business workflows as well as insulating the end user from the specifics of the data storage and operational platforms. Sixth and finally it ensures and enhances the mechanisms in the data management platforms for data validation security privacy and audit trail logging by extending these to the case when the data is used by an external HPC application in an on demand mode.

The details of the present invention both as to its structure and operation can best be understood in reference to the accompanying drawings in which like reference numerals refer to like parts and in which 

The data staging gateway enables data to be automatically staged between the external data sources and the BG L attached file system based on the specifications in the job submission file. It therefore replaces the current practice of performing these data transfers in a manual or ad hoc fashion. The design supports the requirement from financial services customers for running the same application repeatedly in response to changing market or portfolio data that is stored and updated in SQL databases. The data staging gateway also provides the option for applying streaming transformations to the data while writing or reading the files on the HPC files system during the data staging operations. For example these files may be block compressed on the fly to save disk storage or even to optimize the subsequent I O performance on the HPC platform. In the current design the data staging gateway is integrated with the other components of the middleware for simplicity of deployment. However when the data server is only accessible over a wide area network it is preferable especially for large files to optimize the long haul data transfer by implementing the data extraction and compression modules as stored procedures on the data server with the compressed files being directly transferred to the BG L file system. Other potential server side data transformations such as encryption or statistical data scrambling can also be implemented to protect data privacy on the external network and HPC file system.

The job dispatching module carries all the information for submitting computational jobs to the HPC platform or to the platform specific job scheduler. For example for MPI jobs submitted using the mpirun command e.g. MPICH2 implementation of MPI as can be found by a search for unix.mcs.anl.gov mpi mpich will require the parameters for specifying the executable filename the number of processors the processor partition and numerous other runtime options. Like most UNIX Linux command line programs mpirun is usually invoked via shell scripts but this approach is problem specific and ad hoc in nature. The job submission file whose schema is an extension of the XML based JSDL definition The Job Submission Description Language Specification Version 1.0 projects jsdl wg document draft ggf jsdl spec en 21 establishes the syntax of the job submission parameters and helps to normalize job submission specifications and facilitates the use of cross platform interaction between middleware and external clients using web services.

The proprietary financial risk application is an example of a Monte Carlo calculation for estimating Value at Risk as described earlier D. Duffie and J. Pan An overview of value at risk Journal of Derivatives Vol. 4 1997 p. 7 . This application was implemented on a parallel supercomputer Blue Gene L N. R. Adiga et al An Overview of the Blue Gene Computer IBM Research Report RC22570 September 2002 .

This prototype Value at Risk code has many of the features that are required to understand the deployment and performance of the generic class of financial risk applications on HPC platforms in a production setting.

The end to end execution of this application proceeds in three phases namely data pre staging computation and data post staging as described here. The input data for this application consists of all the necessary data on portfolio holdings the simulation data for generating scenarios and the various algorithm parameters e.g. the number of scenarios to be evaluated . For this application the input data was roughly 300 MB which was distributed across 44 files. These files were extracted from a 4 GB database using standard SQL queries and procedures in a pre staging phase. We note that this pre computation phase would not be required if the HPC platform could efficiently access the external database via a programmatic interface as noted earlier the long latencies and completion uncertainties of remote communication make it very inefficient to provide direct database connectivity for a space partitioned distributed memory HPC platform like BG L. In the parallel computation phase the required 300 MB of input data is copied to each compute node and independent Monte Carlo simulations are performed. These independent simulations take a random set of realizations generated from the risk factor distributions to generate market scenarios which are then used to price the instruments in the portfolio for each scenario. The output from the individual compute nodes are written to disk for post processing and analysis. In the final post staging phase these results are saved to the SQL database for archiving and further analysis.

This prototype application is typical of the intra day market risk calculations that are routinely performed in many large banks. The input data for this application changes between successive calculations only for those variables that are based on market conditions such as equity prices exchange rates yield curves and forward curves. We estimate that in production settings a typical large bank might hold about 250 000 instruments in its portfolio of which 20 may need to be priced by Monte Carlo while the remaining 80 may be priced by closed form approximations. In addition roughly 100 000 scenarios are required in the Monte Carlo simulations to obtain empirical profit loss distributions for estimating the relevant Value at Risk quantiles with the required statistical confidence.

