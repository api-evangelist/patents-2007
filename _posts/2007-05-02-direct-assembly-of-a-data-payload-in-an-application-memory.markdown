---

title: Direct assembly of a data payload in an application memory
abstract: A system and method for direct assembly of data payload in an application memory. A transmission control protocol offloading process is applied by network interface card components on a packet header to provide a direct memory access (DMA) task with precise application memory location pointers to a DMA engine. The DMA engine uses the DMA task to place the data payload directly in the application memory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07813339&OS=07813339&RS=07813339
owner: Tehuti Networks Ltd.
number: 07813339
owner_city: Hertzliya
owner_country: IL
publication_date: 20070502
---
The present invention relates generally to application memory management in a communications network by a network interface card NIC and in particular to reduction of data transfer latency when high level protocol data is transferred between the network and a computer application memory.

Computer applications that communicate over a network require a considerable amount of central processing unit CPU processing power to decipher the packet based complex Transmission Control Protocol TCP Internet Protocol IP . Each network packet must be processed through a protocol stack multiple protocols on transmit and receive ends of a network connection.

During the protocol stack process multiple protocol headers e.g. TCP IP Ethernet are added in a specific order to the data payload at the transmit end of the connection. These headers are necessary for the data transmission across the network. When a packet is received at the receiver end of the connection the packet is processed again through the protocol stack and the protocol headers are removed in an opposite order until the data is recovered and available to a user.

Packet size is determined by the network maximum transfer unit MTU . When data transmitted between two applications is longer than the MTU the data is divided into multiple separated packets. More CPU resources are needed as the number of packets increases. When the speed of the network increases the demands on the CPU escalate as well. Using a direct memory access DMA device can help free CPU resources by allowing the system to access the CPU memory without CPU intervention. However DMA does not reduce the CPU protocol stack processing and usually require additional memory to organize receiving packets before sending them to the application memory. This step adds latency to the data transfer and takes up precious resources.

TCP offload engines TOE devices have been developed to free the CPU processing resources by performing some or all of the TCP and IP processing for the computer. The data payloads of the processed packets still need to be aggregated in order using a dedicated memory and transferred to the application memory. That is the application expects to receive the data in order. Normally a memory is used to hold out of order received packets until all the holes in the sequential data are filled. Thus the offloading process does not eliminate the need for data aggregation.

Direct Data Placement DDP is a developing protocol described in the DDP Protocol Specification published by the Internet Engineering Task Force IETF working group on Oct. 21 2002. DDP may enable an Upper Layer Protocol ULP to send data to a Data Sink i.e. a computer or any other medium capable of receiving data without requiring the Data Sink to place the data in an intermediate buffer. When data arrives at the Data Sink a NIC can place the data directly into the ULP s receive buffer. This may enable the Data Sink to consume substantially less memory bandwidth than a buffered model because the Data Sink is not required to move the data from an intermediate buffer to the final destination. This can also enable the network protocol to consume substantially fewer CPU cycles than if the CPU was used to move data and remove the bandwidth limitation of being only able to move data as fast as the CPU can copy the data.

DDP is much harder to achieve with network applications over TCP IP where exemplarily data can arrive out of order because of the nature of the sockets application programming interface API used by applications. One protocol that does achieve DDP over TCP IP is iSCSI which transports the SCSI storage protocol over TCP IP. The iSCSI protocol benefits from the fact that storage applications generally do not use the sockets API and are required to provide buffers for all data ahead of that being received from the network. The iSCSI protocol uses tags that indicate exactly where received data should be placed and has mechanisms to limit the expense of dealing with out of order TCP IP data. However SCSI is a network storage protocol not a communication protocol.

Various attempts to solve some of the problems above are known in the art. For example US Patent Application No. 20040249998 by Rajagopalan et al. deals with uploading TCP frame data to user buffers and buffers in system memory. The payload data is uploaded to user buffers in system memory and partially processed frame data is uploaded to legacy buffers allocated in operating system memory space. U.S. Pat. No. 7 012 918 to Williams deals with DDP disclosing a system comprising a host and a NIC or host bus adapter. The host is configured to perform transport protocol processing. The NIC is configured to directly place data from a network into a buffer memory in the host. U.S. Pat. No. 7 010 626 to Kahle deals with data pre fetch disclosing a method and an apparatus for pre fetching data from a system memory to a cache for a DMA mechanism in a computer system. U.S. Pat. No. 6 996 070 to Starr et al deals with a TCP IP offload device with reduced sequential processing and discloses a TOE device that includes a state machine that performs TCP IP protocol processing operations in parallel. If some of these solutions write the data directly to the memory they either need to process the TCP stack or use additional memory for data payload aggregation.

There is therefore a widely recognized need for and it would be highly advantageous to have new efficient ways to approach an application memory. In particular there is a need to find inexpensive solutions or methods of transferring data to and from the application memory with less CPU processing power or less dedicated processing time for protocol processing and with minimum latency.

The present invention discloses a method and system for performing assembly of TCP data payload directly on a TCP application memory. The application runs on a host. The method removes the need for reassembling an out of order packet before writing the data directly to the application memory and removes the need to use intermediate buffers that require intensive memory copies. The invention also advances the TOE implementation by freeing more CPU resources and by reducing the latency in data delivery from the TCP packet to the application memory.

The essence of the present invention lies in the writing of data payload directly from the network to an application memory irrespective of the order of received packets. To do so the invention uses a network interface card NIC to perform data aggregation directly in the application memory. A dedicated algorithm implemented by hardware a pre processor a microcontroller and a Sub processor see and dedicated firmware run on the internal microcontroller perform the direct placement and aggregation using a DMA engine. Information received after TCP offload processing is combined with pointer management. The method described below is applied to a TCP packet of an already established connection after application memory pointers for the application memory buffers allocated to this connection are placed in a database in the NIC.

Pre processor receives incoming packets from the network and verifies and processes each packet header. This processing fetches TCP ports and IP addresses destination and source that allow the pre processor to generate search tasks to the connection registers to fetch the relevant connection information to be used by micro controller for the TCP offloading process. The pre processor also provides micro controller with execution tasks and with relevant information for the processing of these tasks. Exemplary tasks include updating of database with new application memory pointers and receiving packets.

Micro controller performs the TCP offload processing processes the TCP stack . Micro controller is also responsible for updating connection registers . For example when a connection is established micro controller generates a connection register and inserts it into database . The generated connection register includes an internal pointer Int pointer to the link list which stores the buffers allocated by the application in the application memory for the relevant connection. In other words the connection registers are needed for the offload processing and also for holding the Int pointer to link list . Micro controller then generates a fetch command to Sub processor

Sub processor handles the application memory buffers allocated to each connection allocating them to the relevant TCP data payload . It also provides commands and information such as start DMA task data write address and data length to the DMA engine. In other words Sub processor fetches the memory application pointer from Link List fetches the data payload from pre processor provides them to the DMA engine and generates a DMA task. The DMA engine places the data payload directly on the application memory using the information provided by Sub processor .

The DMA operation performed by DMA engine can be effected by using descriptors exchanged between the NIC and a device driver located in the host and not shown . The driver sends the location and the size of the application memory buffers to the NIC using descriptors. The NIC sends the device driver indications about the relevant application memory buffers to be used by the NIC also using descriptors. When the NIC sends data to the application memory the NIC micro controller uses the descriptors to calculate the size and the location of the transactions that will be used to transfer the data.

The combination of TCP offload processing on Sub processor and the handling of Link List provide the system with an inventive ability to use DMA engine to place the data directly on the application memory. After the data is written onto the application memory the application needs to be notified of the written data location. This is done by micro controller which sends to the device driver a descriptor that points out this valid data location to the driver. If a TCP packet is received out of order at the NIC the data payload of the packet will be placed exactly on its dedicated location on the application memory in the order it was received. After the missing packets are received and placed on the application memory so that the packet order is restored micro controller sends a respective descriptor to the device driver.

a. performs direct data placement to an application memory using a NIC that processes the TCP stack without using additional memory for data aggregation 

All publications patents and patent applications mentioned in this specification are herein incorporated in their entirety by reference into the specification to the same extent as if each individual publication patent or patent application was specifically and individually indicated to be incorporated herein by reference. In addition citation or identification of any reference in this application shall not be construed as an admission that such reference is available as prior art to the present invention

