---

title: Centralized polling service
abstract: A centralized polling system is set forth for providing constant time select call functionality to a plurality of polling tasks in an operating system kernel. In one aspect, the CPS registers for and thereby captures events of interest on a continual basis. Polling tasks are supplied with active events thereby eliminating the need to repetitively poll large numbers of inactive sockets. An exemplary embodiment of the CPS includes a system interface to the operating system kernel, a data structure for maintaining a profile for each of the polling tasks, and an application programming interface for registering the polling tasks, receiving the active sockets and corresponding read/write event types via the system interface, updating the profile within the data structure for each of the polling tasks, and returning the current read and write ready sockets to respective ones of the polling tasks.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08006005&OS=08006005&RS=08006005
owner: Mitel Networks Corporation
number: 08006005
owner_city: Ottawa, Ontario
owner_country: CA
publication_date: 20070921
---
The present specification relates generally to client server systems and more particularly to the problem of socket handling for VoIP and collaborative systems.

VoIP and collaborative systems currently use standard commercial operating systems such as VxWorks in order to leverage economies of scale. VxWorks is a single process operating system derived from the general purpose BSD 4.3 of UNIX systems where all concurrent threads or tasks share a global memory space. Originally each UNIX process is assumed to have a small number e.g. 64 of file descriptors also known as sockets but VxWorks extends it to a larger set e.g. 2048 .

Standard commercial operating systems supply socket handling facilities whose design is based on assumptions that are not valid for VoIP and collaborative systems. For example VxWorks utilizes a polling system that reaches its maximum efficiency with extensive use of relatively few sockets whereas VoIP systems typically have a large number of peripheral devices each of which needs to be supplied with socket services that even under heavy load conditions result in relatively light socket usage.

Typically I O multiplexing is performed over a large number of sockets using industry standard i.e. BSD 4.3 compatible socket services such as select or poll . However the use of standard socket services results in high computing overhead and poor scalability. Specifically on each polling task the operating system OS kernel must search across many sockets to find the relatively few ones with active events and it must do so for each of several applications that are active at one time.

In general there are three basic models or methods for socket I O multiplexing. A first method is to spawn a task i.e. a thread for each TCP connection wherein each task handles one socket I O. However this method works only for small and simple systems. It cannot be applied to a system handling a large number of sockets because it results in a large number of tasks being created in the system causing high memory cost and excessive system processing overhead.

A second method is to use a single task to control i.e. multiplex a group of sockets. This task continually makes a select call i.e. select with a predetermined non zero timeout value which allows the select call to wait until an I O event arrives at any given socket before the waiting time expires.

The third method referred to herein as the polling model is similar to the second method except that it periodically polls the set of sockets wherein each poll call i.e. poll returns immediately. By polling the sockets the computational overhead is reduced when compared to the second method by making system calls less frequently and collecting more I O ready socket events with each call. One problem however with the polling model method is that it introduces a response delay to the I O events.

It should be noted that VxWorks does not support poll . However a person of ordinary skill in the art will appreciate that a select call having zero timeout value is logically equivalent to poll . Both select and poll are theoretically of the same computational complexity although poll may have a small factor of improvement over select in some operating systems . Since all such systems nonetheless require sequential checking of the list of N sockets for the sake of convenience this specification occasionally uses the terms select and poll interchangeably.

Heuristic methods have been used to find the best value of polling interval or polling window PW for a particular system. Usually PW is related to a set of system attributes such as maximum number of IP phones supported maximum traffic throughput Calls Per Hour minimum I O delay etc.

An increase in N results in an increase in PW. However an increased N is also associated with an increased traffic requirement which in turn requires a smaller PW. Reducing the PW value increases the traffic throughput or reduces the I O delay but increases the CPU consumption. For a given system with limited CPU resources the choice of PW ends up being a tradeoff of system attributes to control I O multiplexing overhead within an acceptable range.

As an example of the foregoing the MN3300 system manufactured by Mitel Networks Corporation is a distributed TCP IP client server system where a single Integrated Communication Platform ICP controller or a cluster of interconnected ICPs plays a server role to support up to N 1400 IP phone clients. Each IP phone establishes a TCP socket connection with the ICP for signaling purposes. However the ICP controller typically must perform I O multiplexing over a very large number of TCP sockets to communicate with the IP phones. The computational cost of this I O multiplexing is directly proportional to N total number of sockets causing high overhead of O N computational time as well as scalability problems when N is large. In an effort to address the scalability problem in some commercial implementations of the MN3300 system the VxWorks kernel has been re compiled for select service to support 2048 4096 and 8192 file descriptors and thereby accommodate an increased number of IP phones.

In the prior art MN3300 system one task is responsible for polling TCP sockets every PW 40 milliseconds. Thus for an MN3300 system designed for N 1400 phones and about 8000 CPH traffic throughput under a saturated level of CPU usage reducing PW from 40 milliseconds to 20 milliseconds can cause some iPBX service outage. To support a larger system say N 3000 the MN3300 system needs to increase its CPU capacity.

A centralized polling service CPS is set forth that provides equivalent functionality to prior art select services such as the VxWorks select service but with a computing complexity of only O 1 . The CPS as set forth in greater detail below provides optimal I O multiplexing for user applications without affecting the kernel select service.

From a computational complexity point of view the method and apparatus set forth herein are optimal in that a predictable constant number of steps are executed to detect I O related events regardless of the number of sockets. In other words the disclosed method eliminates the scalability problem discussed above in building large TCP IP client server distributed systems and the resulting computing time is independent of N total number of TCP sockets .

The centralized polling service CPS set forth in detail herein below combines elements of polling and event systems to create a hybrid polling event system. Specifically applications continue to periodically scan for events thereby eliminating the overhead of pushing individual events to applications. However applications can also register or subscribe to events of interest thereby eliminating the overhead of polling systems at both kernel and application layers being required to search through multiple devices looking for ones with active events.

Thus in one aspect whereas it is known in the art that on each polling task the OS kernel must search across many sockets for each active application to find the relatively few sockets with active events the CPS set forth herein effectively centralizes polling services thereby eliminating redundant searching for multiple applications.

In another aspect the CPS set forth herein also eliminates the prior art requirement that the polling task poll each of many sockets. The ability of the CPS to register for events similar to event system capabilities results in capturing events of interest on a continual basis. Thus polling tasks can be supplied with active events that eliminate the need to repetitively poll large numbers of inactive sockets.

A more extensive discussion of the prior art and additional details of the solution summarized above are provided hereinafter reference being had to the accompanying drawings wherein like numerals refer to like parts throughout.

With reference to an exemplary communication system is shown comprising a communication switch such as the MN3300 ICP manufactured by Mitel Networks Corporation connected to a local area network or LAN and to the Public Switched Telephone Network PSTN . A plurality of IP telephone devices such as Phone Phone . . . Phone are connected to the LAN .

A person of skill in the art will appreciate that the configuration of is representative of a typical converged communication network and that numerous variations in configuration components etc. are possible.

In the exemplary embodiment of the ICP functions in a server role to support up to N 1400 IP phone clients. Each IP Phone establishes a TCP socket connection with the ICP for signaling purposes. The ICP performs I O multiplexing over a very large number of TCP sockets in order to communicate with the IP phones. As discussed briefly above the use of industry standard i.e. UNIX BSD 4.3 compatible socket services such as select or poll to perform socket I O multiplexing within the ICP which results in a high computational cost that is directly proportional to N the total number of sockets . For a prior art MN3300 ICP incorporating a 450 MHz PowerPC processor a single scan of 1400 TCP sockets takes approximately 10 milliseconds. Overall the networking I O functionality consumes approximately 25 of CPU resources even when there is no call processing traffic between the ICP and the IP phones. As the system line size N increases the computational overhead increases as well with the result that the existing platform hardware architecture eventually needs to be replaced with a more powerful CPU in order to meet increasing computing resource demands.

The Centralized Polling Service CPS set forth herein below with reference to the exemplary embodiments of completes the select functionality for any number of sockets in a small constant time e.g. typically less than 8 microseconds instead of 9.8 milliseconds as in the prior art or approximately 1000 times faster than prior art solutions . Furthermore the CPS set forth herein is fully scalable in that its computing time remains unchanged even if the total number of TCP sockets increases to N 10 000 or more.

An immediate practical commercial benefit of the CPS described herein is that it provides cost effective alternatives to building large enterprise systems and server based applications. For example it is possible to significantly increase the IP phone line size for existing IPBX systems without requiring any major architectural changes and investments.

As discussed briefly above the operating system OS for the exemplary ICP in the Mitel MN3300 ICP platform is VxWorks which is a popular UNIX like operating system suitable for real time embedded computing. VxWorks provides UNIX BSD 4.3 compatible select service as defined below 

Three parameters pReadFds pWriteFds and pExceptFds of the fd set file descriptor mask point to file descriptor fd sets wherein each bit corresponds to a particular file descriptor. The pReadFds parameter points to a bit mask that specifies the file descriptors to check for reading. Bits that have been set cause select to remain pending until data is available on any of the corresponding file descriptors. The pWriteFds parameter points to a bit mask that specifies the file descriptors to check for writing. Bits that have been set cause select to remain pending until any of the corresponding file descriptors become writable. The pExceptFds parameter points to a bit mask that specifies the file descriptors to check for exception conditions. The pExceptFds parameter is currently unused in VxWorks but is provided for UNIX call compatibility.

Thus a select on a set of socket file descriptors for a read event can monitor any connected socket to determine when data has arrived and is ready to be read without blocking the FIONREAD parameter to the ioctl system call is used to determine exactly how much data is available . A select on a set of socket file descriptors for a read event can also monitor any listening socket to determine when a connection can be accepted without blocking and can also monitor any socket to detect whether an error has occurred on the socket.

A select on a set of socket descriptors for a write event can monitor any connecting socket to determine when a connection is complete and determine when more data can be sent without blocking i.e. at least one byte can be sent and to detect if an error has occurred on the socket.

A select for an exception event will return true for sockets that have out of band data available to be read. A select also returns a true for sockets that are no longer capable of being used e.g. if a closed or shutdown system call has been executed against them .

The width parameter defines how many bits are examined in the file descriptor sets and should be set to either the maximum file descriptor value in use plus one or simply to kernel hard coded FD SETSIZE. For example in an exemplary MN3300 system such as depicted in the VxWorks kernel may be re compiled with FD SETSIZE 8192.

If pTimeOut is NULL select will block indefinitely. If pTimeOut is not NULL but points to a timeval structure with an effective time of zero the file descriptors in the file descriptor sets will be polled and the results returned immediately. If the effective time value is greater than zero select will return after the specified time has elapsed even if none of the file descriptors are ready.

When select returns it reports how many file descriptors are ready and it zeros out the file descriptors and sets only the bits that correspond to file descriptors that are ready.

A well known prior art BSD kernel implementation of select involves awakening all pended tasks or processes in the system at select so that each task can check its input bit masks for a descriptor in connection with which an I O event has arrived. A task making use of select has to perform a sequential search on the returned bit masks again in order to find out which socket fd is ready.

As discussed above this amounts to an overhead of O N computational time complexity where N is the maximum number of socket file descriptors used. In short the computing time of select is directly proportional to N causing high computing overhead and poor scalability problems when N is large.

The centralized polling service CPS set forth in greater detail below dynamically manages a profile for each polling task which contains the task ID current polling sockets and the polling results. It provides similar functionality as a select call but bypasses the standard kernel select service. The CPS set forth herein obtains the kernel socket asynchronous I O events and updates the polling tasks profiles in constant time. This guarantees an O 1 computing complexity in offering the polling services. The O N overhead in prior art select calls is thereby eliminated. The CPS set forth herein independently provides an optimal I O multiplexing choice for user applications without affecting the kernel select service.

Turning to a functional block diagram of the CPS is set forth. A person of skill in the art will appreciate that the block diagram of and the following description thereof are merely illustrative. A detailed description of an exemplary CPS API is set forth below with reference to .

As shown in read and write socket events on each of the fds are observed and handled by the OS kernel and supplied to the rest of the CPS via a callback function of VxWorks discussed in greater detail below with reference to . The callback function invokes CPS functionality for each event. Read and Write Byte Masks frequently also referred to as Socket Maps are indexed by the fds and contain the Service Number of the application that has registered to be informed of the event. CPS uses the Service Number to index into a Service List as discussed in greater detail below with reference to . The Service List is a list of records with each list containing among other elements Read and Write Stores for storing read and write socket events for the application identified by the Service Number i.e. CPS writes the fd of the event into the appropriate store . At suitable intervals the applications can poll for active events.

Turning briefly to the CPS Application Programming Interface API the standard select service requires callers to provide an input bit mask of file descriptors fds at each call even if there is no change to the bit mask since the last call by the same caller. This requires O N time for the select service to check the input bit masks . In the exemplary MN3300 systems once the IP Phones are powered on and registered with the ICP controller the TCP sockets remain open at all times 99.999 availability . An aspect of an exemplary embodiment of CPS is that it is not necessary for a user task to provide such socket bit mask information each time. Instead CPS keeps centralized polling socket maps for each caller task and allows the task to dynamically add clear the socket from its maps.

Similarly the standard select service returns the total number of active file descriptors and an output bit mask. Thus the caller task must search the bit masks to find which file descriptor is active which also takes O N time. In CPS this overhead is eliminated since the active file descriptors are kept on the Service List comprising Read and Write Stores. The CPS select call returns the number of active socket fds and the caller task simply performs get next actions to retrieve all active file descriptors.

To illustrate an exemplary embodiment a polling model for IPBX I O multiplexing is described followed by a description of an O 1 time complexity centralized polling service and a description of implementation results and performance comparisons.

To bring an offline IP Phone or IP device on line with the ICP there are two consecutive phases a Signaling Transport Link STL Setup phase and a Device Registration phase. In the STL Setup phase a TCP Socket connection is established between the remote IP device and the ICP or a TCP 3 way handshake protocol is completed. If a secured link is to be established additional security negotiations are performed as well. In the Device Registration phase the first segment of application data is sent as the IP phone e.g. IP Phone IP Phone etc. initiates a registration protocol with the ICP over the STL. If the protocol transactions are successful the IP phone will be put to In Service state by the ICP .

After the Device Registration phase the IP device applications are ready to provide services. For example IP Phone can make a call to IP Phone by initiating a call processing protocol with the ICP over the STL. If the protocol is successfully completed the ICP will set up a bi directional RTP voice streaming path directly between the two phones. Similarly other services such as maintenance service heartbeat service and network management service can all be provided through the corresponding signaling protocols over the STL TCP links.

ICP I O multiplexing is used to send and receive data through the TCP sockets after STL Setup phase. From an operational service point of view ICP I O multiplexing supports three cases IP Phone Service IP Trunking Service and Application Service. The IP Phone Service provides basic iPBX telephony services wherein IP phones IP Phone IP Phone etc. initiate TCP connections to ICP and wherein up to 1400 TCP sockets may terminate at a single ICP controller . With IP Trunking Service a cluster of ICPs may also be interconnected through TCP socket connections for fault tolerance networking topology and system service sharing purposes. In this case the ICP initiates TCP connections to other ICPs. According to the Application Service other IP Device applications or desk top applications may have their own TCP connections to the ICP in the event that the IP Phone Service TCP connections cannot be shared.

For the sake of simplicity the following description of an ICP multiplexing model is set forth for the IP Phone Service and IP Trunking Service cases only and since they cover all types of TCP socket connections.

In the exemplary MN3300 system ICP I O multiplexing is based on a polling model where a single polling task periodically performs a select call for read write data on a group of sockets associated for a particular service. For example the IP Phone Service performs such a polling task while IP Trunking performs another polling task of the same structure. In MN3300 systems the polling task is implemented as a standard C class.

At step other system maintenance actions are performed and if a socket has been opened or deleted then the socket record is updated in the Socket Map which as discussed in greater detail below with reference to is an array or table data structure for holding all current subscribed socket records and using socket fd as an indexing key. For example in IP Phone service 1400 TCP sockets are recorded on this map when 1400 IP phones come on line.

At step the BSD 4.3 select call is executed where the timeout value is set to zero such that it is a non blocking call whether or not there is any socket I O event. The select call returns all read ready and write ready I O events as discussed above.

The sockets are read at step which represents the major input part of the I O multiplexing operation. After select call is returned the number of active I O sockets is known. The GetNextReadyFd method in is used to retrieve socket events. Corresponding actions are taken based on the state of a given socket. For example if a socket with a read event is in the listening state then a standard BSD accept is applied to the new connection which creates a new socket and returns a file descriptor fd pointing to the new socket. For this new socket additional socket processing is immediately handed over to a higher level socket handler task so that the polling task can continue to read the next socket. On the other hand the socket handler task may decide whether or not time consuming secured connection negotiations e.g. SSL handshakes are required. Once this has been completed a socket subscripting request is sent back to the polling task for notification that the socket is ready to receive and transmit application data. It should be noted that the socket subscripting message cannot be processed at step . Instead the socket subscripting message is processed when the polling task loops back to step . If the negotiation is successful the new socket is then set to a Subscribed state and the socket fd is put on the Socket Map for detecting read ready events Read Byte Mask to receive the incoming data. If the negotiation fails the TCP connection is then closed. If the socket is in the subscribed state then a standard BSD read action is applied to the socket to receive the application data.

As discussed in connection with Task Queue Processing Loop the connecting sockets may have pending connections to complete. If select call step indicates a write ready event on a connecting socket then step is performed to process outgoing connections. As when accepting a connection if the connection is to be a secured link a security negotiator task may be notified of the new connection. If the connection is successful the new subscribed socket is put on the Socket Map for detecting Read ready events. Thereafter the incoming data on this new link can be received from later polling cycles.

In general the socket data output function is handled exclusively in the Task Queue Processing Loop step and the socket data input function is performed in Read Sockets step . The polling task invokes the select call step every 40 milliseconds and provides a comprehensive set of socket services both for accepting client e.g. IP Phone service connection requests or for initiating inter ICP connection requests e.g. IP Trunking service .

According to an exemplary embodiment CPS is implemented in C . The code segment of the CPS API shown in is executed to access CPS services. Specifically a polling task calls RegisterForSocketService to register with CPS and receive a valid larger than zero service number . A polling task can also stop using the CPS for example at system shut down by calling DeRegisterForSockService . It should be noted that for system safety and reliability considerations the task ID and service number have to be used together to access other services of the CPS.

The Add call allows the polling task to add a new socket fd for a given I O event READ 0 WRITE 1 to its polling Socket Map. The clear function allows the polling task to remove a socket fd for a given event from its polling Socket Map.

In the normal polling cycle the CPS select call step is used by the polling task to find the current total number of active sockets to read or write respectively. Similar to BSD 4.3 select the CPS select call allows the user task to specify a timeout value in milliseconds which is set to 0 by default for the polling model. Finally GetNextReadyFd call is used to retrieve the I O ready file descriptors reported by CPS Select call.

Accordingly a person of skill in the art will appreciate that CPS models the three part BSD 4.3 select call functionality as follows 1 updating input Socket Mask 2 finding the total number of I O ready sockets and 3 retrieving the output sockets. As discussed in greater detail below every CPS API function call in is completed in O 1 time.

Turning now to the CPS system interface according to an aspect of an embodiment of CPS the same asynchronous I O events are obtained as received by the BSD select utility within the operating system kernel. The raw input information that the BSD select receives is needed to write a fast and scalable CPS select facility. The specific information needed is the active socket file descriptor fd and the corresponding I O event type READ or WRITE .

As shown in a classic implementation of select is utilized wherein the kernel call sowakeup struct socket so struct sockbuf sb SELECT TYPE wakeupType awakens all processes pending on select . A user callback hook is included within the sowakeup call as an alternative for supporting asynchronous I O. This callback hook is a global function pointer for forwarding struct socket and SELECT TYPE i.e. SELREAD 0 SELWRITE 1 two types of parameters to the user callback function.

A Socket Map is a list with max fd 1 entries where max fd is the maximum number of sockets supported by the system. For example in an MN3300 system supporting 1400 IP Phones max fd is configured to 2000 and the memory cost for two maps is only 4K bytes. In this way a Socket Map can be indexed by the socket fd. The Socket Map is expressed as an octet thus called Byte Mask which is different from the bit mask of the BSD 4.3 select service. For example a byte operation is faster than a bit operation when accessing the Socket Maps using the callback function from the kernel. A Byte Mask that has been initialized to zero or empty may be used by the CPS to record a service number which represents a polling task s subscription on the indexed fd for the I O event associated with the store.

Several observations can be made. First a Socket Map is logically partitioned by service numbers. All entries with the same service number are associated with a partition where the service number is the number granted to a polling task at its registration time by the CPS. Second a Socket Map can accommodate at most 255 polling tasks at the same time. Increasing the entry s size from byte to word effectively eliminates this restriction in CPS. Third a socket fd cannot be subscribed to by two polling tasks with different service number at the same time. Socket collisions two applications attempting to read and or write to the same socket are thereby prevented in CPS.

BSD4.3 select service is not confined by restrictions such as the third observation above. Multiple applications can access a socket from two BSD4.3 select calls. Although socket collision is not supported directly by CPS it can be implemented by an overlay mechanism allowing a list of service numbers to be associated with an entry of the Socket Map. However in most applications supporting socket collision is not necessary. In fact in MN3300 IP PBX systems the third observation secure and reliable communications must be enforced.

As shown in a Service List data structure keeps a list of polling records in CPS. After successfully registering with the CPS a polling task creates a polling record on the Service List where the service number granted to the polling task is the index of the polling record to the list. If only a few tasks perform networking I O multiplexing such as in the exemplary MN3300 ICP controller then in practical terms the Service List is very short. CPS registration can be completed in only O 1 time because the length of the Service List is a small constant number. For a large Service List other suitable data structures e.g. hash table free service number pool can be used. Regardless a polling task needs to register with CPS only once during system configuration before polling the sockets.

A polling record contains all necessary information about a polling task such as Task ID. Two circular lists are provided Socket Read Store SRS and Socket Write Store SWS . Each store is associated with a Wakeup Flag and a Wakeup Semaphore binary semaphore depicted schematically at which are not needed to support the MN3300 polling model i.e. select call with zero timeout but are set forth to show that CPS can provide generic BSD4.3 select functionality with a non zero timeout value.

SRS SWS are used for keeping current read write ready socket file descriptors for the corresponding polling task. Each store has a sysPtr that points to the current available entry for the system e.g. kernel callback function to write a socket fd that has an I O ready event that has arrived. After each write operation sysPtr is moved so as to point to the next available entry. Each store also has a usrPtr that points to the current entry that a polling task can read. After each read operation the current entry is cleaned by resetting to 0 and usrPtr is moved so as to point to the next available entry. In a polling model sysPtr moves forward along the store whenever the system reports a new fd but usrPtr is only updated after each polling interval when a polling task comes back to catch up with the system s updates.

If the size of SRS SWS is too small it is possible that the sysPtr may come around the store and catch up with usrPtr creating a SRS SWS Read Write collision which must be avoided. However SRS and SWS collision detection can be achieved by simply checking whether sysPtr points to an entry that has not been cleaned i.e. larger than 0 before the system adds a new I O ready fd to the store.

Since the maximum call processing traffic pattern and the polling window are known the required size of SRS and SWS can easily be calculated. In the implementation of for example a store size of 1000 is able to support a traffic volume that is three times larger than the maximum operational volume 8000 calls per hour . The total memory cost for two stores is only 8 Kbytes.

With the foregoing as background basic run time cases are set forth below to illustrate how the above data structures are used in CPS algorithms for providing a set of polling services.

As shown in when wakeupCallback struct socket so SELECT TYPE wakeupType is called out from the VxWorks kernel it indicates that a file descriptor fd has an I O event SELREAD or SELWRITE step in the flowchart of .

At step from the given so so fd indexed to the right of the Socket Map in if the entry is empty a YES at step then the call exits step . Otherwise the value of the entry is used as the service number to index the Service List to find the relevant record for the polling task step .

At step the file descriptor fd is added directly to the polling task s fd list which is ready to be retrieved. Step is skipped over in the event that the Wakeup Flag is not set i.e. a NO determination at step . For the polling model where the CPS select call has a timeout value of zero i.e. a YES determination that the Wakeup Flag has been set at step the polling task is awakened step . Step illustrates that the CPS polling service can model all standard BSD select services.

It will be recalled from that each record in the Service List contains read and write event stores for each service number. In addition each record contains the Wakeup Flag and Wakeup Semaphore for that service number. These entities impart the ability of CPS to provide the time out functions supplied by the standard BSD select call. If CPS receives a select call from an application that supplies a time out value other than 0 it sets the Wakeup Flag and pends the call on the Wakeup Semaphore thereby providing a time out function as supplied by BSD Unix.

A polling task uses the CPS API RegisterForSockService int taskId int serviceNum to register with the CPS wherein taskId can be any unique number identifying the caller and a reference of serviceNum allows the caller to receive a service number from the CPS. Upon receiving the registration request CPS checks its Service List to find the index of the next empty entry. A polling record is then set up and the index value is assigned to serviceNum.

When de registering from the CPS i.e. using the DeRegisterForSockService int taskId int serviceNum call the registered polling record directly indexed by the given serviceNum is cleared or deleted.

A polling task uses the CPS API Add int fd int ioEvent int serviceNum int taskId to add a new socket fd against an interested soEvent READ 0 WRITE 1 for a polling task identified by serviceNum and tasked.

In the MN3300 polling model of after replacing the BSD select service with the CPS service CPS Add is called from the Task Queue Processing Loop of step in the three cases shown at step 

In step the new serviceNum can be written into the entry if and only if the entry has not been taken by any polling task. This provides protection against a socket collision as discussed above.

The algorithm of takes O 1 time provided that step handling Add Racing Condition ARC is completed in a constant time. Consequently it will be appreciated that ARC constitutes a challenge in developing a fast and scalable O 1 CPS select or polling service. The following is an explanation of what ARC is and how it is solved in constant time according to CPS.

Consider the example of adding a subscribed socket for a read event i.e. case ARC READ . When a TCP connection arrives on a listening socket the socket becomes readable. The polling task accepts the connection by creating a new socket for the connection and returns a file descriptor pointing to the new socket. This is done in the Read Sockets block step in . However the polling task cannot immediately call CPS Add since it has to pass additional processing such as secured negotiations to the higher level tasks. Once the higher level tasks have completed such additional processing a socket subscribing message is sent back to the polling task which is handled in the first block step of the main processing loop. Eventually the polling task calls the CPS Add with ioEvent ARC READ.

In the above scenario there is a random time interval between the time that the connection negotiation has been completed and the CPS Add call is initiated. During this time the far end IP phone is ready to send the first segment of application data but the CPS Socket Read Byte Mask has not yet been set. If the IP Phone s application data arrives during this time interval the kernel will invoke the CPS wakeupCallback function which exits at step of giving rise to the ARC problem namely the application data is consequently lost.

The standard BSD select service has no such problem because the BSD select call checks at the kernel level if any I O event has already arrived for the sockets given by the input socket bit mask. In order to avoid the O N sequential searches and to achieve O 1 time complexity performance the ARC problem is solved by the CPS API Add which directly adds the socket fd to the polling task s Socket Read Store thereby compensating for any missed updating of the SRS by wakeupCallback .

Therefore step handling Add Racing Condition ARC finds the polling record indexed by the serviceNum writes the fd to the entry pointed by sysPtr on the SRS and moves sysPtr to point to the next entry on the store .

It will be apparent that step may be performed in O 1 time. Thus the time complexity of the CPS Add algorithm shown in is O 1 . A person of skill in the art will appreciate that performance of step within the exemplary MN3300 system allows up to 1400 IP phones to reliably come online.

Since ARC relates to a racing condition it is also possible that CPS Add may sometimes be called before the first segment of application data arrives on a new connection. In this case wakeupCallback updates the SRS such that at step a duplicated socket fd read event is added to the SRS leading to at most one read miss on a subscribed socket when 0 bytes of data is read.

As shown in a polling task uses CPS API Clear int fd int ioEvent int serviceNum int taskId to remove or clear a previously added socket fd against an interested ioEvent READ 0 WRITE 1 for a polling task identified by serviceNum and taskId. When a TCP connection is down this function is used to remove the TCP socket from the polling task s Socket Map .

A validation check is performed step by comparing the given taskId against the stored taskId if a match is found then this check is passed and the polling record is removed from the Service List. Otherwise the check returns INVALID step . Step is similar to step in .

Step in provides protection for the Socket Map since only the polling task with an identical service number can clear its own Socket Map entry. The CPS Clear method ends at step . A person of skill in the art will appreciate that CPS Clear is simpler than CPS Add and is also an O 1 algorithm.

The CPS Select int ioEvent int serviceNum int taskId int timeout 0 call returns the number of active sockets available for a given ioEvent. shows its algorithm design in an exemplary embodiment of CPS.

A validation check is performed step by comparing the given taskId against the stored taskId if a match is found then this check is passed and the polling record is removed from the Service List. Otherwise the check returns INVALID step .

At step the value of the service number is used to index the Service List to find the relevant record for the polling task.

The major function of the CPS select occurs at step wherein a fixed number of arithmetic operations are carried out to determine how many entries exist between two pointers on the store. A store with M entries is implemented by a linear array of M entries with modulo operations to circulate back from the last entry to the first entry.

Similar to step of the wakeupCallback flowchart shown in step in is not necessary for the MN3300 polling model but is nonetheless described herein to show how the CPS select can fully model BSD select. In the MN3300 polling model the polling task is used to call BSD select every 40 milliseconds. However the polling task is able to call CPS select every 20 milliseconds since CPS select is more efficient than BSD select. CSP select is an O 1 algorithm as demonstrated above.

After the call wakeup of step step is performed once more step and the number of sockets is returned step .

It should be noted that the CPS select call does not require an input socket bit mask or an output socket bit mask both of which are built in data structures described in connection with .

A validation check is performed step by comparing the given taskId against the stored taskId if a match is found this check is passed. Otherwise the check returns INVALID step .

At step the value of the service number is used to index the Service List to find the relevant record for the polling task.

The major function of the CPS GetNextReadyFd occurs at steps through wherein a constant number of arithmetic operations are used to check whether usrPtr is behind sysPtr thereby ensuring that usrPtr never goes beyond sysPtr for the ring store. In short GetNextReadyFd takes O 1 time to get an I O ready socket fd. In BSD select it takes O N time to find a ready socket by sequentially searching the output socket bit mask.

In summary the seven cases Wakeup Callback from the kernel Registering with the CPS De register from the CPS Add socket to the Socket Map Remove the socket from the Socket Map CPS Select call and Retrieve the I O Ready socket described above with reference to set forth an exemplary embodiment of the CPS API and CPS. Each of the foregoing cases is implemented with O 1 time complexity. That is no matter how many sockets or file descriptors are involved in I O multiplexing CPS always takes constant time to respond to each CPS service request. From a computational point of view CPS is an optimal solution for I O multiplexing.

Experimental test results have been obtained using a standard MITEL MN3300 IP PBX system where the PBX switching platform i.e. ICP Controller in incorporated a 450 MHz PowerPC 603e CPU and 512 MB main memory to support up to 1400 IP phones.

During tests of 500 1000 and 1400 phones the online IP phones generated 6000 calls per hour signaling traffic to the ICP Controller across the TCP IP network . Measurements of the execution times were taken for BSD select and CPS select for each of the above described cases.

I O multiplexing is one of the major performance bottlenecks in iPBX systems such as the MN3300 system discussed herein. Among hundreds of tasks in the system the IP Phone polling task itself consumes about 30 CPU cycles for 1400 phones. With the CPS algorithms set forth herein the polling task demands only a small and constant CPU usage ratio wherein the major CPU consumption relates to handling the task queue messages step in and reading data from the selected sockets. As long as the total IP PBX traffic volume does not change increasing the number of IP phones does not increase the polling task s CPU overhead.

To summarize a fast and scalable I O multiplexing method is set forth for direct implementation in VoIP PBX and other collaborative systems. The centralized polling service CPS set forth herein is characterized by the following an O 1 algorithmic solution optimized for I O multiplexing that is applicable to general TCP IP Client Server networking architectures which is a generic solution capable of modeling standard BSD select service and which is portable across all VxWorks platforms.

Large systems with up to 10 000 IP phone lines can be developed in a cost effective manner. Existing legacy MN3300 systems can be upgraded to double the line size without requiring more powerful CPU hardware architectures. Also I O multiplexing overhead in large switching systems has been a challenging issue in the industry. Designing special purpose VLSI chips and hardware architectures for I O multiplexing represents another technological direction to speed up data transfer and reduce CPU loading. The CPS set forth herein provides a set of algorithms implemented in C as a cost effective software solution to this problem.

The present specification sets forth an exemplary embodiment. Other embodiments variations and applications are possible. For example the CPS set forth herein originated from solving I O multiplexing problems in a distributed client server system based on TCP connections. However CPS may be used in distributed client server systems based on UDP connections as well and may be applied to hybrid architectures with mixed connection types with TCP sockets UDP sockets and other network connection types supported by the platforms.

Also although the CPS set forth herein addresses VxWorks operating system platforms it can be applied to I O multiplexing in other operating systems having similar asynchronous I O event services with both file descriptor and event type information. For instance it is possible to use the CPS service on Linux platforms after modifying the Linux operating system kernel to provide a similar I O wakeup call feature as VxWorks.

Furthermore although the CPS is applied to IP PBX iPBX systems in this specification a person of skill in the art will understand that CPS can be applied to other switching systems. Many IT or Internet service providers offer various commercial services e.g. search services directory services banking services marketing services etc. for large numbers of users based on a client server architecture on distributed networking systems TCP IP or other legacy networks . I O multiplexing performance at the server side is closely related to the server throughput and response performance. Accordingly the CPS as set forth herein can help to build large scalable servers and provide cost effective services in many different commercial applications.

Many features and advantages are apparent from the detailed specification herein and it is intended by the appended claims to cover all such features and advantages. Further since numerous modifications and changes will readily occur to those skilled in the art it is not desired to limit the claim scope to the exact construction and operation illustrated and described and accordingly all suitable modifications and equivalents may be resorted to falling within the scope of the claims.

