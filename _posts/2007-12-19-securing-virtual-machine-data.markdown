---

title: Securing virtual machine data
abstract: One embodiment of the present invention is a method including: (a) representing virtual primary disk data and state data of a virtual machine in a unit of storage; (b) exposing the virtual primary disk data of the virtual machine to a guest of the virtual machine to allow the guest to access the virtual primary disk data; and (c) preventing the guest from accessing the state data for the virtual machine.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09354927&OS=09354927&RS=09354927
owner: VMware, Inc.
number: 09354927
owner_city: Palo Alto
owner_country: US
publication_date: 20071219
---
This application claims the benefit of U.S. Provisional Application No. 60 871 234 filed Dec. 21 2006 U.S. Provisional Application No. 60 884 568 filed Jan. 11 2007 and U.S. Provisional Application No. 60 886 072 filed Jan. 22 2007.

This application is related to application Ser. No. 11 960 460 entitled STORAGE ARCHITECTURE FOR VIRTUAL MACHINES and to application Ser. No. 11 960 491 entitled IMPLEMENTATION OF VIRTUAL MACHINE OPERATIONS USING STORAGE SYSTEM FUNCTIONALITY each naming Hiltgen and Schmidt as inventors each filed on even date herein and each incorporated herein by reference.

One or more embodiments of the present invention relate to virtual machines and more specifically to encoding methods for virtual machine state data.

In general state data for a virtual machine may be encoded in some form of computer readable media. In some cases storage local to an underlying hardware platform can be used. In some cases storage array technology may be used to share pools of underlying storage amongst multiple computational systems. In some cases other forms of media including portable removable media such as USB mass storage whether based on flash memory disk or other storage technology may be used. Regardless of the storage technologies used methods are desired whereby aspects of the virtual machine state may be protected or secured from undesired access.

Some embodiments of the present invention address one or more of the above identified needs. In particular one embodiment of the present invention includes a method comprising a representing virtual primary disk data and state data of a virtual machine in a unit of storage b exposing the virtual primary disk data of the virtual machine to a guest of the virtual machine to allow the guest to access the virtual primary disk data and c preventing the guest from accessing the state data for the virtual machine.

In accordance with one or more embodiments of the present invention methods for encoding data used by virtual machines provide isolation and or security for certain types or portions of virtual machine data while allowing a virtualization system to expose other types or portions of such data to a guest application or operating system. In some embodiments these methods allow a virtualization system to expose data that encodes a virtual disk while still securing data such as non disk data that encodes virtual machine state virtual hardware configuration and or snapshot or checkpoint states. Typically an encoding of virtual machine state includes backing state data corresponding to internal states of devices memory and other system components virtualized by or for a given virtual machine. As such read access to such state date e.g. by a guest may leak or compromise sensitive information while write access may afford malicious code or users with an attractive vector for attack.

It has been discovered that a unit of storage can be prepared or presented in a manner that secures virtual machine state data encoded therein from undesired exposure to a guest application or operating system executing in coordination with the virtual machine while still facilitating access e.g. by a guest computation to other data e.g. virtual primary disk data also encoded in the unit of storage. In general undesired exposure of virtual machine state data to a guest of a virtual machine can risk corruption of the state data and perhaps other non disk data. Whether such corruption is accidental or intentional the corruption can be avoided by limiting guest access to only an appropriate subset of data encoded in the storage unit.

In some virtualization system embodiments an appropriate subset includes data that is exposed to the guest computation as the information content of a disk virtualized by the virtual machine. In some embodiments mutually exclusive units of storage are employed for each virtual machine supported in a virtualization system. Typically in such embodiments the storage allocated for a particular virtual machine is partitioned in such a way that the virtualization system may expose virtual disk data while restricting access to non disk data e.g. virtual machine state data corresponding to internal states of devices memory and other system components virtualized . Various techniques such as encryption and or use of offsets may also be implemented to prevent circumvention of the restricted access. In some embodiments partitions are organized and partition information is manipulated such that a reduced apparent size of storage that encodes both disk and non disk data is presented to the guest and conceals non disk data beyond an apparent extent of the storage.

The methods may be used for encoding virtual machine data using any of a variety of underlying storage technologies including local disk storage network storage even portable and or removable media. Accordingly based on the description herein it will be apparent to persons of ordinary skill in the art that the term non disk data refers to data that encodes information other than that exposed as a disk by a virtualization system or other similar software system rather than to any exclusion of underlying storage technology. For concreteness of description an example of non disk data is backing state data corresponding to internal states of devices memory and other system components virtualized by or for a given virtual machine. Similarly an example of disk data is information exposed to a guest application or operating system by a virtualization system as a virtual primary secondary tertiary . . . disk. For avoidance of doubt underlying encodings of both disk and non disk data may reside in media that include or constitute disks.

The following describes embodiments in which virtual machine data is encoded in units of storage allocated from pooled network storage shared amongst virtualization systems. Although network storage units provide a useful descriptive context in which to illustrate embodiments network storage is not essential. Rather based on the description herein persons of ordinary skill in the art will appreciate that embodiments of the present invention may be used in the context of other storage technologies and configurations to selectively expose certain portions of virtual machine state while securing and or isolating other portions.

As used herein the term network storage refers generally to storage systems and storage array technology including storage area network SAN implementations network attached storage NAS implementations and other storage architectures that provide a level of virtualization for underlying physical units of storage. In general such storage architectures provide a useful mechanism for sharing storage resources amongst computational systems. In some cases computational systems that share storage resources may be organized as a coordinated system e.g. as a cluster or cooperatively managed pool of computational resources or virtualization systems . For example in a failover cluster it may be desirable to share or at least failover virtual machine access to some storage units. Similarly in a managed collection of virtualization systems it may be desirable to migrate or otherwise transition virtual machine computations from one virtualization system to another. In some cases at least some computational systems may operate independently of each other e.g. employing independent and exclusive units of storage allocated from a storage pool or pools provided and or managed using shared network storage.

Generally either or both of the underlying computer systems and storage systems may be organizationally and or geographically distributed. For example some shared storage particularly storage for data replication fault tolerance backup and disaster recovery may reside remotely from a computational system that uses it. Of course as will be appreciated by persons of ordinary skill in the art remoteness of shared storage is a matter of degree. For example depending on the configuration network storage may reside across the globe across the building across the data center or across the rack or enclosure.

While embodiments of the present invention particularly cluster organized and or enterprise scale systems may build upon or exploit data distribution replication and management features of modern network storage technology further embodiments may be used in more modest computational systems that employ network storage technology. For example even a single computer system may employ SAN type storage facilities in its storage architecture. Thus while some embodiments utilize network storage that can be shared and while at least some underlying elements thereof may be remote persons of ordinary skill in the art will understand that for at least some embodiments network storage need not be shared or remote.

In some embodiments of the present invention particularly those that use SAN type storage arrays block level I O access to virtual machine state data can afford performance advantages. Accordingly certain embodiments are described in which non commingled encapsulated representations of virtual machine state are maintained in distinct storage volumes or LUNs of a SAN. Nonetheless other embodiments including those that use NAS type or filesystem mediated access mechanisms still allow a virtualization system to exploit the described encapsulation and or isolation methods to limit access e.g. by a guest application or operating system to underlying virtual machine data.

For concreteness embodiments are described which are based on facilities terminology and operations typical of certain processor architectures and systems and based on terminology typical of certain operating systems virtualization systems storage systems and network protocols and or services. That said the embodiments are general to a wide variety of processor and system architectures including both single and multi processor architectures based on any of a variety of instruction set architectures to numerous operating system implementations and to systems in which both conventional and virtualized hardware may be provided. As described herein the embodiments are also general to a variety of storage architectures including storage virtualization systems such as those based on storage area network SAN or network attached storage NAS technologies as well as storage architectures that use local disk media removable media etc.

Accordingly in view of the foregoing and without limitation on the range of underlying processor hardware or system architectures operating systems storages architectures or virtualization techniques that may be used in embodiments of the present invention are described. Based on these descriptions and on the claims that follow persons of ordinary skill in the art will appreciate a broad range of suitable embodiments.

In the illustrated collection storage area network SAN technology is used for at least some storage needs of computational systems participating in the collection. In general network storage systems including SAN based system provide a level of virtualization for underlying physical storage elements e.g. individual disks tapes and or other media where the characteristics and or configuration of particular storage elements may be hidden from the systems that employ the storage. SAN based systems typically provide an abstraction of storage pools from which individual storage units or volumes may be allocated or provisioned for block level I O access. In the illustrated collection a switched fabric topology consistent with Fibre Channel SAN technology is shown in which switches A B C and or directors are used to mediate high bandwidth access typically using a SCSI Small Computer System Interface command set to an extensible and potentially heterogeneous set of storage resources A B C D E F G e.g. SATA Serial ATA and or SCSI disks tape drives as well as arrays thereof e.g. RAID i.e. Redundant Array of Inexpensive Disks . Such resources may be distributed and if desirable may provide data replication and or off site storage elements. Fibre Channel is a gigabit speed network technology standardized in the T11 Technical Committee of the InterNational Committee for Information Technology Standards INCITS .

In general a variety of different types of interconnect entities including without limitation directors switches hubs routers gateways and bridges may be used in topologies or sub topologies that include point to point arbitrated loop switched fabric portions. Fibre Channel and non Fibre Channel technologies including those based on iSCSI protocols i.e. SCSI command set over TCP IP or ATA over ethernet AoE protocols may be used in embodiments of the present invention. Similarly any of a variety of media including copper pair optical fiber etc. may be used in a network storage system such as SAN .

Although not specifically illustrated in persons of ordinary skill in the art will recognize that physical storage is typically organized into storage pools possibly in the form of RAID groups sets. Storage pools are then subdivided into storage units e.g. storage volumes that are exposed to computer systems e.g. as a SCSI LUN on a SAN communicating via Fibre Channel iSCSI etc. . In some environments storage pools may be nested in a hierarchy where pools are divided into sub pools. In general persons of ordinary skill in the art will understand the SCSI derived term LUN Logical Unit Number to represent an address for an individual storage unit and by extension an identifier for a virtual disk of other storage device presented by a network storage system such as SAN . By convention the term LUN is used throughout this description however based on the description herein persons of ordinary skill in the art will appreciate that this is done without limitation and that any suitable identifier may be employed to identify an individual storage unit in embodiments of the present invention.

Embodiments of the present invention may be understood in the context of virtual machines or virtual computers that are presented or emulated within a virtualization system such as virtualization system executing on underlying hardware facilities . However in addition migration from or to a computational system embodied as a conventional hardware oriented system may be supported in some systems configured in accordance with the present invention. Nonetheless for simplicity of description and ease of understanding embodiments are described in which individual computational systems are embodied as virtualization systems that support one or more virtual machines.

Although certain virtualization strategies designs are described herein virtualization system is representative of a wide variety of designs and implementations in which underlying hardware resources are presented to software typically to operating system software and or applications as virtualized instances of computational systems that may or may not precisely correspond to the underlying physical hardware.

The term virtualization system as used herein refers to any one of an individual computer system with virtual machine management functionality a virtual machine host an aggregation of an individual computer system with virtual machine management functionality and one or more virtual machine hosts communicatively coupled with the individual computer system etc. Examples of virtualization systems include commercial implementations such as for example and without limitation VMware ESX Server VMware and ESX Server are trademarks of VMware Inc. VMware Server and VMware Workstation available from VMware Inc. Palo Alto Calif. operating systems with virtualization support such as Microsoft Virtual Server 2005 and open source implementations such as for example and without limitation available from XenSource Inc.

As is well known in the field of computer science a virtual machine VM is a software abstraction a virtualization of an actual physical computer system. Some interface is generally provided between the guest software within a VM and the various hardware components and devices in the underlying hardware platform. This interface which can generally be termed virtualization layer may include one or more software components and or layers possibly including one or more of the software components known in the field of virtual machine technology as virtual machine monitors VMMs hypervisors or virtualization kernels. 

Because virtualization terminology has evolved over time and has not yet become fully standardized these terms when used in the art do not always provide clear distinctions between the software layers and components to which they refer. For example the term hypervisor is often used to describe both a VMM and a kernel together either as separate but cooperating components or with one or more VMMs incorporated wholly or partially into the kernel itself. However the term hypervisor is sometimes used instead to mean some variant of a VMM alone which interfaces with some other software layer s or component s to support the virtualization. Moreover in some systems some virtualization code is included in at least one superior VM to facilitate the operations of other VMs. Furthermore specific software support for VMs is sometimes included in the host OS itself.

Embodiments are described and illustrated herein primarily as including one or more virtual machine monitors that appear as separate entities from other components of the virtualization software. This paradigm for illustrating virtual machine monitors is only for the sake of simplicity and clarity and by way of illustration. Differing functional boundaries may be appropriate for differing implementations. In general functionality and software components structures described herein can be implemented in any of a variety of appropriate places within the overall structure of the virtualization software or overall software environment that includes the virtualization software .

In view of the above and without limitation an interface usually exists between a VM and an underlying platform which is responsible for executing VM issued instructions and transferring data to and from memory and storage devices or underlying hardware. A VMM is usually a thin piece of software that runs directly on top of a host or directly on the hardware and virtualizes at least some of the resources of the physical host machine. The interface exported to the VM is then the same as the hardware interface of a physical machine. In some cases the interface largely corresponds to the architecture resources and device complements of the underlying physical hardware however in other cases it need not.

The VMM usually tracks and either forwards to some form of operating system or itself schedules and handles all requests by its VM for machine resources as well as various faults and interrupts. An interrupt handling mechanism is therefore included in the VMM. As is well known in the Intel IA 32 x86 architecture such an interrupt exception handling mechanism normally includes an interrupt descriptor table IDT or some similar table which is typically a data structure that uses information in the interrupt signal to point to an entry address for a set of instructions that are to be executed whenever the interrupt exception occurs. In the Intel IA 64 architecture the interrupt table itself contains interrupt handling code and instead of looking up a target address from the interrupt table it starts execution from an offset from the start of the interrupt when a fault or interrupt occurs. Analogous mechanisms are found in other architectures. Based on the description herein interrupt handlers may be adapted to correspond to any appropriate interrupt exception handling mechanism.

Although the VM and thus applications executing in the VM and their users cannot usually detect the presence of the VMM the VMM and the VM may be viewed as together forming a single virtual computer. They are shown and described herein as separate components for the sake of clarity and to emphasize the virtual machine abstraction achieved. However the boundary between VM and VMM is somewhat arbitrary. For example while various virtualized hardware components such as virtual CPU s virtual memory virtual disks and virtual device s including virtual timers are presented as part of a VM for the sake of conceptual simplicity in some virtualization system implementations these components are at least partially implemented as constructs or emulations exposed to the VM by the VMM. One advantage of such an arrangement is that the VMM may be set up to expose generic devices which facilitate VM migration and hardware platform independence. In general such functionality may be said to exist in the VM or the VMM.

It is noted that while VMMs have been illustrated as executing on underlying system hardware many implementations based on the basic abstraction may be implemented. In particular some implementations of VMMs and associated virtual machines execute in coordination with a kernel that itself executes on underlying system hardware while other implementations are hosted by an operating system executing on the underlying system hardware and VMMs and associated virtual machines executed in coordination with the host operating system. Such configurations sometimes described as hosted and non hosted configurations are illustrated in . However the description herein refers to the physical system that hosts a virtual machine s and supporting components whether in the hosted or non hosted configuration as a virtual machine host. To avoid confusion the hosted configuration will be referred to herein as OS hosted and the non hosted configuration will be referred to as non OS hosted. In the OS hosted configuration an existing general purpose operating system OS acts as a host operating system that is used to perform certain I O operations. In the non OS hosted configuration a kernel customized to support virtual machines takes the place of the conventional operating system.

Different systems may implement virtualization to different degrees virtualization generally relates to a spectrum of definitions rather than to a bright line and often reflects a design choice in respect to a trade off between speed and efficiency and isolation and universality. For example full virtualization is sometimes used to denote a system in which no software components of any form are included in the guest other than those that would be found in a non virtualized computer thus the OS guest could be an off the shelf commercially available OS with no components included specifically to support use in a virtualized environment.

Another term which has yet to achieve a universally accepted definition is para virtualization. As the term implies a para virtualized system is not fully virtualized but rather a guest is configured in some way to provide certain features that facilitate virtualization. For example the guest in some para virtualized systems is designed to avoid hard to virtualize operations and configurations such as by avoiding certain privileged instructions certain memory address ranges etc. As another example many para virtualized systems include an interface within the guest that enables explicit calls to other components of the virtualization software. For some the term para virtualization implies that the OS guest in particular its kernel is specifically designed to support such an interface. According to this definition having for example an off the shelf version of Microsoft Windows XP as the OS guest would not be consistent with the notion of para virtualization. Others define the term para virtualization more broadly to include any OS guest with any code that is specifically intended to provide information directly to the other virtualization software. According to this definition loading a module such as a driver designed to communicate with other virtualization components renders the system para virtualized even if the OS guest as such is an off the shelf commercially available OS not specifically designed to support a virtualized computer system.

Unless otherwise indicated or apparent virtualized systems herein are not restricted to use in systems with any particular degree of virtualization and are not to be limited to any particular notion of full or partial para virtualization.

Network storage systems provide virtualized storage where the physical structure of the storage e.g. individual disks tapes or other media is hidden from the user. Network storage systems provide abstractions of storage pools and or storage units to manage the physical storage. For example physical storage is often organized into storage pools possibly in the form of RAID groups sets. Storage pools can then be subdivided into storage units which are then exposed to computer systems e.g. a SCSI LUN on a SAN communicating via Fibre Channel iSCSI etc. . In some environments storage pools may be nested in a hierarchy where pools are divided into sub pools.

In the illustrated configuration virtualization system supports virtual machines A B and virtualization system supports virtual machines A B. Although virtual machines A and A are depicted in on separate virtualization system embodiments are not so limited. A virtualization system may support larger or smaller numbers of virtual machines and may support different numbers of virtual machines at different times. Virtual machine management system indicates to virtualization system that network storage unit has been allocated for a virtual machine of virtualization system . Likewise virtual machine management system indicates to virtualization system that network storage units and have been allocated for virtual machines A and B.

Storage abstractions provided by network storage systems can be directly used by the virtual machine management system to integrate virtual machine provisioning and storage provisioning. For example one or more storage pools may be assigned to a virtual machine manager. A virtual machine provisioning operation will automatically provision storage units from the assigned storage pools. The degree of exposure of a network storage system to virtual machine management may vary based on needs structure of a particular entity. The degree of integration of facilities of a network storage system and a virtualization system will vary in accordance with the degree of exposure. A small entity for example may wish to task a single group with management of both virtualization systems and storage systems. Full exposure of the raw storage devices can allow for full integration of virtualization management and storage management. Full exposure of raw storage devices to a virtualization management system allows the virtualization management system to organize the raw storage devices into logical units. In such a scenario the virtualization management system may create the abstraction of storage units directly upon the raw storage devices or create additional layers of abstraction such as storage pools.

In another scenario an entity may outsource management of storage devices or utilize a third party vendor that provides access to raw storage devices while managed by the third party vendor allowing the entity to retain some degree of control of the raw storage devices. In this third party scenario the entity responsible for management of the raw storage devices may expose storage pools to the entity managing virtual machines. Hence the raw source devices are hidden from the virtual machine management entity but are provided some latitude in management of the storage. This allows the virtual machine management entity greater ability to tailor the storage to their needs.

A large entity may have an established division for management of their storage devices separate from a division for virtualization. Such a large entity may wish to maintain distinct separation between the divisions. Hence the virtualization division would only have exposure to units of the network storage as allocated by the storage device division perhaps as requested by the virtualization division. However as already stated some degree of integration of network storage management and virtual machine management satisfies some calls for efficiency.

At block the storage pool is selected. At block network storage units are created in the selected storage pool. The network storage units are created in accordance with configuration settings that indicate for example a given size for each network storage unit. The size may be a default size a discrete size selected from multiple options user defined size etc. At block the created network storage units are designated for virtual machine data. A virtual machine manager may mark each of the created network storage units with a value identifying the network storage units as supporting virtual machines. Marking may be implemented by writing a partition type into a root partition for a network storage unit that has been associated with virtual machines. However it is possible that a partition type may be overloaded with multiple associations. In addition to or instead of partition type marking a unique value may be written into the network storage unit. For example a checksum may be coded into the virtual machine data stored in a network storage unit. A discovery operation would then search for both the partition type and the checksum to verify that the network storage unit has been designated for virtual machines. Indication of whether a network storage unit supports a virtual machine may be maintained in a data structure separate and distinct from the network storage units in addition to or instead of marking the created network storage units.

To help illustrate the following example configuration is provided which refers to an industry standard Storage Management Initiative Specification SMI S . Storage array vendors typically provide APIs for third party management. One example is the industry standard SMI S which describes vendor neutral interfaces to manage storage arrays. A virtual machine manager may configure storage i.e. provision logical representations of raw storage for virtual machine provisioning automatically or manually. If automatic configuration is implemented then the virtual machine manager performs Service Location Protocol SLP discovery for SNIA SMI S registered profiles. The virtual machine manager displays a list of discovered network storage in a user interface and prompts a user to enter a user password. After the user enters a valid name password for the discovered network storage the virtual machine manager authenticates. If manual configuration is implemented a user enters an IP address optional port number and name password for the network storage management interface. With the entered information the virtual machine manager authenticates. After authenticating the virtual machine manager connects to the network storage unit and discovers available storage pools. From the available discovered storage pools the virtual machine manager divides the pools into primordial pools and data store pools from which network storage units are created. The virtual machine manager enumerates units e.g. logical unit numbers LUNs of the network storage from the storage pools and compares the enumerated units against a list of in band discovered LUNs across applicable hosts. The virtual machine manager displays root boot LUNs known virtual machine LUNs and all other LUNs separately.

Referring again to at block a unit of network storage designated for support of a given virtual machine is communicated to a corresponding virtualization system. For example a virtual machine manager identifying information for a network storage unit communicates with a virtualization layer on the virtualization system. The virtualization layer may then map the identified network storage unit to a host bus adapter exposed by the virtualization system to the corresponding virtual machine.

Of course creation and designation of network storage units for virtual machines does not necessitate immediate or contemporaneous creation of a virtual machine or communication of a network storage unit to the virtualization system. In general the division of labor between a virtual machine manager and a virtualization layer of a virtualization system represents an implementation choice and implementations can vary greatly. For example a virtual machine manager may only perform storage pool discovery and communicate discovered storage pools to the virtualization layer of a virtualization system. The virtualization layer may be responsible for creation of network storage units designation of the created network storage units for virtual machines and eventual provisioning of a virtual machine. In another example a virtual machine manager may perform all operations from storage unit discovery perhaps even storage unit creation to provisioning and activation of a virtual machine. In this embodiment a virtualization layer is tasked with the responsibility of operating and exposing resources to supported virtual machines executing in coordination therewith. Regardless of the division of labor in a particular embodiment provisioning of a virtual machine involves preparation of a corresponding network storage unit. Preparation involves formatting of a network storage unit and possibly initialization of the network storage unit with certain data for a virtual machine. Preparation of network storage unit may be entirely performed by a virtual machine manager or a virtualization layer collectively performed by a virtual machine manager and a virtualization layer alternately performed etc.

Although the above depicts allocation and preparation of a single unit of network storage for each virtual machine multiple units of network storage may be allocated and prepared for a given virtual machine. Virtual machine provisioning can range from completely encapsulating a single virtual machine in a single network storage unit i.e. storing state data virtual primary disk data virtual hardware configuration data snapshot data virtual secondary disk data etc. to allocating individual network storage units for each aspect of a virtual machine e.g. a network storage unit for each of virtual hardware configuration data state data virtual primary disk data boot sector etc. . In light of these variations the network storage unit that backs state data of a virtual machine is referred to herein as the base unit whether or not the unit also encodes additional data.

At block one or more additional units of network storage are allocated for one or more secondary virtual disks. At block the additional network storage units for the secondary disks are indicated. Various techniques can be utilized to indicate the additional units of network storage allocated for the virtual secondary disks of the virtual machine. For example indications may be written into the base unit to identify the additionally allocated units of network storage as virtual secondary disks e.g. the base unit will identify a globally unique ID such as SCSI Inquiry VPD page 0x83 for a secondary virtual disk . The globally unique ID will allow the virtual secondary disks to be automatically detected and attached to the virtual machine. The indications of the additional network storage units as virtual secondary disks may alternatively or in addition be communicated to the virtualization system. The secondary virtual disk network storage units may also be marked to distinguish them from base units. Control flows from block to block . At block the base unit is indicated to the virtualization system at least for backing state data of the virtual machine.

For a given embodiment the degree of separation of virtual machines across network storage units can range from only separating state data for virtual machines to separating all data for virtual machines. For example a first network storage unit may encode a library of possible virtual hardware configuration. While the state data of two distinct virtual machines would be backed on separate network storage units these two distinct virtual machines may point to the same or different virtual hardware configuration data in the library. In another example two distinct virtual machines may share a virtual secondary disk on a network storage unit while their respective virtual primary disks and state data are stored on mutually exclusive network storage units.

In general prevention of commingling of state data of the state virtual machines may be enforced implicitly or explicitly. For example an embodiment of a virtual machine manager may not present a network storage unit already encoding state data of a virtual machine during virtual machine provisioning. However in some cases a given embodiment of a virtual machine manager may preserve the availability for provisioning multiple virtual machines onto a single network storage unit perhaps for performance comparison purposes dwindling resources etc. Of course the preserved option would involve mounting a file system as intermediary between a virtual machine and a network storage system thus losing integration of the virtualization system in the network storage system and manageability of virtual machines at network storage unit granularity.

As mentioned above data that encodes a virtual machine typically includes multiple data components state data virtual primary disk sometimes referred to as virtual primary disk data virtual hardware configuration data e.g. type of processor type of virtual network card type of virtual storage host bus adapter HBA amount of memory etc. snapshot data and zero or more virtual secondary disks sometimes referred to as virtual secondary disk data . Those of ordinary skill in the art should appreciate that in various embodiments these multiple data components may be aggregated together separated differently across different embodiments further divided etc. The state data of a virtual machine indicates execution state of a virtual machine at a particular time whether suspended or not suspended. For example state data indicates current data in all or a portion of a memory of a virtual machine e.g. instruction and or value data in the virtual machine s RAM cache registers etc. . A boot disk e.g. a boot sector and OS disk image may reside on the primary virtual disk virtual secondary disks or not be present at all for network boot of virtual machines. Virtual hardware configuration data indicates a configuration of a virtual machine. For example virtual hardware configuration data indicates a type of virtual processor type of virtual network card type of virtual storage HBA amount of virtual RAM virtual chipset type and size of a virtual primary disk etc.

The virtual machine manager or virtualization layer of the corresponding virtualization system should allocate enough space in the state data portion of a network storage unit to support the configured memory size of the virtual machine. To support memory growth in the future the virtual machine manager or virtualization layer provisioning functionality may choose to over allocate space to accommodate the growth. The provisioning functionality may later change a virtual disk size or the supported virtual RAM size by extending the unit of network storage updating a top level partition data structure and moving the data around e.g. to the end of the network storage unit in the data structure outlined above . If the virtual disk size is being expanded the virtual disk exposed to a guest will now report a larger size. If RAM is being added the state data region will be increased. The state data region and other regions are typically small compared to virtual disks so this reconfiguration should be a relatively fast operation. The provisioning computation may also choose to use a fragmented model without relocating regions when growing shrinking the network storage unit.

Regardless of the particular embodiment allocating mutually exclusive network storage units for distinct virtual machines dispenses with a traditional file system to store data components of the virtual machine. Dispensing with the traditional file system allows for example a storage virtualization engine to communicate directly with a network storage unit manager assuming a virtual machine stack that comprises a virtual machine a storage virtualization engine i.e. instantiated code that presents virtual storage to a virtual machine a network storage manager e.g. a SAN agent and a communication medium interface . The intermediary file system that would reside between the storage virtualization engine and the network storage manager has been obviated. With at least mutually exclusive network storage units backing state data of a virtual machine the entire virtual machine state is encapsulated in a single base unit.

Since a virtual machine has complete access to an entire network storage unit a guest may also have access to the entire network storage unit. It would be prudent to restrict access to non disk data. Exposing the state data virtual hardware configuration data or snapshot data can potentially leak sensitive information to a guest of the virtual machine. Actions by a guest whether accidental or intentional could result in the deletion or corruption of this data. Restricting guest access to non disk data provides security and reliability. Furthermore the potential harmful and or undesirable exposure of non disk data to a guest of a virtual machine is not limited to virtual machines with data stored in remote shared storage. For example risks may also exist for a virtual machine with data stored in remote unshared storage storage local to the hardware system that supports a given virtual machine e.g. flash memory plugged into a USB port a local disk etc. .

To address this concern non disk data is isolated from disk data. A method for isolating disk data from non disk data would be to limit guest access to a partition of a network storage unit that encodes a virtual primary disk and zero or more partitions that encode virtual secondary disks. For example the virtualization layer on a virtualization system perhaps a virtual machine monitor in the virtualization layer provides a virtualized SCSI disk to the virtual machine that only includes a virtual disk partition. To the virtual machine this looks like a regular virtual disk with its own partition table and partitions. A guest of the virtual machine is no longer able to access the non disk data.

Referring again to after formatting network storage unit virtual machine management system communicates information concerning the primary virtual disk partition to the virtualization layer of virtualization system which in turn exposes only the virtual disk data.

Although the embodiment s described with reference to employs a unit of network storage further embodiments of the inventive techniques can more generally be applied to other types of storage. For example a local storage may be partitioned into multiple partitions. At least two partitions are allocated to a virtual machine but a guest of the virtual machine is restricted to accessing the one or more partitions that encode disk data. The local storage may be organized in accordance with a file system. Different permission levels are assigned to folders. A folder that includes disk data for various virtual machines is set with a lower level permission than is granted to guests. A different folder that includes non disk data is set with a privileged permission level granted to the virtualization layer but not granted to guests. Permission levels may be applied to individual files in addition to setting access permissions for folders or instead of setting permission for folders.

Another method for restricting access to non disk data hides the non disk data from a guest while exposing a global partition table. For example the non disk data whether or not organized into different partitions are located at a location in a partition or storage known to the virtualization layer. The size of the storage folder or partition is truncated when reported to a guest to hide the region occupied by non disk data. Hiding the region occupied by non disk data obviates modification of I O requests with offsets.

Methods that align the beginning of a virtual disk with the beginning of a physical storage for a virtual machine can also be used to conceal non disk data from a guest. A partition table for the virtual disk is stored in a non disk data partition. When a guest requests access to the beginning of the storage the second partition table is fetched and exposed to the guest. Attempted access by the guest to other than the beginning of the storage is allowed if not off the truncated end.

As described above in some embodiments of the present invention a virtual machine directly accesses data encoded in a unit of network storage rather than via an intermediate file system interface. To enforce security and restrict access by a guest of a virtual machine to at least state data if not all non disk data the virtualization layer intercepts requests from a guest of a virtual machine.

For a response to an access request the reverse of the operations depicted in are applied. A response received by system hardware would be passed to virtualization layer . In virtualization layer the network storage management agent extracts a payload from the response and passes the payload e.g. read request data to the virtualization layer disk I O handler . The virtualization layer disk I O handler then modifies any location indication to conceal non disk data locations from the application guest e.g. subtracts an offset and perhaps the operating system guest . The virtualization layer disk I O handler passes the response with the modified location indication to the virtual system . The application guest eventually receives the response to its initiated disk access request with the modified location indication. Although the above describes an embodiment for preventing access to certain if not all non disk data by a guest of a virtual machine the scope of restriction may be expanded to include the virtual machine data. In general access to non disk virtual machine data may be limited to a virtualization layer that supports the corresponding virtual machine.

As previously stated other techniques for concealing non disk data from a guest may be implemented that avoid modifying requests with an offset. For example storage size may be truncated when reported to a guest to conceal a region occupied by non disk data. The previously described alignment technique also avoids modifying access requests from guests with offsets. Avoiding offset modification can be implemented with a less performance intensive set of operations e.g. introducing a compare operation and a branch predict operation to the I O path instead of a set of operations to compute and apply an offset .

Although non disk data locations of the network storage unit are hidden from guests of a virtual machine non disk data locations are exposed to a corresponding virtualization system for certain management operations such as backup operations and snapshot operations. A component of a virtualization layer e.g. a network storage management agent has direct access to a network storage unit. Direct access to a network storage unit allows state data to be written directly to a backing network storage unit for efficient backup of the state data. It is not necessary however for functionality that performs a write for backing state data to be resident in the virtualization layer or embodied as a separate agent. Embodiments may implement the backup writing functionality in the virtual system exposed to a virtual machine in an agent that is separate from the virtual machine and the virtualization layer etc.

Similarly snapshot functionality may be implemented in the exposed virtual system in the virtualization layer in a separate agent as an agent in the virtualization layer etc. Regardless of the particular implementation by exploiting direct access to a network storage system and an n 1 relationship between network storage units and virtual machines a virtualization system can leverage facilities of a network storage system. In general a network storage system application programming interface allows direct access by a virtualization system to snapshot facilities of a network storage management system. Allocation of mutually exclusive network storage units for distinct virtual machines allows one to use network storage management system facilities without the overhead from an intermediary file system.

Modern storage arrays typically provide the ability to snapshot a storage volume. A snapshot is typically implemented as copy on write implemented at the storage array layer where a second volume is created and linked to the first volume. To the client of the array a snapshot results in a second volume that has the exact state of the original volume at the point the snapshot was taken. Arrays provide further capabilities such as refreshing a snapshot to bring it back into synchronization with the base volume as well as severing a link between the two volumes to make clones. Since state data for a single virtual machine is completely stored in a base network storage unit taking a snapshot for a virtual machine can be implemented with a write operation to the base unit and a network storage management system snapshot operation of the base unit and perhaps any supplemental network storage units for the virtual machine.

Once this point is reached where a consistent state can be conveyed for the virtual machine the virtualization layer triggers a snapshot of the base network storage unit to the linked snapshot network storage unit. This causes the underlying facilities of the network storage e.g. network storage management system to copy the data of the base network storage unit and store it as snapshot 1 in the snapshot network storage unit. The virtualization also begins routing I O to the snapshot network storage unit which is written as current state data for the virtual machine. This process repeats as long as snapshot activation continues storing a given number of snapshots in a different network storage unit.

In the context of arrays and volumes the VM snapshot is now stored in a separate volume or volumes if multiple disks are in use by the VM . Multiple VM snapshots can be supported by performing multiple snapshots on the array. In the state data portion of the base volume references can be stored to the snapshot parent base volume so the virtual machine manager can discover the parent child relationship between snapshots. In this approach the copy on write is not implemented in the virtual machine or in a file system layer but in the storage array layer.

Typically storage arrays change the unique ID of the snapped volume to distinguish it from the base volume. Many operating systems especially those with advanced multipath capabilities use the unique ID of a volume as the ID for their disks instead of relying on the SCSI target and unit numbers. In a virtualization environment the same ID for the virtual disk can be exposed to a guest s of a virtual machine otherwise drivers within an OS guest may become confused and may crash the OS. Within the state data portion of the base disk or even a separate location the initial unique ID for the volume s is recorded. When using snapped volumes the virtualization layer will interpose and replace the snapped ID with the initial ID so that the OS guest is unaware that it is operating on a snapshot. Of course this functionality is not limited to snapshotting the state data. A snapshot may be taken of the entire base unit a very active portion of the state data assuming there is a very active and less active state data portions the primary virtual disk and state data all disks and state data etc.

Although an initial snapshot includes all data e.g. all state data all data in a base unit etc. subsequent snapshots may be taken in accordance with a variety of snapshotting techniques. For example each snapshot in a series of snapshots may include all data at the particular moment of the snapshot or only the difference between the current snapshot and a predecessor snapshot which is not necessarily the root snapshot or immediately preceding snapshot. For example a snapshot x may only include differences between itself and snapshot x 3. The snapshot may include a reference to the snapshot x 3 which may include another reference to the base snapshot. Another approach may track snapshot relationships in a table linked list etc. Those of ordinary skill in the art should appreciate that a variety of data structures may be utilized to encode snapshots and or snapshot relationships.

At block the virtual machine is stopped. Although the virtual machine is stopped in the illustrated embodiment embodiments in accordance with the present invention may suspend the virtual machine thus preserving running state if so desired. In either case the virtual machine state in network storage will be consistent and suitable for snapshot. At block the selected snapshot data is loaded from the corresponding network storage unit. At block it is determined whether additional corresponding snapshot data resides on other network storage units e.g. snapshots of virtual secondary disks that correspond to the selected snapshot . If additional snapshots exist then control flows to block . If there are no other corresponding snapshots for the virtual machine then control flows to block . At block the corresponding snapshots in the other network storage units are bound to the selected snapshot. At block the virtual machine is resumed with the selected snapshot.

Maintaining snapshots of virtual machines allows activation of a virtual machine s at different points in time which can be used for a myriad of purposes such as testing agent less back up offline virus scanning and other integrity checks etc. When reverting to a previous snapshot it may be desirable to discard an intermediate snapshot and maintain a snapshot that reflects a most recent state. For example a snapshot to be deleted may correspond to state results from a failed test or experiment. This can also be used for applications that reset and erase any changes made by users upon every reboot such as a kiosk application. Discarding or deleting the intermediate snapshot may reduce complexity and possibly improve performance. In a snapshot hierarchy that maintains children snapshots with deltas a to be deleted snapshot may be rolled back into the predecessor intermediate snapshot thus effectively deleting the intermediate snapshot. Of course the snapshot to be discarded is not necessarily an intermediate snapshot and may be the most recent snapshot. Those of ordinary skill in the art should appreciate the availability of methods for deleting a child snapshot.

In addition to the ability to snapshot virtual machine management may include functionality to clone and or migrate virtual machines running or offline . These functions are typically implemented using redo logs with copy on write semantics. With a network storage unit devoted to a single virtual machine redo logs have been discarded along with the independent intermediary file system. With access to built in facilities of a network storage system virtual machine management can leverage the network storage management operations to efficiently manage virtual machines e.g. clone migrate discovery replicate for disaster recovery etc. . The facilities are exposed to a virtualization system with an application programming interface for network storage management storage API and exposure of network storage units. The functionality that makes calls to the storage API may be implemented as part of a virtual machine manager that resides on a server communicatively coupled to virtualization system part of a virtual machine manager that resides on a virtualization system etc.

Instead of initiating a communication session with every virtualization system or accessing a centralized database of instantiated virtual machines a virtual machine management system can efficiently and automatically discover virtual machines by examining visible network storage units. If a virtual machine management system does not have visibility into storage units of a network storage system or has limited visibility and access then network storage units may be made visible by modifying the network storage mapping and masking configuration. For example a partition table on all visible network storage units may be examined. A unique partition type or other marker as discussed previously may be used to facilitate automatic detection of these network storage units allocated for virtual machines. Within the context of arrays existing array based masking and mapping primitives can be used to control access to virtual machines. Additionally locking capability may be included within a portion e.g. partition of a network storage unit to allow multiple virtualization systems to cooperate when accessing a single VM.

At block one of the virtual machine marked units is selected from the list. At block virtual machine data is loaded from the selected network storage unit. At block the loaded data is examined to identify any virtual secondary disk s and their corresponding network storage unit s . Any identified network storage units are associated with the selected unit and removed from the list. Of course embodiments may mark network storage units that encode virtual secondary disks or other non state data differently. Hence detect and scan operations such as those depicted in blocks and will yield a list of base network storage units. At block snapshot data and corresponding network storage units are identified. Any identified corresponding network storage units are associated with the selected unit and removed from the list. At block an indication of the selected unit and associated units is encoded. Control flows from block back to block . If there are no remaining unselected network storage units in the list then control flows to block . At block the encoding of discovered virtual machines is indicated e.g. presented via a user interface stored in a file etc. .

Instead of streaming pages of memory across an IP network network storage system facilities can be leveraged to implement local move and remote move of a virtual machine. Moving a virtual machine locally transitions virtual machine computations to a different virtualization system while maintaining data for the virtual machine in the same network storage unit. Moving a virtual machine remotely transitions virtual machine computations to a different virtualization system and relocates virtual machine data to a different network storage unit.

As previously stated the movement of a virtual machine may be across a local area network or a wide area network. For network storage systems that support wide area replication e.g. a SAN that supports wide array replication the supported facility can be employed to implement movement of a virtual machine across a wide area network.

Continuous data protection support may also be achieved by running the move command operations 24 7 without the final suspend resume. Periodic check points may be implemented to ensure consistency of the memory image. The periodic check points may occur coincident with the replica updates. At any point in time the virtual machine may be resumed at the destination virtualization system based on the last coherent replica at the destination network storage unit. A customer may be presented with tunable parameters to control how frequently these check points are performed where the maximum setting requires all pages of memory to be written out constantly. The customer is then presented with a tradeoff of performance versus the time delay and potential for data loss as a result.

When a user wishes to create a clone of a virtual machine a virtual machine manager will trigger a snapshot as described above and then request the snapshots to be unlinked. With the capabilities of a network storage system a virtual machine can be cloned expeditiously.

Although the embodiment depicted in illustrates paging out of state data a clone command may replicate disk data as well. In one embodiment the clone command forces the source virtual machine to be stopped or powered off and then copies the disk data of the virtual machine. In another embodiment the clone command causes the virtual machine to be temporarily disabled e.g. triggers a quiesce operation . A quiesce operation may be employed to preserve consistency in the file system of the operating system guest of the disabled virtual machine. Moreover these methods for effectuating a clone command may also be employed for effectuating a snapshot command.

In addition to providing facilities for implementing current virtual machine management commands implementation of virtual machines with network storage introduces additional management commands such as migrate. In general it may be necessary or desirable to move data e.g. a virtual disk of a virtual machine to a different storage pool using a migration facility.

The described embodiments may be provided as a computer program product or software that may include a machine readable medium having stored thereon instructions which may be used to program a computer system or other electronic devices to perform a process according to embodiments of the invention whether presently described or not since every conceivable variation is not enumerated herein. A machine readable medium includes any mechanism for storing or transmitting information in a form e.g. software processing application readable by a machine e.g. a computer . The machine readable medium may include but is not limited to magnetic storage medium e.g. floppy diskette optical storage medium e.g. CD ROM magneto optical storage medium read only memory ROM random access memory RAM erasable programmable memory e.g. EPROM and EEPROM flash memory or other types of medium suitable for storing electronic instructions. In addition embodiments may be embodied in an electrical optical acoustical or other form of propagated signal e.g. carrier waves infrared signals digital signals etc. or wireline wireless or other communications medium.

While the invention s is are described with reference to various implementations and exploitations it will be understood that these embodiments are illustrative and that the scope of the invention s is not limited to them. In addition while our description of virtualization techniques has generally assumed that the virtual machines present interfaces consistent with a hardware system persons of ordinary skill in the art will recognize that the techniques described may be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the present invention implemented as hosted embodiments non hosted embodiments or as embodiments that tend to blur distinctions between the two are all envisioned. Furthermore various operations may be wholly or partially implemented in hardware. For example a hardware implementation may employ a look up table for modification of storage access requests to secure non disk data.

Many variations modifications additions and improvements are possible regardless the degree of virtualization. For example while particular example operations were depicted for illustrative purposes various techniques for exposing a network storage system and various operations for performing moves migrations clones etc. of virtual machines will also be appreciated by persons of ordinary skill in the art. Furthermore while techniques and mechanisms have been described using particular network configurations hardware architectures memory organizations and particular operating system constructs typically IA 32 based architectures systems and Windows operations systems as a descriptive framework persons of ordinary skill in the art will recognize other implementations can be envisioned for use in systems that support other processor instruction set architectures other network or memory configurations and or other operating system constructs.

Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention s . In general structures and functionality presented as separate components in the exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the invention s .

