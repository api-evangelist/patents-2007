---

title: Low overhead access to shared on-chip hardware accelerator with memory-based interfaces
abstract: In one embodiment, a method is contemplated. Access to a hardware accelerator is requested by a user-privileged thread. Access to the hardware accelerator is granted to the user-privileged thread by a higher-privileged thread responsive to the requesting. One or more commands are communicated to the hardware accelerator by the user-privileged thread without intervention by higher-privileged threads and responsive to the grant of access. The one or more commands cause the hardware accelerator to perform one or more tasks. Computer readable media comprises instructions which, when executed, implement portions of the method are also contemplated in various embodiments, as is a hardware accelerator and a processor coupled to the hardware accelerator.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07809895&OS=07809895&RS=07809895
owner: Oracle America, Inc.
number: 07809895
owner_city: Redwood City
owner_country: US
publication_date: 20070309
---
Hardware accelerators are often included in processor based systems such as computer systems to perform specific predefined tasks in hardware rather than in software. Work is offloaded from the processors to the hardware accelerators permitting them to work in parallel on other tasks. Even if no other task is available for the processors to work on the higher performance of the hardware accelerator performing the defined tasks can still result in a performance increase. For example if the software execution of the task requires X cycles and the hardware accelerator execution of the task requires Y cycles where Y is less than X and often much less than X the performance gain is X Y without accounting for software overhead in dispatching the task to the accelerator . Additionally in some cases the hardware acceleration can be more power efficient than performing the same tasks in software. Power efficiency can be even greater if the hardware accelerators are incorporated on the same semiconductor substrate on chip as the processors. Particularly integrating hardware accelerators onto multi core chips such as chip multiprocessors CMP and or chip multithreaded CMT processors can be efficient because the accelerator can be shared among the cores threads.

Currently there is a large amount of software overhead associated with dispatching a task to a shared hardware accelerator e.g. on the order of tens of thousands of processor clock cycles . Access to the hardware accelerator is typically managed by the lowest level and most privileged layer of software in the system. Managing access in this fashion helps ensure that the hardware accelerator is shared in a secure fashion preventing one thread core from disrupting and particularly corrupting the task issued by another thread core to the hardware accelerator and also in a fair fashion so that various threads cores have the opportunity to take advantage of the hardware accelerator. The OS can implement the fairness and security in a non virtualized environment. In a virtualized environment the Hypervisor implements the fairness and security. Typically the overhead incurred is as follows the application transmits a task request to the operating system OS the OS copies the data to be processed from the user space to the kernel space the OS forwards the request to the Hypervisor the Hypervisor programs the accelerator to perform the task and awaits completion the Hypervisor passes the completion to the OS the OS copies the results from kernel space to user space and the OS informs the application that the task is complete. Accordingly much of the overhead is consumed in copying the data back and forth between user space and kernel space as well as the communications between the OS and the Hypervisor.

The size of the software overhead limits the usefulness of the hardware accelerator to those tasks for which the performance gain of using the hardware accelerator is in the tens of thousands of clock cycles or greater. Since the software overhead is experienced for every task issued to the hardware accelerator the size of each individual task must be large enough to compensate for the software overhead. Not all tasks meet these requirements. For example bulk encryption in web servers can be expected to be a large task overall which could greatly benefit from hardware acceleration. However each packet to be encrypted is relatively small and the cost of the software overhead would be prohibitive.

In one embodiment a method is contemplated. Access to a hardware accelerator is requested by a user privileged thread. Access to the hardware accelerator is granted to the user privileged thread by a higher privileged thread responsive to the requesting. One or more commands are communicated to the hardware accelerator by the user privileged thread without intervention by higher privileged threads and responsive to the grant of access. The one or more commands cause the hardware accelerator to perform one or more tasks.

In an embodiment a computer accessible medium stores a plurality of instructions which when executed as a user privileged thread request access to a hardware accelerator and communicate one or more commands to the hardware accelerator without intervention by higher privileged threads responsive to receiving a grant of access to the hardware accelerator. The one or more commands cause the hardware accelerator to perform one or more tasks.

In one embodiment a computer accessible medium stores a plurality of instructions which when executed as a higher privileged thread receive a request to access a hardware accelerator from a user privileged thread and grant access to the hardware accelerator to the user privileged thread responsive to the request. The grant includes data permitting the user privileged thread and the hardware accelerator to communicate without intervention by higher privileged threads.

While the invention is susceptible to various modifications and alternative forms specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood however that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.

The MT cores A B are each configured to execute instructions defined in an instruction set architecture implemented by the MT cores A B. That is the MT cores A B each comprise circuitry configured to execute instructions. As the name implies the MT cores A B may be multithreaded. That is the cores may include circuitry to support multiple active threads in the cores A B at the same time. The cores may select available instructions from different active threads for execution without the intervention of software. Generally a thread may comprise a sequence of instructions that is defined as an executable unit and that may be executed in parallel with other threads. Threads may have dependencies on each other e.g. they may communicate with each other through a defined mechanism such as memory semaphores or may be independent. Threads may be grouped to perform a defined function and the threads may be a process. One or more processes may form an application. Thus an application may comprise one or more threads. Similarly an OS or Hypervisor may comprise one or more processes each of which may comprise one or more threads.

The MT cores A B may implement any desired microarchitecture scalar superscalar pipelined speculative in order out of order etc. . While MT cores A B are used in this embodiment other embodiments may not implement a CMT and may include discrete processing circuitry or any other level of integration with other components. Generally the term processor may refer to any processing circuitry including MT cores and other embodiments. The MT cores will be used as an example herein but other embodiments may use other processor implementations.

The hardware accelerators A B may each comprise circuitry that implements a task or tasks in hardware. Generally the hardware accelerator A B may be controlled by software to perform a task on input data to produce output data that can be read by software. For example the hardware accelerator A B may be configured to accept commands generated by software that identify the desired task the source data and the result location. The tasks for which a given hardware accelerator is designed may vary from embodiment to embodiment across a wide variety of operations. For example embodiments are contemplated in which the hardware accelerators A B implement cryptographic tasks e.g. encryption or decryption extensible markup language XML processing tasks network processing tasks etc. If multiple hardware accelerators are provided different accelerators may implement different tasks and or two or more accelerators may implement the same task as desired.

The memory controller comprises the circuitry to interface to the memory . Various types of memory may be supported by the memory controller . For example static random access memory SRAM dynamic RAM DRAM synchronous DRAM SDRAM double data rate SDRAM DDR SDRAM DDR2 Rambus DRAM RDRAM etc. may be supported.

The I O interface circuit may bridge between the CMT and one or more I O interfaces. For example the I O interfaces may include the peripheral component interconnect PCI PCI Express PCIe HyperTransport Infiniband etc.

The MT cores A B the hardware accelerators A B the memory controller and the I O interface circuit may be coupled in any desired fashion. A shared bus may be used a packet interface may be used etc. A hierarchical indirect coupling may be used. In one embodiment the CMT may include a level 2 L2 cache comprising a plurality of banks and the MT cores A B may be coupled to the banks through a crossbar structure. The accelerators A B the memory controller and the I O interface circuit may be coupled to the L2 cache as well.

The CMT may comprise a single semiconductor substrate integrating the components shown in the CMT . Alternatively other levels of integration and or discrete circuitry may be used in other embodiments.

Turning now to a block diagram is shown illustrating various software that may execute in a system including the CMT shown in and which may interface to a hardware accelerator in the CMT shown in e.g. the hardware accelerator A for one embodiment. Specifically the software components shown include an application an accelerator library a guest operating system OS and a Hypervisor . Each software component may comprise instructions which when executed implement the operation described for that component. Furthermore each software component may comprise one or more processes each of which may comprise one or more threads.

Also shown in is a portion of the memory that is accessible to the application . The portion shown in may be allocated to the application by the guest OS and or the Hypervisor . The application may be executed with address translation enabled and thus may generate virtual addresses to access the memory. The virtual addresses are translated through the MT core s translation hardware to a physical address that actually identifies the memory locations in memory. The translations may be managed by the OS and or the Hypervisor and may be stored in translation tables in memory not shown in . The translations may be loaded into translation hardware such as a translation lookaside buffer or TLB in the MT cores for use during instruction execution. Generally the translation may be performed at a page level of granularity where a page may have any desired size e.g. 4 kilobytes KB 8 KB up to Megabytes or even larger aligned to a page boundary in the memory address space. The portion of memory allocated for use by the application may be referred to as user memory since threads executing with the user privilege level can access the memory.

Three privilege levels are illustrated in the user privilege level the OS privilege level and the Hyperprivilege level. The horizontal dotted lines between the privilege levels divide the software executing at each privilege level. Generally privilege levels may be supported by the MT cores and the more privileged levels e.g. OS privilege level and Hyperprivilege level may have access to more processor and system resources than the less privileged levels have. For example some control registers in the MT cores may be restricted to reading writing only by instructions executing at the higher privilege levels i.e. the more privileged levels . Some instructions may only be executed at certain privilege levels. The use of privilege levels may aid in the controlling software e.g. the OS and or the Hypervisor being able to retain control of application programs such as the application .

The accelerator library is illustrated as straddling the line between the user privilege level and the OS privilege level. The accelerator library may execute entirely in the user level or some threads may execute in the user level and some in the OS level in various embodiments. The accelerator library however may be provided with the OS so that it can be shared by different applications. In other embodiments the accelerator library may not be provided and the application may implement the operation of the accelerator library .

The accelerator library may provide an application programming interface API for the application to access the hardware accelerator A. The accelerator library may include at least one user privileged thread that manages the interface to the hardware accelerator A. Generally the accelerator library may request access to the hardware accelerator A from a higher privileged thread e.g. the Hypervisor in this embodiment . Once access is granted the accelerator library may communicate commands to perform tasks to the hardware accelerator A without intervention by the Hypervisor . Thus the software overhead of communicating with the Hypervisor may be incurred to obtain access to the hardware accelerator A but then may not be incurred for subsequent commands as long as the access is not revoked by the Hypervisor . Shared access to the hardware accelerator A may still be controlled by the Hypervisor which still has visibility to requests from other applications even in other guests using other guest OSs and thus fairness and security may be maintained. However the overhead on a per command basis may be reduced. Smaller tasks may be dispatched to the hardware accelerator A in some embodiments since the software overhead may be amortized across multiple tasks.

Specifically in this embodiment the accelerator library may queue commands to perform tasks from the application for the accelerator A in a command word queue CWQ in the user privileged memory. The accelerator A may be configured to read commands from the CWQ and to write status to the CWQ . Thus the command write status read by the accelerator library need not traverse the layers of privileged software to reach privileged memory or the hardware accelerator A itself.

As illustrated in the application may request access to the hardware accelerator by making an API call to the accelerator library as illustrated by the Request Access arrow from the application to the accelerator library . The accelerator library may pass the access request to the Hypervisor as illustrated by the Request Access arrow from the accelerator library to the Hypervisor . In some embodiments the request may pass through the guest OS as well.

If the Hypervisor grants the request the Hypervisor may return the virtual address of the CWQ as well as the virtual address es of one or more data pages used to communicate the data to the hardware accelerator A to the accelerator library . In one embodiment the accelerator library and or the application may request certain virtual addresses for the CWQ and or the data pages . Particularly requesting the virtual addresses for the data pages may permit the input data for the task to be placed in the data pages before making the request. The accelerator library may provide the virtual addresses of the data pages to the application .

Additionally in response to granting the request the Hypervisor may provide an identifier of the requesting process e.g. thread identifier process identifier etc. to the accelerator A so that the accelerator A may determine that a command is sourced by a permitted thread. The Hypervisor may further provide physical page addresses of the CWQ and the data pages to the accelerator A so the accelerator A may locate the CWQ and the data pages in the user memory. In one embodiment the CWQ and the data pages may be pinned in the memory since the accelerator A effectively is caching the result of translating the virtual addresses to the CWQ and the data pages . In the illustrated embodiment the accelerator A may comprise control registers A C to store CWQ control data including the physical address of the CWQ head and tail pointers etc. page addresses of the data pages and the identifier of the granted thread.

In this embodiment the Hypervisor may also manage a permission word e.g. reference numeral in that indicates whether or not permission to access the accelerator A is granted to the application . The permission word may be checked periodically to ensure that the application still has access to the accelerator A.

The application may write data to the data pages and when data is ready for a task the application may write a command to the accelerator A via a call to the API of the accelerator library CWQ R W from the application to the accelerator library in . The accelerator library may write the command into a command field Cmd in the next available entry of the CWQ CWQ R W from the accelerator library to the CWQ in and may communicate the update to the accelerator A CWQ updates arrow in . For example the accelerator library may increment the tail pointer to the CWQ or may simply indicate that a command has been written. The accelerator A may read the command from the CWQ process the data from the data pages write results to the data pages and update a status field in the CWQ . Cmd R Stat W and Data R W in . The accelerator library may poll the status field to determine when the command is complete. It is noted that while the accelerator library accesses the accelerator A on behalf of the application in this embodiment other embodiments are contemplated in which the application is modified to access the accelerator A directly.

If the Hypervisor revokes the access by the application to the accelerator A e.g. to service another request from a different application not shown or for other reasons the Hypervisor may update the permission word and indicate the revocation to the accelerator A as well dotted arrows in . The accelerator A may update the ID in the register C so that commands from the revoked application are no longer accepted. In one embodiment the Hypervisor may also update the status field in the CWQ if a command is outstanding in the accelerator A when the revocation occurs.

The accelerator library may also include software emulation of the hardware accelerator in case the accelerator is not included in the system is disabled or is otherwise unavailable to the application .

The guest OS may provide OS functionality in the system. In this embodiment the guest OS is executing in a virtualized environment provided by the Hypervisor and there may be other guest OSs as well. In other embodiments virtualization may not be implemented and the Hypervisor may not be provided. In such cases the functionality of managing shared access to the accelerator A described for the Hypervisor herein may be implemented by the OS . Thus in general access to the accelerator A may be managed by a higher privileged thread than the user privileged thread that requests access and generates commands to the accelerator A.

In some embodiments a CWQ may be provided in more than one user privileged address space at the same time e.g. for different applications . The Hypervisor or OS may manage the granting of the accelerator A among the different threads and commands from the CWQs may be processed as the accelerator A is granted to the corresponding user threads. That is the Hypervisor or OS may grant the accelerator A to the various user threads in any arbitrary order according to any desired scheme for arbitrating among the user threads.

Turning now to a flowchart is shown illustrating operation of one embodiment of the Hypervisor or the OS if the Hypervisor is not included in response to a user privileged request for access to the accelerator A e.g. from the accelerator library in the embodiment of . While the blocks are shown in a particular order for ease of understanding other orders may be used. The Hypervisor may comprise instructions which when executed implement the operation illustrated by the flowchart of .

The Hypervisor may determine if the accelerator A is busy that is currently granted to another user privileged thread . If the accelerator A is not busy decision block no leg the Hypervisor may update the permission word in the memory assigned to the requesting thread to indicate that the thread is permitted to access the accelerator A block . The Hypervisor may provide the ID for the granted thread and the physical page addresses of the CWQ and the data pages to the accelerator A block . The Hypervisor may return the virtual page addresses mapping to the physical page addresses to the requesting thread block . An indication that the request was successful may also be returned.

If the accelerator A is busy decision block yes leg the Hypervisor may determine if the access by the other thread is to be revoked decision block . Various factors may be considered in the decision. For example balancing access to the accelerator A among the threads may be considered. Additionally the length of time that the thread has been granted access to the accelerator A may be considered. It may be desirable to ensure that access is granted for at least a minimum period of time to overcome the software overhead incurred to gain access. If access is not revoked decision block no leg the Hypervisor may fail the access request block . For example an indication that the request was unsuccessful may be returned to the requestor. If the accelerator library transmitted the request and has emulation functionality the accelerator library may emulate operation of the accelerator A so that the application makes progress even though the accelerator A is busy serving another user privileged thread. On the other hand if access is revoked decision block yes leg the Hypervisor may update the permission word in the revoked thread s memory so that the revoked thread may detect the revocation block . If the revoked thread had a command outstanding the status field in the CWQ may also be updated to indicate that the command was not completed due to revocation of access. The Hypervisor may also update the permission word and provide the addresses and IDs for the newly granted thread blocks and .

Turning now to a flowchart is shown illustrating operation of one embodiment of the application and or the accelerator library to invoke the accelerator A to perform a task. While the blocks are shown in a particular order for ease of understanding other orders may be used. The application and or accelerator library may comprise instructions which when executed implement the operation illustrated by the flowchart of .

The application may load the source data on which the task is to be performed by the accelerator A into the data pages block . In some cases the source data may already be stored in the data pages e.g. by intelligent placement of the data requesting that the virtual addresses at which the data is stored be used for the data pages etc. . Accordingly block may not be needed in some cases.

The application may transmit a command to the accelerator library . The accelerator library may determine if the application still has permission to access the accelerator A e.g. by checking the permission word . If permission is still granted decision block yes leg the accelerator library may obtain the current tail pointer of the CWQ from the accelerator A block . Alternatively the accelerator library may retain a local copy of the tail pointer e.g. in the memory . The accelerator library may write the command to the CWQ at the entry indicated by the tail pointer block and may inform the accelerator A that a new command is available for processing. For example the accelerator library may increment the tail pointer maintained by the accelerator A. The accelerator library may poll the status field of the entry in the CWQ until a status update is detected block and decision block no leg . Once the status update is detected decision block yes leg the accelerator library may process the status to determine if the task completed successful or an error is detected. If an error is not detected decision block no leg the accelerator library may inform the application that the operation is completed. In some embodiments the application may read the task results from the data pages block . If an error is detected decision block yes leg the accelerator library may perform error processing dependent on the detected error block . For example if the error is that the task was terminated because access was revoked the accelerator library may perform the task in software. If the error was detection of an event that the accelerator A is not designed to handle again the data may be processed in software. Other errors may be handled by the application in an application dependent fashion.

If permission has been revoked decision block no leg the accelerator library may request access to the accelerator A again perform the task in software request access and perform the task in software if access is not granted or even indicate that permission has been revoked to the application which may take action as appropriate block .

Turning now to a flowchart is shown illustrating operation of one embodiment of the accelerator A. While the blocks are shown in a particular order for ease of understanding blocks may be performed in other orders. Blocks may be performed in parallel in combinatorial logic circuitry in the accelerator A. Blocks combinations of blocks and or the flowchart as a whole may be pipelined over multiple clock cycles. In some embodiments the accelerator A may be powered down if not in use to conserve power. In such embodiments the accelerator A may be woken up powered up and initialized for use before performing the operation shown in .

If a new command has not be written to the CWQ the accelerator A may be idle decision block no leg . If a new command has been written to the CWQ decision block yes leg the accelerator A may determine if the thread that transmitted the command is authorized to access the accelerator A decision block e.g. by comparing the ID provided by the Hypervisor to corresponding identification data in the CWQ or in the communication indicating an update to the CWQ . If the command is not from an authorized source decision block no leg the accelerator A may inform the Hypervisor block and abort the command block . The mechanism for informing the Hypervisor is implementation dependent e.g. by comparing the ID provided by the Hypervisor to the ID associated with the communication indicating an update to the CWQ .

If the command is from an authorized source decision block yes leg the accelerator A may load the command from the CWQ and update the head pointer block . The command may include pointers to the data to be processed in the data pages and to the result area in the data pages to which the results are to be written. The pointers may be offsets from a base address and the accelerator A may thus be able to determine the physical addresses within the data pages . Alternatively the pointers may be virtual addresses that the accelerator A maps to physical addresses provided when access was granted.

The accelerator A may execute the command reading the source data and writing the result data block . If an error is detected during the execution of the command decision block yes leg the accelerator A may write the error code to the status field in the CWQ block . If no error is detected a good status may be written to the status field block . In some embodiments operate in place may be supported in which the source and result pages are the same physical pages and the result overwrites the source.

It is noted that in one embodiment threads within a particular application may share the same CWQ if desired. In such embodiments the accelerator library may ensure that one thread accesses the CWQ at any given point in time e.g. using mutually exclusive locks semaphores etc. .

Turning now to a block diagram of another embodiment of the MT core A and the accelerator A is shown. In this embodiment the MT core A includes execution circuitry coupled to a TLB . The accelerator A includes a CWQ a permission word register a physical address buffer and accelerator execution circuitry . The CWQ the permission word register the physical address buffer and the accelerator execution circuitry are coupled.

In the embodiment of the TLB may be configured to translate the data operand of a store instruction in addition to its address for certain types of stores. Specifically the store type used to write commands to the CWQ may cause the TLB to translate the data operand of the store. The data operand may be a pointer to source data and or a result location and thus the pointer is a virtual address for the user privileged thread that executes the store. By using the TLB to translate the pointer the MT core A may provide the physical address of the pointer to the accelerator A. Accordingly pages need not be preallocated and pinned for the data to be transmitted back and forth between the accelerator A and the application . Rather any addresses for which valid translations exist may be used. Moving data in and out of the pinned pages may be avoided in some embodiments. In other embodiments the data pages may still be pinned if desired.

The execution circuitry may generally include the circuitry to fetch and execute instructions in the instruction set architecture implemented by the MT core A. For store instructions the execution circuitry may generate a store virtual address for the target memory location of the store St VA in . The store instruction may include one or more address operands which may be added to produce the store virtual address. Additionally the execution circuitry may transmit the store type St Type in to the TLB so that the TLB may determine if the data is to be translated for this embodiment. Additionally the execution circuitry transmit the store data operation St Data in .

The TLB stores translation mappings from virtual addresses to physical addresses. The TLB may have any construction and capacity. Particularly the TLB may provide both the store virtual address and the store data translation in a variety of ways. For example the TLB may include two or more address ports and may use one port to translate the store virtual address and another port to translate the store data. Alternatively the store virtual address may be translated first on a port and then the store data may be translated in the following clock cycle using the same port. In one implementation that uses a single port the TLB and or related hardware in the execution circuitry may be configured to prevent and or resolve conflicts for access to the TLB port. Alternatively software may schedule stores so that conflicts in accessing the TLB do not occur or the latency of the accelerator may ensure that no conflicts can occur.

In response to the store virtual address the TLB may detect a TLB hit and output the store physical address St PA in . In response to the store type and the store data the TLB may detect a TLB hit and output a data physical address St DPA . An identifier indicating the thread that initiated the store may also be supplied. For example the identifier may be the thread ID and or the process ID in one embodiment. In one embodiment the identifier may further include an OS identifier identifying which guest OS is associated with the thread. In addition to providing the store physical address to the accelerator A the TLB may provide the store physical address back to the execution circuitry e.g. for cache access memory access etc. 

If either the store VA or the store data if translated is a TLB miss a hardware tablewalker may be invoked to search the software managed translation tables for a translation. If a translation is found it may be loaded into the TLB and the store instruction may be reexecuted. If a translation is not found an exception may occur. In other embodiments a hardware table walker is not included and an exception may occur in response to the TLB miss to load the desired translation into the TLB . In some embodiments a TLB miss on the store data may not cause the TLB to be loaded in response to the miss or an exception to be taken to service the miss. Rather the store data may be supplied unmodified to the target and may be marked as untranslated. If the accelerator A is to treat the store data as an address and it is marked as untranslated the accelerator A may signal an error for the task and software may then take corrective action. Such an embodiment may be used e.g. if some stores to the accelerator A provide pointers and thus need translation and other stores to the accelerator A provide control data that does not need to be translated.

The store type may indicate whether or not the store is targeted at the accelerator A and thus may be used to determine if the data operand is to be translated by the TLB . The store type may be defined in a variety of fashions. In one embodiment the SPARC instruction set architecture may be implemented and the store type may comprise the address space identifier ASI of the store. Other embodiments may use different instruction encodings e.g. opcodes to define store types. Still other embodiments may use a virtual address range of the store virtual address to determine the store type e.g. the virtual address range mapped to the accelerator A may be defined to be one store type and other ranges may be defined to be one or more other store types . In the case of the address range the range may be programmable or may be fixed. In the case of the address range the execution circuitry may decode the store virtual address to detect the store type or the TLB may detect it. In still other embodiments a range of the physical addresses may be used and the store type detection may occur after the translation by the TLB . The TLB may also provide translations for load instructions not shown in .

The store physical address and the store data physical address may be conveyed to the accelerator A in the form of a normal store operation e.g. the store physical address may be conveyed as the address of the store operation and the store data physical address may be conveyed as the store data . Based on the target of the store operation the accelerator A may be able to determine that the store data is an address. For example if the store writes a command or part of a command into the CWQ the accelerator A may determine that the store data is a pointer to source data control state or target location depending on its location in the CWQ .

The CWQ may be similar to the CWQ shown in except that the CWQ is implemented in the accelerator A in one embodiment. Thus the accelerator A may control the head and tail pointers and the MT core A may write a set of addresses to write the tail entry in the queue. The permission word register may store a permission word similar to the permission word in . The accelerator library may check the permission word and the Hypervisor may write the permission word similar to the above description. The accelerator A may check the ID provided with a store against the permission word for stores that write into the CWQ . The accelerator execution circuitry may perform the checking and may read the commands for the CWQ and perform the identified tasks.

Since the accelerator A is supplied with translated pointers through the TLB the accelerator A may also need to be informed when a TLB invalidation or shootdown is being performed. The TLB invalidations are typically performed to invalidate translation data that has been deleted from the translation tables e.g. by the OS or the Hypervisor to reclaim the page for other uses. The execution circuitry may execute an instruction to invalidate a TLB entry or entries and the TLB invalidate may be transmitted to the TLB . The TLB may forward the TLB invalidate to the accelerator A which may return an acknowledgement Ack in when the accelerator A has processed the invalidation. The TLB may delay completion of the TLB invalidate until the acknowledgement is received from the accelerator A.

The TLB invalidate may occur at any level of granularity. For example the invalidate may be for a single page in which case the TLB may transmit the physical address of the page being invalidated to the accelerator A. Alternatively a TLB invalidate may invalidate all of the TLB in which case the TLB may signal that a TLB invalidate is occurring and the accelerator A may invalidate all pages. If TLB invalidations are deemed infrequent enough the TLB may signal a TLB invalidation and the accelerator A may invalidate all pages in response to any TLB invalidate.

The physical address buffer may store physical addresses that the accelerator A currently has access to e.g. as part of a current task or as part of a task queued in the CWQ . The physical address buffer may be used to detect that a TLB invalidate affects one or more tasks that have been dispatched to the accelerator A. If a physical address is invalidated in the physical address buffer the corresponding task or tasks that access data at the physical address are affected. The accelerator execution circuitry may abort a task that is pending in the CWQ updating that status to indicate abort so that software may determine why the task is not performed . If the task is in flight it may be selectively aborted. For example if the task is in place overwriting source data with result data the task may not be aborted because it may be difficult to recover the input data and complete the task.

If the TLB invalidate does not affect a page used by a command that is in progress decision block no leg then the TLB invalidate may affect a command that is pending in the CWQ . If so decision block yes leg the accelerator A may abort the pending command block and update the status of the aborted entry to indicate that the command was aborted block . If not decision block no leg then no update is needed in response to the TLB invalidate. In either case the accelerator A may delete the cached page address es affected by the TLB invalidate from the physical address buffer block and may acknowledge the TLB invalidate block . The accelerator A may handle the aborted commands in any fashion. For example the aborted commands may be skipped without consuming accelerator resources subsequent to the skip i.e. temporarily consumed resources may be freed .

If the TLB invalidate affects a page used by a command that is in progress decision block yes leg the accelerator A may determine if the in progress command is abortable decision block . If the in progress command is abortable decision block yes leg the accelerator A may abort the command block and may update the status in the CWQ to indicate that the command was aborted block . If the in progress command is not abortable decision block no leg the accelerator A may wait for the command to complete block and update the status in the CWQ to indicate that the command completed successfully block . The accelerator A may delete the cached page address es affected by the TLB invalidate from the physical address buffer block and may acknowledge the TLB invalidate block .

As mentioned above a command that performs in place processing may not be abortable in one embodiment. In other embodiments there may be other reasons that a command is not abortable. For example if the command is almost completed it may be a better performance choice to complete the processing rather than abort. If aborting would leave sensitive information e.g. a password an encryption key etc. available unprotected in memory the command may not be aborted until the sensitive information is no longer available. Any set of one or more non abortable conditions may be implemented in various embodiments.

Turning now to a block diagram of a computer accessible medium is shown. Generally speaking a computer accessible medium may include any media accessible by a computer during use to provide instructions and or data to the computer. For example a computer accessible medium may include storage media such as magnetic or optical media e.g. disk fixed or removable tape CD ROM or DVD ROM CD R CD RW DVD R DVD RW volatile or non volatile memory media such as RAM e.g. synchronous dynamic RAM SDRAM Rambus DRAM RDRAM static RAM SRAM etc. ROM Flash memory non volatile memory e.g. Flash memory accessible via a peripheral interface such as the Universal Serial Bus USB interface etc. microelectromechanical systems MEMS as well as media accessible via transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link. The computer accessible medium in may store one or more of the application the accelerator library the guest OS and or the Hypervisor . Generally the computer accessible medium may store any set of instructions which when executed implement a portion or all of the flowcharts shown in one or more of and .

Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.

