---

title: Deadlock avoidance skid buffer
abstract: Under some conditions, requests transmitted between different devices in a computing system may be blocked in a way that prevents the request from being processed, resulting in a deadlock condition. A skid buffer is used to allow additional requests to be queued in order to remove the blockage and end the deadlock condition. Once the deadlock condition is removed, the requests are processed and the additional buffer entries in the skid buffer are disabled.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08726283&OS=08726283&RS=08726283
owner: NVIDIA Corporation
number: 08726283
owner_city: Santa Clara
owner_country: US
publication_date: 20070604
---
The present invention generally relates to avoid deadlock conditions between devices in a computing system and more specifically to removing a deadlock condition caused by queued requests that are transferred between devices in the computing system.

Current processing systems use conventional interfaces such as PCI Express and HyperTransport to communicate between devices within the system. These protocols specify rules for transmitting requests between the different devices in order to prevent deadlock conditions. In some cases one or more of the devices in the system do not adhere to the rules and requests between different devices are blocked in a way that prevents the request from being processed resulting in a deadlock condition.

Accordingly what is needed in the art is a system and method for ending the deadlock condition to allow request processing to resume.

Under some conditions requests transmitted between different devices in a computing system may be blocked in a way that prevents the request from being processed resulting in a deadlock condition. A skid buffer is used to allow additional requests to be queued in order to remove the blockage and end the deadlock condition. Once the deadlock condition is removed the requests are processed and the additional buffer entries in the skid buffer are disabled.

Various embodiments of a method of the invention for avoiding deadlock during request processing include determining a deadlock condition exists that prevents processing of queued requests received from a processing unit enabling additional entries to accept additional requests and remove the deadlock condition processing the queued requests and disabling the additional entries after the additional requests are output and the additional entries are empty.

Various embodiments of the invention include a system for avoiding deadlock during request processing. The system includes a processing unit configured to output requests to a skid buffer. The skid buffer includes an incoming request FIFO skid FIFO entries and a deadlock detection unit. The deadlock detection unit is configured to determine a deadlock condition exists that prevents queued requests received from the processing unit that are stored in the incoming request FIFO from being output enable the skid FIFO entries to accept additional requests and remove the deadlock condition and disable the skid FIFO entries after the additional requests are output from the skid FIFO entries to the incoming request FIFO and the skid FIFO entries are empty.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

A processing subsystem is coupled to memory bridge via a bus or other communication path e.g. a PCI Express Accelerated Graphics Port or HyperTransport link in one embodiment processing subsystem is a graphics subsystem that delivers pixels to a display device e.g. a conventional CRT or LCD based monitor . Processing subsystem may be implemented using one or more integrated circuit devices such as programmable processors application specific integrated circuits ASICs and memory devices. Data and program instructions for execution by processing subsystem may be stored in system memory or memory within other devices of system . In embodiments of the present invention where processing subsystem is a graphics subsystem the instructions may be specified by an application programming interface API which may be a conventional graphics API such as Direct3D or OpenGL.

A system disk is also connected to I O bridge . A switch provides connections between I O bridge and other components such as a network adapter and various add in cards and . Other components not explicitly shown including USB or other port connections CD drives DVD drives film recording devices and the like may also be connected to I O bridge . Communication paths interconnecting the various components in may be implemented using any suitable protocols such as PCI Peripheral Component Interconnect PCI Express PCI E AGP Accelerated Graphics Port HyperTransport or any other bus or point to point communication protocol s and connections between different devices may use different protocols as is known in the art.

Processing subsystem can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying laws of physics to determine position velocity and other attributes of objects image rendering operations e.g. vertex shader geometry shader and or pixel shader programs and so on. Processing subsystem may transfer data from system memory into internal on chip memory process the data and write result data back to system memory where such data can be accessed by other system components including e.g. CPU or another processing subsystem .

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip. The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

The connection of processing subsystem to the rest of system may also be varied. In some embodiments processing subsystem is implemented as an add in card that can be inserted into an expansion slot of system . In other embodiments a processing subsystem is integrated on a single chip with a bus bridge such as memory bridge or I O bridge . In still other embodiments some or all elements of processing subsystem are integrated on a single chip with CPU .

A processing subsystem may include any amount of local memory including no local memory and may use local memory and system memory in any combination. For instance processing subsystem can be a graphics processor in a unified memory architecture UMA embodiment in such embodiments little or no dedicated graphics local memory is provided and processing subsystem uses system memory exclusively or almost exclusively. In UMA embodiments processing subsystem may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high speed link e.g. PCI E connecting processing subsystem to system memory e.g. via a bridge chip.

Furthermore any number of processing subsystem can be included in system to create a parallel processing subsystem. For instance multiple processing subsystem can be provided on a single add in card or multiple add in cards can be connected to communication path or one or more of the processing subsystems can be integrated into a bridge chip. The processing subsystems in a parallel processing subsystem may be identical to or different from each other for instance different processing subsystems may have different numbers of cores different amounts of local memory and so on. Where multiple processing subsystems are present they can be operated in parallel to process data at higher throughput than is possible with a single processing subsystem . Systems incorporating one or more processing subsystems may be implemented in a variety of configurations and form factors including desktop laptop or handheld personal computers servers workstations game consoles embedded systems and so on.

Under some conditions requests transmitted via communication paths between different devices in a system may be blocked in a way that prevents the request from being processed resulting in a deadlock condition. When deadlock occurs no requests are processed and one or more devices in system are unable to continue processing data. A skid buffer is used to allow additional requests to be queued by enabling additional buffer entries in order to end the deadlock condition. Once the deadlock condition is removed the requests are processed and the additional buffer entries in the skid buffer are disabled.

Since the organization of addresses is not known to devices other than processing subsystem any requests to read or write PS surface received by memory bridge from CPU are reflected to processing subsystem before being output to system memory . The reflection operation translates the request address as needed to properly access PS surface . The reflection operation can cause a deadlock condition to occur when the CPU is attempting to output an EOI end of interrupt transaction to a device via memory bridge .

CPU includes an output queue that stores outgoing requests including EOIs for output by CPU . In particular write requests from CPU to system memory are output to memory bridge . Any write requests to PS surface are reflected to processing subsystem via write request . Processing subsystem includes a skid buffer and an upstream write queue . Incoming write requests including reflected requests are stored in skid buffer as described in conjunction with for processing by processing subsystem . Reflected requests are output back to memory bridge via upstream write queue with the address translation information that is needed to complete the request. In addition to the reflected requests processing subsystem produces snoop requests when the request being output may be cached by CPU . The snoop request output by memory bridge to CPU is snoop request and it ensures that any data cached by CPU is flushed to system memory . Under certain conditions a snoop request from a conventional processing unit may be blocked at the input to CPU and if the conventional processing unit blocks inputs from CPU deadlock can occur as described in conjunction with . By using skid buffer in processing subsystem the deadlock condition may be removed as described in conjunction with .

In step CPU determines if an EOI is ready for output by CPU to memory bridge and if not the method proceeds to step . Otherwise in step CPU determines if the output interface to memory bridge is stalled preventing CPU from outputting the EOI. If in step CPU determines that the output is not stalled then in step CPU outputs the EOI to memory bridge . In step CPU receives the snoop request from memory bridge via snoop request and processes the snoop request. The snoop request returns an acknowledgement to memory bridge that indicates that the request can proceed.

When the snoop request address is not in a cache within CPU then the acknowledgement is output quickly. When the snoop request address is in a cache but has not been modified the acknowledgement is also output quickly. When the snoop request indicates that the request is a write the cache line matching the address is invalidated since the content will be stale and the acknowledgement is output. When the snoop request address is cached in CPU and is dirty i.e. has been modified CPU first flushes the cache line to write the data back to system memory and then outputs the acknowledgement.

In step memory bridge completes the request to access PS surface . If in step CPU determines that the output is stalled then CPU repeats step until the output is no longer stalled. Under some circumstances the output remains stalled and a deadlock condition exists for the output of CPU and the output of processing subsystem . In particular when CPU has an EOI at the output and is stalled CPU cannot write a cache line to system memory and acknowledge a snoop request via snoop request when the snoop request address is cached and dirty. If processing subsystem has output a snoop request to CPU for an address is cached and dirty the system is deadlocked and skid buffer functions to eliminate the stall at the output of CPU to remove the deadlock condition.

Under normal conditions i.e. when a deadlock condition does not exist new requests received at inbound request are pushed directly into inbound request FIFO and are not stored in skid FIFO entries . When a deadlock condition is detected by deadlock detection unit skid FIFO entries are enabled and new requests received at inbound request are pushed into skid FIFO entries and pushed into inbound request FIFO as they are popped from skid FIFO entries . Deadlock detection unit determines that a deadlock condition exists when a valid inbound request is provided at inbound request pop is negated and inbound request FIFO is full causing skid buffer full to be asserted for a predetermined number of cycles as described in conjunction with . Once the requests stored in skid FIFO entries are all pushed into inbound request FIFO deadlock detection unit disabled skid FIFO entries and normal operation resumes.

As shown in skid buffer is positioned in processing subsystem . In other embodiments of the present invention skid buffer is positioned in memory bridge or any other device along the request path between CPU and processing subsystem . is a block diagram of a memory bridge that includes skid buffer in accordance with one or more aspects of the present invention. Memory bridge receives requests and EOI transactions from output queue of CPU . Skid buffer enables skid FIFO entries when a deadlock condition is detected and negates the stall output from memory bridge to CPU to accept new inputs from output queue . When skid buffer is positioned in memory bridge it is not necessary to also include a skid buffer in processing subsystem . Therefore processing subsystem can instead use a conventional input queue such as a FIFO instead of skid buffer .

If in step deadlock detection unit determines that skid FIFO entries are full then in step deadlock detection unit asserts skid buffer full and rejects inbound request for storage in skid FIFO entries . Skid FIFO entries should be a number of entries that is sufficient to end a deadlock condition and allow a previously blocked inbound request to be accepted by skid buffer . Returning to step if skid buffer determines that the additional entries are not enabled then in step deadlock detection unit determines if inbound request FIFO is full. If in step inbound request FIFO is not full then in step deadlock detection unit negates skid buffer full and accepts inbound request for storage in inbound request FIFO . When skid FIFO entries are not enabled inbound request passes through skid FIFO entries or bypasses skid FIFO entries and is stored directly in inbound request FIFO . If in step inbound request FIFO is full then in step deadlock detection unit asserts skid buffer full and rejects inbound request for storage in inbound request FIFO .

In step deadlock detection unit determines if inbound request FIFO is stalled i.e. if pop is negated blocking request from being output. If in step deadlock detection unit determines that inbound request FIFO is not stalled then in step deadlock detection unit clears the deadlock counter. If in step deadlock detection unit determines that inbound request FIFO is stalled then in step deadlock detection unit determines the deadlock counter has reached a limit meaning that skid buffer has not accepted an inbound request since inbound request FIFO is full and has been stalled for a predetermined number of cycles indicating that a deadlock condition may exist. The limit may be a programmed value or a static value. In some embodiments of the present invention different counters are used to monitor different possible deadlock conditions and in step deadlock detection unit determines if any one of the deadlock counters has reached a corresponding limit.

If in step deadlock detection unit determines that the deadlock counter has not reached the limit then in step deadlock detection unit increments the deadlock counter. Otherwise in step deadlock detection unit enables skid FIFO entries in order to remove the deadlock condition. The additional entries provided by skid FIFO entries are enabled until deadlock detection unit disables those entries as described in conjunction with . In step deadlock detection unit clears the deadlock counter.

When a processing unit initiates peer to peer communications with processing subsystem a deadlock condition may result. Therefore skid buffer is also used to remove that deadlock condition and allow CPU processing unit and processing subsystem to process peer to peer requests. In some embodiments of the present invention processing unit is a video engine that produces write requests for PS surface . The write requests for PS surface are reflected by CPU to processing subsystem for address translation. When a snoop request from processing subsystem is blocked by CPU and processing subsystem blocks additional requests from output queue a deadlock condition results. Processing subsystem may block requests from output queue when upstream write queue and the inbound request FIFO within skid buffer is full. When the deadlock condition results the additional entries provided by the skid FIFO entries within skid buffer are enabled and the deadlock condition is removed.

The invention has been described above with reference to specific embodiments. Persons skilled in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

