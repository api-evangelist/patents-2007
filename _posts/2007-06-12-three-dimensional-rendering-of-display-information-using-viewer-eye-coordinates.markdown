---

title: Three dimensional rendering of display information using viewer eye coordinates
abstract: Game data is rendered in three dimensions in the GPU of a game console. A left camera view and a right camera view are generated from a single camera view. The left and right camera positions are derived as an offset from a default camera. The focal distance of the left and right cameras is infinity. A game developer does not have to encode dual images into a specific hardware format. When a viewer sees the two slightly offset images, the user's brain combines the two offset images into a single 3D image to give the illusion that objects either pop out from or recede into the display screen. In another embodiment, individual, private video is rendered, on a single display screen, for different viewers. Rather than rendering two similar offset images, two completely different images are rendered allowing each player to view only one of the images.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07884823&OS=07884823&RS=07884823
owner: Microsoft Corporation
number: 07884823
owner_city: Redmond
owner_country: US
publication_date: 20070612
---
The technical field relates generally to computer processing and more specifically to rendering display information in three dimensions.

Often touted as the holy grail of gaming three dimensional 3D gaming has not yet reached the commercial success desired by many game developers and suppliers. There are several problems associated with 3D gaming. 3D displays for the home market are not readily available. Typical displays are single purpose in that a display is configured either for two dimensional 2D rendering or 3D rendering but not both. Displays configured to render in both 2D and 3D are typically prohibitively expensive. Further very little 3D game content exists. Content producers typically do not want to invest in a new technology until the technology is proven and consumers typically do not want to invest in the technology if there is limited content available. Additionally true 3D content requires multiple cameras to film objects from different viewpoints.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description Of Illustrative Embodiments. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Three dimensional 3D content for video games and related applications is rendered on a console utilizing existing hardware technology. No new display technology need be developed. Software game developers can incorporate 3D capability into their video games with minimal work. 3D capabilities also are utilized to provide private multiple views wherein one viewer can not see another viewer s content.

In an example configuration video games are rendered using multiple camera views. The multiple views are generated within the rendering engine. Thus software game developers are not required to encode dual images into a specific hardware format. Content from a single default camera perspective is provided to the engine and first and second camera viewpoints are generated therefrom. The first and second camera viewpoints are slightly offset from the default camera viewpoint. When a viewer sees the two slightly offset images the user s brain combines the two offset images into a single 3D image to give the illusion that objects either pop out from or recede into the display screen. In an example embodiment two camera views a right camera view and a left camera view are generated from a default camera view. The left camera view is generated by subtracting an offset from the default camera view and the right camera view is generated by adding an offset to the default camera view. The camera view perceived by a viewer is between the left and right camera views.

In another example configuration individual private video is rendered on a single display screen for different viewers. Rather than rendering two similar offset images for a single 3D image two completely different images are rendered allowing each player to view only one of two 2D two dimensional images. Thus viewers can view a full screen display rather than a split screen display. This technique can be expanded to accommodate any number of viewers. Utilizing this technique multiple players can view various aspects of a common game and or completely different games. In various implementations the individual video is temporally multiplexed spatially multiplexed or a combination thereof.

In an example embodiment three dimensional 3D rendering of content such as video game content is performed by a graphics processing unit GPU rather than a central processing unit CPU . This allows a software game developer to easily generate software that can be rendered in 3D. In the case of game applications the 3D rendering is performed on the game console such as for example an XBOX game console. The software game developer is not required to generate software specifically designed for 3D rendering. The developer can generate software that calls an API application programming interface that performs the transformation into 3D renderable display information thus enabling a stereoscopic render state. The software game developer is insulated from the task of rendering two separate views encoding dual images into a specific hardware format and rendering this view to the display s encoding format. The software game is insulated from the addition of the second camera the calibration of that camera the generation of two scenes and the encoding format for the display. However the software game developer is provided control over which scenes are rendered in 3D and which are not. Thus the software game developer can turn on a 3D mode state of the like providing an indication that content is to be rendered in 3D on a frame by frame basis.

In example embodiments utilization of the 3D rendering technique described herein can provide rendering of an entire game or application in 3D or rendering of selected portions of a game or application in 3D. Further utilization of the herein described 3D rendering technique allows multiple viewers to view different rendered display information wherein one viewer can not view another viewer s display information. The 3D rendering technique can be utilized with any appropriate display device such as a monitor a television or the like. In an example embodiment the 3D rendering technique is utilized with goggles or the like to provide temporal multiplexing.

Three dimensional 3D displays incorporate two images a left image and a right image corresponding to each eye of a viewer. A game also referred to as a title produces a single image comprising the two separate images. To generate a 3D image a scene from the two cameras left L and right R is rendered. In an example embodiment the 3D rendering is performed by the GPU of a processor such as an XBOX game console for example. Thus the CPU of the processor does not have to recalculate for each rendered pass game play audio physics etc. The L and R camera positions are derived as an offset from a default camera. The camera positions and angles are adjustable to achieve a desired 3D effect. Once the two scenes are rendered and captured they are processed by the GPU for a final render pass. This pass is processed at a chosen resolution and does the encoding of the textures to the desired encoding format of the display.

Point is located behind the focal plane farther from the cameras while point is located in front of the focal plane closer to the cameras . If the focal plane is mapped directly to the display of a processor or the like as is customary in most implementations objects in front of the focal plane are in negative parallax space and will appear to pop out of the display. Likewise objects behind the focal plane are in positive parallax space and will appear behind the display. Objects that are coincident with the focal plane are said to have zero parallax and appear to coincide with the display.

The parallax space of a camera dictates its relative representation in camera space. For example objects in positive parallax space will have a lower screen space x coordinate value when viewed by the left camera and a higher x coordinate when viewed by the right camera. Objects that have zero parallax will occupy the same screen space coordinates for both cameras. This effect does not produce geometrically accurate results. As a consequence of the varying viewpoints objects appear spatially shifted between any pair of stereo images. As shown in which illustrates a stereo image rendered from the two cameras and the object is shifted to the right in the left camera image and to the left in the right camera image. If the two images depicted in were superimposed on top of one another they would produce a 3D effect in negative parallax space. This means that the donut would appear to pop out of the screen at viewers.

One of the problems with this model of stereoscopic calibration is that objects only appear clearly visible if they are close to the focal plane and thus demonstrate unexaggerated levels of stereoscopic separation. The skewing of cameras enforces a focal range upon viewers and makes it difficult to focus on increasingly hyper stereo objects that fall outside of this range. Thus very close or very distant objects become a great deal harder to view.

Another shortcoming of the classic approach depicted in is that it focuses on being geometrically accurate without being physically accurate. That is the approach does not correlate the configuration of the virtual cameras to the real world. The 3D rendering approach in accordance with the present invention on the other hand attempts to accurately map the separation of a viewer s eyes in the real world to the separation of the virtual cameras in the game world. By removing inconsistencies in the viewing configuration of the cameras the viewer is guaranteed to receive stereo images that are physically accurate that place no greater strain on the eyes than viewing 3D objects in the real world.

In gaming scenarios enforcing a specific finite focal plane distance is unacceptable because a viewer becomes constrained and unable to choose where she wants to focus at all times. Objects in the scene that are closer to the focal plane will always appear more crisp more in focus and easier to view than objects farther away. In accordance with the 3D rendering technique of the present invention a parallel camera configuration is utilized wherein as depicted in the cameras converge at infinity. This forces the entire game world to constantly be in focus and mitigates fatigue of a viewer s eye. Thus the primary focal plane is located at a distance of infinity and all rendered objects exist in the negative parallax space. Accordingly a viewer can focus on any object in the world effectively allowing her to determine at will where to place the focal plane. Thus the positive and negative parallax spaces are constantly changing as the user focuses at variable distances into the virtual world which mimics real world ocular operation and stereopsis. For example a way to conceptualize this mechanism is for one to hold up a finger a few inches in front of one s nose with a specific object of interest for example a chair along the same line of sight as the finger but further away. When one focuses on the finger and closes the left eye one notices that the object of interest in the background as viewed from the open right eye is shifted to the right. That is the right eye sees the object of interest in positive parallax because the focal plane is at the finger. Next one can re focus now on the object of interest and again close the left eye. One will observe that the right eye perceives the finger in negative parallax and that the right eye correctly sees the finger as shifted to the left.

To achieve 3D rendering in accordance with the present invention a mapping is created of real world coordinates to virtual game coordinates. Although this description is in the context of a game it is to be understood that 3D rendering as described herein is applicable to any appropriate application. The mapping of real world coordinates to virtual game coordinates allows the determination of the relative distance between virtual cameras based on the average distance between a person s eyes. Eye coordinates are mapped directly into screen space and ultimately into the game s world space coordinate system and refined to take the viewer s distance from the display screen into account. This mapping is described with reference to a vertex transformation processing pipeline.

The ultimate goal is to transform what is seen by a virtual 3D camera into a 2D representation with apparent depth. Vertices begin in object space which is centered about the origin as modeled in an art tool or the like. Upon transformation by the world matrix block the vertices now exist in the world coordinate system of a game or any appropriate application . The world matrix transformation allows a single model to be created relative to the origin but placed at several different locations throughout the game world. Next vertices are transformed by the view matrix block which orients the vertices about a particular location and direction in 3D space corresponding to an in game camera. As a result all vertices are transformed such that they are visible to a camera seated at the origin looking down the negative or positive z axis. This space is referred to as the view or camera space because it orients the world relative to an arbitrary camera orientation.

The projection matrix block transforms vertices from camera space into projection space. Projection space is defined as a coordinate system spanning from w to w in the x and y directions and either 0 to w or 0 to w in the z direction. Conceptually this matrix transforms a viewing frustum from camera space into an unnormalized coordinate system such that the near viewing plane maps to the front face of the projection space and the far viewing plane maps to the back face of the projection space.

The perspective divide block performs a division operation. When vertices of the form x y z 1 are multiplied by the projection matrix transform block they are transformed into the form of x y z w and are not normalized. In order to normalize these coordinates and produce a sense of geometric depth all coordinates are divided by w. This operation yields a coordinate of the form x w y w z w 1 . At this point the vertices are in normalized device space which spans from 1 to 1 in the x and y directions and 0 to 1 or 0 to 1 in the z direction. Vertices can now be clipped because the coordinate system is normalized and a perspective sense has been added by effectively shrinking objects as they approach the far viewing plane. The final step in the pipeline vertex transformation processing pipeline is performed by the viewport matrix transformation block . A goal of the viewport matrix transformation block is to take vertices bounded by a unit hemi cube 1 to 1 in xy 0 to 1 in z and map them into a screen based coordinate system or a user defined system . This implies that vertex coordinates will range from 0 to width and 0 to height in the xy and from 0 to 1 in the z for depth operations .

As mentioned above in accordance with an example embodiment of the present invention eye coordinates are mapped directly into screen space and ultimately into the game s world space coordinate system and refined to take the viewer s distance from the display screen into account. This mapping allows the determination of the relative distance between virtual cameras based on the average distance between a person s eyes. Eye coordinates are mapped directly into screen space and subsequently into the game s world space coordinate system and refined to take into account the viewer s distance from the display screen.

In an example scenario assume a total eye separation of 3 inches wherein the left and right eyes are each 1.5 inches away from the center of the nose. This individual eye to nose distance is referred to as the eye separation distance. If we attempt to map this value directly to the display without taking viewing distance into consideration a separation of 1.5 inches for each eye maps directly to a span of 1.5 inches on the surface of the display. Next this distance is mapped into the normalized device space of the vertex transformation processing pipeline by dividing the separation distance by the width of the display. Because normalized device space ranges from 1 to 1 in the x range the normalized device separation is equal to the eye separation divided by half the display width. At this point the eye to nose real world distance is mapped into the virtual normalized device space. The viewport transformation block is skipped because of the assumption of a standard full screen viewport transformation.

Following the vertex transformation processing pipeline backwards this distance is transformed inversely through the perspective divide block and the projection matrix transform block . Performing an inverse transformation results in an offset of 0.014125 applied to the cameras in view space to achieve a physically correct calibration offset derived from the separation between the viewer s eyes. Now that the correct amount of offset to apply to the stereo cameras has been determined each camera is translated by this amount. Because the coordinates are view space coordinates the offset is applied as a lateral shift along the x axis in both the positive and negative directions.

Because the virtual cameras are parallel and their separation is physically based there is no need to dynamically shift the separation or perform any stereo calculations per frame. In this configuration the game is rendering world geometry as accurately as possible with reverence to the physical dimension and unit system. As a natural consequence this method frees the viewer from fatigue as the eye is no longer taxed or constrained to view an artificially and arbitrarily stereographic image.

At step a single default camera viewpoint and associated content are received. That is the location of the single perspective camera viewpoint and the content associated with the single perspective camera viewpoint are received. First and second perspectives are generated at step . The first and second perspectives are offset from the received default view as described above. In an example embodiment the first and second perspectives represent left and right perspectives. The first and second perspectives camera viewpoints are slightly offset from the default camera viewpoint. The left camera viewpoint is generated by subtracting an offset from the default camera viewpoint and the right camera viewpoint is generated by adding an offset to the default camera viewpoint. Composite content is generated at step . The composite content comprises the two offset views. The composite content is transformed into content renderable in 3D at step . The transform as described above includes configuring the virtual cameras to comprise parallel views such that the effective focal length is infinity and includes mapping the real world coordinates such as the viewer s eye separation to game world coordinates. The transformed composite content is provided for rendering in 3D at step .

In an example embodiment instead of using the left and right images to produce one 3D image the herein described 3D rendering technique also can be used to provide two unique 2D images to two people playing a game on the same screen. The display data can alternate between the left and right eyes. As an analogy consider shutter glasses although the same theory applies to displays that do not require glasses. The displaying of information is synchronized with the display such that a shutter for the left eye is open when the image for the left eye is on the display screen and the shutter for the right eye is open when the image for the right eye is displayed on the screen. If the images for the left and right eye are shown in rapid succession one gets the illusion of seeing one 3D image because the brain will persist the preceding image and blend the two. Furthermore the left eye never sees data intended for the right eye and vice versa because when the shutter for the left eye is open the shutter for the right eye is closed. It is to be understood that the use of goggles is as an example application and should not be limited thereto. This technique is applicable to a system that does not require goggles.

Without the goggles a player would see both views simultaneously and could possibly see what the other player is doing. To avoid seeing the other players display data e.g. in a game like poker a third view is added. By adding a third view it can be ensured that a player without goggles would only see garbage. Every third frame for example could be garbage such that someone without glasses would see player 1 s P1 image player 2 s P2 image and then the garbage image in rapid succession. If the images are shown fast enough the output would be so garbled as to make screen cheating difficult. In order to decode the correct image for P1 and P2 the glasses would then have to shutter in a different pattern. P1 s shutter pattern would be open closed closed and P2 s pattern would be closed open closed. In multiplayer scenarios two distinct views enable two people logged in to the console to see separate dashboard settings. For example player 1 might have a Kameo theme and background while player 2 has a Gears of War theme. Both players can see their own custom settings.

In an example embodiment a synchronizing module or the like could be synchronized to the vertical blank interrupt in the console. The synchronizing module could drive the synchronization of goggles. The monitor TV or the like would receive un encoded images in sequence left image right image left image right image etc. and the synchronization module would ensure that the glasses would shutter accordingly.

A computer system can be roughly divided into three component groups the hardware component the hardware software interface system component and the applications programs component also referred to as the user component or software component . In various embodiments of a computer system the hardware component may comprise the central processing unit CPU the memory both ROM and RAM the basic input output system BIOS and various input output I O devices such as a keyboard a mouse a monitor and or a printer not shown among other things. The hardware component comprises the basic physical infrastructure for the computer system.

The applications programs component comprises various software programs including but not limited to compilers database systems word processors business programs videogames and so forth. Application programs provide the means by which computer resources are utilized to solve problems provide solutions and process data for various users machines other computer systems and or end users . In an example embodiment application programs perform the functions associated with rendering display information in three dimensions as described above.

The hardware software interface system component comprises and in some embodiments may solely consist of an operating system that itself comprises in most cases a shell and a kernel. An operating system OS is a special program that acts as an intermediary between application programs and computer hardware. The hardware software interface system component may also comprise a virtual machine manager VMM a Common Language Runtime CLR or its functional equivalent a Java Virtual Machine JVM or its functional equivalent or other such software components in the place of or in addition to the operating system in a computer system. A purpose of a hardware software interface system is to provide an environment in which a user can execute application programs.

The hardware software interface system is generally loaded into a computer system at startup and thereafter manages all of the application programs in the computer system. The application programs interact with the hardware software interface system by requesting services via an application program interface API . Some application programs enable end users to interact with the hardware software interface system via a user interface such as a command language or a graphical user interface GUI .

A hardware software interface system traditionally performs a variety of services for applications. In a multitasking hardware software interface system where multiple programs may be running at the same time the hardware software interface system determines which applications should run in what order and how much time should be allowed for each application before switching to another application for a turn. The hardware software interface system also manages the sharing of internal memory among multiple applications and handles input and output to and from attached hardware devices such as hard disks printers and dial up ports. The hardware software interface system also sends messages to each application and in certain cases to the end user regarding the status of operations and any errors that may have occurred. The hardware software interface system can also offload the management of batch jobs e.g. printing so that the initiating application is freed from this work and can resume other processing and or operations. On computers that can provide parallel processing a hardware software interface system also manages dividing a program so that it runs on more than one processor at a time.

A hardware software interface system shell referred to as a shell is an interactive end user interface to a hardware software interface system. A shell may also be referred to as a command interpreter or in an operating system as an operating system shell . A shell is the outer layer of a hardware software interface system that is directly accessible by application programs and or end users. In contrast to a shell a kernel is a hardware software interface system s innermost layer that interacts directly with the hardware components.

As shown in an exemplary general purpose computing system includes a conventional computing device or the like including a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS containing basic routines that help to transfer information between elements within the computing device such as during start up is stored in ROM . The computing device may further include a hard disk drive for reading from and writing to a hard disk hard disk not shown a magnetic disk drive e.g. floppy drive for reading from or writing to a removable magnetic disk e.g. floppy disk removal storage and an optical disk drive for reading from or writing to a removable optical disk such as a CD ROM or other optical media. The hard disk drive magnetic disk drive and optical disk drive are connected to the system bus by a hard disk drive interface a magnetic disk drive interface and an optical drive interface respectively. The drives and their associated computer readable media provide non volatile storage of computer readable instructions data structures program modules and other data for the computing device . Although the exemplary environment described herein employs a hard disk a removable magnetic disk and a removable optical disk it should be appreciated by those skilled in the art that other types of computer readable media which can store data that is accessible by a computer such as magnetic cassettes flash memory cards digital video disks Bernoulli cartridges random access memories RAMs read only memories ROMs and the like may also be used in the exemplary operating environment. Likewise the exemplary environment may also include many types of monitoring devices such as heat sensors and security or fire alarm systems and other sources of information.

A number of program modules can be stored on the hard disk magnetic disk optical disk ROM or RAM including an operating system one or more application programs other program modules and program data . A user may enter commands and information into the computing device through input devices such as a keyboard and pointing device e.g. mouse . Other input devices not shown may include a microphone joystick game pad satellite disk scanner or the like. These and other input devices are often connected to the processing unit through a serial port interface that is coupled to the system bus but may be connected by other interfaces such as a parallel port game port or universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video adapter . In addition to the monitor computing devices typically include other peripheral output devices not shown such as speakers and printers. The exemplary environment of also includes a host adapter Small Computer System Interface SCSI bus and an external storage device connected to the SCSI bus .

The computing device may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be another computing device e.g. personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computing device although only a memory storage device floppy drive has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN . Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computing device is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computing device can include a modem or other means for establishing communications over the wide area network such as the Internet. The modem which may be internal or external is connected to the system bus via the serial port interface . In a networked environment program modules depicted relative to the computing device or portions thereof may be stored in the remote memory storage device. It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

While it is envisioned that numerous embodiments of rendering display information in three dimensions are particularly well suited for computerized systems nothing in this document is intended to limit the invention to such embodiments. On the contrary as used herein the term computer system is intended to encompass any and all devices capable of storing and processing information and or capable of using the stored information to control the behavior or execution of the device itself regardless of whether such devices are electronic mechanical logical or virtual in nature.

The various techniques described herein can be implemented in connection with hardware or software or where appropriate with a combination of both. Thus the methods and apparatuses for rendering display information in three dimensions or certain aspects or portions thereof can take the form of program code i.e. instructions embodied in tangible media such as floppy diskettes CD ROMs hard drives or any other machine readable storage medium wherein when the program code is loaded into and executed by a machine such as a computer the machine becomes an apparatus for implementing rendering display information in three dimensions.

The program s can be implemented in assembly or machine language if desired. In any case the language can be a compiled or interpreted language and combined with hardware implementations. The methods and apparatuses for rendering display information in three dimensions also can be practiced via communications embodied in the form of program code that is transmitted over some transmission medium such as over electrical wiring or cabling through fiber optics or via any other form of transmission wherein when the program code is received and loaded into and executed by a machine such as an EPROM a gate array a programmable logic device PLD a client computer or the like. When implemented on a general purpose processor the program code combines with the processor to provide a unique apparatus that operates to invoke the functionality of rendering display information in three dimensions. Additionally any storage techniques used in connection with rendering display information in three dimensions can invariably be a combination of hardware and software.

While rendering display information in three dimensions has been described in connection with the example embodiments of the various figures it is to be understood that other similar embodiments can be used or modifications and additions can be made to the described embodiments for performing the same functions of rendering display information in three dimensions without deviating therefrom. Therefore rendering display information in three dimensions as described herein should not be limited to any single embodiment but rather should be construed in breadth and scope in accordance with the appended claims.

