---

title: Reducing layering overhead in collective communication operations
abstract: A communication method for collective operations is provided which eliminates the need for a three layered structure. Instead, the method of the present invention employs a control structure which is specifically designed to keep track of the progress of the collective communication operations and the facilities for the handling of asynchronous message passing. The method of the present invention thus eliminates the need for an intermediary, point-to-point communication layer that is required to follow message matching semantics for sending messages, receiving messages and for awaiting message arrivals. Application layer tasks and user defined programs running in the communication layer share access to the control structure as a mechanism for synchronizing and scheduling the progress of the collective communication operation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08056090&OS=08056090&RS=08056090
owner: International Business Machines Corporation
number: 08056090
owner_city: Armonk
owner_country: US
publication_date: 20070629
---
This invention was made with Government support under Agreement No. NBCH3039004 awarded by DARPA. The Government has certain rights in the invention.

This invention relates in general to collective operations in a multinode data processing system and more particularly to collective operations as carried out in multinode data processing systems. More particularly the present invention is directed to systems and methods which provide a control structure for such operations in a manner in which layering overhead is reduced.

In order to provide useful background information to better frame the environment in which the present invention is employed consideration is given to what is most likely viewed as the paradigm application in which collective operations are performed. Accordingly attention is directed to the calculation of the largest one of a large set of numbers. The set of numbers is divided up and they are parceled out to a number of independent data processing units each one of which is capable of determining a maximum value for the set of numbers that it has been given either directly or by the passage of a boundary address for a subset of the whole set of numbers . Clearly this is the type of operation that can be parceled out again to another set of data processing nodes. The net result is that operations of this sort referred to as collective operations are ones that parceled out with data moving down a structured tree of processing elements and with resulting data being passed back up the tree. In the basic example illustrated herein each node in the tree computes a maximum value for the set that it has been assigned and returns that result to the node from which it received the data further up the tree a tree root at top picture being assumed so as to make sense of the use of the word up . That node then picks a maximum value from the set of data returned to it and in turn passes that result further up the tree. Such is the basic nature and function of so called collective operations. While this example illustrates the general principles and justifications for the use of collective operations it should be noted that in general the data that is passed up and down the branches of a tree structure is often of a significant size and is not limited to a single number. It is often numeric however and often has a specific structure. The present invention is directed to structures and processes which underlie collective operations.

Collective communication operations play a very important role in high performance computing. In collective communication data are redistributed cooperatively among a group of processes. Sometimes the redistribution is accompanied by various types of computation on the data and it is the results of the computation that are redistributed. The de facto message passing programming model standard namely the Message Passing Interface MPI defines a set of collective communication interfaces including MPI BARRIER MPI BCAST MPI REDUCE MPI ALLREDUCE MPI ALLGATHER MPI ALLTOALL etc. These are application level interfaces and are more generally referred to as APIs. In MPI collective communications are carried out on communicators which define the participating processes and a unique communication context.

Functionally each collective communication is equivalent to a sequence of point to point communications for which MPI defines MPI SEND MPI RECEIVE and MPI WAIT interfaces and variants . MPI collective communication operations are implemented with a layered approach that is the collective communication routines handle semantic requirements and translate the collective communication function call into a sequence of SEND RECV WAIT operations according to the algorithms used. The point to point communication protocol layer guarantees reliable communication. A communication protocol stack often consists of several layers each provides certain functionality and service to a higher layer. The MPI point to point layer itself is sometimes built on other point to point communication layers some of which are not of the two sided communication model. One such example is the IBM Parallel Environment MPI and IBM LAPI a Low level Application Program Interface set of definitions and functions . The MPI point to point communication layer in IBM PE MPI is called Message Passing Client Interface MPCI which is built on top of point to point active message functionalities provided by the IBM Low level Application Programming Interface LAPI . LAPI consists of interfaces for the source side of the data making transfer requests and handler interfaces for upper layer functionality to be carried out by LAPI on its behalf. There are three types of handlers in LAPI including send side completion handler receive side header handler and receive side completion handler. The collective communication operations in IBM PE MPI interfaces with MPCI which in turn interfaces with IBM LAPI.

Despite its advantages the layered approach has performance issues one of which is the locking overhead in a threaded environment. A lock is required for each layer to protect its internal data structure. Multiple lock unlock costs are paid when control goes through multiple layers. In the above example to complete a collective communication operation the MPI layer of IBM PE MPI may make multiple calls to the MPCI layer each one resulting in a LAPI function call and each requiring the following sequence 

Another issue is that the interfaces provided by a two sided point to point communication lower layer for example MPCI are generic and may not be convenient to serve certain special requirements of a particular upper layer. The MPCI protocol complies to the MPI point to point communication semantic which sometimes complicates things more than necessary. One of these cases is transferring a large message involved in collective communication operations. To send a large message in standard mode MPCI implements the rendezvous protocol in which the sender sends a message envelop to the receiver and waits for the receiver s signal on sending the data. This can add substantial overhead and increase implementation complexity. In collective communication the message envelope is not required to be delivered to the receiver for message matching purpose when send is posted before the receive. Message matching semantics enforced by the two sided point to point communication interface is not necessary for collective communication operation. Another example is the implementation of MPI Reduce. MPI Reduce combines inputs from all participating processes using a combine operation specified through the interface and returns the results in a receive buffer of one process called the root of the reduce operation . The task that performs the reduce operation needs to receive some inputs from other tasks. With the point to point send receive protocol temporary buffers are allocated and extra copies are used to store those inputs at the MPI layer before carrying on the reduce operation. A third example is a small message MPI Bcast where the message available at one process referred to as the root is transferred to all of the other participating processes. Implementation of MPI Bcast is often based on tree algorithms in which the message is sent from the root of the tree to internal nodes of the tree and then forwarded along the branches. With the point to point send receive protocol a node can only receive the message after all nodes along the branches from the root to the node made the bcast call. Delay at any internal node in calling beast delays the completion of the beast at the downstream nodes along the branch.

In the process shown in this is shown using the layered implementation of MPI Barrier as an example. The algorithm for BARRIER requires log N round of communications by each process logarithms are assumed to be base 2 . During round j process i sends a 0 byte message to process i 2 mod N and receives a 0 byte message from process i N 2 mod N . A message consists of an envelope and a payload of user data. Messages without user data payloads are referred to as 0 byte messages. These may also be referred to as control messages or zero payload messages. The send in a new round cannot start until the receiver of the previous round completes. At MPI level the algorithm is implemented by a loop of nonblocking mpci recv and blocking mpci send calls followed by mpci wait to make sure the mpci recv completes. MPCI calls LAPI Xfer to send messages and loops on calling LAPI Msgpoll for message completion. MPCI registers the generic header handler the completion handler and the send completion handler for message matching and other two sided point to point communication semantics.

The active message style interfaces for example LAPI provides more flexibility to upper layers of the communication protocol for example MPI in comparison with two sided point to point communication interfaces. To address those issues the present application describes methods to implement MPI collective communications on top of active message point to point communication layer directly bypassing the MPI two sided point to point communication layer. Improved collective communication performance is achieved through reduced protocol layering overhead.

The present invention provides a method for message transfer in a data processing system between tasks in support of collective operations as that term is used herein and as described above. The method includes updating state information for the collective operations in a control structure. The updating is carried out by tasks running in an application layer. A message passing request is initiated from one of the tasks directly to a communication layer interface in a communication layer having point to point message passing communication protocol capabilities. The message is delivered to at least one targeted task in the application layer from the communication layer. Execution of at least one user defined routine in the application layer is triggered upon delivery or upon completion of the sending operation and the message is thus handled asynchronously. The state information in the control structure of the targeted task is updated upon arrival of the message by a user defined routine. The state information in the control structure of the initiating task is also updated upon completion of message sending by a user defined routine.

Additional features and advantages are realized through the techniques of the present invention. Other embodiments and aspects of the invention are described in detail herein and are considered a part of the claimed invention.

The recitation herein of desirable objects which are met by various embodiments of the present invention is not meant to imply or suggest that any or all of these objects are present as essential features either individually or collectively in the most general embodiment of the present invention or in any of its more specific embodiments.

In order to provide the best description of the present invention IBM s PE MPI and IBM LAPI are used as exemplar environment. In the approach provided by the present invention MPI collective communication routines make LAPI data transfer calls directly whenever a message needs to be sent. The transfers are nonblocking. Receiving data is handled by handlers registered with LAPI. The activations of those handlers are asynchronous. Together the collective communication routine and the handlers cooperate to carry out the collective communication algorithms. Once the collective routine reaches synchronization point the routine enters a loop of LAPI message polling function calls to drive asynchronous communication and poll for completion indication. The handlers may advance the algorithms to the next synchronization point. A synchronization point in the algorithm is an operation that has a dependency on the completion of a previous receive operation or the local process s call to the collective communication routine. Completion of one collective communication at a process is defined by no more synchronization points left and all data transfers are complete locally. Collective communication routines and the handlers coordinate through control data structures cached with a MPI communicator object which records current state of the algorithm. The MPI BARRIER is used again as an example. It is a simpler case since there is no application data being transferred or received.

The method of the present invention is exemplified by the layers transfers and structure shown in . The most significant differences between and are that the process illustrated in only employs two layers and that the method shown in employs a control structure whose variables are described in detail below. It is also noted that while the control structure is shown as being present in the MPI layer it is shared and is also accessible from user defined programs handlers running in the LAPI layer. The details of the exchanges shown are provided below.

With the approach employed in the present invention a LAPI header handler and a completion handler are registered with LAPI during initialization time. The communicator object data structure is augmented to include two bit vectors bitvec cur and bitvec next each of size log N where N is the number of processes in the group associated with the communicator. Bitvec cur is used to track messages from which a process has arrived for the current MPI BARRIER operation. Bitvec next is used to track early arrival for the next barrier operation. Initially the bits in both vectors are cleared. There is also integer counter cur val added to keep track of the synchronization point. In the described MPI Barrier algorithm every transfer is a synchronization point. Cur val is initialized to 0. As used herein the term bit vector refers to an identified region of storage having a specified length in terms of the number of bits present and wherein processing associated with this vector is generally performed on a bit wise basis that is one bit at a time except perhaps for clearing resetting operations.

When the MPI Barrier routine is called at process i it first grabs a lock to make sure it has exclusive access to the control structure. A simple lock is sufficient for this purpose since no blocking communication is allowed while holding the lock therefore the length of lock holding is short. More expensive mutex locking is not needed mutex stands for mutual exclusion and refers to a type of lock used to avoid simultaneous access of a shared resource . Next it copies the value of bitvec next to bitvec cur and sets bit 0 of bitvec cur to 1 one . All of the bits in bitvec next are cleared. Then it scans bitvec cur to find bit k such that bits 0 through bit k of bitvec cur are all 1 and the k 1 bit of bitvec cur is 0. Cur val is set to k. After releasing the lock the barrier routine calls LAPI Xfer in a loop to transfer 0 byte messages to processes i 2 mod N where 0 j

The collective communication sequence number of the current barrier is stored in the header or envelope of the 0 byte message as this phrase is defined above together with the context number associated with the communicator and distance between the source and the target processes. Here distance between two processes is the absolute value of the rank of first process minus the rank of second process. When the header of the message arrives at the target process LAPI actives the header handler for the BARRIER operation and passes the header to the handler. The handler locates the corresponding communicator object using the context number and the control data structures for the BARRIER operation. It grabs the lock and compares the local collective communication sequence number and the one in the message header. Processing occurs as follows 1 An error is raised if the former is greater than the latter. 2 If the former is smaller bit j in bitvec next is set to 1 where j log distance and the header handler returns after releasing the lock. No completion handler is necessary in this situation. 3 If the two are equal bit j in bitvec cur is set to 1. Next the header handler checks if j equals cur val. If that is the case the current synchronization point is satisfied by its activation and the header handler scans bitvec cur to find the next right most 0 bit and updates cur val accordingly. It then sets up the completion handler and passes both the old and new values of cur val s and t to the completion handler. The header handler returns after releasing the lock. If j is not equal to cur val no completion handler is needed. The header handler simply returns after releasing the lock. If the completion handler gets activated for BARRIER operation it calls LAPI Xfer in a loop to send messages to processes i 2 mod N where i is the local process and s j

The chart shown in illustrates the performance results from MPI Barrier benchmark tests with both the approach shown in and the new approach on a cluster consisting of 32 IBM System P nodes running AIX as the operating system . The new approach clearly improves MPI Barrier performance. The function flow of the approach for other collective communication operations is similar. The synchronization points may be different but the idea is same and those skilled in the message passing arts can easily modify the above example to fit in those operations. The chart in shows five different situations based upon differing numbers of tasks. In each case 2 tasks 4 tasks 8 tasks 16 tasks and 32 tasks it is seen that the method of the present invention results in a significant reduction in barrier time. It is additionally noted that the improvement in barrier time reduction increases with the number of tasks.

There are however added complexities in the present approach for other collective communication operations which allow users to specify how much data needs to be processed and redistributed during collective communication. The extra logic is employed in the following three situations 

First When the data length in a message is greater than 128 bytes return from LAPI Xfer does not guarantee completion of sending the message. To address this similar calls to LAPI Xfer for other collective communication also specify LAPI send completion handlers. The responsibility of the send completion handler is to increment an integer counter send complete in the control data structure. Once the collective communication routine finds cur val is greater than the number of synchronization points it continues the polling loop instead of breaking out until the value of send complete equals the number of LAPI Xfer calls being posted.

Second The header handlers should return a data buffer so that LAPI can move the incoming data to the buffer. It is possible that when a header handler gets activated the collective communication routine has not yet been called by the local process. For messages smaller than a certain threshold the data is cached in early arrival buffers maintained by the MPI library. Flow control based on tokens is the method used for managing the early arrival buffers. The token flow control is easily incorporated in the present approach. If the message is smaller than certain threshold and there is token available the source simply calls LAPI Xfer. When the message arrives earlier than the target process calling the corresponding collective communication routine the header handler moves the data into the early arrival buffer and stashes the address of the earlier arrival buffer in the control data structure. When the collective communication routine is called the early arrived data is consumed as part of the synchronization advancing process. If tokens are used up at the sender process LAPI Xfer is called to send a 0 byte message with the data address at the source process recorded in the header. No send completion hander is scheduled for this LAPI Xfer. When this control message arrives at the target process the header handler stores this information in the control data structure if it is an early arrival with a flag indicating the data is still at the source process. Once the synchronization point advance gets to this message a reply control message is sent back to the source of the data which triggers the data actually being transferred by another LAPI Xfer. The send completion hander is scheduled for this transfer.

Third When the data length is greater than the threshold the message is treated as smaller message without a token. The present invention uses a simpler and more efficient solution in which the source process records the data address in the control data structure and skips calling LAPI Xfer. On the target side if a message with large application data needs to be received LAPI Xfer is called to send a 0 byte message to the source process with the receive buffer address in the header. The control message triggers the data being actually transferred to the target.

While the invention has been described in detail herein in accordance with certain preferred embodiments thereof many modifications and changes therein may be effected by those skilled in the art. Accordingly it is intended by the appended claims to cover all such modifications and changes as fall within the true spirit and scope of the invention.

