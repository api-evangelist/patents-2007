---

title: Complexity-based adaptive preprocessing for multiple-pass video compression
abstract: Multiple-pass video encoding systems and techniques are described which utilize statistics taken during a first-pass encoding to create complexity measurements for video data which is to be encoded. By analyzing these complexity measurements, preprocessing decisions, such as, for example, the determination of strength of denoise filters, can be made with greater accuracy. In one implementation, these complexity measurements take the form of calculation of temporal and spatial complexity parameters, which are then used to compute a unified complexity parameter for each group of pictures being encoded.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08238424&OS=08238424&RS=08238424
owner: Microsoft Corporation
number: 08238424
owner_city: Redmond
owner_country: US
publication_date: 20070209
---
Transform coding is a compression technique used in many audio image and video compression systems. Uncompressed digital image and video is typically represented or captured as samples of picture elements or colors at locations in an image or video frame arranged in a two dimensional 2D grid. This is referred to as a spatial domain representation of the image or video. For example a typical format for images consists of a stream of 24 bit color picture element samples arranged as a grid. Each sample is a number representing color components at a pixel location in the grid within a color space such as RGB or YIQ among others. Various image and video systems may use various different color spatial and time resolutions of sampling. Similarly digital audio is typically represented as time sampled audio signal stream. For example a typical audio format consists of a stream of 16 bit amplitude samples of an audio signal taken at regular time intervals.

Uncompressed digital audio image and video signals can consume considerable storage and transmission capacity. Transform coding reduces the size of digital audio images and video by transforming the spatial domain representation of the signal into a frequency domain or other like transform domain representation and then reducing resolution of certain generally less perceptible frequency components of the transform domain representation. This generally produces much less perceptible degradation of the digital signal compared to reducing color or spatial resolution of images or video in the spatial domain or of audio in the time domain.

According to one possible definition quantization is a term used in transform coding for an approximating non reversible mapping function commonly used for lossy compression in which there is a specified set of possible output values and each member of the set of possible output values has an associated set of input values that result in the selection of that particular output value. A variety of quantization techniques have been developed including scalar or vector uniform or non uniform with or without dead zone and adaptive or non adaptive quantization.

The quantization operation is essentially a biased division by a quantization parameter which is performed at the encoder. The inverse quantization or multiplication operation is a multiplication by the quantization parameter performed at the decoder.

In general video compression techniques include intraframe compression and interframe compression. Intraframe compression techniques compress individual frames typically called I frames or key frames. Interframe compression techniques compress frames with reference to preceding and or following frames which are typically called predicted frames P frames or B frames.

In addition to the mechanisms described above video encoding can also benefit from the use of preprocessing prior to encoding to provide for more efficient coding. In one example denoise filters are used to remove extraneous noise from a video source allowing a later encoding step to operate with greater efficiency.

However with typical video encoding it is difficult to know how exactly to perform preprocessing in order to create the most efficient encoding with the fewest number of visible artifacts. What is needed is a mechanism for gaining knowledge about a video source which can be used to facilitate preprocessing decisions.

Multiple pass video encoding systems and techniques are described. In various implementations these systems and techniques utilize statistics taken during a first pass encoding to create complexity measurements for video data to be encoded. In one implementation through analyzing these complexity measurements preprocessing decisions such as the determination of strength of denoise filters are made. In one implementation temporal and spatial complexity parameters are calculated as the complexity measurements. These parameters are then used to compute a unified complexity parameter for each group of pictures being encoded.

In one example implementation a method of determining parameters for pre processing of a group of one or more pictures is described. The example method comprises determining one or more complexity parameters for the group of pictures and encoding the group of pictures in a video stream based at least in part on the one or more complexity parameters.

In another example implementation a system for encoding video is described. The example system comprises a first pass video encoding module which is configured to analyze one or more frames in a video sequence and to calculate one or more encoding parameters to be used in encoding the one or more frames in the video sequence. The example system also comprises a complexity based adaptive preprocessing module which is configured to determine one or more complexity parameters for the one or more frames and to determine preprocessing filters to be used during encoding the one or more frames based on the one or more complexity parameters. The example system also comprises a second pass video encoding module which is configured to apply preprocessing filters to the one or more frames based on the preprocessing filter parameters and to encode the filtered frames into encoded video stream data.

In another example implementation one or more computer readable media are described which contain instructions which when executed by a computer cause the computer to perform an example method for encoding video. The example method comprises performing a first pass analysis on one or more frames in a video sequence in order to calculate one or more encoding parameters to be used in encoding the one or more frames in a video sequence. The example method also comprises determining one or more complexity parameters for the one or more frames based on the one or more encoding parameters determining preprocessing filters to be used during encoding the one or more frames based on the one or more complexity parameters applying preprocessing filters to the one or more frames based on the preprocessing filter parameters and performing a second pass analysis on the one or more frames to encoding the filtered frames into encoded video stream data.

The exemplary techniques and systems described herein allow for and perform additional preprocessing on video data in a multiple pass video encoding system. After a first pass is performed video statistics based on the first pass encoding are analyzed to determine complexity parameters from the first pass encoding process. These complexity parameters are then used to control preprocessing performed on the video data. In one example the preprocessing performed is the application of a filter. The preprocessed video data is then encoded by a later pass of the encoding system. By determining and utilizing complexity data the systems and techniques described herein can use the content of the encoded video to make more informed decisions about what preprocessing should or should not be performed. This results in more efficient video encoding that is more accurate to the qualities of the video being encoded. Additionally the techniques described herein offer very little overhead in the encoding process and so do not overly complicate encoding.

Multiple pass video encoders generally perform a first encoding on video data in order to determine statistics about the video data. These statistics are then used to create controls for later processing and encoding. By using information gained during a first pass analysis multiple pass encoding systems are able to perform processing and encoding that is more accurately directed toward the particular nature of the video being encoded. This tuning of the process results in an eventual encoded video stream that either has a lower bit rate has fewer visible artifacts or both.

In the particular illustrated example the preprocessing takes the raw video data as input and applies preprocessing filters or other techniques to it before passing it to a second pass encoding where the processed video data is then encoded into a final encoded video stream . In other implementations the preprocessing may simply analyze the statistics provided it by the first pass encoding and give control data to the second pass encoding which would then take the raw video data as input and encode the raw video data according to the control data. The result is a final encoded video stream which is then output from the video encoding system. Note also that in alterative implementations more than two passes may be used before outputting a final encoded video stream.

In the illustrated implementation the display system comprises a first pass video encoding module . In one implementation this module is configured to accept raw video data and perform a first pass encoding of the video data. As discussed above this first pass is performed in order to acquire statistics about the video data that can then be used in later encoding. Additionally in various implementations the first pass video encoding module may also produce a first pass encoded video stream which may or may not be used in later encoding.

The illustrated implementation also shows a complexity based adaptive preprocessing module which is configured to perform preprocessing on the raw video data or the first pass encoded data in alternative implementations before final encoding. Then in the illustrated implementation a final encoding is performed by the second pass encoding module which is configured in one implementation to accept preprocessed video data from the complexity based adaptive preprocessing module and perform a final encoding on it. In alternative implementations additional video encoding modules not illustrated may also be included in the system and or the two or more encoding modules may be combined into a single module.

Next the process continues to block where a first encoding is performed in order to generate encoding statistics. In some implementations the encoding is performed according to the VC 1 video encoding standard. In other implementations other standards may be used including but not limited to Windows Media Players 7 8 and 9 H.264 MPEG 2 MPEG 4. During the process of block various statistics may be reported. However for the ease of description the techniques described herein will be performed only with reference to two statistics the frame size and quantization parameter for each frame encoded during the process of block . Thus in one implementation only quantization parameters and frame sizes for each frame are recorded after this first pass. In another implementation if variable quantization parameters are used during the first pass an average over the frame is recorded for use during preprocessing. In alternative implementations other statistics may be collected which provide additional information about complexity and can be used in preprocessing.

Next the process continues to block where the system determines complexity parameters from the encoding statistics determined at block . Particular examples of processes to determine complexity parameters are described below. Next at block the system encodes the video data based on the complexity parameters determined at the process of block . Particular examples of processes to encode video data using complexity parameters are described below as well.

Finally in one implementation the encoded video stream created at block is output by the system . In alternative implementations additional encoding or post processing modifications may be made to the video stream before output but for the sake of simplicity these implementations are not illustrated.

Next at block the process begins a loop to analyze each partitioned group of pictures. Then at block the system determines a spatial complexity parameter for the currently analyzed group of pictures. This is followed at block by the system determining a temporal complexity parameter for the current group of pictures. Descriptions of temporal and spatial complexity will follow.

The last illustrated block in the loop is at block where a unified complexity parameter is determined for the group of pictures. While in some implementations including ones described below the unified complexity parameter is determined through manipulation of the previously determined temporal and spatial complexity parameters in some implementations the unified complexity parameter by be determined through other analysis. In yet other implementations a unified parameter may not be calculated at all but instead individual parameters such as the spatial and temporal complexity parameters computed in blocks and may be used for preprocessing. Finally at block the loop is repeated for the next group of pictures.

Each of the ideas illustrated in seeks to capture the idea that different source data may be more or less complex and different raw video data may exhibit complexities in different ways. Thus a video containing a static room is easier generally to encode than a video capturing a busy street. By bifurcating complexities into temporal and spatial complexities the calculations used to measure complexity are made simpler both to understand and to implement.

Example images and illustrate differences in spatial complexity. In one implementation spatial complexity captures the idea of the number of details in a video frame. Thus in the example shown image which contains many shapes some of which are overlapped contains a non trivially greater amount of spatial complexity than does image which has only a single circle in it.

By contrast in one implementation temporal complexity captures the difficulty in predicting one frame from a previously encoded frame. An example of this is illustrated in images and . Please note that in each of the two images and movement within the image is illustrated through the use of arrows and dotted figures this is merely an abstraction of movement that would take place over the course of various frames within a group of pictures. In the examples of images and image shows a lower temporal complexity than does image . This is because while image has a high spatial complexity its only movement and thus the only part of the frame that needs to be predicted is a simple sideways movement of the triangle . In contrast image shows a large movement of the circle which provides a more difficult task of prediction and therefore raises the level of temporal complexity of the group of pictures represented by image .

The process begins at block where an I frame is located for the group of pictures being analyzed. As mentioned above in a preferred implementation there is only one I frame within the group of pictures. Next the quantization parameter and frame size are determined for this I frame. In one implementation this determination may consist solely of looking up the recorded values for the quantization parameter and the frame size for the I frame. In another when variable quantization parameters are used an average quantization parameter is found for the I frame to ease later computations.

Next at block the quantization parameter and frame size for the I frame are multiplied and at block this product is set as the spatial complexity parameter for the group of pictures. Thus for a quantization parameter and frame size for the I frame of QPand Size respectively the spatial complexity parameter for every frame in the group of pictures is calculated by Size In alternative implementations the calculation of the spatial complexity parameter may be modified by scaling either or both of the input statistics before combining them into the final parameter. Thus one or both of the quantization parameter and frame size may be scaled exponentially or may be multiplied by a scale before calculating a spatial complexity parameter.

The process begins at block where one or more P frames are located for the group of pictures being analyzed. As mentioned above in a preferred implementation there is only one I frame and a collection of P frames as well as B frames within the group of pictures. Next at block a loop is performed to analyze each P frame within the group of pictures.

At block the quantization parameter and frame size are determined for the particular P frame being analyzed. In one implementation this determination may consist solely of looking up the recorded values for the quantization parameter and the frame size for the P frame. An another when variable quantization parameters are used an average quantization parameter is found for the P frame to ease later computations.

Next at block the quantization parameter and frame size for the P frame are multiplied. Thus for a quantization parameter and frame size for the P frame of QPand Size respectively the a first product is calculated for the P frame by Size While this product does capture the general concept that lower temporal complexity should lead to a smaller frame size at a given QP experimentation has discovered that the above measure is largely related to spatial complexity. Thus given the same amount of motion and the same QP a scene with higher spatial complexity is likely to have a bigger sized P frame compared to a low spatial complexity scene. In some implementations of encoders this is due to imperfections in the capturing process and motion estimation processes.

To account for this correlation at block the product given above is divided by the spatial complexity parameter for the P frame. As discussed above in the illustrated implementation of this spatial complexity parameter was calculated for every frame in the group of pictures by calculating it for the I frame in the group. This gives a more accurate measure for the temporal complexity of P frame as 

Next in order to have a single temporal complexity parameter for the group of pictures an average of the temporal complexity parameters for the P frames in the group of pictures is taken. This is performed by the system in block . Finally at block this average is set as the temporal complexity parameter for the group of pictures.

The illustrated process begins at block where the temporal and spatial complexity parameters are normalized. In one implementation this normalization is performed according to the following two equations 

Next at block the normalized temporal complexity parameter is scaled according to a predetermined exponent. This is done to adjust the relative strength of the spatial and temporal complexities within the unified complexity paramters. In one implementation a value of 0.5 is used as an exponent for the temporal complexity parameter. Next at block the scaled temporal complexity parameter and the spatial complexity parameter are multiplied and at block this product is set as the unified complexity parameter for the group of pictures. Thus the unified complexity parameter is found as where is the scaling exponent used in block . It should be noticed that this equation can be written in an equivalent fashion as This alternative form demonstrates more clearly the capability of the exponent as a relative strength control between the two particular complexity parameters.

Next at block the scaled complexity parameter is normalized to form an appropriate filter strength value. In the case of the VC 1 encoding one implementation gives the scaling and normalization calculations according to the following equation FilterStrength 2048 10 Where is the exponential scale of block e.g. 1.2 in a VC 1 encoding system and the operator represents a right bit shift operation. Additionally in some implementations if the resulting FilterStrength value is outside of the proper range for the filters being used the number is clipped. Thus in an exemplary VC 1 implementation FilterStrength is clipped to reside in the range 0 8 . Next at block the filters are applied to the group of pictures or raw video associated therewith according to the calculated filter strength. The loop then repeats for additional groups of pictures at block .

It should be noted that the estimated complexities C C and C may be used in alternative implementations to make better encoding decisions in other encoding and preprocessing modules. For example and not by way of limitation the system may make rate control decisions at to what quantization parameter second quantization parameter or P or B frame delta quantization parameters to use if the system considers the three complexity parameters from multiple frame altogether. In another example a quantization module of an encoding system may benefit from the use of complexity parameters such as using a bigger deadzone for quantization in the case of a high value for C.

The above surface approximation techniques can be performed on any of a variety of computing devices. The techniques can be implemented in hardware circuitry as well as in software executing within a computer or other computing environment such as shown in .

With reference to the computing environment includes at least one processing unit and memory . In this most basic configuration is included within a dashed line. The processing unit executes computer executable instructions and may be a real or a virtual processor. In a multi processing system multiple processing units execute computer executable instructions to increase processing power. The memory may be volatile memory e.g. registers cache RAM non volatile memory e.g. ROM EEPROM flash memory etc. or some combination of the two. The memory stores software implementing the described techniques.

A computing environment may have additional features. For example the computing environment includes storage one or more input devices one or more output devices and one or more communication connections . An interconnection mechanism not shown such as a bus controller or network interconnects the components of the computing environment . Typically operating system software not shown provides an operating environment for other software executing in the computing environment and coordinates activities of the components of the computing environment .

The storage may be removable or non removable and includes magnetic disks magnetic tapes or cassettes CD ROMs CD RWs DVDs or any other medium which can be used to store information and which can be accessed within the computing environment . The storage stores instructions for the software implementing the described techniques.

The input device s may be a touch input device such as a keyboard mouse pen or trackball a voice input device a scanning device or another device that provides input to the computing environment . For audio the input device s may be a sound card or similar device that accepts audio input in analog or digital form or a CD ROM reader that provides audio samples to the computing environment. The output device s may be a display printer speaker CD writer or another device that provides output from the computing environment .

The communication connection s enable communication over a communication medium to another computing entity. The communication medium conveys information such as computer executable instructions compressed audio or video information or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media include wired or wireless techniques implemented with an electrical optical RF infrared acoustic or other carrier.

The techniques described herein can be described in the general context of computer readable media. Computer readable media are any available media that can be accessed within a computing environment. By way of example and not limitation with the computing environment computer readable media include memory storage communication media and combinations of any of the above.

The techniques herein can be described in the general context of computer executable instructions such as those included in program modules being executed in a computing environment on a target real or virtual processor. Generally program modules include routines programs libraries objects classes components data structures etc. that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer executable instructions for program modules may be executed within a local or distributed computing environment.

For the sake of presentation the detailed description uses terms like calculate generate and determine to describe computer operations in a computing environment. These terms are high level abstractions for operations performed by a computer and should not be confused with acts performed by a human being. The actual computer operations corresponding to these terms vary depending on implementation.

In view of the many possible variations of the subject matter described herein we claim as our invention all such embodiments as may come within the scope of the following claims and equivalents thereto.

