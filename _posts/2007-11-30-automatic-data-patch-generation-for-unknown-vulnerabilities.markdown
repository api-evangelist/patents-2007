---

title: Automatic data patch generation for unknown vulnerabilities
abstract: The claimed subject matter provides a system and/or method that generates data patches for vulnerabilities. The system can include devices and components that examine exploits received or obtained from data streams, constructs probes and determines whether the probes take advantage of vulnerabilities. Based at least in part on such determinations data patches are dynamically generated to remedy the hitherto vulnerabilities.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08613096&OS=08613096&RS=08613096
owner: Microsoft Corporation
number: 08613096
owner_city: Redmond
owner_country: US
publication_date: 20071130
---
Recently there has been a rise in zero day attacks e.g. computer threats that expose undisclosed or unpatched computer application vulnerabilities . Unfortunately current practice in vulnerability analysis and protection generation is generally manual. Zero day attacks can be considered extremely dangerous as they exploit computer security holes for which no solution is currently available. Usually zero day attacks are released before or on the same day that a particular vulnerability is identified to the public.

Automatic signature generation for zero day attacks has generated much attention in recent times. Recent attempts to provide solutions to thwart zero day attacks have included generating attack signatures for a single attack variant searching for long invariant substrings from network traffic as signatures. Finding multiple invariant substrings from network traffic can include for example observing that multiple invariant substrings must often be present in all variants of a worm payload for the worm to function properly. These substrings typically correspond to protocol framing control data like return addresses and poorly obfuscated code. Such an approach however suffers from significant false positives and false negatives because legitimate traffic often contains multiple invariant substrings and such polymorphic attacks could hijack control data without using an invariant substring. Moreover the foregoing approach generates signatures from network traffic alone. The fundamental drawback of such mechanisms therefore is that carefully crafted attack traffic can mislead them to generate incorrect signatures.

Other approaches employed to overcome zero day attacks have included leveraging information about vulnerable applications for improving both the accuracy and the coverage of signatures. For example one technique employs protocol specifications to provide more protocol context to attack signatures and attempts to generalize the signature for observed attack instances. Moreover the signatures provided are finite state automaton FSA inferred from clusters of similar connections or sessions. The edges in the connection level finite state automata FSA can be either fields or messages and the edges in a session level finite state automaton FSA can be connections. Such an approach generalizes the signatures by replacing certain variable data elements with a wildcard. Shortcomings associated with such an approach are that it is dependent on attack instances observed. This can make resulting signatures too specific. For example attack variants that make use of different message sequences cannot be captured by this approach. Also wildcard based generalizations typically cannot filter attack variants of buffer overrun vulnerabilities. Additionally the validity of this approach can lead to over generalization and false positives.

Further mechanisms employed to overcome zero day attacks have included utilization of address space randomization based zero day detectors and regular expression based protocol specifications to generate signatures for buffer overrun vulnerabilities. These signatures however generally do not contain any protocol context but typically only a pattern matching predicate for a particular protocol message. Signatures without protocol context can result in false negatives when different message sequences lead to the same attack and result in false positives when pattern matching a message at a non vulnerable protocol state. Further since these mechanisms use the length of the vulnerable input field as the buffer limit for the buffer overrun condition in the signature this can cause false negatives for attacks that have shorter buffer length than that of the observed attack instance.

As will have been noted the foregoing techniques for overcoming zero day attacks have typically focused on and or have been dependent on attack instances observed. Other approaches for producing signatures to counter zero day attacks have included manipulating packet payloads and observing program reactions to them. Such approaches have involved generating signatures in three steps 1 constructing probes by randomizing address like strings 2 detecting exploits e.g. code fragments or sequences of commands that take advantage of vulnerabilities in order to cause unintended or unanticipated behavior to occur in computer software and or hardware by observing memory exception upon probe utilization and 3 generating signatures by finding in the attack input the bytes that cannot take random values. In step 3 probes are typically constructed for each byte other than the address string by randomizing its value. Nevertheless this approach has two limitations. First the probing scheme randomizes each byte rather than leveraging data format information. Such a strategy generates significantly more probes particularly when multiple messages are involved in an attack. Additionally the scheme works more reliably for text based protocols than binary ones because of a lack of protocol knowledge for binary data formats. Second the approach can only detect control flow hijacking attacks. For example such an approach cannot detect exploits of Windows Metafile WMF vulnerabilities.

A further signature generation approach to counteract zero day attack vulnerabilities and more particularly to find attack invariants has been to flip bits of original attack data to generate probes. However such an approach can be prohibitively expensive and on the whole can be impractical.

Yet another strategy for automatic signature generation utilizes program analysis for binary or source code. For example one modality employs dynamic data flow analysis over the execution on attack input and generates a signature in the form of symbolic predicates. Such attack signatures generated by such dynamic data flow analysis can be inherently specific to the attack input used in the data flow analysis. A further methodology that can be adopted utilizes static program analysis to extract the program logic that processes the attack data and triggers the vulnerability. The extracted logic can be expressed in the form of Turing Machines symbolic predicates or regular expressions as vulnerability signatures. Turing Machine based signatures typically however may not terminate and regular expressions are not sufficiently expressive for many vulnerabilities.

The following presents a simplified summary in order to provide a basic understanding of some aspects of the disclosed subject matter. This summary is not an extensive overview and it is not intended to identify key critical elements or to delineate the scope thereof. Its sole purpose is to present some concepts in a simplified form as a prelude to the more detailed description that is presented later.

The claimed subject matter provides a system that automatically generates data patches or vulnerability signatures for vulnerabilities given a zero day attack instance. The claimed subject matter leverages knowledge of data formats to generate new potential attack instances called probes and detects predicts and or determines if instances can exploit vulnerabilities the determination and or prediction can then be utilized to locate further vulnerability signatures. Typically the generated signatures have no false positives and generally a low rate of false negatives. Further the generated signatures are significantly more precise than signatures generated by existing schemes. Moreover the claimed subject matter can produce high quality signatures for a large portion of currently known vulnerabilities and further the signatures produced by the claimed subject matter are superior to those signatures generated by existing schemes and methodologies.

In accordance with an illustrative aspect the subject matter as claimed generates new attack instances based on a single attack instance along with a data format specification without relying on just observed attack instances. Moreover the signature generalization process employed by the claimed subject matter can include a validation aspect. Further signatures generated by the claimed subject matter can incorporate the protocol context in which attacks can happen such as at what protocol state an attack occurred. Additionally the claimed subject matter can discover buffer limits where such discovery can be based at least in part on dynamic flow analysis. Discovery of buffer limits typically can yield zero false positives and harmless false negatives. Furthermore the coverage provided by the signatures generated by the claimed subject matter is much greater because of the use of data format informed probing.

In accordance with a further illustrative aspect the claimed subject matter provides data patch generation for vulnerabilities with informed probing. The subject matter as claimed can receive or obtain data streams that can contain exploits and thereafter can construct probes and determine whether these constructed probes uncover vulnerabilities. Where vulnerabilities are identified the subject matter as claimed can dynamically generate data patches that can be utilized to remedy these hitherto vulnerabilities.

To the accomplishment of the foregoing and related ends certain illustrative aspects of the disclosed and claimed subject matter are described herein in connection with the following description and the annexed drawings. These aspects are indicative however of but a few of the various ways in which the principles disclosed herein can be employed and is intended to include all such aspects and their equivalents. Other advantages and novel features will become apparent from the following detailed description when considered in conjunction with the drawings.

The subject matter as claimed is now described with reference to the drawings wherein like reference numerals are used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding thereof. It may be evident however that the claimed subject matter can be practiced without these specific details. In other instances well known structures and devices are shown in block diagram form in order to facilitate a description thereof.

In the face of the rise in attacks that exploit vulnerabilities and the current practice of manual protection generation the subject matter as claimed automates the process and enables fast accurate patch level protection generation for vulnerabilities given the observation of an exploit e.g. software code code fragment or sequence of commands that take advantage of vulnerabilities in order to cause unintended and unanticipated behavior to occur in machine software and or hardware of the vulnerability.

In particular the claimed subject matter provides fast patch level protection in the form of data patches rather than the more traditional software patch. A data patch typically can serve as a policy for a data filter and is generally based on vulnerabilities and or software flaws that require protection. The data filter can utilize the data patch to identify parts of input data to cleanse as input data is being consumed. As a result the sanitized data stream will not exploit the vulnerability. Vulnerability signatures employed by firewalls to filter malicious network traffic are examples of data patches for network input. Similarly files can be crafted maliciously to exploit vulnerabilities in applications that consume file input. With the rise of such exploits anti virus software vendors have started using vulnerability signatures for data files to defend against new attack variants. It should be noted that the terms data patch and vulnerability signature can be and have with out limitation been employed interchangeably herein. The term data patch when emphasizing its purpose as a patch and the term vulnerability signature when emphasizing its form as a signature.

By being data driven data patches can take the form of signatures that can be automatically distributed and enacted on vulnerable hosts. This style of protection generally cannot be achieved by traditional software patches since applying software patches is inherently user driven. Even with automatic patch download users often still need to enact the downloaded patch by restarting the application or rebooting the machine. Further in enterprise environments patches are typically tested prior to deployment in order to avoid the potential high cost of recovering from faulty patches. In contrast rolling back data patches is as simple as removing vulnerability signatures.

The subject matter as claimed provides systems and methods for automatically generating data patches for vulnerabilities given attack instances. The system leverages knowledge of data formats to generate new potential attack instances and uses a detector as an oracle in order to guide the search of vulnerability signatures. The system in accordance with an aspect of the claimed subject matter assumes knowledge of data formats of attacks such as for example protocol formats that can be employed by network based attacks or file formats that can be utilized by file based attacks. This assumption is reasonable in that the types of data input consumed by applications is typically known and it is common practice to have data formats specified for purposes of interoperability across vendors or simply for the purpose of documentation in a proprietary setting.

The claimed subject matter employs an attack detector to detect attacks with high confidence. Detected attacks are sent to the attack detector which thereafter produces data patches to thwart attacks. The detector constructs new potential attack instances called probes based at least in part on data format information. The probes can be utilized by the attack detector to ascertain whether these potential attack instances could succeed as real viable attacks. Determinations rendered by the attack detector can provide guidance for the construction of new probes to discard attack specific parts of the initial original attack data as well as retain inherent vulnerability specific parts. The attack detector thus provides vulnerability signatures in the form of refinements to data format specifications that embed vulnerability predicates a set of boolean conditions on data fields. For stateful network based attacks the claimed subject matter is able to capture protocol contexts such as for instance protocol states at which attack messages are sent.

The number of probes utilized by the claimed subject matter for deriving vulnerability signatures can for instance be considered a key measure of its efficiency. Accordingly the subject matter as claimed minimizes the number of probes by leveraging semantic information and or constraints in data format specifications. For example the claimed subject matter can enforce dependency constraints across data fields so that invalid probes will not be generated.

In order to describe the coverage of vulnerability signatures the notion of Monomorphic Execution Path MEP and Polymorphic Execution Path PEP can be utilized. The notion of Monomorphic Execution Path MEP considers a single execution path from the point at which attack input is consumed to the point of compromise while Polymorphic Execution Path PEP considers many different paths. To date automatic signature generation utilizing program analysis for binary or source code have typically been Monomorphic Execution Path MEP signatures e.g. execution trace based methods . It nevertheless has remained an open challenge to generate Polymorphic Execution Path PEP signatures in the form of symbolic predicates. One challenge has been the combinatorial explosion in the number of execution paths. Another challenge has been that with potentially large attack data e.g. a very long large files maliciously crafted with many iterative elements the resulting symbolic predicates can contain a large number of conditions e.g. the number of conditions can grow with the number of iterative elements in the input many of which are unnecessary and overly restrictive causing high rates of false negatives. Furthermore in a stateful network based attack that takes place over a sequence of messages the vulnerability may only be triggered by the last message and there maybe other message sequences that can lead to the same last message. In such a scenario the symbolic predicates generated over the message sequence generally will not detect attacks that use other message sequences to reach the same vulnerability. The claimed subject matter in contrast can cope with these challenges much more easily with knowledge of data formats.

As depicted attack detector can be a standalone machine that includes a processor. Illustrative machines that can constitute attack detector can include Personal Digital Assistants PDAs cell phones smart phones laptop computers notebook computers multimedia recording and or playback devices consumer devices appliances hand held devices desktop computers server computing devices etc. Additionally and or alternatively attack detector can be incorporated as a component within a pre existing machine for example a dedicated server computing device. As indicated above attack detector can be in continuous and operative or sporadic but intermittent communication via network topology with network security device and malicious device .

Network topology can include any viable communication technology for example wired and or wireless modalities and or technologies can be employed to effectuate the subject matter as claimed. Moreover network topology can include utilization of Personal Area Networks PANs Local Area Networks LANs Campus Area Networks CANs Metropolitan Area Networks MANs extranets intranets the Internet Wide Area Networks WANs both centralized and distributed and or any combination permutation and or aggregation thereof. Additionally and or alternatively network topology can employ power electricity line communications wherein power distribution wires are utilized for both the simultaneous distribution of data as well as transmission of power electricity .

Network security device can like attack detector be a standalone machine that includes a processor such as laptop computers notebook computers consumer devices appliances hand held devices desktop computers server class computing devices and the like. Additionally and or alternatively network security device can be incorporated as a component within a pre existing machine for example a dedicated desktop personal computer. Accordingly network security device can be effectuated in both hardware and or software and can be configured to permit deny or proxy data through a computer network e.g. network topology that has different levels of trust. The basic utility of network security device can be to transfer traffic between computer networks of different trust levels for instance. Typical examples of networks of different trust levels are the Internet which can be a zone with no trust and internal networks e.g. behind beyond or within internal external barrier which can be zones of higher trust. Network security device can thus be employed to prevent network intrusion to a private and or internal network. In order to effectuate and accomplish its objectives network security device can for example employ network layer and or packet filtering allowing packets through network security device only where they match established rulesets. Further network security device can also employ application layer filters wherein all packets traveling to or from applications e.g. File Transfer Protocol FTP Web Browsers Domain Name Service DNS telnet and the like can be intercepted and certain of these intercepted packets blocked e.g. dropping them without acknowledgement to the originator . Additionally and or alternatively network security device can be effectuated as a proxy device responding to input packets e.g. correction requests and the like in the manner of an application whilst blocking other packets. Proxies make tampering with an internal system from an external network more difficult and misuse of one internal system would not necessarily cause a security breach exploitable from outside network security device .

Very briefly malicious device as depicted like attack detector and similar to network security device can be any standalone machine inclusive of a processing means. Accordingly malicious device can include cell phones smart phones laptop computers Personal Digital Assistants PDAs notebook computers multimedia recording and or playback devices consumer devices appliances hand held devices desktop computers server class computing devices and the like. Alternatively and or additionally malicious device can be incorporated in a pre existing machine for example a Personal Digital Assistant PDA .

Attack detector in accordance with one illustrative aspect can be based on dynamic data flow analysis and can implement three vulnerability conditions. Attack detector can test for arbitrary execution control AEC whether input data is about to be moved into the instruction pointer. Such a test detects attempts to overwrite return addresses on the stack or function pointers on the stack or heap. Attack detector can also assay for arbitrary code execution ACE e.g. before executing instructions the detector tests whether instructions depend on a program s input which detects attempts to execute injected code. Further attack detector can also test for arbitrary function arguments AFA wherein prior to performing certain critical system calls for instance creating a process the detector can check whether certain critical arguments depend on a programs input. Attack detector thus has a low rate of false positives. False positives can accordingly be eliminated completely at the expense of higher rates of false negatives by means of verification processes procedures.

Attack detector in addition to issuing an alert can also provide detailed information about the exploit and the vulnerability. Such detailed information can include the complete data flow history and application state at the moment the vulnerability was detected. Attack detector can also output the positions within the input stream of the values that triggered the alert. For example in the case of an arbitrary execution control AEC condition these can be the bytes that were about to be loaded into the instruction pointer. In the case of an arbitrary code execution ACE condition these can be the bytes that were about to be executed. Furthermore attack detector can output information about the instruction that triggered the alert. For instance the detector can output whether or not an arbitrary execution control ACE alert was triggered while executing a ret instruction overwritten return address or indirect jmp or call instructions overwritten function pointer .

As illustrated attack detector can include an interface component herein after referred to as interface that can receive input in the form of a data stream. The input or data stream obtained or received can relate to suspicious data and can be obtained from crash dumps or from honeyfarms. Interface can thereafter output with high confidence a data patch vulnerability signature e.g. whether or not the suspicious data received as input contained exploits that can be generated by filter component . Additionally and or alternatively interface can receive data from a multitude of other sources such as client applications services users clients devices and or entities involved with a particular transaction a portion of a transaction and thereafter convey the received information to filter component for further analysis.

Interface can provide various adapters connectors channels communication pathways etc. to integrate the various components included in system into virtually any operating system and or database system and or with one another. Additionally interface can provide various adapters connectors channels communication modalities etc. that provide for interaction with various components that can comprise system and or any other component external and or internal data and the like associated with system .

Filter component can instrument applications that are to be monitored and can track how its input e.g. network packets and or files propagate in that application s address space as the application executes. Filter component can utilize this information to test for a wide range of vulnerability conditions. For instance before executing ret instructions determining whether or not input data has propagated into return addresses on the stack. A small set of vulnerabilities of this type can be sufficient to provide broad coverage for low level control and or data flow vulnerabilities such as for example buffer overflows arbitrary vulnerabilities that can result in code injection or overwriting of function pointers or return to libc style attacks.

Filter component can test for impending movement of input data into instruction pointers thereby detecting attempts to overwrite return addresses on a stack or function pointers on a stack or heap. Further filter component can before executing an instruction test whether or not the instruction depends on the program s input. Such analysis can detect attempts to execute injected code. Additionally filter component can before performing critical system calls e.g. creating a process check whether certain critical arguments depend on the program s input.

In addition to issuing an alert filter component can also provide detailed information about the exploit and the vulnerability. This can include a complete data flow history and the application state at the moment the vulnerability was detected. Filter component can thereafter output the positions within the input stream of the values that triggered the alert. For example filter component in the case of arbitrary execution control AEC conditions can output bytes that were about to be loaded into the instruction pointer. As a further example filter component in the case of arbitrary code execution ACE conditions can output bytes that were about to be executed. Moreover filter component can output information about the instruction that triggered the alert. For instance filter component can output whether an arbitrary execution control AEC alert was triggered while executing a ret instruction e.g. overwritten return address or an indirect jmp or call instruction e.g. overwritten function pointer .

In order to facilitate the foregoing filter component can utilize and or include data analyzer and probe generator and analyzer . Data analyzer can parse data according to a data format specification giving semantics and structure to the raw data. Data analyzer can operate either online or off line for example. For instance when data analyzer operates online it can serve as the main mechanism in data filters to prevent network or file based intrusions in so doing data analyzer can perform vulnerability driven filtering of application level protocol traffic and or prevent file based attacks that exploit file parsing vulnerabilities.

Data analyzer in order to parse data efficiently and effectively can utilize a data format specification that specifies a protocol format using a domain specific language. Recent work has advocated specifying protocol formats using domain specific languages and then using a generic protocol analyzer runtime to parse protocol traffic according to the specification. These domain specific languages typically are able to map the protocol structure over the raw network data parsing packets into messages that contain various fields as expressed in the specification and extracting the protocol state information based on the sequence of messages already parsed. Accordingly data analyzer can employ a data format specification that specifies a protocol format using a domain specific language such as a Generic Application level Protocol Analyzer GAPA language to specify protocol formats and for analysis.

A Generic Application level Protocol Analyzer GAPA language specification e.g. a Spec can specify message format protocol state machine and message handlers of which there is one per protocol state and carries out protocol state transitions. The message format can be expressed in an enhanced Backus Naur Form BNF format similar to those utilized in Request for Comments RFCs . Run time values of earlier parts of a message can determine how later parts of the message are parsed. For instance a size field before an item array can indicate the number of items to parse at runtime a field value in one message can determine how to parse parts of a subsequent message. Such context sensitive characteristics of data formats are well supported in the Generic Application level Protocol Analyzer GAPA language through enhancing the Backus Naur Form BNF notation and embedding code in the Backus Naur Form BNF rules. Nevertheless as will be understood by those of ordinary skill in the art other data format specification languages e.g. yacc lex bison and the like can equally be utilized without departing from the intent spirit and or ambit of the claimed subject matter and as such all such variants and permutations are deemed to fall within the purview of the subject matter as claimed.

It should be noted at this stage that since the data patch generated by the claimed subject matter is a refinement of the data format specification that is to be fed into a data analyzer of a network security device e.g. network security device or an antivirus program for patch equivalent protection the refinement can include a vulnerability predicate on fields of the input. If the predicate evaluates to be true the input can be classified as an exploit and removed. In a Generic Application level Protocol Analyzer GAPA language the predicate can be expressed as a condition in an if statement that can be inserted into an appropriate data handler. For network data the data handler can be the message handler at the protocol state at which attacks can happen. To adapt a Generic Application level Protocol Analyzer GAPA language for file data the entire file can be treated as one message the protocol state machine specification can contain just one state and there can be just one data handler.

Data analyzer can thus check whether input e.g. an attack instance violates data format constraints e.g. the number of bytes in a byte array must correspond to the value of its size field . Where data analyzer uncovers violated constraints such constraints can be conveyed to probe generator and analyzer .

Probe generator and analyzer can obtain violated constraints from data analyzer and thereafter can construct probes that satisfy the constraint e.g. changing the size field to correspond to the size of the byte array in the original attack packet . The constructed probe can thereafter be fed back to the attack detector for example via a feedback facility to provide a predictive mechanism whereupon a determination as to the constructed probe s viability to exploit vulnerabilities can be ascertained. If the constructed probe is reported unsuccessful at this stage namely the constructed probe failed to exploit vulnerabilities the generated data patch produced by attack detector can simply be the data format specification that enforces the corresponding data format constraint. If on the other hand the probe is successful an attack predicate can be generated. Typically an attack predicate can be a conjunction of boolean conditions with each data field of the message used in the attack equaling the value of the attack input.

Of course this predicate generally incurs no false positives in attack detection but nevertheless can be restrictive and generally admits only one attack input as its protocol state. Accordingly conditions specific to the original attack input can be relaxed by probe generator and analyzer so that more attack variants can be admitted. This can be accomplished by probe generator and analyzer generating probes potential attack variants based on the data format and forwarding these to attack detector via a predictive feedback facility e.g. a facility that based at least in part on the potential attack variants generates predictions as to whether attack variants pose a threat . Thus if the predictive feedback facility associated with attack detector determines that a generated probe is a new attack variant probe generator and analyzer can adjust vulnerability predicates so that the confirmed new attack instance can be admitted as well. For example if all values of a data field have been tried and the predictive feedback facility associated with attack detector classifies all the corresponding probes as attacks then that particular field can be categorized as a don t care field and probe generator and analyzer can remove the condition on the data field from the predicate.

One challenge in probe generation can be that probe generator and analyzer must generate legitimate messages that satisfy protocol semantics including session semantics and cross field correlation in a message. For example the session ID field of all messages of the session should generally be the same. Some examples of field correlations within a message are length field sizeof all fields checksum field checksum all fields hash field hash another field . It is important that such semantics are obeyed for otherwise answers to illegitimate probes provided by the predictive feedback feature associated with attack detector can be misleading. It should be noted without unnecessary limitation to the claimed subject matter that probes can typically contain multiple protocol messages including those messages that precede the offending message for session initiation and application context setup.

As indicated earlier the efficiency metric of such a probing scheme can be the number of probes. Some probes can be parallelized while some need to be sequential namely construction of a later probe can be dependent on the outcome of an earlier probe. Accordingly given sufficient attack detectors e.g. multiple attack detectors parallel probes can be sped up by evaluating probes in parallel.

Since it is typically infeasible to burden the predictive feedback feature associated with attack detector with all combinations of values in each data field that appears in the input probe generator and analyzer can reduce the number of probes in the following ways.

First there may be iterative elements in the attack data such as a sequence of records which may be expressed as records record records nil or itemArray size unit8 items int byte in the data format specification. Such iterative elements can also introduce numerous repetitive data fields which may not all matter to vulnerability predicate construction e.g. it could be the case that just a particular iterative element triggers the vulnerability . Because probe generator and analyzer can recognize iterative elements based on the specification probe generator and analyzer can issue a probe that contains just one element to see whether the number of elements matters and then based on the answer provided by the predictive facility associated with attack detector probe generator and analyzer can issue subsequent probes. Being able to recognize iterative elements in attacks can significantly reduce the number of probes generated.

Second by obeying protocol semantics and eliminating illegitimate probes probe generator and analyzer can reduce the number of probes needed. It is possible that an expressed constraint may not actually be a constraint in an implementation. For example an application may allow a size constraint to be violated. Accordingly a probe can be constructed and used to determine whether such a constraint matters.

Additionally for network input that contains a sequence of messages it can be highly likely that the vulnerability predicate is only dependent on the last message. This is because vulnerabilities are often localized. Given that message handlers for each message are typically separate code pieces if an attack happens during the handling of the message the vulnerability likely affects the message handling for that message only. In this case having conditions on data fields of earlier messages will cause false negatives in the resulting vulnerability predicate which will not capture the attack variants that take a different message sequence. In other words it can be assumed that only the contents of the last message determines whether the input exploits the vulnerability and that the data analyzer naturally traces the protocol context before reaching the handler of the last message.

Each of protocol analyzer and or file analyzer can utilize protocol formats specified in domain specific languages and thereafter employ a generic protocol analyzer runtime to parse protocol traffic according to the specification. The domain specific language typically can specify message format protocol state machine and message handlers generally one per protocol state and carries out protocol state transitions. Message formats can be expressed in enhanced Backus Naur Form EBNF . Run time values of earlier parts of a message can determine how later parts of the message are to be parsed. For example the size field before an item array can be indicative of the number of items to parse at runtime a field value in one message can determine how to parse parts of a subsequent message.

As illustrated probe generator and analyzer can include probe constructor attack predicate generator buffer overrun component iteration removal component and field removal component . Probe constructor can construct probes that satisfy a particular constraint. For instance probe constructor can change a size field to correspond to the size of the byte array in an original attack packet. Constructed probes generated by probe constructor can be fed back to a prognosticative mechanism affiliated with attack detector so that determinations can be made as to whether or not constructed probes viably exploit vulnerabilities. Where a constructed probe is found not to practicably exploit vulnerabilities data patches and or vulnerability signatures generated by attack detector can simply include a data format specification that enforces a corresponding data format constraint. Conversely where the probe is found to feasibly exploit vulnerabilities an attack predicate can be generated by attack predicate generator .

Attack predicate generator typically can generate an attack predicate that is a conjunction of boolean conditions with each data field of the message utilized in the putative attack equaling the value of attack input. Attack predicates can typically be generated from the attack input itself together with its data format. As has been noted earlier attack predicates so generated typically do not incur false positives in attack detection. Consequently such generated attack predicates can be very restrictive generally admitting but one attack input as its protocol state. In order to counter this attack predicate generator can attempt to relax conditions specific to the original attack input so that more attack variants can be admitted. Such a relaxation feature can be accomplished by generating probes potential attack variants based at least in part on data formats and directing these to a predicative feedback mechanism associated with attack detector . Accordingly if the predicative feedback feature associated with attack detector ascertains that a generated probe can be a new attack variant attack predicate generator can adjust vulnerability predicates so that confirmed new attack variants can be admitted as well. For instance where all values of a data field have been tried and the prognosticative feature affiliated with attack detector categorizes all corresponding probes as attacks then that particular data field can be flagged as a don t care field and attack predicate detector can remove the condition on the data field from the predicate.

Buffer overrun component can be utilized in conjunction with probe constructor and or attack predicate generator to find out whether buffer overrun vulnerabilities occur. In order to make this determination buffer overrun component can utilize a heuristic wherein if an offending byte in the input lies in the middle of a byte or unicode string then buffer overrun component can indicate a buffer overrun condition and can for example add the following condition as a refinement 

The rationale for the utilization for the foregoing heuristic and why it is accurate depends on the type of alert generated. In the case of arbitrary code execution ACE alerts if the application were about to execute a ret instruction when the alert is issued then a return address on the stack can be corrupted by input data. It is almost certain that the cause of this condition will be a stack buffer overrun.

If on the other hand the application were about to execute an indirect jmp or call instruction then the corresponding function pointer can contain input data. Even if this does represent legitimate application behavior e.g. the application intends to use input data as a function pointer an accurate specification should declare this function pointer as a four byte integer and not as a substring of a byte or unicode string.

In the case of arbitrary execution control AEC alerts if the application was about to execute an instruction that originated in the middle of a byte or unicode string in the input. It appears highly likely that this was caused by a buffer overrun.

In the case of arbitrary function arguments AFA alerts the application was about to make a critical system call with a parameter that originated in the middle of a byte or unicode string in the input. If this was legitimate application behavior this parameter should appear as a separate data field in an accurate specification. In this case the parameter would not lie in the middle of a larger string field. Again this is a strong indication of a buffer overrun.

Probe generator analyzer can also include iteration removal component . Many popular input formats include arbitrary sequences of largely independent elements records . For example HTML files contain sequences of tags. Most audio and video files consist of sequences or chunks compressed audio or video data. Windows metafile WMF files can consist of a sequence of drawing command records. In all cases the file contains a sequence of records. Rendering applications typically have different handler functions for different record types. For instance Windows metafile WMF record types can include Ellipse Rectangle or Escape. Each record type can be processed by a corresponding handler function. Vulnerabilities typically can be located in a particular handler function. In this case any input that contains a malicious record can be considered an attack irrespective of what other records may exist in the input. This can pose special challenges to execution trace based signature generation mechanisms.

In order to cope with such iterative elements more easily the claimed subject matter can utilize an iteration removal component that removes all iterative elements that are not necessary to exploit a vulnerability. This not only reduces the number of probes needed but also improves the accuracy of the generated signatures. At a high level the claimed subject matter generates probes from which some of the iterative elements have been removed e.g. by iteration removal component and thereafter the generated probes can be dispatched to a predictive facility associated with attack detector . If the generated probe still exploits the vulnerability then the iterative elements that were removed by iteration removal component can be omitted. Otherwise one or more of the iterative elements that had been removed can be added as they can be considered necessary.

In the theoretical worst case this procedure may not lead to any simplifications of the input namely if all iterative elements in the input are necessary to exploit a vulnerability. However for real world vulnerabilities the procedure can rapidly converge to a probe in which only one or two iterative elements are left.

In accordance with an illustrative aspect of the claimed subject matter iterative removal component can identify offending data in the input and thereafter map this information to a particular iterative element the offending element. For instance given the heuristic observation that typically only one or two iterative elements are necessary to exploit the vulnerability iterative removal component can generate a first probe by removing all iterative elements except for the offending element. If such a minimal probe does not succeed as an attack iterative elements that have been removed can be added back into the probe to make it functional for example. Iteration typically can be hierarchical namely an iterative element can have another iterative structure associated with it and so on. Accordingly iterative removal component can first add back all iterative elements from the original input at the lowest hierarchical level where the offending byte is located for instance. This probe for example can then be dispatched to the prognosis mechanism associated with attack detector . If the prognosis facility perceives the probe as an attack iterative removal component can attempt to eliminate iterative elements at the lowest level using a divide and conquer mechanism split all iterative elements at this level into N parts for some N 1 N denoting an integer for example. Iterative removal component can then construct N parallel probes with each probe dropping one of the N parts. If a probe is successful the associated iterative elements can be dropped otherwise iterative removal component can further divide the 1 N portion of the interactive elements N ways and send another round of parallel probes. Iterative removal component can repeat this until it finds iterative elements that must exist to make a probe functional.

If the offending byte does not belong to any iterative element or a successful attack has not been obtained after adding iterative elements at all levels around the offending byte then iterative removal component can commence adding iterative elements that do not contain the offending byte. In this case iterative removal component can take an approach similar to how elements containing the offending byte were added back. The difference being that iterative removal component approaches adding iterative elements that do not contain the offending byte in a top down manner starting from the highest hierarchical level. In accomplishing this iterative removal component can once again utilize a divide and conquer approach to find what elements at the highest level must be added back. Then for each element that must be added back iterative removal component can go down one level inside it and use the same mechanism to find out what elements in this lower level that must be added back. Iterative removal component can repeat this process until it reaches the lowest level.

Once iterative element removal has been accomplished by iterative removal component field removal component can eliminate don t care fields so that probes can be constructed over the remaining data fields and to locate additional values of the data fields for which an attack succeeds. Sending probes in a combinatoric fashion typically cannot be tolerated as this can result in an exponential number of probes. Instead field removal component can evaluate k tuples of fields where k is an integer greater than 0 setting values of all other fields to their respective values in the original attack input. In order to accomplish this field removal component can obey the data format semantics elucidated earlier with respect to probe construction.

As will be readily apparent to those of ordinary skill the heuristics employed by iteration removal component are provided as aids to exposition of the claimed subject matter and should not be construed as limiting or confining the subject matter as claimed. Accordingly other variations of the aforementioned heuristics can be employed with equal effect and as such any and all such variations permutations and or derivatives thereof will be considered to fall within the scope and intent of the claimed subject matter.

Field removal component can employ without limitation the following sampling heuristic to further reduce the number of probes for evaluating each field 1 For any base type data field such as an integer try its minimum value maximum value and sample some number of random values below and or above the input value. 2 For character strings generate a random string by taking a random non zero value for each byte and send several samples of such random strings. 3 For byte arrays utilize a random value for each byte rather than non zero values. When all samples for a field lead to probes that exploit a vulnerability field removal component can consider the field as a don t care field and remove it from the predicate.

At the end of this process probe generator and analyzer can be left with a set of input fields fand values for each fields V such that an attack can be triggered where value f Vfor all i. The output of probe generator and analyzer can be the conjunction of these conditions which can be considered to be an approximation of the vulnerability predicate. Nevertheless it is possible to have more complex vulnerability conditions that can involve complex calculations on multiple fields.

It is to be appreciated that store can be for example volatile memory or non volatile memory or can include both volatile and non volatile memory. By way of illustration and not limitation non volatile memory can include read only memory ROM programmable read only memory PROM electrically programmable read only memory EPROM electrically erasable programmable read only memory EEPROM or flash memory. Volatile memory can include random access memory RAM which can act as external cache memory. By way of illustration rather than limitation RAM is available in many forms such as static RAM SRAM dynamic RAM DRAM synchronous DRAM SDRAM double data rate SDRAM DDR SDRAM enhanced SDRAM ESDRAM Synchlink DRAM SLDRAM Rambus direct RAM RDRAM direct Rambus dynamic RAM DRDRAM and Rambus dynamic RAM RDRAM . Store of the subject systems and methods is intended to comprise without being limited to these and any other suitable types of memory. In addition it is to be appreciated that store can be a server a database a hard drive and the like.

The independent components may be used to further fill out or span an information space and the dependent components may be employed in combination to improve quality of common information recognizing that all sensor input data may be subject to error and or noise. In this context data fusion techniques employed by data fusion component may include algorithmic processing of sensor input data to compensate for inherent fragmentation of information because particular phenomena may not be observed directly using a single sensing input modality. Thus data fusion provides a suitable framework to facilitate condensing combining evaluating and or interpreting available sensed or received information in the context of a particular application.

Typically the mechanism such as a push button or the enter key on the keyboard can be employed subsequent to entering the information in order to initiate for example a query. However it is to be appreciated that the claimed subject matter is not so limited. For example nearly highlighting a checkbox can initiate information conveyance. In another example a command line interface can be employed. For example the command line interface can prompt e.g. via text message on a display and an audio tone the user for information via a text message. The user can then provide suitable information such as alphanumeric input corresponding to an option provided in the interface prompt or an answer to a question posed in the prompt. It is to be appreciated that the command line interface can be employed in connection with a graphical user interface and or application programming interface API . In addition the command line interface can be employed in connection with hardware e.g. video cards and or displays e.g. black and white and EGA with limited graphic support and or low bandwidth communication channels.

In view of the exemplary systems shown and described supra methodologies that may be implemented in accordance with the disclosed subject matter will be better appreciated with reference to the flow chart of . While for purposes of simplicity of explanation the methodologies are shown and described as a series of blocks it is to be understood and appreciated that the claimed subject matter is not limited by the order of the blocks as some blocks may occur in different orders and or concurrently with other blocks from what is depicted and described herein. Moreover not all illustrated blocks may be required to implement the methodologies described hereinafter. Additionally it should be further appreciated that the methodologies disclosed hereinafter and throughout this specification are capable of being stored on an article of manufacture to facilitate transporting and transferring such methodologies to computers.

The claimed subject matter can be described in the general context of computer executable instructions such as program modules executed by one or more components. Generally program modules can include routines programs objects data structures etc. that perform particular tasks or implement particular abstract data types. Typically the functionality of the program modules may be combined and or distributed as desired in various aspects.

It should be noted that method outlined above provides but one illustrative instance that can be employed to effectuate the claimed subject matter. As will be readily appreciated by those cognizant in this field of endeavor the claimed subject matter is not necessarily so limited as a subset of the acts or a disparate ordering of the acts elucidated therein can be undertaken without departing from the intent and scope of the subject matter as claimed. Accordingly any and all such modifications derivations combinations and or permutations are deemed to fall within the ambit and intent of the claimed subject matter.

The claimed subject matter can be implemented via object oriented programming techniques. For example each component of the system can be an object in a software routine or a component within an object. Object oriented programming shifts the emphasis of software development away from function decomposition and towards the recognition of units of software called objects which encapsulate both data and functions. Object Oriented Programming OOP objects are software entities comprising data structures and operations on data. Together these elements enable objects to model virtually any real world entity in terms of its characteristics represented by its data elements and its behavior represented by its data manipulation functions. In this way objects can model concrete things like people and computers and they can model abstract concepts like numbers or geometrical concepts.

The benefit of object technology arises out of three basic principles encapsulation polymorphism and inheritance. Objects hide or encapsulate the internal structure of their data and the algorithms by which their functions work. Instead of exposing these implementation details objects present interfaces that represent their abstractions cleanly with no extraneous information. Polymorphism takes encapsulation one step further the idea being many shapes one interface. A software component can make a request of another component without knowing exactly what that component is. The component that receives the request interprets it and figures out according to its variables and data how to execute the request. The third principle is inheritance which allows developers to reuse pre existing design and code. This capability allows developers to avoid creating software from scratch. Rather through inheritance developers derive subclasses that inherit behaviors that the developer then customizes to meet particular needs.

In particular an object includes and is characterized by a set of data e.g. attributes and a set of operations e.g. methods that can operate on the data. Generally an object s data is ideally changed only through the operation of the object s methods. Methods in an object are invoked by passing a message to the object e.g. message passing . The message specifies a method name and an argument list. When the object receives the message code associated with the named method is executed with the formal parameters of the method bound to the corresponding values in the argument list. Methods and message passing in OOP are analogous to procedures and procedure calls in procedure oriented software environments.

However while procedures operate to modify and return passed parameters methods operate to modify the internal state of the associated objects by modifying the data contained therein . The combination of data and methods in objects is called encapsulation. Encapsulation provides for the state of an object to only be changed by well defined methods associated with the object. When the behavior of an object is confined to such well defined locations and interfaces changes e.g. code modifications in the object will have minimal impact on the other objects and elements in the system.

Each object is an instance of some class. A class includes a set of data attributes plus a set of allowable operations e.g. methods on the data attributes. As mentioned above OOP supports inheritance a class called a subclass may be derived from another class called a base class parent class etc. where the subclass inherits the data attributes and methods of the base class. The subclass may specialize the base class by adding code which overrides the data and or methods of the base class or which adds new data attributes and methods. Thus inheritance represents a mechanism by which abstractions are made increasingly concrete as subclasses are created for greater levels of specialization.

As used in this application the terms component and system are intended to refer to a computer related entity either hardware a combination of hardware and software software or software in execution. For example a component can be but is not limited to being a process running on a processor a processor a hard disk drive multiple storage drives of optical and or magnetic storage medium an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a server and the server can be a component. One or more components can reside within a process and or thread of execution and a component can be localized on one computer and or distributed between two or more computers.

Artificial intelligence based systems e.g. explicitly and or implicitly trained classifiers can be employed in connection with performing inference and or probabilistic determinations and or statistical based determinations as in accordance with one or more aspects of the claimed subject matter as described hereinafter. As used herein the term inference infer or variations in form thereof refers generally to the process of reasoning about or inferring states of the system environment and or user from a set of observations as captured via events and or data. Inference can be employed to identify a specific context or action or can generate a probability distribution over states for example. The inference can be probabilistic that is the computation of a probability distribution over states of interest based on a consideration of data and events. Inference can also refer to techniques employed for composing higher level events from a set of events and or data. Such inference results in the construction of new events or actions from a set of observed events and or stored event data whether or not the events are correlated in close temporal proximity and whether the events and data come from one or several event and data sources. Various classification schemes and or systems e.g. support vector machines neural networks expert systems Bayesian belief networks fuzzy logic data fusion engines . . . can be employed in connection with performing automatic and or inferred action in connection with the claimed subject matter.

Furthermore all or portions of the claimed subject matter may be implemented as a system method apparatus or article of manufacture using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device or media. For example computer readable media can include but are not limited to magnetic storage devices e.g. hard disk floppy disk magnetic strips . . . optical disks e.g. compact disk CD digital versatile disk DVD . . . smart cards and flash memory devices e.g. card stick key drive . . . . Additionally it should be appreciated that a carrier wave can be employed to carry computer readable electronic data such as those used in transmitting and receiving electronic mail or in accessing a network such as the Internet or a local area network LAN . Of course those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.

Some portions of the detailed description have been presented in terms of algorithms and or symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and or representations are the means employed by those cognizant in the art to most effectively convey the substance of their work to others equally skilled. An algorithm is here generally conceived to be a self consistent sequence of acts leading to a desired result. The acts are those requiring physical manipulations of physical quantities. Typically though not necessarily these quantities take the form of electrical and or magnetic signals capable of being stored transferred combined compared and or otherwise manipulated.

It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like. It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the foregoing discussion it is appreciated that throughout the disclosed subject matter discussions utilizing terms such as processing computing calculating determining and or displaying and the like refer to the action and processes of computer systems and or similar consumer and or industrial electronic devices and or machines that manipulate and or transform data represented as physical electrical and or electronic quantities within the computer s and or machine s registers and memories into other data similarly represented as physical quantities within the machine and or computer system memories or registers or other such information storage transmission and or display devices.

Referring now to there is illustrated a block diagram of a computer operable to execute the disclosed system. In order to provide additional context for various aspects thereof and the following discussion are intended to provide a brief general description of a suitable computing environment in which the various aspects of the claimed subject matter can be implemented. While the description above is in the general context of computer executable instructions that may run on one or more computers those skilled in the art will recognize that the subject matter as claimed also can be implemented in combination with other program modules and or as a combination of hardware and software.

Generally program modules include routines programs components data structures etc. that perform particular tasks or implement particular abstract data types. Moreover those skilled in the art will appreciate that the inventive methods can be practiced with other computer system configurations including single processor or multiprocessor computer systems minicomputers mainframe computers as well as personal computers hand held computing devices microprocessor based or programmable consumer electronics and the like each of which can be operatively coupled to one or more associated devices.

The illustrated aspects of the claimed subject matter may also be practiced in distributed computing environments where certain tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules can be located in both local and remote memory storage devices.

A computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by the computer and includes both volatile and non volatile media removable and non removable media. By way of example and not limitation computer readable media can comprise computer storage media and communication media. Computer storage media includes both volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital video disk DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the computer.

With reference again to the exemplary environment for implementing various aspects includes a computer the computer including a processing unit a system memory and a system bus . The system bus couples system components including but not limited to the system memory to the processing unit . The processing unit can be any of various commercially available processors. Dual microprocessors and other multi processor architectures may also be employed as the processing unit .

The system bus can be any of several types of bus structure that may further interconnect to a memory bus with or without a memory controller a peripheral bus and a local bus using any of a variety of commercially available bus architectures. The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS is stored in a non volatile memory such as ROM EPROM EEPROM which BIOS contains the basic routines that help to transfer information between elements within the computer such as during start up. The RAM can also include a high speed RAM such as static RAM for caching data.

The computer further includes an internal hard disk drive HDD e.g. EIDE SATA which internal hard disk drive may also be configured for external use in a suitable chassis not shown a magnetic floppy disk drive FDD e.g. to read from or write to a removable diskette and an optical disk drive e.g. reading a CD ROM disk or to read from or write to other high capacity optical media such as the DVD . The hard disk drive magnetic disk drive and optical disk drive can be connected to the system bus by a hard disk drive interface a magnetic disk drive interface and an optical drive interface respectively. The interface for external drive implementations includes at least one or both of Universal Serial Bus USB and IEEE 1394 interface technologies. Other external drive connection technologies are within contemplation of the claimed subject matter.

The drives and their associated computer readable media provide nonvolatile storage of data data structures computer executable instructions and so forth. For the computer the drives and media accommodate the storage of any data in a suitable digital format. Although the description of computer readable media above refers to a HDD a removable magnetic diskette and a removable optical media such as a CD or DVD it should be appreciated by those skilled in the art that other types of media which are readable by a computer such as zip drives magnetic cassettes flash memory cards cartridges and the like may also be used in the exemplary operating environment and further that any such media may contain computer executable instructions for performing the methods of the disclosed and claimed subject matter.

A number of program modules can be stored in the drives and RAM including an operating system one or more application programs other program modules and program data . All or portions of the operating system applications modules and or data can also be cached in the RAM . It is to be appreciated that the claimed subject matter can be implemented with various commercially available operating systems or combinations of operating systems.

A user can enter commands and information into the computer through one or more wired wireless input devices e.g. a keyboard and a pointing device such as a mouse . Other input devices not shown may include a microphone an IR remote control a joystick a game pad a stylus pen touch screen or the like. These and other input devices are often connected to the processing unit through an input device interface that is coupled to the system bus but can be connected by other interfaces such as a parallel port an IEEE 1394 serial port a game port a USB port an IR interface etc.

A monitor or other type of display device is also connected to the system bus via an interface such as a video adapter . In addition to the monitor a computer typically includes other peripheral output devices not shown such as speakers printers etc.

The computer may operate in a networked environment using logical connections via wired and or wireless communications to one or more remote computers such as a remote computer s . The remote computer s can be a workstation a server computer a router a personal computer portable computer microprocessor based entertainment appliance a peer device or other common network node and typically includes many or all of the elements described relative to the computer although for purposes of brevity only a memory storage device is illustrated. The logical connections depicted include wired wireless connectivity to a local area network LAN and or larger networks e.g. a wide area network WAN . Such LAN and WAN networking environments are commonplace in offices and companies and facilitate enterprise wide computer networks such as intranets all of which may connect to a global communications network e.g. the Internet.

When used in a LAN networking environment the computer is connected to the local network through a wired and or wireless communication network interface or adapter . The adaptor may facilitate wired or wireless communication to the LAN which may also include a wireless access point disposed thereon for communicating with the wireless adaptor .

When used in a WAN networking environment the computer can include a modem or is connected to a communications server on the WAN or has other means for establishing communications over the WAN such as by way of the Internet. The modem which can be internal or external and a wired or wireless device is connected to the system bus via the serial port interface . In a networked environment program modules depicted relative to the computer or portions thereof can be stored in the remote memory storage device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers can be used.

The computer is operable to communicate with any wireless devices or entities operatively disposed in wireless communication e.g. a printer scanner desktop and or portable computer portable data assistant communications satellite any piece of equipment or location associated with a wirelessly detectable tag e.g. a kiosk news stand restroom and telephone. This includes at least Wi Fi and Bluetooth wireless technologies. Thus the communication can be a predefined structure as with a conventional network or simply an ad hoc communication between at least two devices.

Wi Fi or Wireless Fidelity allows connection to the Internet from a couch at home a bed in a hotel room or a conference room at work without wires. Wi Fi is a wireless technology similar to that used in a cell phone that enables such devices e.g. computers to send and receive data indoors and out anywhere within the range of a base station. Wi Fi networks use radio technologies called IEEE 802.11x a b g etc. to provide secure reliable fast wireless connectivity. A Wi Fi network can be used to connect computers to each other to the Internet and to wired networks which use IEEE 802.3 or Ethernet .

Wi Fi networks can operate in the unlicensed 2.4 and 5 GHz radio bands. IEEE 802.11 applies to generally to wireless LANs and provides 1 or 2 Mbps transmission in the 2.4 GHz band using either frequency hopping spread spectrum FHSS or direct sequence spread spectrum DSSS . IEEE 802.11a is an extension to IEEE 802.11 that applies to wireless LANs and provides up to 54 Mbps in the 5 GHz band. IEEE 802.11a uses an orthogonal frequency division multiplexing OFDM encoding scheme rather than FHSS or DSSS. IEEE 802.11b also referred to as 802.11 High Rate DSSS or Wi Fi is an extension to 802.11 that applies to wireless LANs and provides 10 Mbps transmission with a fallback to 5.5 2 and 1 Mbps in the 2.4 GHz band. IEEE 802.11g applies to wireless LANs and provides 20 Mbps in the 2.4 GHz band. Products can contain more than one band e.g. dual band so the networks can provide real world performance similar to the basic 10BaseT wired Ethernet networks used in many offices.

Referring now to there is illustrated a schematic block diagram of an exemplary computing environment for processing the disclosed architecture in accordance with another aspect. The system includes one or more client s . The client s can be hardware and or software e.g. threads processes computing devices . The client s can house cookie s and or associated contextual information by employing the claimed subject matter for example.

The system also includes one or more server s . The server s can also be hardware and or software e.g. threads processes computing devices . The servers can house threads to perform transformations by employing the claimed subject matter for example. One possible communication between a client and a server can be in the form of a data packet adapted to be transmitted between two or more computer processes. The data packet may include a cookie and or associated contextual information for example. The system includes a communication framework e.g. a global communication network such as the Internet that can be employed to facilitate communications between the client s and the server s .

Communications can be facilitated via a wired including optical fiber and or wireless technology. The client s are operatively connected to one or more client data store s that can be employed to store information local to the client s e.g. cookie s and or associated contextual information . Similarly the server s are operatively connected to one or more server data store s that can be employed to store information local to the servers .

What has been described above includes examples of the disclosed and claimed subject matter. It is of course not possible to describe every conceivable combination of components and or methodologies but one of ordinary skill in the art may recognize that many further combinations and permutations are possible. Accordingly the claimed subject matter is intended to embrace all such alterations modifications and variations that fall within the spirit and scope of the appended claims. Furthermore to the extent that the term includes is used in either the detailed description or the claims such term is intended to be inclusive in a manner similar to the term comprising as comprising is interpreted when employed as a transitional word in a claim.

