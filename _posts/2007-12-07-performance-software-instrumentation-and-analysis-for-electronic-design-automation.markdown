---

title: Performance software instrumentation and analysis for electronic design automation
abstract: Various methods and apparatuses are described that provide instrumentation and analysis of an electronic design. A method for providing performance instrumentation and analysis of the electronic design includes defining a first and second set of intended software instrumentation test points and an associated first and second set of performance analysis units. The method further includes instrumenting the first and second sets of software instrumentation test points and the associated first and second sets of performance analysis units to a first model and a second model, respectively. The method further includes creating a first and a second set of software instances associated with the first and second sets of intended software instrumentation test points and associated sets of performance analysis units during run time of a first simulation and a second simulation of the electronic design associated with the first model and second model, respectively.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08229723&OS=08229723&RS=08229723
owner: Sonics, Inc.
number: 08229723
owner_city: Milpitas
owner_country: US
publication_date: 20071207
---
A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the software engine and its modules as it appears in the Patent and Trademark Office Patent file or records but otherwise reserves all copyright rights whatsoever.

Embodiments of the invention generally relate to electronic design automation. An aspect is related to performance software instrumentation and analysis of the electronic design at different levels of abstraction.

High level system description methodologies can allow designers and system architects to modify the traditional system design approach. Some languages such as SystemC may allow system architects to model a circuit or system at a transaction level in order to test the specified functionality and get early performance reports.

SystemC is a set of library routines and macros implemented in C which makes it possible to simulate concurrent processes each described by C syntax. Software objects instantiated in the SystemC framework may communicate in a simulated real time environment using signals of all the datatypes offered by C some additional ones offered by the SystemC library as well as user defined.

The traditional design cycle for an integrated circuit still follows or often runs concurrently with this architectural modeling. The design will be written in a hardware description level usually at RTL register transfer level to be synthesized and finalized into silicon.

Some prior approaches are not able to quickly and efficiently monitor and analyze a circuit or system performance early in the design cycle resulting in a longer design cycle and slower time to market for the design.

Various methods and apparatuses are described for an electronic design automation EDA modeling tool that includes a definition module that defines a first full set of possible software instrumentation test points for a simulation of an electronic system being modeled with a first executable behavioral model coded at a first level of abstraction. The definition module also defines a second full set of possible software instrumentation tests for a simulation of the electronic system being modeled with a second executable behavioral model coded at a second level of abstraction.

The EDA modeling tool also includes a population module that populates both instances of the first set of possible software instrumentation test points in the first model and instances of the second set of possible software instrumentation test points in the second model. A performance instrumentation platform then creates a first set of software objects associated with a subset of the first set of possible software instrumentation test points that are enabled by the user for the run of the first simulation as well as create a second set of software objects associated with a subset of the second set of possible software instrumentation test points that are enabled by the user for the run of the second simulation.

The second full set of possible software instrumentation test points is configured to capture similar measurement quantities as the first full set of possible software instrumentation test points. The modeling tool provides users with performance instrumentation in order to analyze and debug their electronic designs for all of the different phases of their product development. For example a comparison module may compare the similar measurement quantities between the first and second enabled subsets of instrumentation points to discern differences in the executable behavioral models coded at different levels of abstraction.

While the invention is subject to various modifications and alternative forms specific embodiments thereof have been shown by way of example in the drawings and will herein be described in detail. The invention should be understood to not be limited to the particular forms disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the invention.

In the following description numerous specific details are set forth such as examples of specific data signals named components connections etc. in order to provide a thorough understanding of the present invention. It will be apparent however to one of ordinary skill in the art that the present invention may be practiced without these specific details. In other instances well known components or methods have not been described in detail but rather in a block diagram in order to avoid unnecessarily obscuring the present invention. Further specific numeric references such as a first instance of a transactor may be made. However the specific numeric reference should not be interpreted as a literal sequential order but rather interpreted that the first instance of a transactor is different than a second instance of a transactor. Thus the specific details set forth are merely exemplary. The specific details may be varied from and still be contemplated to be within the spirit and scope of the present invention. In general various methods and apparatuses are described for an EDA modeling tool that includes a definition module that defines a first full set of every possible software instrumentation test point for a simulation of an electronic system. The electronic system is typically modeled with a first executable behavioral model coded at a first level of abstraction such as coded at RTL. The definition module also defines a second full set of every possible software instrumentation test point for a simulation of the electronic system being modeled with a second executable behavioral model coded at a different level of abstraction than the first level of abstraction.

The EDA modeling tool also includes a population module that populates both instances of a first subset of the possible software instrumentation test points in the first executable behavioral model and instances of a second subset of the possible software instrumentation test points in the second executable behavioral model. A performance instrumentation platform then creates a first set of software objects associated with one or more of the first subset of possible software instrumentation test points that are actually selected by a user to be enabled for a run of the first simulation. The performance instrumentation platform then also creates a second set of software objects associated with one or more of the second subset of possible software instrumentation test points that are selected by the user to be enabled for a run of the second simulation.

The second subset of possible software instrumentation test points is configured to capture similar measurement quantities as the first subset of possible software instrumentation test points. The modeling tool provides users with performance instrumentation in order to analyze and debug their electronic designs for all of the different phases of their product development.

An architectural phase of the design of the electronic system can be optimized with a high level SystemC model that is instrumented with software instrumentation test points and performance analysis units that analyze data collected by the test points. A low level RTL model can collect data and provide analysis based on using software instrumentation test points and performance analysis units that analyze data collected by the test points. In one embodiment these test points and performance analysis units correspond to those of the SystemC model. Collected data and analysis of the two models e.g. RTL and SystemC can then be compared to determine performance of the electronic system. The modeling tool provides consistent instrumentation and a common post processing infrastructure to allow easy comparisons between different models e.g. SystemC RTL silicon . For example the first executable behavioral model having the first level of abstraction may be the SystemC model and the second executable behavioral model having the second level of abstraction may be the RTL model.

As discussed many electronic designs are written at a register transfer level RTL description of a circuit or system in a hardware description language. Generally a RTL description describes what intermediate information i.e. information stored between clock cycles in an electronic circuit is stored in registers where it is stored within the design and how that information moves through the design as it operates. The RTL description of an electronic design is a functional description at least one level of abstraction higher than the individual gate layout of the electronic system e.g. gate level implementation Netlist . The RTL description fully and explicitly describes virtually all of the logic and sequential operations of the circuit. RTL descriptions are commonly written in standard languages such as Verilog or VHDL and are intended for logic synthesis and mapping with a commercial logic synthesizer.

A transaction level model can be a software coded model of a circuit or a system where data transfers are simplified and abstracted. This model is typically written in a high level software language such as C C or SystemC. RTL may be considered Transaction Level 0 TL0 and correspond directly with signals in the open core protocol specification. Transaction Level 1 TL1 modeling may correspond to a given protocol s phase level on an individual transfer basis and be cycle accurate. Transaction Level 2 TL2 modeling may correspond to one or more open core protocol transfers or transactions at the same time and can be transaction accurate but cycle approximate. For example if a system implements burst data transfers between components this burst may be expressed using a single function call in the implementation language. The difference in the number of application programming interface calls or events can cause timing inaccuracies between these different levels of abstraction.

A transaction can be a functional unit of data transfer through a system. A transaction may be an aggregate of multiple data transfers but is tied by a semantic unity. For example a transaction may merely be group transfers at consecutive addresses between the same two components of the system.

The definition module may define the first set of possible software instrumentation test points prior to run time or at run time of the first simulation of an electronic system being modeled with the first executable behavioral model coded at the first level of abstraction. The definition module also then defines a second set of intended software instrumentation test points via the creation layer prior to run time or at run time of a second simulation of the electronic system being modeled with a second executable behavioral model coded at the second level of abstraction. The definition module includes plug in programs for interfacing with the definition module in order to add things such as additional definitions of possible instrumentation test points .

The population module populates instances of a first subset of possible software instrumentation test points in the first executable behavioral model as well as populates instances of a second subset of possible software instrumentation test points in the second executable behavioral model . For example the population module generates instances through of the first subset of possible software instrumentation test points containing two of the three possible defined instrumentation test points in this example. The population module populates the generated instances of the first subset of possible software instrumentation test points through in the RTL executable behavioral model . The population module populates similar generated instances of a second subset of possible software instrumentation test points through in the SystemC executable behavioral model . The performance instrumentation platform then creates a first set of software objects associated with one or more of the first subset of possible software instrumentation test points that are selected by the user to be enabled for the run of the simulation of an instance of the RTL executable behavioral model . The performance instrumentation platform also creates a second set of software objects associated with one or more of the second subset of possible software instrumentation test points that are selected by the user to be enabled for the run of the simulation of an instance of the SystemC executable behavioral model . The subset of enabled test points by the performance instrumentation platform may include all or merely some of the sets of software instrumentation test points and populated in the RTL model and the SystemC model of the electronic system.

A comparison module may later compare the similar measurement quantities between the first and second enabled subsets of instrumentation points to discern differences in the executable behavioral models coded at different levels of abstraction.

The population module instruments the first set of intended software instrumentation test points in the first model prior to or during the run time of the first simulation. The population module instruments the second set of intended software instrumentation test points in the second model prior to or during the run time of the second simulation.

The performance instrumentation platform creates a first set of software objects associated with an enabled subset of the first set of intended software instrumentation test points during run time of the first simulation. The performance instrumentation platform creates a second set of software objects associated with an enabled subset of the second set of intended software instrumentation test points during run time of the second simulation. Various logical links and allow communication between the first model and application programming interfaces API and . The logical links and also allow communication between the second model and the APIs and . A logical link may be a functional call to from the APIs. In one embodiment the API is a SystemC API and the API is a RTL API. Each model e.g. RTL model SystemC model receives inputs signals via the input interface . Each model sends outputs signals via the output interface . Each model may include a hardware performance counter to analyze test point input signals and interface with APIs and . In one embodiment a single creation layer corresponds to each population module .

The performance instrumentation platform manages the software objects and associated instrumentation test points of the first executable behavioral model and the software objects and associated instrumentation test points of the second executable behavioral model by enabling and disabling some or all of the first and second set of test points and based on a given instance of that electronic system either prior to or during the runtime of the simulation. User control for enabling and disabling the test points is provided by means of the user analysis environment .

The performance instrumentation platform also determines if the electronic system is functioning properly during run time of the first and second simulations. A deadlock situation in which a simulation is not able to proceed any further can be avoided based on the performance instrumentation platform determining a cause of the deadlock situation. The performance instrumentation platform includes temporary states and configurations for managing software objects. The performance instrumentation platform retrieves raw data via a logical link from the execution of the software objects and during run time of the first and second simulations. The performance instrumentation platform samples the raw data at a certain frequency via a logical link.

In one embodiment the first executable behavioral model is a SystemC model and the second executable behavioral model is a RTL model. The first set of software objects will not be synthesized by a logic synthesis tool if associated with the SystemC model which is a high level abstraction model. The second set of software objects may be configured to be coded at a lower abstraction level in register transfer level RTL to be synthesized by the logic synthesis tool. Alternatively the second set of software objects may be configured to be coded at an abstraction level in register transfer level RTL without being synthesized by the logic synthesis tool.

The EDA modeling tool further includes a recording database that receives data from the performance instrumentation platform via logical link . A user analysis environment interfaces with the recording database via a logical link . The user analysis environment interfaces with the performance instrumentation platform via a logical link .

In certain embodiments of the present disclosure the EDA modeling tool further includes a first and a second set of performance analysis units and associated with the first and second set of test points and respectively. The performance analysis units and perform calculations on raw data collected by the test points and during run time of the first and second simulations. Thus the performance analysis units and reduce the amount of raw data to be sent to the database leading to quicker feedback and debugging of the electronic system.

The definition module defines the first and the second set of test points and and the associated first and the second set of performance analysis units and prior to run time of the first and second simulations. In a similar manner the population module instruments the first set of test points and the associated first set of performance analysis units to the first model and instruments the second set of test points and associated second set of performance analysis units to the second model prior to run time of the first and second simulations.

The performance instrumentation platform may create both the first set of software objects associated with an enabled subset of the first set of intended software instrumentation test points and a first set of software objects associated with an enabled subset of the first set of performance analysis units during run time of the first simulation. The performance instrumentation platform may create both the second set of software objects associated with an enabled subset of the second set of intended software instrumentation test points and a second set of software objects associated with an enabled subset of the second set of performance analysis units during run time of the second simulation.

The performance instrumentation platform manages the software objects and associated instrumentation test points by enabling and disabling the first and second set of test points based on a given instance of the electronic system. The performance analysis units are further described in connection with .

The definition module population module and the performance instrumentation platform in combination form a component object model based performance instrumentation infrastructure that is configured for use with any type of instrumented module in current use or available in the future. The performance instrumentation infrastructure can greatly reduce the design cycle by capturing interesting observations and executing analysis algorithms to help identify key performance bottlenecks.

The electronic system further includes the memory scheduler and memory controller for accessing and storing data associated with an off chip memory component not shown such as dynamic random access memory DRAM . The interconnect memory scheduler and memory controller may include software instrumentation test points e.g. performance observation points POP that collect data associated with the various transactions. A test point or POP is a software object that provides visibility to a specific event related to transactions and resources. In one embodiment each event gets logged in a defined format such as in an ASCII file using SCV SystemC Verification library transaction recording.

In one embodiment block includes initiator agents and target agents and and test points and . Block includes test point . Block includes initiator agents and target agents and and test points .

A simulation based on a SystemC model can be performed on the electronic system . The SystemC model provides a detailed instrumentation and high simulation speed with the quality of the analysis results depending on the SystemC model accuracy. The SystemC model provides transaction tracing from initiator to target across the interconnect . Performance measurements include end to end times of a given transaction round trip latencies average peak best performance figures over a certain time window internal model measurements analysis of queues e.g. occupancy size quality of service QoS tracing thread arbitration link arbitration comparison of multiple simulations and other similar performance metrics.

The basic instrumentation based on test points as illustrated in captures detailed activity found in the interconnect . A large amount of information is sent to the SCV analysis output file. A large analysis database with full post processing capability can perform the analysis which is ideal for early exploration or system debug.

An instrumentation test point inserted inside the interconnect allows performance related activities observable by the test point to be monitored. Raw performance data represents observed activities at each test point during a simulation of an electronic system can also be collected to a transaction based database e.g. SCV that allows post process analysis.

Each test point contains one or more test point variables that are configured based on a design s configuration. For example the design may be configured with the test point being located on an initiator agent to monitor activities with respect to the usages of all request side resources e.g. FIFO queues .

In one embodiment a number N of test point variable s for the test point equals the number of initiator threads. Each test point variable FIFO n where n can be ranged from 0 to the number of initiator threads 1 is used to monitor the nth FIFO queue associated with the initiator thread n. When the initiator threads are collapsed only one test point variable FIFO0 is available to monitor activities of the collapsed FIFO queue.

Depending on configurations in the initiator agent may have three threads and the target agent may be configured with merely one thread for the design. Therefore the test point may have three test point variables FIFO0 FIFO1 and FIFO2 that each monitor a FIFO queue and the test point may have one test point variable FIFO0. This multi variable test point concept facilitates the configuration flexibility.

Returning to the three test point variables for test point are the FIFO0 FIFO1 and FIFO2. Since a test point can have multiple variables a transaction trace stream can contain multiple transaction trace lines. Activities i.e. events associated with each test point variable are monitored in a transaction by transaction fashion and can be collected during simulation along the variable s trace line. Raw performance data that represents or corresponds to activities i.e. events of each transaction needs to be defined by an architect of the interconnect .

Each test point variable can have one or more test point elements that are used to store the transaction based raw performance data collected during the run time of the simulation. An architect defines the semantics of each of the test point elements associated with its test point variable. In a transaction based tracing scheme two activities a beginning one and an ending one may be used to identify a transaction. Optional activities along the time line between the beginning and ending of a transaction can be defined if needed. illustrates as an example an embodiment of a trace line with multiple test point elements associated with a test point variable. In one embodiment EnqT and DeqT are test point elements defined for test point variable ReqFlop.FIFO0 where EnqT and DeqT represent the beginning and ending respectively of each transaction.

The architectural semantic of the test point elements can be as follows. Test point Element EnqT means the en queuing time when a free buffer entry of the req flop FIFO0 queue is consumed taking into account the width conversion this can due to the acceptance of a new initiator packet. Test point element DeqT is defined as the de queuing time when the req flop FIFO0 queue is freeing up one buffer entry taking into account the width conversion this can due to the departure of a packet down to the SL fabric .

In addition to use the above test point elements to monitor activities additional information such as the packet s identification can also be collected along those activities using other test point elements. When the SCV library is used the SCV transaction generator s begin transaction end transaction and the SCV transaction generator handler s record attribute API functions can be used to generate i.e. record transactions along each trace line for each transaction.

In one embodiment the method further includes collecting performance data using the enabled subset of intended software instrumentation test points at block . The method further includes disabling the instrumentation test points not included in the enabled subset of the intended software instrumentation test points at block . The method further includes recording the performance data to a transaction based database at block . The method may later compare the similar measurement quantities between the enabled subsets of software instrumentation points to discern differences in the executable behavioral models coded at different levels of abstraction.

A performance analysis unit is a software object in run time processing that performs analysis on data collected one or more associated the test points in order to optimize simulation speed and minimize analysis database size. For example performance analysis unit is associated with enabled test points that collect data associated with transactions occurring across the block . The performance analysis unit performs analysis of this collected data during run time of a simulation associated with the electronic system in order to quickly monitor debug and analyze performance of the electronic system.

In one embodiment each performance analysis unit performs analysis on a filtered specific dataflow. A filtered data flow may originate from initiator on thread 2 with a specific address range. Each performance unit is triggered on a specific event. For an OCP based transaction the performance unit starts measuring performance parameters on a particular register access e.g. end of the boot sequence core initialization beginning of DMA transfers .

A performance analysis unit or performance analysis counter is a software instrumentation instance that can be attached to one or more test points inside a populated and enable subset and thus instrumented instance. illustrates this concept by showing that two defined performance analysis counters and available for the interconnect design are attached to test points and respectively.

Each performance analysis unit allows performance related activities to be monitored and or collected by its attaching instrumentation test point s and to be further analyzed during simulation time. The analysis algorithm used by each performance analysis unit can 1 utilize the configuration information e.g. the maximum queue size of a FIFO associated with the test point location 2 use as inputs of the simulation time the current counter value of a performance analysis unit and the stored values of all test point elements of interested from the attaching instrumentation test point s merely the oldest value of each POP element associated with the same and the oldest transaction can be used and 3 after the computation save the result as the new counter value for future usage.

In one embodiment a performance analysis unit includes at least one simple defined analysis algorithm as follows. An average performance analysis unit can be used to compute the average value of interest during a period of time. A count performance analysis unit can be used to count the occurrences of interest during a period of time. A utilization performance analysis unit can be used to calculate the utilization of interest during a period of time. The maximum dimension of the resource of interest must be provided while initializing the counter instance.

Other examples for a performance analysis unit or counter include the following. A transaction performance analysis counter can be used to obtain statistics at a per transaction basis including start time end time request path latency response path latency end to end latency and page hit miss statistics. An arbitration performance analysis counter can be used to monitor a thread level parallelism in a simulation. Thread level parallelism is a per cycle measure of the number of threads that are contending for a resource in an arbitration unit. The number of threads active at different part of the simulation can vary. There might be times in a simulation when only one thread is active. Other times multiple threads may be active. And other times no thread might be active. A measure of how many threads are available for an arbitration unit to choose from has an effect on the performance on the interconnect. A deadlock performance analysis unit can be used to raise a flag if a deadlock was detected in the system. The performance analysis units can include an average performance analysis unit to compute the average value of interest during a period of time a count performance analysis unit to count the occurrences of interest during a period of time and a utilization performance analysis unit counter to calculate the utilization of interest during a period of time.

In another embodiment the underlying hardware components may have their own hardware counters that are capable of analyzing usages of some critical resources not just during simulation but also the real use of the final product. In this case a performance analysis unit software instance is not associated with each hardware component. Instead a POP can be used to monitor each hardware counter s value directly.

Referring to conceptually the analysis algorithm for a performance analysis unit can be invoked when all inputs used by the algorithm become available. This also indicates that the availability of the last in terms of simulation time the collecting time POP element of interest is used as the trigger. The algorithm associated with performance analysis unit is triggered upon receiving collected data from the test point .

One advantage in using a performance analysis unit instead of doing post process analysis is to allow a long simulation run with minimum file input output I O and to do useful performance analysis on the fly. Therefore the simulation can run much faster due to less file I O and can generate only a small database containing not the raw performance data but the final analyzed performance results out of all performance analysis units

The analysis results of a performance analysis unit is computed during simulation time and saved as the counter value from time to time after every computation. In some cases the history i.e. the usage characteristic of a performance analysis unit can be more important than the final counter result. Thus it is also important to provide a way to collect the counter value from time to time. The sampling scheme introduced below in allows the counter value of each performance analysis unit to be retrieved and recorded to the database periodically.

In one embodiment when a performance analysis unit e.g. counter is sampled by the library s Platform Sampling Engine it picks up the counter content and dumps both the simulation time and the counter content or other specified data to a transaction trace line associated with this performance analysis unit. The sampling rate can be adjusted according to the length of a simulation run so that the generated counter trace is small and will not significantly affect the simulation time.

In some embodiments we have one transaction trace stream for the Platform Sampling Engine with the stream including multiple transaction trace lines one for each performance analysis unit to reduce software overhead. Before the end of the simulation values stored in all performance analysis units are recorded to the database via the support block that performs formatting. This may assist in post processing analysis.

The available test points and performance analysis units for each interconnect are defined by an architect first then instrumented by taking configuration into account into the SystemC and RTL modules accordingly. instrumented code is merely used during simulation time.

Table 1 shows the initial set of POPs defined for an example architecture. The right most three columns indicate whether a POP is expected to be instrumented to the specified product entity i.e. the RTL module the SystemC TL1 model and the SystemC TL2 model.

The method further includes instrumenting the second set of software instrumentation test points and associated second set of performance analysis units to a second model having a second level of abstraction that models the electronic design at block . The method further includes creating a first set of software instances associated with an enabled subset of the first set of intended software instrumentation test points and associated enabled subset of the first set of performance analysis units during run time of a first simulation of the electronic design associated with the first model at block . The method further includes creating a second set of software instances associated with an enabled subset of the second set of intended software instrumentation test points and associated enabled subset of the second set of performance analysis units during run time of a second simulation of the electronic design associated with the second model at block . The method further includes collecting performance data using the enabled subset of the first and second set of intended software instrumentation test points at block . The method further includes invoking at least one performance analysis algorithm installed in the performance analysis units when collected data from the associated instrumentation points becomes available to each performance analysis unit at block . The method further includes disabling the instrumentation test points not included in the enabled subset of the intended software instrumentation test points and disabling associated performance analysis units at block .

In one embodiment the method further includes setting sampling rates for the enabled subset of the first and second set of intended software instrumentation test points and associated first and second set of performance analysis units during run time of the first and second simulations at block . The method further includes recording the analyzed data of the performance analysis units to a transaction based database at block . The performance analysis units are configured with a plurality of performance analysis algorithms to analyze performance of the electronic design during run time of the first and second simulation and thus reduce the amount of data recorded to the transaction based database.

In some embodiments the first set of software instrumentation test points and associated first set of performance analysis units of the first simulation correspond to the second set of software instrumentation test points and associated second set of performance analysis units of the second simulation.

The method further includes managing the first set of software instrumentation test points and associated first set of performance analysis units of the first executable behavioral model and the second set of software instrumentation test points and associated second set of performance analysis units of the second executable behavioral model with a performance software instrumentation module that enables and disables the first and second sets of test points and associated performance analysis units and determines if the electronic system is functioning properly during run time of the first and second simulations at block .

Aspects of the above design may be part of a software library containing a set of designs for components making up the Interconnect and associated parts. The library cells are developed in accordance with industry standards. The library of files containing design elements may be a stand alone program by itself as well as part of the EDA toolset.

The EDA toolset may be used for making a highly configurable scalable System On a Chip SOC inter block communication system that integrally manages input and output data control debug and test flows as well as other functions. In an embodiment an example EDA toolset may comprise the following a graphic user interface a common set of processing elements and a library of files containing design elements such as circuits control logic and cell arrays that define the EDA tool set. The EDA toolset may be one or more software programs comprised of multiple algorithms and designs for the purpose of generating a circuit design testing the design and or placing the layout of the design in a space available on a target chip. The EDA toolset may include object code in a set of executable software programs. The set of application specific algorithms and interfaces of the EDA toolset may be used by system integrated circuit IC integrators to rapidly create an individual IP core or an entire System of IP cores for a specific application. The EDA toolset provides timing diagrams power and area aspects of each component and simulates with models coded to represent the components in order to run actual operation and configuration simulations. The EDA toolset may generate a Netlist and a layout targeted to fit in the space available on a target chip. The EDA toolset may also store the data representing the interconnect and logic circuitry on a machine readable storage medium.

Generally the EDA toolset is used in two major stages of SOC design front end processing and back end programming.

Front end processing includes the design and architecture stages which includes design of the SOC schematic. The front end processing may include connecting models configuration of the design simulating testing and tuning of the design during the architectural exploration. The design is typically simulated and tested. Front end processing traditionally includes simulation of the circuits within the SOC and verification that they should work correctly. The tested and verified components then may be stored as part of a stand alone library or part of the IP blocks on a chip. The front end views support documentation simulation debugging and testing.

In block the EDA tool set may receive a user supplied text file having data describing configuration parameters and a design for at least part of an individual intellectual property IP block having multiple levels of hierarchy. The data may include one or more configuration parameters for that IP block. The IP block description may be an overall functionality of that IP block such as an interconnect. The configuration parameters for the interconnect IP block may be number of address regions in the system system addresses how data will be routed based on system addresses etc.

The EDA tool set receives user supplied implementation technology parameters such as the manufacturing process to implement component level fabrication of that IP block an estimation of the size occupied by a cell in that technology an operating voltage of the component level logic implemented in that technology an average gate delay for standard cells in that technology etc. The technology parameters describe an abstraction of the intended implementation technology. The user supplied technology parameters may be a textual description or merely a value submitted in response to a known range of possibilities.

The EDA tool set may partition the IP block design by creating an abstract executable representation for each IP sub component making up the IP block design. The abstract executable representation models TAP characteristics for each IP sub component and mimics characteristics similar to those of the actual IP block design. A model may focus on one or more behavioral characteristics of that IP block. The EDA tool set executes models of parts or all of the IP block design. The EDA tool set summarizes and reports the results of the modeled behavioral characteristics of that IP block. The EDA tool set also may analyze an application s performance and allows the user to supply a new configuration of the IP block design or a functional description with new technology parameters. After the user is satisfied with the performance results of one of the iterations of the supplied configuration of the IP design parameters and the technology parameters run the user may settle on the eventual IP core design with its associated technology parameters.

The EDA tool set integrates the results from the abstract executable representations with potentially additional information to generate the synthesis scripts for the IP block. The EDA tool set may supply the synthesis scripts to establish various performance and area goals for the IP block after the result of the overall performance and area estimates are presented to the user.

The EDA tool set may also generate an RTL file of that IP block design for logic synthesis based on the user supplied configuration parameters and implementation technology parameters. As discussed the RTL file may be a high level hardware description describing electronic circuits with a collection of registers Boolean equations control logic such as if then else statements and complex event sequences.

In block a separate design path in an ASIC or SOC chip design is called the integration stage. The integration of the system of IP blocks may occur in parallel with the generation of the RTL file of the IP block and synthesis scripts for that IP block.

The EDA toolset may provide designs of circuits and logic gates to simulate and verify the operation of the design works correctly. The system designer codes the system of IP blocks to work together. The EDA tool set generates simulations of representations of the circuits described above that can be functionally tested timing tested debugged and validated. The EDA tool set simulates the system of IP block s behavior. The system designer verifies and debugs the system of IP blocks behavior. The EDA tool set tool packages the IP core. A machine readable storage medium may also store instructions for a test generation program to generate instructions for an external tester and the interconnect to run the test sequences for the tests described herein. One of ordinary skill in the art of electronic design automation knows that a design engineer creates and uses different representations to help generating tangible useful information and or results. Many of these representations can be high level abstracted and with less details or top down views and can be used to help optimize an electronic design starting from the system level. In addition a design process usually can be divided into phases and at the end of each phase a tailor made representation to the phase is usually generated as output and used as input by the next phase. Skilled engineers can make use of these representations and apply heuristic algorithms to improve the quality of the final results coming out of the final phase. These representations allow the electric design automation world to design circuits test and verify circuits derive lithographic mask from Netlists of circuit and other similar useful results.

In block next system integration may occur in the integrated circuit design process. Back end programming generally includes programming of the physical layout of the SOC such as placing and routing or floor planning of the circuit elements on the chip layout as well as the routing of all metal lines between components. The back end files such as a layout physical Library Exchange Format LEF etc. are generated for layout and fabrication.

The generated device layout may be integrated with the rest of the layout for the chip. A logic synthesis tool receives synthesis scripts for the IP core and the RTL design file of the IP cores. The logic synthesis tool also receives characteristics of logic gates used in the design from a cell library. RTL code may be generated to instantiate the SOC containing the system of IP blocks. The system of IP blocks with the fixed RTL and synthesis scripts may be simulated and verified. Synthesizing of the design with Register Transfer Level RTL may occur. The logic synthesis tool synthesizes the RTL design to create a gate level Netlist circuit design i.e. a description of the individual transistors and logic gates making up all of the IP sub component blocks . The design may be outputted into a Netlist of one or more hardware design languages HDL such as Verilog VHDL Very High Speed Integrated Circuit Hardware Description Language or SPICE Simulation Program for Integrated Circuit Emphasis . A Netlist can also describe the connectivity of an electronic design such as the components included in the design the attributes of each component and the interconnectivity amongst the components. The EDA tool set facilitates floor planning of components including adding of constraints for component placement in the space available on the chip such as XY coordinates on the chip and routes metal connections for those components. The EDA tool set provides the information for lithographic masks to be generated from this representation of the IP core to transfer the circuit design onto a chip during manufacture or other similar useful derivations of the circuits described above. Accordingly back end programming may further include the physical verification of the layout to verify that it is physically manufacturable and the resulting SOC will not have any function preventing physical defects.

In block a fabrication facility may fabricate one or more chips with the signal generation circuit utilizing the lithographic masks generated from the EDA tool set s circuit design and layout. Fabrication facilities may use a standard CMOS logic process having minimum line widths such as 1.0 um 0.50 um 0.35 um 0.25 um 0.18 um 0.13 um 0.10 um 90 nm 65 nm or less to fabricate the chips. The size of the CMOS logic process employed typically defines the smallest minimum lithographic dimension that can be fabricated on the chip using the lithographic masks which in turn determines minimum component size. According to one embodiment light including X rays and extreme ultraviolet radiation may pass through these lithographic masks onto the chip to transfer the circuit design and layout for the test circuit onto the chip itself.

The EDA toolset may have configuration dialog plug ins for the graphical user interface. The EDA toolset may have an RTL generator plug in for the SocComp. The EDA toolset may have a SystemC generator plug in for the SocComp. The EDA toolset may perform unit level verification on components that can be included in RTL simulation. The EDA toolset may have a test validation testbench generator. The EDA toolset may have a dis assembler for virtual and hardware debug port trace files. The EDA toolset may be compliant with open core protocol standards. The EDA toolset may have Transactor models Bundle protocol checkers OCPDis2 to display socket activity OCPPerf2 to analyze performance of a bundle as well as other similar programs

As discussed an EDA tool set may be implemented in software as a set of data and instructions such as an Instance in a software library callable to other programs or an EDA tool set consisting of an executable program with the software cell library in one program stored on a machine readable medium. A machine readable storage medium may include any mechanism that provides e.g. stores and or transmits information in a form readable by a machine e.g. a computer . For example a machine readable medium may include but is not limited to read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices DVD s EPROMs EEPROMs FLASH magnetic or optical cards or any other type of media suitable for storing electronic instructions. The instructions and operations also may be practiced in distributed computing environments where the machine readable media is stored on and or executed by more than one computer system. In addition the information transferred between computer systems may either be pulled or pushed across the communication media connecting the computer systems.

Some portions of the detailed descriptions above are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of operations leading to a desired result. The operations are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

The modeling tool may have its instructions executable code sequences data files etc stored on a machine readable storage medium. A machine readable storage medium may include any mechanism that provides e.g. stores and or transmits information in a form readable by a machine e.g. a computer . For example a machine readable medium may include but is not limited to read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices DVD s electrical optical digital signals EPROMs EEPROMs FLASH magnetic or optical cards or any other type of media suitable for storing electronic instructions.

In block the circuit layout of a verified an IP block may be integrated with the entire integrated circuit. One or more lithographic masks may be generated from to be used for the manufacturing of a chip based upon that layout.

In block the chip verified with the modeling tool may be fabricated using CMOS process employing 1.0 um 0.35 um 0.25 um 0.13 um 90 nm etc. technologies.

Some portions of the detailed descriptions above are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of operations leading to a desired result. The operations are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the above discussions it is appreciated that throughout the description discussions utilizing terms such as processing or computing or calculating or determining or displaying or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

While some specific embodiments of the invention have been shown the invention is not to be limited to these embodiments. For example most functions performed by electronic hardware components may be duplicated by software emulation. Thus a software program written to accomplish those same functions may emulate the functionality of the hardware components. The functionality accomplished by one module may be coded into another module. The hardware logic consists of electronic circuits that follow the rules of Boolean Logic software that contain patterns of instructions or any combination of both. The invention is to be understood as not limited by the specific embodiments described herein but only by scope of the appended claims.

