---

title: Methods and systems for hardware acceleration of database operations and queries
abstract: Embodiments of the present invention provide a database system that is optimized by using hardware acceleration. The system may be implemented in several variations to accommodate a wide range of queries and database sizes. In some embodiments, the system may comprise a host system that is coupled to one or more hardware accelerator components. The host system may execute software or provide an interface for receiving queries. The host system analyzes and parses these queries into tasks. The host system may then select some of the tasks and translate them into machine code instructions, which are executed by one or more hardware accelerator components. The tasks executed by hardware accelerators are generally those tasks that may be repetitive or processing intensive. Such tasks may include, for example, indexing, searching, sorting, table scanning, record filtering, and the like.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08244718&OS=08244718&RS=08244718
owner: Teradata US, Inc.
number: 08244718
owner_city: Dayton
owner_country: US
publication_date: 20070827
---
This application claims priority to U.S. Provisional Patent Application No. 60 823 579 filed on Aug. 25 2006 entitled Methods Devices and Systems for Accelerating Databases by Joseph I. Chamdani which is incorporated herein by reference in its entirety.

This application is related to the following U.S. Patent Applications and Patents which are herein incorporated by reference in their entirety U.S. patent application Ser. No. 11 895 998 filed on Aug. 27 2007 entitled HARDWARE ACCELERATED RECONFIGURABLE PROCESSOR FOR ACCELERATING DATABASE OPERATIONS AND QUERIES by Jeremy Branscome et al. and U.S. patent application Ser. No. 11 895 997 filed on Aug. 27 2007 entitled PROCESSING ELEMENTS OF A HARDWARE ACCELERATED RECONFIGURABLE PROCESSOR FOR ACCELERATING DATABASE OPERATIONS AND QUERIES by Jeremy Branscome et al.

This invention relates generally to database systems. More particularly it relates to database systems that are optimized by using hardware acceleration.

Despite their different uses applications and workload characteristics most systems run on a common Database Management System DBMS using a standard database programming language such as Structured Query Language SQL . Most modern DBMS implementations Oracle IBM DB2 Microsoft SQL Sybase MySQL PostgreSQL Ingress etc. are implemented on relational databases which are well known to those skilled in the art.

Typically a DBMS has a client side where applications or users submit their queries and a server side that executes the queries. On the server side most enterprises employ one or more general purpose servers. However although these platforms are flexible general purpose servers are not optimized for many enterprise database applications. In a general purpose database server all SQL queries and transactions are eventually mapped to low level software instructions called assembly instructions which are then executed on a general purpose microprocessor CPU . The CPU executes the instructions and its logic is busy as long as the operand data are available either in the register file or on chip cache. To extract more parallelism from the assembly code and keep the CPU pipeline busy known CPUs attempt to predict ahead the outcome of branch instructions and execute down the code path speculatively. Execution time is reduced if the speculation is correct the success of this speculation however is data dependent. Other state of the art CPUs attempt to increase performance by employing simultaneous multithreading SMT and or multi core chip multiprocessing CMP . To take advantage of these changes have to be made at the application or DBMS source code to manually create the process thread parallelism for the SMT or CMP CPUs. This is generally considered highly as very complex to implement and not always applicable to general purpose CPUs because it is workload dependent.

Unfortunately general purpose CPUs are not efficient for database applications. Branch prediction is generally not accurate because database processing involves tree traversing and link list or pointer chasing that is very data dependent. Known CPUs employ the well known instruction flow or Von Neumann architecture which uses a highly pipelined instruction flow rather than a data flow where operand data is pipelined to operate on data stored in the CPUs tiny register files. Real database workloads however typically require processing Gigabytes to Terabytes of data which overwhelms these tiny registers with loads and reloads. On chip cache of a general purpose CPU is not effective since it s relatively too small for real database workloads. This requires that the database server frequently retrieve data from its relatively small memory or long latency disk storage. Accordingly known database servers rely heavily on squeezing the utilization of their small system memory size and disk input output I O bandwidth. Those skilled in the art recognize that these bottlenecks between storage I O the CPU and memory are very significant performance factors.

However overcoming these bottlenecks is a complex task because typical database systems consist of several layers of hardware software etc. that influence the overall performance of the system. These layers comprise for example the application software the DBMS software operating system OS server processor systems such as its CPU memory and disk I O and infrastructure. Traditionally performance has been optimized in a database system horizontally i.e. within a particular layer. For example many solutions attempt to optimize various solutions for the DBMS query processing caching the disk I O etc. These solutions employ a generic narrow approach that still fails to truly optimize the large performance potentials of the database system especially for relational database systems having complex read intensive applications.

Accordingly it would be very desirable to provide a more complete solution for database systems than what is currently available. As will be described herein the present invention provides a relatively complete solution that utilizes hardware acceleration for query processing and a vertical solution approach.

As mentioned previously a general purpose database system consists of many layers that influence the overall application performance. Traditionally performance has been optimized horizontally within a single layer but based on general purpose requirements that are not focused on database applications or a class of database applications. Instead of optimizing independently at each layer in a generic way the present invention applies optimizations more directly and efficiently in a vertical approach across these layers. For example the present invention may apply various optimizations suitable for decision support system DSS applications and web based search applications. These applications are generally known to be read intensive applications and utilized on relational DBMS s. In the present invention optimizations are strategically applied where they can gain orders of magnitude of performance but are still efficient in cost implementation etc.

The present invention employs a custom computing C2 solution that provides a significant gain in performance for enterprise database applications. In the C2 solution a node or appliance may comprise the host or base system that is combined with hardware accelerated reconfigurable processors HARP . These HARPs are specially designed to optimize the performance of database systems and its applications especially relational database systems and read intensive applications.

A host system may be any standard or pre existing DBMS system. In general such systems will comprise a standard general purpose CPU a system memory I O interfaces etc.

The HARPs are coupled to the host system and are designed to offload repetitive database operations from the DBMS running on the host system. The HARPs utilize dataflow architecture processing elements that execute machine code instructions that are defined for various database operations. The C2 solution may employ a node that is scalable to include one HARP or multiple HARPs. In addition the C2 solution may use a federated architecture comprising multiple nodes i.e. multiple DBMS servers that are enhanced with the C2 solution.

In some embodiments the C2 solution employs an open architecture and co processor approach so that the C2 hardware can be easily integrated into existing database systems. Of note the hardware acceleration of the C2 solution utilizes novel machine code database instructions to execute certain fragments of a query in a dataflow and using parallel pipelined execution.

In the present invention the C2 solution also comprises software that orchestrates the operations of the DBMS running on the host system and the HARPs. The C2 software is configured with a flexible layered architecture to make it hardware and database system agnostic. Thus the C2 software is capable of seamlessly working with existing DBMSs based on this open architecture.

In general the C2 software receives the query from the DBMS and breaks the query down into query fragments. The C2 software then decides which of these query fragments can be appropriately handled in software in the C2 software itself or back in the originating DBMS or ideally with hardware acceleration in the HARPs. All or part of the query may be processed by the C2 software and HARPs.

In addition in order to maximize the efficiency of the hardware acceleration the C2 solution stores its databases in compressed column store format and utilizes various hardware friendly data structures. The C2 solution may employ various compression techniques to minimize or reduce the storage footprint of its databases. The column store format and hardware friendly data structures allow the HARPs or C2 software to operate directly on the compressed data in the column store database. The column store database may employ columns and column groups that are arranged based on an implicit row identifier RID scheme and RID to primary column to allow for easy processing by the HARPs. The hardware friendly data structures also allow for efficient indexing data manipulation etc. by the HARPs.

For example the C2 solution utilizes a global virtual address space for the entire database to greatly simplify and maximize efficiency of create read update and delete operations of data in a database. In some embodiments the columns and column groups are configured with a fixed width to allow for arithmetic memory addressing and translation from a virtual address to a physical memory address. On demand and speculative prefetching may also be utilized by the C2 solution to hide I O latency and maximize HARP utilization. Various indexing structures that are optimized for hardware execution of query fragments are also employed in the C2 software .

Due to the comprehensive nature of the present inventions in the C2 solution the figures are presented generally from a high level of detail and progress to a low level of detail. For example illustrate exemplary systems and topologies enabled by the present invention. illustrate the architecture of the C2 software. illustrates the architecture of a HARP module. illustrate the database format and data structures employed by the C2 solution of the present invention. And illustrates an example execution of a SQL query by the C2 solution of the present invention. Reference will now be made in detail to the exemplary embodiments of the invention which are illustrated in the accompanying drawings. Wherever possible the same reference numbers will be used throughout the drawings to refer to the same or like parts.

Application may be any computer software that requests the services of DBMS . Such applications are well known to those skilled in the art. For example application may be a web browser in which a user is submitting various search requests. Of course application may be another system or software that is consuming the services of DBMS and submitting queries to DBMS .

Client represents the hardware and software that supports the execution of application . Such clients are well known to those skilled in the art. For example client may be a personal computer or another server.

DBMS is any computer software that manages databases. In general DBMS controls the organization storage management retrieval of data in a database. As is well known these types of systems are common for supporting various SQL queries on relational databases and thus may also be known as a RDBMS . Due to its open architecture various DBMS systems may be employed by the present invention. Typical examples of DBMSs include Oracle DB2 Microsoft Access Microsoft SQL Server PostgreSQL and MySQL.

In some embodiments and for purposes of explanation DBMS is shown comprising C2 software interfacing MySQL software via an API . MYSQL software is open source software that is sponsored and provided by MySQL AB and is well known to those skilled in the art. Of course any DBMS software such as those noted above may be employed in the present invention.

C2 software orchestrates the execution of a query forwarded from DBMS and thus operates in conjunction with MySQL software . For example in the C2 software SQL queries are broken down into query fragments and then routed to the most appropriate resource. A query fragment may be handled in C2 hardware i.e. HARP module . HARP module is further described with reference to . The query fragment may also be processed in the C2 software itself or returned for handling by MySQL software .

In general C2 software utilizes a flexible layered architecture to make it hardware and database system agnostic. For example C2 software may operate as a storage engine of MySQL software . As is well known MySQL software may provide an API for storage engines which C2 software may plug in to. API comprises the software that specifies how the C2 software and MySQL software will interact how they will request services from each other such as SQL queries and results.

As a storage engine C2 software may employ the MySQL API to provide various storage mechanisms indexing facilities locking levels and ultimately provide a range of different functions and capabilities that are transparent to MySQL software . As noted above this is one aspect of how the present invention overcomes the generic approach in known solutions without having to sacrifice performance for functionality or fine tune the database. Of note although shows a single storage engine MySQL software may be coupled to multiple storage engines not shown in addition to C2 software . C2 software is also described in further detail with reference to .

Network represents the communication infrastructure that couples application and DBMS . For example network may be the Internet. Of course any network such as a local area network wide area network etc. may be employed by the present invention.

Storage infrastructure comprises the computer storage devices such as disk arrays tape libraries and optical drives that serve as the storage for the databases of system . Storage infrastructure may employ various architectures such as a storage area network network attached storage etc. which are known to those skilled in the art.

In some embodiments the C2 solution stores its databases in storage infrastructure in column store format. Column store format is where data is stored in columns or groups of columns. Column store format is advantageous for data fetching scanning searching and data compression. The column store format may employ fixed width columns and column groups with implicit RIDs and a RID to primary key column to allow for arithmetic memory addressing and translation. This allows HARPs to utilize hardware processing for database processing such as column hopping and to operate directly on the compressed data in the columns.

In contrast in typical DBMS environments data is stored in row store format. Row store format is sometimes considered by those skilled in the art for having better performance in data updates and record retrieval thus it is sometimes considered to have better functionality over column store databases in most applications with a high ratio of updates over reads. In the present invention however the C2 solution achieves better performance by using hardware acceleration with a column store database yet it still delivers the functionality and benefits of row store databases. The column store format used by the C2 solution of the present invention is further described with reference to .

First the basic C2 node will be explained which comprises a single host system and a single HARP module . Variations of this basic node will then be explained to show how the basic node can be scaled up and how multiple nodes can be employed in a federated architecture.

The basic C2 node topology may comprise a host system and a hardware accelerated reconfigurable processor HARP module . Collectively host and HARP module may be referred to as a node or appliance. In some embodiments host system and HARP module are coupled together over a known communications interface such as a PCIe or HyperTransport HT interface. In terms of packaging host system and HARP module may be built on one or more cards or blades that are bundled together in a common chassis or merely wired together. In the C2 solution host system and HARP module may be flexibly packaged using a modular form factor for ease of installation and scaling.

The host system may comprise a general purpose CPU such as a Xeon x86 processor by the Intel Corporation and a memory such as a dynamic random access memory. Such types of host systems are well known to those skilled in the art. In general in the C2 solution host system will be used to process parts of a query that are less time consuming i.e. slow path portion such as server client connection authentication SQL parsing logging etc. However in order to optimize performance the bulk of query execution i.e. the fast path portion is offloaded to the HARP module .

Host system may run MySQL software and also run C2 software that orchestrates query processing between MySQL and HARP . In particular C2 software will decompose a query into a set of query fragments. Each fragment comprises various tasks which may have certain dependencies. C2 software will determine which fragments and tasks are part of the fast path portion and offload them to the HARP module . Appropriate tasks for the selected query fragments are sent to HARP module with information on the database operation dependency graph. Within the HARP module tasks are further broken down into parallel pipelined machine code operations known as MOPs and executed in hardware.

HARP module comprises processing logic HARP logic and a relatively large memory HARP memory for hardware accelerating database operations of the node. In some embodiments HARP module is configured to handle various repetitive database tasks such as table scanning indexing etc. In the C2 solution HARP module can receive high level database query tasks not just low level read write or primitive computation tasks as is typically for a general purpose processor in the form of machine code database instructions.

HARP logic is the hardware that executes machine code database instructions for the database tasks being handled by HARP module . To adapt to application requirement changes the HARP logic is designed to have hardware re configurability. Accordingly in some embodiments HARP logic is implemented using field programmable gate arrays FPGAs . However any type of custom integrated circuit such as application specific integrated circuits ASICs may be implemented as HARP logic .

When it is implemented as a rigid ASIC it is also possible to keep the reconfigurability of HARP module by embedding FPGA cores in the ASIC i.e. a mixed implementation . The reconfigurability of HARP module may have significance in allowing the C2 hardware platform to be re programmed to adapt to changing application needs.

For example a software patch or release may include a new FPGA image s that upgrade HARP module in a manner similar to the way software or firmware can be upgraded. These new FPGA images may be downloaded by offlining the target HARP module to fix functional bugs add new features for functionality or better performance or any other application customer specific adaptation.

Multiple FPGA images could be stored in an Electrically Erasable Programmable Read Only Memory EEPROM or flash memory of the FPGA. Each FPGA image may then have its own unique functionality. One image could be used to speed up a fast loader bulk operation which is normally done when there are no queries in the system either parsed or offloaded . Another image could be used or loaded if an application requires a lot of text processing structured unstructured or semi structured and needs additional acceleration specific to text search regular expressions and other work text related operations. Yet another image could be loaded for pattern matching queries related to DNA or protein search in bio informatics applications. These FPGA images may be activated one at a time depending on customer application setup or dynamically loaded based on current active application workload.

A more fine grained FPGA image loading unloading could be employed in the present invention. In these embodiments the FPGAs could support very fast programming i.e. on the order or sub microseconds similar to the effect of context process thread switching in operating systems. Based on the type of query submitted to HARP module a corresponding FPGA image may be pre loaded together with its appropriate query state in HARP memory .

HARP memory serves as the memory of HARP module . In order to maximize the efficiency of the HARP logic the HARP memory may be implemented using relatively large amounts of memory. For example in some embodiments the HARP memory in a HARP module may comprise 256 Giga Bytes or more of RAM or DRAM. Of course even larger amounts of memory may be installed in HARP module . HARP logic and HARP memory are further described with reference to .

In addition to the basic C2 node a scale up C2 node topology may be used as an extension of the basic C2 node. As shown host system may now be coupled to a plurality or array of 1 N HARP modules . In this type of node a PCIe switch or other suitable switching fabric may couple these components together with storage infrastructure . Of course other internal arrangements for a scale up C2 node may be utilized in the present invention.

Going further a scale out topology can be used for multiple C2 nodes. As shown the scale out topology may comprise various combinations of either the basic or scale up C2 nodes. For example as shown the scale out topology may comprise a federation of Nodes M where each node may have its own storage infrastructure or multiple nodes may share common storage infrastructure e.g. a SAN attached storage array . In Node is shown as a basic C2 node while Node M is shown as a scale up node. A control node is also shown and manages the operations of Nodes M. Control node is shown as a separate node however those skilled in the art will recognize the role of control node by any of Nodes M. Other variations in node hierarchy and management are within the scope of the present invention. Of course this topology may also comprise a variety of combinations of nodes.

In contrast as shown in the SQL query may submitted to a C2 system having a DBMS that comprises a top layer DBMS software i.e. MySQL and C2 software . C2 software interfaces with the DBMS software to orchestrate and optimize processing of the SQL query.

In particular C2 software may identify portions of the query i.e. the fast path portion which is better handled in hardware such as HARP module . Such portions may be those fragments of the query that are repetitive in nature such as scanning indexing etc. In the prior art system the DBMS is limited by its own programming the operating system and the general purpose CPU. The present invention avoids these bottlenecks by offloading fast path portions of a query to HARP module .

As shown HARP module comprises HARP logic and a HARP memory to accelerate the processing of SQL queries. In order maximize the use of HARP module the present invention may also utilize column store databases. Whereas the prior art system is hindered by the limitations of a standard row store database. These features also allow the present invention to maximize the performance of the I O between the operating system and storage.

For ease of implementation C2 software may be implemented on well known operating systems. The operating system will continue to be used to perform basic tasks such as controlling and allocating memory prioritizing system requests controlling input and output devices facilitating networking and managing files and data in storage infrastructure . In some embodiments various operating systems such as Linux UNIX and Microsoft Windows may be implemented.

Those skilled in the art will also recognize that other advantages and benefits may be achieved by the embodiments of the present invention. For purposes of explanation the present disclosure will now describe the C2 software hardware data structures and some operations in further detail.

As noted C2 software orchestrates the processing a query between MySQL software and HARP module . In some embodiments C2 software runs as an application on host system and as a storage engine of MySQL software . illustrates an architecture of the C2 software . As shown C2 software comprises a query and plan manager a query reduction rewrite module an optimizer a post optimizer rewrite module a query plan generator an execution engine a buffer manager a task manager a memory manager a storage manager an answer manager an update manager shared utilities and a HARP manager . Each of these components will now be briefly described.

Query and plan manager analyzes and represents the parsed query received from the MySQL software annotates the query and provides an annotation graph representation of the query plan. Query reduction rewrite module breaks the query into query fragments and rewrites the query fragments into tasks. Rewrites may be needed for compressed domain rewrites and machine code database instruction operator rewrites. Optimizer performs cost based optimization to be done using cost model of resources available to C2 software i.e. HARP module resources of C2 software itself using software operations or MySQL software .

These modules interact with each other to determine how to execute a query such as a SQL query from MySQL software . The data structures output by the query plan generator will be the same data structure that the optimizer and the rewrite module will operate on. Once a parsed SQL query has been represented in this data structure converted for example from MySQL query and plan manager rewrites the query such that each fragment of the query can be done entirely in MySQL software in C2 software or in HARP module . Once the final query representation is available the rewrite module goes through and breaks the graph into query fragments.

Post optimizer module is an optional component that rewrites after the optimizer for coalescing improvements found by optimizer . Query plan generator generates an annotations based template driven plan generation for the query tasks. Execution engine executes the query fragments that are to be handled by software or supervises the query execution in HARP module via HARP manager .

Buffer manager manages the buffers of data held in the memory of host and for the software execution tasks handled by host . Task manager orchestrates the execution of all the tasks in HARP module and C2 software execution engine .

Memory manager manages the virtual address and physical address space employed by C2 software and HARP module in HARP memory . In some embodiments memory manager utilizes a 50 bit VA addressing i.e. in excess of 1 petabyte . This allows C2 software to globally address an entire database and optimize hardware execution of the query tasks.

Storage manager is responsible for managing transfers of data from HARP memory to from storage infrastructure . Answer manager is responsible for compiling the results of the query fragments and providing the result to MySQL software via the API .

Update manager is responsible for updating any data in the database stored in storage infrastructure . Shared utilities provide various utilities for the components of C2 software . For example these shared utilities may include a performance monitor a metadata manager an exception handler a compression library a logging and recovery manager and a bulk incremental loader.

HARP manager controls execution of the tasks in HARP module by setting up the machine code database instructions and handles all interrupts from any of the hardware in HARP module . In some embodiments HARP manager employs a function library known as a Hardware Acceleration Function Library HAFL in order to make its function calls to HARP module .

As shown a SQL query is received in the RDBMS layer i.e. MySQL software . MySQL software then passes the SQL query via API to C2 software . In C2 software the SQL query is processed. At this layer C2 software also manages retrieving data for the SQL query if necessary from storage infrastructure or from host system .

In order to communicate with HARP module HARP manager employs the HAFL layer in order to make its function calls to HARP module . In order to allow for variances in hardware that may exist in HARP module the system software stack may also comprise a hardware abstraction layer HAL . Information is then passed from C2 software to HARP module in the form of machine code database instructions via an interconnect layer. As noted this interconnect layer may be in accordance with the well known PCIe or HT standards.

Within HARP module the machine code database instructions are parsed and forwarded to HARP logic . These instructions may relate to a variety of tasks and operations. For example as shown the system software stack provides for systems management task coordination and direct memory access to HARP memory . In HARP logic machine code database instructions are interpreted for the various types of processing elements PE . HARP logic may interface with HARP memory i.e. direct memory access by utilizing the memory management layer.

In addition to its PEs processing core may comprise a task processor a memory manager a buffer cache and an interconnect . One or more these components may be duplicated or removed from the other processing cores and . For example as shown core may be the sole core that includes task processor and an interconnect . This architecture may be employed because cores and are connected via switching fabric and may operate logically as a single processor or processor core. Of course one skilled in the art will recognize that various redundancies may be employed in these processing cores as desired.

Task processor is the hardware that supervises the operations of the processing cores and . Task Processor is a master scheduling and control processing element disconnected from the direct dataflow of the execution process for a query. Task processor maintains a running schedule of machine code database instructions which have completed are in progress or are yet to execute and their accompanying dependencies the Task Processor may also dispatch machine code database instructions for execution and monitor their progress. Dependencies can be implicit or explicit in terms of strong intra or inter processor release criteria. Machine code database instructions stalled for software assist can be context switched by the Task Processor which can begin or continue execution of other independent query tasks to optimize utilization of execution resources in HARP logic .

Memory manager is the hardware that interfaces HARP memory . For example memory manager may employ well known memory addressing techniques such as translation look aside buffers to map the global database virtual address space to a physical address in HARP memory to access data stored in HARP memory .

Buffer cache serves as a small cache for a processing core. For example temporary results or other meta data may be held in buffer cache .

PCIe interconnect is the hardware that interfaces with host system . As noted interconnect may be a PCIe or HT interconnect.

PEs represent units of the hardware and circuitry of HARP logic . As noted PEs utilize a novel dataflow architecture to accomplish the query processing requested of HARP logic . In particular PEs implement execution of an assortment of machine code database instructions that are known as Macro Ops MOPs and Micro Ops UOPs . MOPs and UOPs are programmed and executed by the PEs to realize some distinct phase of data processing needed to complete a query. MOPs and UOPs are just example embodiments of machine code database instructions other types of instruction sets for high level database operations of course may be used by the C2 solution.

PEs pass logical intermediate MOP results among one another through a variable length dataflow of dataflow tokens carried across an interconnect data structure which is a physical data structure and not a software data structure termed an Inter Macro Op Communication IMC path. Of note the IMC paths and self routing fabric allow HARP module to utilize a minimal amount of reads writes to HARP memory by keeping most intermediate results flowing through the IMCs in a pipelined parallel fashion. IMC may be temporarily stored in buffer caches and interconnect fabric however IMCs can also be dispatched out through interconnect to other PEs on another HARP module.

In the dataflow concept each execution step as implemented by a MOP and its accompanying UOP program can apply symmetrically and independently to a prescribed tuple of input data to produce some tuple of result. Given the independence and symmetry any number of these tuples may then be combined into a list matrix or more sophisticated structure to be propagated and executed in pipelined fashion for optimal execution system throughput. These lists of tuples comprised fundamentally of dataflow tokens are the intermediate and final results passed dynamically among the MOPs via IMC.

Although the dataflow travels over physical links of potentially fixed dimension the logical structure of the contents can be multi dimensional produced and interpreted in one of two different ways either with or without inherent internal formatting information. Carrying explicit internal formatting information allows compression of otherwise extensive join relationships into nested sub list structures which can require less link bandwidth from fabric and intermediate storage in buffer cache at the cost of the extra formatting delimiters increased interpretation complexity and the restriction of fixing the interpretation globally among all consumers. Without inherent formatting a logical dataflow may be interpreted by the consumer as any n dimensional structure having an arbitrary but consistent number of columns of arbitrary but consistent length and width. It should be noted that the non formatted form can be beneficial not only in its structural simplicity but in the freedom with which consumer MOPs may interpret or reinterpret its contents depending upon the purpose of the execution step a consumer is implementing.

The dataflow used in realizing a given query execution can be described by a directed acyclic graph DAG with one intervening MOP at each point of flow convergence and bifurcation one MOP at each starting and ending point as well as any point necessary in between i.e. single input output MOP . The DAG must have at least one starting and one ending point although any larger number may be necessary to realize a query. MOPs which serve as the starting point are designed to begin the dataflow by consuming and processing large amounts of data from local storage. Ending point MOPs may terminate the dataflow back into local storage or to a link which deposits the collected dataflow result table list into host CPU memory. An example of a DAG for a well known TPC H query is shown in .

As mentioned above MOP DAGs can physically and logically converge or bifurcate programmatically. The physical convergence is accomplished with a multi input MOPs which relate inputs in some logical fashion to produce an output comprised of all inputs e.g. composition merge etc. . The physical bifurcation is accomplished by means of multicast technology in the IMC fabric which dynamically copies an intermediate result list to multiple consumer MOPs. These mechanisms work together to allow realization of any desired DAG of MOP execution flow.

In the present invention each MOP is configured to operate directly on the compressed data in the column store database and realizes some fundamental step in query processing. MOPs are physically implemented and executed by PEs which depending on specific type will realize a distinct subset of all MOP types. MOPs work systematically on individual tuples extracted either from local database storage in HARP memory or the IMC dataflow producing output tuples which may be interpreted by one or more MOP processes downstream.

UOPs are the low level data manipulators which may be combined into a MOP specific UOP program accompanying a MOP to perform analysis and or transformation of each tuple the MOP extracts. MOPs which utilize UOP programs are aware of the dependency distributing selected portions of each tuple to the underlying UOP engine extant within all PEs supporting such MOPs. For each set of inputs from each tuple the UOP program produces a set of outputs which the MOP may use in various ways to realize its function.

For example one manner a MOP may use UOP output is to evaluate each tuple of a list of tuples for a set of predicating conditions where the MOP decides either to retain or to drop each tuple based on the UOP result. Another manner is for the UOP to perform an arithmetic transformation of each input tuple where the MOP either appends the UOP result to form a larger logical tuple or replaces some portion of the input tuple to form the output tuple.

Given a finite number of execution resources in PEs the full MOP dataflow DAG needed to execute a query may be partitioned into segments of connected MOPs called tasks. These tasks are then scheduled by task processor for execution in a sequential fashion as MOP execution resources become available in PEs . Significant in this process is the propagation of the execution dataflow among these tasks such that the entire query result is accurately and consistently computed regardless of how each task is apportioned and regardless of the latency between scheduling each task.

One method that may be employed in HARP logic is to treat each task atomically and independently terminating the dataflow back into local storage in HARP memory at the end of each task and restarting that dataflow at the beginning of the subsequent task by reloading it from HARP memory . In some embodiments a more efficient method may be employed to pipeline tasks at their finer constituent MOP granularity where at least one MOP of a new task may begin execution before all MOPs of the previous task have finished. This fine grained method is referred to as Task Pipelining .

Keeping the dataflow alive over task boundaries is a key to realizing the extra efficiency of Task Pipelining. To accomplish this in the C2 solution IMCs may include the ability to dynamically spill or send their dataflow to an elastic buffer backed by HARP memory pending the awakening of a consumer MOP which will continue the dataflow. On scheduling the consumer MOP IMCs are able to fill dynamically reading from the elastic buffer in HARP memory as necessary to continue execution pulling out any slack that may have built up in the dataflow while waiting for the scheduling opportunity. Task Pipelining with these mechanisms then may provide a more efficient use of execution resources down to the MOP granularity such that a query may be processed as quickly as possible.

High latency low bandwidth non volatile storage in storage infrastructure often holds the contents of a query workset due to the sheer volume of data involved. Because execution rates can outstrip the bandwidth available to read from such storage tasks requiring latent data can shorten execution time by starting and progressing their dataflow execution at the rate the data arrives instead of waiting for an entire prefetch to complete before beginning execution. This shortcut is referred to as Prefetch Pipelining. The C2 solution may employ both on demand prefetching and speculative prefetching. On demand prefetching is where data is prefetched based on the progress of the dataflow. Speculative prefetching is where data is prefetched based on an algorithm or heuristic that estimates the data is likely to be requested as part of a dataflow.

In the present invention realizing Prefetch Pipelining is accomplished by having one or more MOPs beginning a task s dataflow are capable of accepting data progressively as it is read from slow storage in storage infrastructure . IMCs are capable of filling progressively as data arrives as are all MOPs already designed to read from local storage in HARP memory . Given that support MOPs can satisfy the requirement of executing progressively at the rate of the inbound dataflow and accomplish efficient Prefetch Pipelining.

As shown processing core may comprise scanning indexing PE and XCAM PE as its set of PEs . As noted PEs are the physical entities responsible for executing MOPs with their underlying UOPs and for realizing other sophisticated control mechanisms. Various incarnations of processing elements are described herein where each incarnation supports a distinct subset of the MOP and control space providing different and distinct functionality from the perspective of query execution. Each of the different PE forms is now addressed where those which support MOPs employing UOP programs implicitly contain a UOP processing engine.

Scanning Indexing PE implements MOPs which analyze database column groups stored in local memory performing parallel field extraction and comparison to generate row pointers row ids or RIDs referencing those rows whose value s satisfy the applied predicate. For some MOP forms a metadata Value List which is an abstract term for a logical tuple list flowing through an IMC containing a column of potentially sparse row pointers may be given as input in which case the scan occurs over a sparse subset of the database. For other forms scanning occurs sequentially over a selected range of rows.

The selection predicate is stipulated through a micro op UOP program of finite length and complexity. For conjunctive predicates which span columns in different column groups scanning may be done either iteratively or concurrently in dataflow progression through multiple MOPs to produce the final fully selected row pointer list.

Inasmuch as the Scanning Indexing PE optimizes scanning parallelism and is capable of constructing and interpreting compacted what are known as bitmap bundles of row pointers which are a compressed representation of row pointers sparse or dense that can be packed into logical tuples flowing through an IMC it operates most efficiently for highly selective predicates amplifying the benefits thereof. Regardless its MOP support locates specific database content.

Scanning Indexing PE also implements MOPs which project database column groups from HARP memory search and join index structures and manipulate in flight metadata flows composing merging reducing and modifying multi dimensional lists of intermediate and final results. Depending on the MOP input is one or more Value Lists whose content may be interpreted in a one or two dimensional manner where two dimensional lists may have an arbitrary number of columns which may have arbitrary logical width .

In the context of list reduction a UOP program of finite length and complexity is stipulated as a predicate function to qualify one or more components of the input Value List elements eliminating tuples which do not qualify. List composition involves the combining of related lists into a single output format which explicitly relates the input elements by list locality while list merging involves intermingling input tuples of like size in an unrelated order. Modification of lists involves a UOP which can generate data dependent computations to replace component s of each input tuple.

The Scanning Indexing PE may also be used for joins with indexes like a Group Index which involves the association of each input tuple with potentially many related data components in a one to many mapping as given by referencing the index via a row pointer component contained in each input tuple. MOPs implemented by the Scanning Indexing PE may thus relate elements of a relational database in by query specific criteria which is useful for any query of moderate to advanced complexity.

XCAM PE implements MOPs which perform associative operations like accumulation and aggregation sieving sorting and associative joins. Input is in the form of a two dimensional metadata Value List which can be interpreted as containing at least two columns related by list locality key and associated value.

Accumulation occurs over all data of like keys associatively applying one of several possible aggregation functions like Summation or an atomic compare and exchange of the current accumulator value with the input value component. A direct map mode exists which maps the keys directly into HARP memory employing a small cache not shown to minimize memory access penalties. A local mode of accumulation exists as well to realize zero memory access penalties by opportunistically employing the cache at the risk of incomplete aggregation.

Sieving involves the progressive capture of keys qualifying as most extreme according to a programmable sieving function generating a result list of the original input keys and values such that the last N tuples keys are the most extreme of all keys in the original input. Iterative application of Sieve can converge on a sorted output over groups of some small granularity.

Sorting can also be accomplished through construction and traversal of either hashes or B Trees which are constructed to relate each input key to its associated value with a structure that is efficient to search and join with.

Within each of PEs thus may be a UOP Processing Engine not shown . Whereas PEs execute MOPs in a dataflow fashion at the higher levels embedded UOP Processing Engines in PEs realize the execution of UOPs which embed within their logical MOP parent to serve its low level data manipulation and analysis needs. In some embodiments the UOP processing engine is code flow logic where a UOP program is executed repetitively by a parent Processing Element at MOP imposed boundaries given MOP extracted input data to produce results interpreted by the parent MOP.

Considering the code flow nature each UOP engine has its own program storage persistent register set and execution resources. It is capable through appropriate UOP instructions to accept data selected from the parent MOP and to simultaneously execute specified data manipulation or analysis thereon in combination with some stored register state. In this manner this tiny code flow processor is able to fit seamlessly into the dataflow as a variable latency element which at the cost of increased latency is capable of performing any of the most complex low level data manipulation and analysis functions on the dataflow pouring through. The capability of the MOP to select and present only those data required for UOP processing at a fine granularity minimizes the latency imposed by the UOP code flow maximizing overall dataflow throughput.

The C2 solution utilizes various hardware friendly data structures to assist in hardware accelerating database operations by HARP modules . In general hot columns i.e. columns having active or frequent access stay in the HARP memory so that they can be accessed randomly fast. Warm Columns i.e. columns having less active access also stay in the HARP memory but occasionally they may be evicted to a disk in storage infrastructure . Cold columns usually be held in storage infrastructure but may be partially brought into HARP memory e.g. for one time usage. In some embodiments date columns in the Sorted Compressed format will be held in the memory of host system and accessed by the software running on host .

In general there is a single entry point for HARP module to identify all the database columns. In particular as shown in a root table points to all the available table descriptors . The table descriptors in turn point to their respective table columns . Each table stores multiple columns in the VA memory space. Each of these tables will now be further described.

As noted root table identifies all the tables accessed by HARP module . In some embodiments each entry in the table takes 8 bytes. When needed multiple Root Table blocks can be chained by a next pointer. The Descriptor Pointers in the root table points to the individual table descriptors. The indices of the Descriptor Pointers also serve as the table ID. To simplify the hardware design a CSR Control Status Register may be employed to store the Root Table information as long as the hardware accessible Table IDs and Descriptors information is retained in HARP module .

Each database defined table has a table descriptor . All the table descriptors may reside in the HARP memory . A table descriptor may comprise different groups of data. A group may contain one or more columns. Within a group the data is organized as rows. A group of data resides in a memory plane which is allocated to it. A data element in a particular plane has direct reference to its corresponding element in another plane. The relationship of the addresses among all the element pairs is the same arithmetical computation. The table descriptor is portable because the present invention utilizes a global virtual address space. In other words when copying the table descriptor from one virtual memory location to another all the information in the table is still valid.

In the C2 solution the data structures of the database are architected to optimize database data processing in HARP hardware. All table columns column groups indices and meta data are defined in a global database virtual address space DBVA . A reserved DBVA section is allocated for table descriptors as part of the meta data. Table descriptors include information about a table such as the table name number of rows number of columns column groups column names width s within a column group etc. In addition to the information of data layout and access information in the VA space the table descriptors also have information about the compression types algorithms used for each individual column. In the present invention hardware can directly use this information to accomplish database queries and table element insertion update and deletion.

Of note in the present invention the columns or column groups possess an implicit row id RID . A RID is considered implicit because it is not materialized as a part of a column or column group. Instead each column and column group is designated a starting RID which corresponds to an address in the global database virtual address space which is then mapped to a physical address in HARP memory . Since each column and column group is a fixed width the RID can provide the basis for arithmetically calculating the memory address of any data in the column or column group.

In some embodiments all columns are packed together in the single DBVA. In addition a meta data structure may be employed to facilitate certain column accesses. For example as shown a row pointer primary key index may comprise a sorted list of primary keys and their associated row id RID in a column or column group. Of course a B tree index may be used as an alternative to this type of index.

In the present invention two active sets of database regions are maintained i.e. a main database region and an augment region for newly added data. Query processing operates on both regions and is accelerated by the HARP module . The augment region is utilized to hold new inserted items. Optionally the augment region may be rolled into the main region. For example as shown in RIDs 1 n are the main region while RIDs n 1 etc. comprise the augment region.

Deletion updates may be committed into the main region right away. To alleviate the drastic changes across all the columns in a table the present invention may allocate a valid or invalid bit. A row deletion in a table therefore becomes a trivial task of setting the appropriate bit in every column group in the table.

C2 Software will decompose this query into 24 MOPs to send to HARP module along with their dependency information which establishes the topology of the dataflow from MOP to MOP. All MOPs are started and hardware processing begins in pipelined fashion with each MOP s results being fed to one or more downstream consumers over one or more dedicated logical IMC connections.

The responsibility of the first MOP ScanCol is to reference HARP memory to find all the customers in the CUSTOMER table who belong to the BUILDING market segment producing into IMC all matching CUSTOMER references in the form of one RID per qualified row. RevIndex then traverses a reverse index residing in pre built to relate customers to their one or more orders residing in the ORDERS table outputting references to all orders made by the given customers. Because the CUSTOMER references are no longer necessary and to boost performance by reducing utilization of IMC transmission resources over IMC the ListProject removes the original customer references after the reverse index join leaving only the ORDER references. The ScanRPL MOP then scans these orders O ORDERDATE column retaining ORDER references only to those orders whose order date occurs before the date 1995 03 15 .

Progressing onward through IMC the dataflow entering RevIndex consists of ORDER table references RIDs which have satisfied all criteria mentioned thus far each order was placed by a customer in the BUILDING market segment before the date Mar. 15 1995. To finish evaluating the WHERE clause of the illustrated SQL query statement these orders must be qualified in terms of certain properties of their related line items.

The purpose of the RevIndex MOP is then to associate each of the qualifying orders to its one or more constituent line items from the LINEITEM table returning appropriate references thereto. At this point the flow contains a two column tuple list relating ORDER references RIDs to LINEITEM RIDs multicasting identical copies of these tuples into IMC and IMC. ListProject extracts only the LINEITEM RID column from the dataflow in preparation for ProjRpl which extracts each line item s L SHIPDATE column value feeding these ship dates to IMC. ListCompose consumes IMC along with IMC executing a composition of the input lists to create a three column tuple list where each tuple contains an ORDER RID an associated LINEITEM RID and its ship date. ListSelect consumes the composed list from IMC and selects only those tuples having ship date older than Mar. 15 1995 thus completing the WHERE clause requirements.

Again at the output of ListSelect the dataflow still logically appears as a three column tuple list where each tuple relates an ORDER RID to one of its associated LINEITEM RIDs and that line item s ship date. It should be noted in this flow that multiple distinct LINEITEM RIDs may appear in different tuples with an identical ORDER RID a definite possibility here since a single order may be comprised of an arbitrary number of line items in the target database and this query specifically requests only those line items satisfying the ship date criteria. The redundancy of ORDER RIDs in the list suggests an aggregation step will be needed to realize the SUM of the SQL select statement but before that some more data must be gathered and calculations done.

IMC and IMC both carry the output of ListSelect identically. ListProject extracts only the LINEITEM RID column from IMC passing that on to both ProjRpl and ProjRpl which fetch each referenced LINEITEM s L EXTENDEDPRICE and L DISCOUNT respectively. Those procured extended price and discount data are then composed together by ListCompose to form a two column tuple to be carried via IMC . ListTupleArith implements the arithmetic process of computing L EXTENDEDPRICE 1 L DISCOUNT on a per tuple basis before sending this arithmetic result to ListCompose . In the meantime ListProject extracts the ORDER RID column from the output of ListSelect such that ListCompose can make a two column composition relating within each tuple an ORDER RID to its line item s arithmetic product.

The final hardware step to complete the query involves fully evaluating the SELECT clause including its SUM aggregation function. The remainder of the MOP flow of beginning with the output of ListCompose is dedicated to this process.

AssocAccumSum receives from IMC with each of the two column tuples relating an ORDER RID to one of its line item s L EXTENDEDPRICE 1 L DISCOUNT product computing a summation of these values independently for each distinct ORDER RID. For example a given ORDER RID may appear twice in IMC once in two different tuples having two distinct LINEITEMs which satisfied all criteria thus far. Each of these LINEITEMs would have generated its own product in ListTupleArith such that the aggregation process of AssocAccumSum must sum them together. The result is a distinct sum of products over each distinct ORDER RID realizing the SQL SUM aggregation function here named REVENUE within the query.

Once the aggregation has completed for a given ORDER RID ListProject extracts the ORDER RID itself passing it to ProjRpl ProjRpl and ProjRpl . These MOPs gather in parallel the referenced orders O ORDERDATE O SHIPPRIORITY and O ORDERKEY respectively while ListCompose forms a two column tuple consisting of O SHIPPRIORITY and O ORDERKEY. ListCompose meanwhile forms a two column tuple comprised of O ORDERKEY and REVENUE. The final MOP ListCompose composes the two two column tuple lists into a final four column tuple list which satisfies the SQL query and its SELECT statement.

It should be noted in this example that the SQL query SELECT actually stipulates L ORDERKEY. But an optimization may be applied here knowing that O ORDERKEY is functionally equivalent when used in this manner thus avoiding the need to carry any LINEITEM RIDs beyond IMC or IMC.

In we have described how an SQL statement gets mapped into a logical MOP DAG directed acyclic graph which gets executed in a dataflow fashion with IMC chaining between MOPs. illustrates an exemplary dataflow through PEs in HARP logic for the same TPC H SQL query shown in . As noted C2 Software will decompose this query task into 10 PE stages to send to HARP module along with their MOP and UOP instructions and dependency information.

Stage 1 is performed by Scanning PE is to find all the customers in CUSTOMER table that is in BUILDING market segment and passes the results C RIDs of matching customer records in an IMC to Indexing PE .

Stage 2 is a join operation of C CUSTKEY O CUSTKEY performed by Indexing PE using a reverse index method. Each C RID of Stage 1 s matching customer records corresponds to an O RID hitlist of ORDER table records given a customer may place multiple orders. The results O RIDs are passed in an IMC to Scanning PE .

Stage 3 is performed by Scanning PE to read the O ORDERDATE field of all the matching orders ORIDs that Stage 2 outputs compare for 

Stage 4 is a join operation of O ORDERKEY L ORDERKEY performed by Indexing PE using a reverse index method. Each O RID of Stage 3 s matching order records corresponds to an L RID hitlist of LINEITEM table records given an order may have multiple line items. The results L RIDs are passed in an IMC to Scanning PE .

Stage 5 is performed by Scanning PE to read the L SHIPDATE field of all matching line items L RIDs that Stage 4 outputs compare for Mar. 15 1995 and passes the results L RIDs in 3 IMCs to Indexing PE and .

Stage 6 is a column extraction projection operation done by Indexing PE and to get L ORDERKEY L EXTENDEDPRICE and L DISCOUNT column.

Stage 8 is an aggregation operation of REVENUE of each L ORDERKEY group done by XCAM PE based on outputs of Indexing PE and . As the SQL statement defines REVENUE is calculated as the sum of L EXTENDEDPRICE 1 L DISCOUNT . Note that even though the GROUP BY defines the group key as concatenation of L ORDERKEY O ORDERDATE O SHIPPRIORITY the group key is simplified to L ORDERKEY since it is already a unique identifier. The output of XCAM PE is a pair list of group key L ORDERKEY with its REVENUE.

Stage 9 done by Indexing PE and is a column extraction of O ORDERDATE based on L ORDERKEY output of XCAM PE .

Stage 10 done by XCAM PE is a sieve ORDER BY operation of REVENUE O ORDERDATE to output top N groups with largest REVENUEs. These outputs are placed at a result buffer area in HARP memory ready to be retrieved by DBMS software .

Other embodiments of the invention will be apparent to those skilled in the art from consideration of the specification and practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only with a true scope and spirit of the invention being indicated by the following claims.

