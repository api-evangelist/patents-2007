---

title: Secure pre-caching through local superdistribution and key exchange
abstract: A distributed peer-to-peer document archival system provides the version-control, security, access control, linking among stored documents and remote access to documents usually associated with centralized storage systems while still providing the simplicity, personalization and robustness to network outages associated with personal and peer-to-peer storage systems.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08046328&OS=08046328&RS=08046328
owner: Ricoh Company, Ltd.
number: 08046328
owner_city: Tokyo
owner_country: JP
publication_date: 20070330
---
The present invention relates generally to document archiving and document distribution and in particular to a distributed secure peer to peer document archival system.

In a typical business workgroup IT infrastructure two basic functions must be provided. The first is to insure that team members are able to access their documents and share them with other members. The second is to insure that no one else can access those documents. The first function typically requires a dedicated file server centralized backups dedicated network static IP address and domain name service the second requires firewalls account and password management and physical security for one s servers. Even when membership of a team is clearly defined and relatively static such an infrastructure is difficult and expensive for a small business to maintain. It is even more difficult when a team is made up of members from several different organizations and who might collaborate in some areas and compete in others.

The groupware model provides features that are especially useful to a single work group company or other well defined group of collaborators that wish to maintain a group memory. These features include remote access to documents restricted access for non group members security version control and unique handles for documents to allow both linking and the creation of compound documents. Groupware systems are most often provided by centralized architectures such as file servers and Web based content managers.

Conversely the personal archive model has features to support the mobile distributed and loose knit organizations that are becoming increasingly prevalent in today s business world. Knowledge workers in these environments tend to work on many projects at once and simultaneously belong to many overlapping and potentially competing communities. They are also increasingly mobile and often find themselves in environments with slow partitioned or no network access. Knowledge workers in these environments need a sharable personal archive one that is easy for a single person to maintain works both on and off line and supports an intuitive limited publication model that allows an ad hoc working group to share some pieces of information while keeping others confidential. These features all suggest a decentralized solution where each user maintains his or her own archive and shares certain files with others as is provided today by PDAs locally stored email archives and traditional paper based documents.

From a user s perspective the main difference between the centralized and decentralized solution is whether control naturally lies with the publisher or the reader of a document. On the Web the publisher of a site or his designated site administrator has ultimate control and responsibility over who has access to a document who can modify it and whether past versions are made available. The publisher may also decide to take a site down entirely thus denying access to everyone. With email and paper based solutions it is the reader who has control. Anyone who receives a paper document has the ability to share it with someone else simply by making a photocopy and once someone receives a paper document it is very difficult for the original author to take it back. Similarly email is often forwarded to others sometimes with modifications or annotations made by the person doing the forwarding. The decision to grant or deny access to a document is distributed among those who already have access with limitations imposed through social and sometimes legal rules.

Whether publisher or reader control is better depends on the organization the environment in which the information is being produced and used and sometimes on who is doing the judging. Centralized solutions such as password and firewall protected Web servers work well in environments where there are clearly defined groups of people who need access to clearly defined sets of documents and where there is a clear distinction between authors and consumers of information. In more collaborative environments where group boundaries are fuzzier a distributed solution is often better. Most workers today fall somewhere between these two environments engaging in both ongoing and ad hoc collaborations and thus need the advantages of both centralized and decentralized systems.

A personal document archive system according to the present invention provides for secure publication of compound documents to a limited audience. The present invention has been reduced to practice by the inventors and is referred to herein generally as Makyoh. Features include version control secure storage permanent handles for publications and versions URIs and the ability to build compound documents and organize documents into directory trees. It also provides features including robust redundant storage an intuitive paper like publication and access control model and the ability to operate in environments with slow partitioned or no network access.

The present invention introduces the idea of a feed a term borrowed from but otherwise not to be confused with news feeds used on the Web. A feed in accordance with the present invention can represent a mutable document wherein each new feed entry represents a new version of the document. A feed can also represent a publication channel where each feed entry is its own content e.g. blog entries or messages on a message board. Each individual entry in a feed can be accessed through its own unique URI. The present invention provides special URIs for accessing the latest entry in a feed useful for representing version controlled documents and for accessing a merged view of all known entries useful for representing blogs and other aggregations of multiple documents over time .

Entries can be posted to a feed from multiple machines Makyoh servers and if desired by multiple authors. Authoring distribution and reading of documents are all completely decentralized. The ability to publish is garnered by obtaining the publication key for a feed.

To access a particular document or feed a user must possess that document s or feed s key. Each document as represented by a file or set of files is associated with a unique key called a document key. A document key grants the ability to both identify and decrypt the file or set of files that make up a single fixed version of the associated document. Each feed and its entries is associated with two unique keys called a subscription key and a publication key. A subscription key grants the ability to both identify and decrypt the file or set of files that make up entries in the associated feed but does not grant the ability to add new entries to the feed. A publication key grants the ability to both identify and decrypt the file or set of files that make up entries in the associated feed and also grants the ability to add new entries to the feed through a process called publication. A user can grant access to a document or feed to someone else by giving the appropriate key. The receiver will then import the key into his or her personal Makyoh server. In an embodiment of the present invention the key is encrypted using the user s passphrase and stored in a private directory on his or her personal Makyoh server s local disk.

A personal document archive system according to the present invention provides robust secure document storage and sharing without requiring any of the infrastructure required in conventional archiving systems. It is robust without requiring dedicated servers scheduled backup or even a reliable network and it is secure without the need for account password firewalls or secure server rooms. To an end user and his applications the archive appears to be a local disk. Once the user has entered his passphrase to unlock the system his entire archive is available in this manner. In a particular embodiment of the present invention each file and directory is actually stored on disk in its own encrypted file called a blob Binary Large OBject . Each blob has its own unique 128 bit symmetric decryption key. No one can access the contents of an archive without the appropriate key even if they steal the hard drive. Blob decryption keys can be listed in encrypted directory blobs but not in all cases. For example the key for a blob representing a single file document might only exist as a printed 2D barcode representing the document key.

As stated above conventional centralized and decentralized systems typically differ in how control over a document is divided between the publisher and the reader with centralized systems leaving more control in the hands of the publisher and decentralized systems giving the reader more control. In accordance with the present invention control over how resources e.g. files directories and feed entries can be accessed and modified is more evenly balanced between publishers and consumers than is the case in either typical central server systems like the Web or decentralized systems like email. In particular the present invention ensures the following needs are met for readers publishers and re publishers e.g. readers who are also publishers e.g. readers who modify material they have read and then publish the modified material .

While striving to satisfy the needs of publishers compared to Web based publishing systems a system according to the present invention tends to grant more power to readers and re publishers. This is for two reasons First as was stated above there are many environments where giving end readers the ability to re distribute and republish information is far more efficient than central control. Words like sharing and republishing give nightmares to executives in the music and movie industries but this kind of communication is the norm when it comes to internal office communication especially when dealing with paper documents. Second most technology trends are pointing towards more reader control rather than less. Local storage capacity continues to increase and local CPUs continue to get faster while mobile network speeds and the batteries necessary to power them are improving far more slowly. Web pages that might disappear are cached not just by Google but by non profit organizations like The Internet Archive the Memory Hole and even independent bloggers. Content sites that were once published exclusively on the Web are increasingly offering podcasting and RSS feeds that make it easy for readers to download content and read or listen to it from their own local cache. Meanwhile Digital Rights Management DRM systems that are designed to restore power to publishers in the music movie and book industries are finding their schemes cracked soon after release and security experts say the very idea of DRM is fundamentally flawed.

The present invention has been reduced to practice by the inventors and is referred to hereinafter generally as Makyoh. A prototype version of the Makyoh personal server has been implemented on a Java based server. Encryption storage versioning digital signature functions peer to peer distribution and server discovery have all been implemented.

Makyoh provides robust and secure document storage and document sharing without needing any of the conventional infrastructure as described above for example. It is robust without requiring dedicated servers scheduled backup or even a reliable network and it is secure without the need for account passwords firewalls or secure server rooms. In accordance with an embodiment of the present invention to an end user and his applications a Makyoh archive appears to be a local disk actually a locally running WebDAV server what is sometimes called a Web Folder . Once the user has entered his passphrase to unlock the Makyoh system his entire archive is available in this manner i.e. as a local disk . As with all WebDAV servers his archive can also be viewed as a web page using a standard web browser. In a particular embodiment each file and directory is stored in its own encrypted file called a blob for Binary Large OBject on persistent storage media such as a hard drive or removable media e.g. devices popularly referred to as thumbnail drives . Each blob has its own unique 128 bit symmetric decryption key. Consequently no one can access the contents of an archive without the appropriate key even if they steal the storage media. Blob decryption keys can be listed in encrypted directory blobs but not in all cases. For example the key for a blob representing a single file document might only exist as a printed 2D barcode representing the document key.

A personal document archive system also referred to herein as Makyoh in accordance with the present invention is shown in . The system comprises a plurality of personal servers referred to herein variously as personal servers Makyoh personal servers Makyoh servers servers and the like for receiving and storing documents and for serving documents. The figure illustrates as an example four portable personal servers such as laptop computers hand held data devices cell phones etc. It will be apparent that the personal servers can also be traditionally non portable computing devices such as desktop PCs and the like. Communication among the personal servers can be by any suitable wireless technology e.g. Bluetooth IEEE 802.11 and so on or over any suitable wired technology e.g. ethernet serial port connections and so on .

The personal servers collectively provide both secure storage of resources and a secure peer to peer model for publishing resources to a limited audience. Each personal server stores a Makyoh archive comprising of one or more resources where a resource is a file directory feed or feed entry. Each Makyoh archive can be thought of as an encrypted locally cached mirror of both resources that have been created locally and those that have been created on other Makyoh servers and subsequently published. A Makyoh archive is implemented using a combination of encrypted blobs representing files directories and feed keys a set of decryption keys and feed entry files each of which is associated with a particular feed. Typically every user will have stored on her personal server her own locally stored Makyoh archive. Additional details of a Makyoh archive will be given below.

A Makyoh personal server performs three main functions. First the server maintains an encrypted version controlled personal archive. Second the server acts as a local mirror of resources that have been published by other Makyoh personal servers . Finally the server distributes these mirrored resources to other Makyoh personal servers with which it comes into contact. In this way every Makyoh personal server functions as a personal archive as a node and router in a peer to peer network and as a mirror for nearby archives. All personal servers are able to participate in routing and mirroring activities but since all resources are encrypted only those who know the decryption key for a given resource are able to read its contents.

Because blobs are always encrypted they can be distributed freely without worrying about revealing sensitive information. In particular whenever a user accesses a resource his local Makyoh server will automatically find all nearby Makyoh servers using an open protocol called Bonjour and distribute all the blobs associated with that document to all other Makyoh servers in the area. This process referred to herein as local superdistribution accomplishes two things First it automatically creates an encrypted backup of the user s documents on all the other machines Makyoh servers in the area. Second it pre caches documents that the user might want to share with other people in the area.

A communication interface represents hardware and software elements for communication with users and other Makyoh servers . For example the communication interface can include one or more connectors to which a display device and an input device e.g. keyboard are connected and related drivers for interacting with the display device and the input device. The communication interface can include connectors e.g. ethernet for wired communication with other Makyoh servers . The communication interface can include a wireless transceiver e.g. Bluetooth or 802.11 compliant hardware for wireless communication with other Makyoh servers .

The data processing component is shown executing an operating system OS . For example in an embodiment of present invention the OS can be the Microsoft Windows operating system the Apple OS X operating system or the Linux operating system. The data processing component is also shown executing two application programming interfaces API one called a trusted user API and the other called a remote user API . These APIs working in conjunction with functionality of the OS provide application level programs with functionality in accordance with the present invention. The APIs are discussed in further detail below.

The APIs provide services for higher level applications . In a particular embodiment of the present invention one such application is a Java based server. The server application includes all the WebDAV WEB based Distributed Authoring and Versioning functionality necessary for mounting a resource or full archive as a disk under the OS e.g. Microsoft Windows operating system Apple OS X operating system Linux operating system etc. . The archive can then be browsed and modified using the operating system s standard file browsing user interface or any other suitable file browsing application.

Every resource in a Makyoh archive explained below can be associated with a unique URI Universal Resource Identifier referred to herein as a hash URI . This special type of URI follows the general URI format commonly used with Web browsers having the following specific form 

Hash URIs function as both identifiers and keys and thus can be used to both retrieve encrypted blobs from nearby servers and to decrypt those blobs once they are retrieved. Once retrieved the remaining fields let the server know how the blob contents should be decrypted and presented to the user.

Access control in Makyoh is primarily done using hash URIs. Once someone imports a hash URI often simply called a key into his Makyoh archive he has access to the contents of the file it identifies. Makyoh also uses special kinds of files namely directory blobs and feedkey blobs to grant access to a large and possibly extensible set of files given a single hash URI. In general users will interact with three kinds of hash URIs document keys which give access to a single immutable file or directory tree subscription keys which give the ability to read feed entries for a particular feed and publication keys which give the ability to both read feed entries for a particular feed and publish new entries for that feed.

A hash URI can be used directly as a hyperlink similar to how URLs are embedded in email and web pages. All that would be necessary is to write a browser plug in to access the new URI format and retrieve the necessary blobs from some data store. However this kind of usage is discouraged in Makyoh because it is not very flexible in terms of access if a user has access to a document that contains a hash URI for another document he automatically has access to both. If at a later time the author wanted to allow access to only the first document he would need to edit its contents and remove all mentions of the second hash URI before handing out the hash URI to the first document.

Instead of using a hash URI directly it is preferable to use the archive directory structure that is presented in the trusted user API which is based on the ID of a document or feed. As with hash URIs the path to a particular document or feed entry is the same for all Makyoh users but unlike hash URIs an archive path does not reveal the document s decryption key. Users who already have the key and thus have been given access to the document or feed will be able to access the file or files at the given path while other users will not.

Makyoh provides a personal archive and typically every user will run his own individual personal server . The personal server maintains an encrypted local copy of all the user s entire archive and will also replicate encrypted documents on nearby servers . This distinguishes Makyoh from conventional distributed document stores like FreeNet or OceanStore which assign each file to specific nodes in a distributed network of storage servers. Makyoh presents two separate APIs 

The first API is the trusted user API shown in . As the figure illustrates the Makyoh archive appears via the trusted user API as a virtual file system also referred to as the archive view . It is virtual in that the file system structure that is presented to the trusted user is not necessarily that of the underlying organization of the constituent files of the Makyoh archive as they are stored on the storage device. The virtual view presents abstractions of the underlying physical files that constitute the Makyoh archive. The virtual view can be any suitable file structure view a common paradigm of course is the hierarchical file structure. For purposes of discussion a virtual hierarchical file system will be assumed.

As illustrates the trusted user API presents the Makyoh archive as a file system of folders and documents organized in a directory hierarchy virtual file system archive view . Additional details of this archive view are discussed below. The Makyoh archive can be accessed either using a standard Web browser communicating with the local Makyoh server also referred to as localhost using HTTP or as a part of the local file system sometimes called Web Folders using the WebDAV protocol. The trusted user API is only available from locally generated connections that is connections to localhost and only after the user has authenticated with the Makyoh server using his passphrase.

The second API is the remote user API also shown in . The remote user API presents to other Makyoh servers so called un trusted users the raw files comprising the Makyoh archive e.g. feed entry files encrypted blobs and so on as they are actually stored in the storage component . These raw files are also accessible via HTTP and WebDAV protocols and are used by other Makyoh servers to find and retrieve needed blobs and feed entries and to push blobs and feed entry files onto yet other servers.

From an authenticated user s perspective the virtual file system view of the Makyoh archive comprises two kinds of resources documents and feeds. A document is an immutable file or directory tree while a feed specifies a distribution channel through which one may subscribe to new documents called entries that are published to the feed. Each document and feed is associated with a unique URI Universal Resource Identifier which serves both as an identifier and a decryption key allowing access to the resource. Documents are immutable a URI pointing to a document is guaranteed to always point to the same exact contents. Feeds are mutable in that new entries can be published to a given feed. Each feed entry is identifiable by its own URI and will itself point to an immutable document that represents the contents of the entry. A feed can be used as a publication channel where each feed entry is its own content e.g. blog entries or messages on a message board or it can represent a mutable version controlled document where each new feed entry represents a new version of the document.

A docs directory contains documents which are immutable i.e. do not change. A feeds directory contains feed entries which are mutable by virtue of receiving entries published by the local server and by remote servers . The user can decrypt and view those documents in the docs directory and feed entries in the feeds directory for which he has imported the appropriate document subscription or publication key. A keyring directory contains all keys that the user has ever imported. In an embodiment of the present invention these keys are been encrypted using the user s passphrase as a symmetric key and stored in a private directory on the local server .

Documents are stored under the docs directory in respective subdirectories . Each subdirectory is named by an identifier referred to as the blob Id which is defined as the SHA 1 hash of the encrypted contents of the blob representing the file or root directory for the document written as a lowercase 40 character hexadecimal string. For example where the document is a single file e.g. my document.pdf the name of the subdirectory within which that file is presented is based on the SHA 1 hash of the encrypted contents of the file s corresponding blob. For example suppose the SHA 1 hash of the encrypted contents of the encrypted blob representing my document.pdf is the text string 

If a document consists of a directory of files then the name of the subdirectory is based on the SHA 1 hash of the directory blob corresponding to the directory of files. The directory blob is an invisible file which stores information about the contents of the directory itself e.g. a list of files and or sub directories. For example shows that subdirectory contains a directory of files called my web page . The directory file schematically indicated in by the dash lined box contains information about the directory my web page . The name of the subdirectory is based on the SHA 1 hash of the encrypted contents of its directory file and in an embodiment of the present invention the pathnames might appear as 

Referring to feeds are stored under the feeds directory . Each feed is stored in a feed subdirectory named by the feed s ID which is defined as the fingerprint of the public key used to verify the feed s signature described later . Each feed directory contains a subdirectory for each entry named by the creation time of the entry followed by a period . followed by the SHA 1 hash of the contents of the feed entry file. The creation time should be encoded in Coordinated Universal Time UTC in the form yyyyMMdd T HHmmss Z where hh is the hour in 24 hour format and T and Z are the literal characters T and Z.

Within a feed subdirectory is a file or a directory tree representing the entry. For example a feed with two entries might appear as 

Feeds also contain up to three other directories a scratch directory . . . a latest directory . . . latest and a merged directory . . . merged . If a user has the ability to publish to a given feed the scratch directory will be available in the corresponding subdirectory . This is an editable local only directory that can later be published as a feed entry. The contents of the scratch directory are not available to other Makyoh servers until they are published. If a feed contains at least one published entry then corresponding latest and merged directories will be available. The latest directory always contains a copy of the latest known entry determined by the entry s timestamp. The merged directory contains a merge of all paths contained within all known entries.

For example if a feed contains two entries one containing the path . . . images thing1.jpg and the other containing the path . . . images thing2.jpg a listing of . . . merged images would show both thing1.jpg and thing2.jpg. The directory structure might appear as 

The keyring directory is a directory containing all keys that a user has ever imported. Keys are represented as key files with the extension .makyoh . Key files for document keys contain the hash URI of the file or directory that represents the document associated with the key. As will be explained below there are two kinds of keys for a feed a subscription key and a publication key. Key files for a feed s subscription key contain the hash URI of the subscription feedkey blob. Similarly key files for a feed s publication key contain the hash URI of the publication feedkey blob.

Local users can perform the usual HTTP and WebDAV requests GET PUT HEAD MKCOL PROPFIND LOCK UNLOCK DELETE MOVE COPY and OPTIONS POST is not currently supported . In addition local users i.e. users on the localhost may perform various operations by performing an HTTP GET request to the localhost on the appropriate port with the query parameter op e.g. GET http localhost 8088 op create . The following operations are provided 

Referring to connections to Makyoh server from remote Makyoh servers are presented with a view physical view of the files as they are stored on the storage device of the storage component of the Makyoh server . This is compared to the archive logical or virtual view that is presented a trusted user described in .

In a particular embodiment the remote user is presented with a blobs directory and an entries directory . The blobs directory simply contains encrypted blob files each with the SHA 1 hash of its encrypted file contents as its filename. For example 

The entries directory contains feed entry files each within a subdirectory named with the feed s ID. The entry file itself is named by the creation time of the entry followed by a period . followed by the SHA 1 hash of the contents of the feed entry file. As described above the creation time should be encoded in Coordinated Universal Time UTC in the form yyyyMMdd T HHmmss Z where hh is the hour in 24 hour format and T and Z are the literal characters T and Z. For example 

In accordance with an embodiment of the present invention the files and directories presented in the remote view are the actual files and directory structure as stored on disk. Remote servers can perform a subset of the HTTP and WebDAV type 2 requests e.g. GET PUT HEAD MKCOL PROPFIND LOCK UNLOCK and OPTIONS. Other requests e.g. POST DELETE MOVE or COPY will return with a Bad Request error.

Referring now to blob files are immutable and represent just a single version of a file as it existed at a particular time. In each blob file is illustrated by a document icon and a lock icon. The document icon associated with a blob file represents the contents of the file which the associate lock icon indicates that the contents are encrypted. The encrypted contents of the blob files are decrypted using their respective symmetric decryption keys . Each decryption key is illustrated in with an arrow leading to the encrypted blob file for which it is serves as the decryption key.

As stated above a blob file is immutable i.e. a given instance of a blob file cannot be modified. A user can nonetheless make modifications for example by reading in the file making the desired edits writing out the modified contents into an entirely new blob file along with its own unique ID and decryption key . A blob file along with its ID and key are automatically computed based on the contents of the file being encrypted. The file is first prepended with a null terminated header consisting of the blob s type currently blob directory or feedkey the document s length in bytes and an optional salt string all separated by spaces. This plaintext is then compressed using a known algorithm called the DEFLATE algorithm and encrypted with the known Advanced Encryption System algorithm using the MD5 hash of the plaintext as the encryption key. The ID for the resulting blob is the SHA 1 hash of the encrypted blob s contents encoded as a 40 hex digit lowercase string. More formally 

The header serves two purposes. First it guarantees that even zero length documents can generate an MD5 hash. Second it includes an optional salt which can be used to generate a blob file with a different ID than would be obtained if no salt was used. This can be less efficient in terms of storage but provides additional privacy against some kinds of attacks.

One advantage of using hashes for a blob s key and ID is that the process is entirely determined by document contents multiple copies of the same exact document will produce the same blob file and blob Id even if the documents were independently published by different people. This reduces the amount of storage an archive uses especially in cases where the same file appears in several different documents directory trees. The only exception is when a publisher adds the optional salt to their headers which by design creates a different blob and blob Id based on the salt.

A directory blob is simply a list of hash URIs pointing to the files and subdirectories the directory contains encoded and encrypted in blob format as described above. Directory blobs have the type directory. For example the decrypted contents of a directory blob containing two files and a subdirectory might consist of the following 

A feed key blob is a file containing keys necessary for decrypting verifying and optionally for creating publishing feed entries. Feed keys come in two forms subscription keys which give read only access to a feed and publication keys which grant both the ability to read entries and to publish new entries. The feed key file consists of the following fields each separated by a linefeed n . The entire contents are then encrypted and encoded as a blob as described above.

A feed s ID is defined as the 160 bit key fingerprint of the feed s verify key in accordance with the OpenPGP Format standard encoded as a 40 character lowercase hexadecimal string.

A feed entry file is a file that contains information about an entry to a feed. The feed entry file comprises the following fields each separated by a linefeed n . These contents are not encoded as an encrypted blob though the Entry field shown in is encoded in encrypted form as described below .

The keyring is a collection of keys i.e. hash URI s the user has imported. In one instantiation of the invention the keyring is implemented as a private directory stored on the local Makyoh server . Referring to when a user logs into a Makyoh server for the very first time a personal keyring directory is automatically created. When a key is imported it is encrypted using the user s passphrase as the symmetric key and the resulting encrypted file is then stored in the keyring directory . When the user logs in using his passphrase Makyoh bootstraps by decrypting all key files in the user s keyring directory. The process of importing keys hash URI s is explained further below.

A typical usage scenario of the present invention will now be described. As an example imagine a user Alan is attending business negotiations with a competitor and the user s documents are stored in his personal Makyoh archive running on his laptop. When Alan accesses an outline of the negotiation strategy on his laptop the encrypted blob s for that outline will be replicated by his laptop on all other laptops running Makyoh in the area. If the key for that document is never revealed then the user has effectively securely backed up a copy of his document on the laptops of everyone else in the meeting. Conversely the user s Makyoh server is likewise backing up documents of other laptops when documents on those laptops are accessed. If Alan s laptop is later stolen he can recover his document by purchasing a new laptop installing Makyoh and re importing his key s and Makyoh would then automatically retrieve all the necessary blobs from the other laptops in the area. In a particular embodiment of the present invention the key is the hash URI described above. Users carry hash URI s one for each document or directory of documents or feed entries and pass them around to other users to give them access to the information. The hash URI is a small amount of data on the order of a hundred or so bytes that can be conveniently stored in a key file on a storage device e.g. thumbdrive on a printable medium e.g. linear barcode two dimensional barcode etc and so on.

Now imagine that later in the meeting a colleague Bob asks for a copy of Alan s strategy outline. The file may be very large especially if it contains multimedia and might take several minutes to transfer over wireless or even USB thumbdrive. However because Alan s Makyoh had already distributed the encrypted blobs that make up the document to the other laptops including Bob s the data is already on Bob s laptop. Alan need only give Bob the associated key file hash URI stored in a file which will typically be less than a couple hundred bytes. Because keys are so small they can be transmitted quickly and securely in a variety of ways that are not possible with larger files. For example they can be printed on business cards as 2 dimensional barcodes beamed to a PDA via infrared transmitted by human touch using a technology such as NTT s RedTacton or through more traditional means such as Bluetooth or even instant messaging. Within a few seconds the colleague can access the document even if the original transmission of the blobs already completed at this point had taken several minutes.

The foregoing usage description illustrates various operations of the present invention which will now be discussed in more detail in connection with the process descriptions in the figures to follow. The processing can be performed by a suitable data processing component of the Makyoh server such as shown in . The processing described in figures to follow can be embodied in suitable computer program code that is executed by the data processing component .

When a document is accessed from an archive the blobs IDs associated with it are automatically added to a list of files to be pushed to other servers the Put Blob Queue and any blobs required by the document that are not found are added to a list of files to get from other servers the Get Blob Queue . Similarly when a feed entry is accessed the corresponding feed entry file is added to a list of feed entries to push to other servers the Put Feed Entry Queue and its feed Id is added to a list of feeds to check for new entries on other servers the Get Feed Queue . Requests added to the Get Blob Queue Put Blob Queue the Get Feed Queue and the Put Feed Entry Queue expire after a certain amount of time by default one hour after which they are removed from the respective queue. Typically these queues are implemented as data structures in the memory of the Makyoh server. However it will become apparent from the discussion below that other mechanisms are possible.

Refer to for a description of the general flow for accessing a document in accordance with an embodiment of the present invention. The requestor i.e. a trusted user will specify to a Makyoh server the local server the pathname of the document to be accessed. In the trusted API a user specifies a full pathname for the file or directory to retrieve for example 

In a particular embodiment of the present invention the pathname leads to the encrypted blob file from which a cleartext representation of the requested document will be obtained. When the encrypted blob is obtained a key the hash URI is used to decrypt the content of the obtained blob. The discussion that follows will describe the processing that takes place in the local Makyoh server.

In a step the blob Id of the requested document is determined based on the pathname specified by the requestor. In a particular embodiment of the present invention the blob Id is the name of the subdirectory in the pathname. Using the example above suppose the pathname given by the requestor is 

In a step a determination is made whether the blob Id is already in the keyring. In the particular embodiment described above the keyring directory contains key files each of which contains the hash URI of the file or document that represents the document associated with the key. Recall that the hash URI includes the blob Id. A search is performed of the hash URI s in the key files for the blob Id determined from step thus identifying the key file associated with the requested document. If it is determined in step that the blob Id is not found then the requested document is deemed not found and a suitable response is sent in a step to the requestor indicating that the requested document was not found.

If it is determined in step that the blob Id is found in one of the key files then a determination is made in a step whether a local copy of the requested blob file is stored in the docs directory of the requestor s local Makyoh server. If not then in a step a pull request is queued on a Get Blob Queue by placing the blob Id in the queue in order to attempt to obtain the requested document from another remote Makyoh server. In a step the Get Blob Queue is serviced as will be discussed in more detail below. The blob Ids in the Get Blob Queue can be serviced with each document access or after some predetermined number of document accesses have occurred or after a predetermined period of time has elapsed or based other suitable criteria or based on some combination of the foregoing. In an embodiment of the present invention requests added to the Get Blob Queue expire after a certain amount of time e.g. one hour after which they are removed from the queue.

If in a step it is determined that the blob was successfully retrieved from a remote Makyoh server and stored in the storage component of the local server then processing proceeds to step discussed below. If it is determined in step that the blob was not successfully retrieved e.g. no other Makyoh servers contain the blob then a suitable response is sent in step to the requester indicating that the requested document was not found.

If a local copy of the requested blob was found step or a copy of the requested blob file was retrieved from a remote Makyoh server step then in a step the push blob service is performed as described in . As will become clear from the explanation of this will cause copies of the requested blob to be distributed to other remote servers. In a step the blob key contained in the hash URI stored in the key file associated with the requested document is obtained and used to decrypt the encrypted blob file. The resulting clear text constitutes the requested document.

Processing of contents is then handed over to the application performing the access. A determination is made in a step whether the requested document is in fact a directory or an actual document e.g. a PDF file . If the application is a browser or the OS s windowing system then it can present the file step or directory step to the user. In the case of a directory the user might select one of the documents in the directory and initiate an access thus repeating the foregoing to obtain the selected document. Another application might take an action that does not display anything to the user e.g. it might read its configuration information from the accessed file.

Referring back to for a moment recall that in an alternative embodiment of the present the Makyoh server can be embodied in a document processing device such as a printer or a fax machine and so on. In one context of a user can make a request on his personal server such as a laptop or PDA to access a document. In another context the user can make a similar request on a printer device or fax machine to access the document to be printed or faxed . The device can be configured as a Makyoh server and access the documents in accordance with including obtaining the document s from another Makyoh server if necessary and distributing the document s to other Makyoh servers in addition to printing or faxing the obtained document s .

Referring to the general flow for accessing a feed entry is similar to the flow for accessing a document. As described above the user would specify a pathname in terms of the archive view presented to a trusted user. In the case of a feed however the path for a particular feed entry might look like 

 feeds 2f267747fd8b6212aed119ec05f42bc014f2ed7 20070306T161144Z.bca9e1954824a32b1f8424511b3f01340ffe231b my entry.pdf

 feeds a2693f77fd8b6212aed119ec05f42bc014f2ed7 20070215T121022Z.f294e1954824a32b1f842451b3f01340ffe1194 my entry dir images thumbnail.jpg

In a step the feed Id of the requested feed is determined based on the pathname specified by the requester at the local Makyoh server. In a particular embodiment of the present invention the feed Id is the name of the subdirectory in the pathname.

In a step a determination is made whether a feedkey associated with the feed Id is known. In an embodiment this can be accomplished by maintaining an associative list called the Feedkey List and searching it. The Feedkey List allows the lookup of feedkey files for a particular feed Id. When the user first logs in with his or her passphrase the Feedkey List is initialized to contain all feedkeys for which a subscription key or a publication key exists in the user s keyring and for which the associated blob is stored in the user s local repository. The process by which this initialization is discussed in more detail below. A search is performed in the Feedkey List for the feedkey associated with the feed Id determined from step thus identifying the feedkey file associated with the requested feed. If the feed Id and associated feedkey is not found then a suitable response is sent in a step to the requestor indicating that the requested feed was not found.

If the feed Id is found in the Feedkey List then a determination is made in a step whether a local copy of an entry file for requested feed is stored in the entries directory of the requestor s Makyoh server . If the feed Id is not found in the Feedkey List then in a step a pull request is queued on a Get Feed Queue by placing the feed Id in the queue in an attempt to obtain the entry file for the requested feed from another remote Makyoh server. In a step the Get Feed Queue is serviced as will be discussed in more detail below. The feed Ids in the Get Feed Queue can be serviced with each feed access or after some predetermined number of feed accesses have occurred or after a predetermined period of time has elapsed or based other suitable criteria or based on some combination of the foregoing. In an embodiment of the present invention requests added to the Get Feed Queue expire after a certain amount of time e.g. one hour after which they are removed from the queue.

If in a step it is determined that the entry file was successfully retrieved from a remote server and stored in the local storage component then processing proceeds to step discussed below. If it is determined in step that the entry file was not successfully retrieved e.g. no other Makyoh servers contain the entry file then a suitable response is sent in step to the requester indicating that the requested document was not found.

If a local copy of the entry file for the requested feed was found step or a copy of the entry file was retrieved from another Makyoh server step then in a step the Entry field is decrypted using the feedkey file retrieved from Feedkey List associated with the requested feed to obtain the hash URI for the file or root directory associated with the requested feed entry. In a step this hash URI is imported into the keyring to be detailed below. In a step the document path associated with the hash URI is calculated by concatenating the string docs the Blob ID specified in the hash URI the string and the filename specified in the Hash URI. For example suppose the hash URI is 

Every Makyoh server maintains set of servers with which it should share blobs and feed entries called that server s neighborhood. Generally speaking a neighborhood is limited to those servers running Makyoh that can be considered nearby. For example in one embodiment of the invention the neighborhood of a given Makyoh server also referred to as the local server is defined as those other Makyoh servers also referred to as remote servers that are communicating on the same local subnet as the local server. Note that nearby may or may not imply physical proximity. For example while most servers on a local subnet will likely to be physically near each other some may be physically remote for example if they are connected through a VPN Virtual Private Network . What is important is that distribution is limited to machines that have a higher than average probability of either eventually being able to decrypt the blobs being transmitted or of themselves redistributing the blobs to a machine that can decrypt them. In this example users on the same subnet are probably part of the same organization and are therefore likely to share documents with one another.

Other embodiments might use other criteria for what constitutes a neighborhood. For example a neighborhood might include both a user s work machine and home machine. As another example the Makyoh servers of people who regularly communicate via email instant messaging or phone might be considered neighbors even though they are physically thousands of miles apart and communicate on different subnets. These servers might be in each other s neighborhood only while communication is in progress e.g. when the users are communicating over the phone to each other or might continue to be in each other s neighborhood for some time after communication has ceased.

In an embodiment a local Makyoh server is notified whenever a machine running Makyoh joins or leaves the local subnet using an open protocol called Bonjour generically known as Multicast DNS DNS Service Discovery . Whenever the local Makyoh server is notified of a new server it automatically determines whether the newly joining server has the blobs and entry files on the Get and Put Blob Queues using HTTP Head and HTTP PROPFIND requests and then performs the appropriate push or pull of the files as necessary using HTTP GET and HTTP PUT requests. A similar set of actions is taken for all known servers in the local server s neighborhood whenever a new request is added. The files held on each remote server are cached so requests need not be made more than once per session.

In another embodiment of the invention a local Makyoh server s neighborhood is defined as the set of servers running Makyoh within a particular organization as determined by using DNS resource discovery to query that organization s Domain Name Service server for all Makyoh servers running in the organization. In this embodiment new servers join the neighborhood by using the open DNS UPDATE protocol. In another embodiment the neighborhood of a local Makyoh server is explicitly set e.g. through the use of configuration files.

In another embodiment the neighborhood of a local Makyoh server is defined as the set of other servers running Makyoh with which direct wireless communication can be established that is those within wireless range . In this embodiment new servers join the neighborhood by broadcasting their existence over the wireless channels to any other Makyoh servers within range.

In another embodiment the neighborhood of a local Makyoh server is defined as the set of machines running Makyoh with which other recent network traffic has recently been communicated. For example if a user initiated an instant message IM chat with another user each of their personal Makyoh servers would join the other s neighborhood. Their personal Makyoh servers would also join each other s neighborhoods when one user sent email to the other when one user called the other on the telephone etc.

In another embodiment a remote Makyoh server is automatically added to a local server s neighborhood if the remote Makyoh server attempts to initiate a GET or PUT on the local Makyoh server. This embodiment insures that servers using different criteria for a neighborhood will still reciprocate joining each other s neighborhood. Of course one might also combine different definitions of neighborhood for example by including both servers on the local subnet and servers within wireless range or use multiple definitions for neighborhood and one can imagine still other definitions of neighborhood.

A session refers to the time from when the local server detects a remote server e.g. is announced by Bonjour to the time when the remote server quits its application or otherwise leaves the neighborhood. When a remote server leaves the neighborhood it has effectively quit or logged out . All record of the files it held is discarded. This is done in part because typically there is no direct way to tell whether a new server that is being announced is one that had been previously known to the local server. Servers typically do not have unique IDs and server IP addresses may change for example in the case of DHCP dynamic host configuration protocol .

In the usage scenario above Alan s laptop had replicated his encrypted strategy outline onto Bob s laptop. If a new user Carl had joined the group subsequent to replication of the encrypted strategy outline by Alan s Makyoh server then Carl will not have a copy. However when Alan later gives Bob his key hash URI for example by scanning a barcode then Bob will import Alan s key and by so doing Carl will receive a copy of the encrypted strategy outline by operation of the processing described in . Carl would then need only obtain the key from Alan or even Bob.

Now suppose that Dan enters the group. He does not have a copy of Alan s encrypted strategy outline. Suppose further that no one has imported Alan s key within the last hour assuming blob Ids are removed from the queue after one hour stale ids are discussed below since Dan s joining the group. As will be explained below stale ids are removed from the queues. What this means for Dan is that when he joins Alan s server will not send a copy of the outline because the id in Alan s Put Blob Queue will have been deleted. Nonetheless Dan can still obtain a copy of Alan s outline simply by importing Alan s key after logging in and per the processing of a copy of the outline will be replicated on Dan s Makyoh server.

If it is determined in step that the candidate server does contain the blob Id then in a step a GET request is performed on that server to obtain the corresponding blob the new blob . The new blob is serviced in a step additional details of which will be discussed below. A determination is made in a step whether the new blob was rejected or not. If the new blob was rejected then in a step that candidate server is marked as not having the blob so that in step the NO branch will be taken for this server. If the new blob is not rejected then processing continues with the next blob Id in the Get Blob Queue steps .

If there is a match then in a step the blob is stored in subdirectory with the blob Id as the blob s filename in the storage of the local server. In a step any requests for blob Id are removed from the Get Blob Queue. In a step a push blob service is performed as described in below. As will become apparent in this context the push blob service will serve to distribute push the received blob to other Makyoh servers e.g. . Thus in the embodiment where a device such as a printer or fax is configured as a Makyoh server if the documents had to be obtained from another server the documents will be distributed to other Makyoh servers by operation of servicing the Get Blob Queue.

If it is determined in step that the target server does not contain the blob Id then in a step a PUT request is performed on that server to send the corresponding blob to that server. Processing then continues with the next target server in the REMOTE list steps . When every server is processed then processing continues with the next blob Id in the Put Blob Queue steps .

If in step it is determined that the target server contains the feed Id or that the determination cannot be made then in a step a PROPFIND request is performed on the candidate server to obtain a directory listing of that candidate server s feed Id directory . Steps and are iterated for each feed entry file see also in that is listed in the candidate server s directory . Thus in a step a determination is made whether a candidate feed entry file in the list is already stored locally. If so then the next feed entry file in the list is processed steps .

If the local Makyoh server does not already have a copy of the candidate feed entry file then in a step a GET request is performed on the candidate server to obtain a new feed entry file for the local server. The new feed file is processed in a step which will be discussed shortly. Processing then continues with the next feed Id in the Get Feed Queue steps .

A validated feed entry is stored in step in the storage of the local server. If the feed key is for a feed Id in the Feedkey list step then the entry key is obtained from the feed key file in a step . In a step the entry field of the Feed Entry is decrypted using the entry key to obtain its hash URI which is then imported in a step in the manner shown in .

Referring to in a step a request to push the blob to other servers is queued by adding blob Id to the Put Blob Queue. A determination is made in a step whether a hash URI corresponding to the blob Id is in the local server s keyring. In one embodiment of the invention this determination is accomplished by searching each key file and comparing the blob Id part of the enclosed hash URI to the blob Id being pushed. In another embodiment the keyring is stored in a temporary database which allows fast lookup of hash URIs based on their blob Id component. If an appropriate hash URI is not found in step then in a step the Put Blob Queue is serviced in accordance with processing shown in and the processing is complete.

Returning to step if an appropriate hash URI is found then in a step the blob contents are decrypted using the key specified in the hash URI. A determination is made in a step whether the blob is a feedkey. This determination is made by examining the blob type as specified in the blob s header.

If the blob is of type feedkey then in a step the feed Id of the feed corresponding to that feedkey is obtained by calculating the fingerprint of the feedkey s verify key e.g. by using the known method specified in the OpenPGP standard. The feedkey is then associated with the calculated feed Id in the Feedkey List in a step . Then in a step a request to retrieve any new feed entries from other servers is queued by adding the feed Id to the Get Feed Queue. The Get Feed Queue is then serviced in a step in accordance with processing shown in . Then in a loop represented by steps and a number of locally stored feed entries associated with feed Id are determined by listing the directory associated with feed Id. The number of locally stored feed entries so listed is determined by a configuration parameter and may include all none or some number of such entries. If the number so listed is less than the number of entries locally stored for the feed Id then the most recent entries are listed as based on the timestamp in the feed entry s filename . The feed entries determined in steps and are then queued to be pushed to other servers in a step by adding each feed entry s path to the Put Feed Entry. Once all listed entries are added the Put Feed Entry Queue is serviced in a step in accordance with . The Put Blob Queue is then serviced in a step in accordance with and the processing is complete.

Returning to step if the blob is not of type feedkey then another determination is made in step whether the blob is a directory by examining the blob type as specified in the blob s header. If the received blob is not a directory e.g. if it is a normal content file of type blob then the Put Blob Queue is serviced in a step per and the processing is complete. Returning to step if the blob is of type directory then a loop is performed over each hash URI listed in the directory blob steps and in which each hash URI is imported in a step in accordance with . Once the loop over the directory s listed hash URIs is complete the Put Blob Queue is then serviced in a step per and the processing is complete.

Login processing includes steps which define a loop that is performed on a local directory of encrypted key files stored on the physical local disk of the server. This directory is in a private local configuration directory it s not distributed over either the trusted API or the remote API. Each file contains one key in the keyring encrypted using the user s passphrase. Recall that the directories in are virtual and thus only shown to the trusted local user. The virtual key files are presented as plaintext and are intended as an easy way for him to access his keys so he can give them to someone else. The keyring directory is also only available after the user logs in and thus can t be looped during login.

For each of the user s key files the key file is decrypted in a step using the user s passphrase in order to access its contents namely the hash URI. In a step the blob Id is obtained from the hash URI. If in a step it is determined that the blob associated with the obtained blob Id is not locally stored i.e. stored in the user s server then the next key file in the user s keyring is processed steps . The determination uses the type field in the header part of the decrypted blob discussed above in the Blob file format section.

If in step it is determined that the blob associated with the obtained blob Id is locally stored then the blob is decrypted in a step using the decryption key obtained from the hash URI to obtain cleartext content. If in a step it is determined from the cleartext content that the blob is a file then the next key file in the user s keyring is processed steps .

If in step it is determined from the cleartext content that the blob is a feedkey then the feed Id is obtained in a step from the signature of the verify key. The feed Id and the blob are then added to the Feedkey List in a step . Processing then continues with the next key file in the user s keyring steps .

New files and directories are created in Makyoh in a scratch directory using standard WebDAV methods in particular PUT COPY DELETE MOVE and RENAME . These files and directories are only accessible locally and are not distributed to other Makyoh servers. To make the contents of scratch directories available to other servers they must first be published by executing an HTTP GET request for the path to be published with the query parameter op createdoc . The Makyoh server will then ensure that blob files associated with each file and directory being published are made available to remote servers in subdirectory import the associated hash URI s into the local keyring and push associated blobs out to known remote servers.

Referring to when a file is published in a step a blob header consisting of the file s length the file s type blob or feedkey and an optional salt is prepended to the file and a blob key is determined in a step by computing the MD5 hash of the resulting prepended file. The prepended file is then encrypted in a step using the blob key as a symmetric key for example using the known Advanced Encryption Standard AES 128 to produce the blob blob file . The blob Id is then calculated in a step by computing the SHA 1 hash of the resulting encrypted blob file. Then in a step the blob is stored in subdirectory using the calculated blob Id as its filename. The hash URI is then generated for the blob in a step by concatenating the text hash id the blob Id aes128 key the blob key content type the MIME type of the file being published name and the filename of the file. This hash URI is then imported into the keyring in a step per processing described in . As discussed above this service will cause the newly added file to be distributed.

In the context of a user s laptop or PDA or similar device the user creates the document or otherwise receives a new document. If the user desires to add it to his Makyoh archive then he can invoke the process described in . In the context of a document handling device such as a printer or a fax or a scanner a user or another machine can send a document to the device to be printed or faxed or the user may place thd document on the scanner to be scanned. If the device is also configured as a Makyoh server the received document can be viewed as a new document and trigger the process of to incorporate the received document in the device s Makyoh archive and also distribute the received document to other Makyoh servers.

The device would receive unencrypted image or file data. The device would then publish the document store the encrypted blobs locally and give the user a key to decrypt the document e.g. in the form of a 2D barcode . In an embodiment of the present invention the device would not store the key locally or indeed have a keyring at all that way the data remains completely secure.

Referring to when a directory is published a blank directory file is created in step . Then each child that is each file or subdirectory is processed in a loop steps and in which first a determination is made in step whether the child is a directory. If the child is not a directory i.e. if it is a file then in step child is published by the method described above and in . The resulting hash URI is then added in step to the directory file created earlier. If in step the child is determined to be a directory then the child is published in step by recursively applying the method described here after which the resulting hash URI is added in step to the directory file created earlier. Once all files and subdirectories in the published directory are processed the loop completes step and the directory file created earlier is published in a step by the method described above and in .

A feed must be created before any entries can be published to it. Feed creation is accomplished in one embodiment of the invention by executing an HTTP GET request with the query parameter op create which will generate feedkeys for the new feed and then publish those feedkeys. Referring to a new publication feedkey is generated in step by generating an asymmetric key pair for example using the known OpenPGP standard for the write key and verify key fields of the key and a random symmetric key is generated for the feedkey s Entry Key field. The file is then published in step using the method described above and in . The subscription feedkey that corresponds to the created publication feedkey is then computed in step by removing the verify key field from the publication feedkey. This subscription key is then published in step and the creation process is complete.

New feed entries are created and published for a feed by executing an HTTP GET request for the path corresponding to the scratch directory containing the entry to be published with the query parameter op publish . The Makyoh server will then ensure that the entry feed entry file is made available to remote servers in subdirectory ensure that blob files representing all files and directories that make up the contents of the entry are made available to remote servers in subdirectory import the associated hash URI s into the local keyring and push the entry file and associated blobs out to known remote servers.

Referring to first a determination is made in step whether a publication feedkey associated with the feed Id to be published can be found in the Feedkey List. If not then an error is reported in step and processing is complete. If a publication feedkey is found then another determination is made in step for whether the root of the entry that is the entry s main contents is a directory. If not i.e. if the entry consists of just a single file then the entry s root file is published in a step . The feed entry file is then created in a step using the entry key specified in the publication feedkey to encrypt the hash URI of the newly published entry root using the write key specified by the publication feedkey to sign the contents of the feed entry. The feed entry file is then stored in a step in the feed s subdirectory . A request to push the feed entry is added to the Put Feed Entry Queue in step that queue is serviced in step per this serves to distribute the received feed entry to other Makyoh servers and processing is complete.

Returning to step if the entry root is a directory then the directory is published in step using the method described above and in . The process then continues through steps as described above.

Makyoh is designed to protect against many kinds of attacks many of which have already been mentioned. To summarize Makyoh protects against the following threats 

