---

title: Visual toolkit for a virtual user interface
abstract: An Integrated Development Environment (IDE) () for creating a touchless Virtual User Interface (VUI)  is provided. The IDE can include a development window () for graphically presenting a visual layout of user interface (UI) components () that respond to touchless sensory events in a virtual layout of virtual components (), and at least one descriptor () for modifying a touchless sensory attribute of a user component. The touchless sensory attribute describes how a user component responds to a touchless touchless sensory event on a virtual component.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08578282&OS=08578282&RS=08578282
owner: NaviSense
number: 08578282
owner_city: Plantation
owner_country: US
publication_date: 20070307
---
This application claims the priority benefit of U.S. Provisional Patent Application No. 60 782 252 entitled Visual Toolkit for a Sensory User Interface filed Mar. 15 2006.

This application also incorporates by reference the following Applications 11 683 410 entitled Method and Device for Three Dimensional Sensing 11 683 412 entitled Application Programming Interface API for Sensory Events 11 683 415 entitled Virtual User Interface Method and Device Thereof and 11 683 416 entitled Touchless Tablet Method and Device Thereof all filed on the same day as this Application.

The present embodiments of the invention generally relate to the field of software programming more particularly to programming development environments.

Integrated Development Environments IDE assist developers with writing and debugging code during the creation of software applications. An IDE provides tools or modules that a developer can employ to facilitate rapid and quality development of code. The tools and modules provide functionality through program reference libraries or from device drivers that provide a layer of abstraction from the underling hardware resources. An IDE can be a compositional studio wherein a programmer can build code using facilities offered by the IDE. An IDE can contain a set of classes or libraries in a particular format which a developer can reference during code creation. The classes or libraries may be associated with a particular software process or a hardware resource. An IDE provides an integrated approach to program development in that programs or utilities are available from directly within the IDE for creating the software application. For example an IDE can support a programming language a compiler a linker and a debugger. A multitude and variation of these resources can be provided to give the developer flexibility in using one resource over the other and to use them together. An IDE can employ an object oriented programming languages to facilitate ease of development. Various levels of programming such as providing access to functions methods variables data and classes can be provided through the IDE to allows the developer immediate access to the underlying code. The levels of access target a specific feature that is generally already available within the levels of programming. The parameters can be precipitated from the underlying native code to a user interface allowing a developer to adjust aspects of the programming within the IDE.

An IDE is a packaged environment that contains a collection of programs and modules that together facilitate the ease of code development. The IDE manages interaction within the modules and coordinates the building of code. An IDE can be targeted to various target platforms wherein programming operations of the platform are already available. An IDE is in general a wrapper for functions and processes already available on the target platform or device. The target platform generally provides an Applications Programming Interface that tells the IDE how to interface with the underlying platform or software processes. An IDE can interface with an API for understanding how to package the code development program and modules. The API opens up a set of communication protocols that can accessed by outside developers or the IDE. The communication protocols allow access for communicating with the underlying resources or behaviors of the platform. However different devices support various access functions which may not be available in APIs or IDEs. Different APIs exist for mouse devices tablets touchpads joysticks and touchscreens as each device has unique operating characteristics.

A motion sensing device can require a special API to define and reveal its own unique operating characteristics. The mechanisms by which testing debugging and validation of code development may depend on the device and the API including the behaviors and properties of the device. A need therefore exists for creating IDEs with APIs that are specific to motion sensing devices.

Embodiments of the invention concern an Integrated Development Environment IDE for creating a touchless Virtual User Interface VUI . The IDE can include a development window for graphically presenting a visual layout of virtual user interface VUI components and a descriptor for modifying touchless sensory attributes of the virtual components. The touchless sensory attributes describe how a virtual component responds to touchless sensory events. The IDE can include a touchless sensing unit communicatively coupled to the integration environment for producing a touchless sensing field for interfacing with the IDE. Using the IDE user components in a User Interface UI can be arranged in a visual layout to generate virtual components within the touchless sensing field to produce a VUI. An object moving within the touchless sensing field at a location in the virtual layout corresponding to a virtual component generates a touchless sensory event on a UI component in the User Interface of the IDE. The UI component can respond to a touchless sensory event on a corresponding VUI in accordance with a touchless sensory attribute. For example a touchless sensory event can be the movement of an object within a certain region the type of movement and the duration of movement within the virtual layout. As an example a touchless sensory attribute can describe how far the object is moves in the virtual layout before an event is triggered.

While the specification concludes with claims defining the features of the invention that are regarded as novel it is believed that the invention will be better understood from a consideration of the following description in conjunction with the drawing figures in which like reference numerals are carried forward.

As required detailed embodiments of the present invention are disclosed herein however it is to be understood that the disclosed embodiments are merely exemplary of the invention which can be embodied in various forms. Therefore specific structural and functional details disclosed herein are not to be interpreted as limiting but merely as a basis for the claims and as a representative basis for teaching one skilled in the art to variously employ the present invention in virtually any appropriately detailed structure. Further the terms and phrases used herein are not intended to be limiting but rather to provide an understandable description of the invention.

The terms a or an as used herein are defined as one or more than one. The term plurality as used herein is defined as two or more than two. The term another as used herein is defined as at least a second or more. The terms including and or having as used herein are defined as comprising i.e. open language . The term coupled as used herein is defined as connected although not necessarily directly and not necessarily mechanically. The terms program software application and the like as used herein are defined as a sequence of instructions designed for execution on a computer system. A program computer program or software application may include a subroutine a function a procedure an object method an object implementation an executable application an applet a servlet a source code an object code a shared library dynamic load library and or other sequence of instructions designed for execution on a computer system.

The term touchless sensing is defined as sensing movement without physically touching the object causing the movement. The term cursor is defined as a cursor on a display and providing control to an underlying object. The cursor can be a handle to an object in the display or a physical object remote from the display but visually controlled using the cursor on the display. The term cursor object is defined as an object that can receive coordinate information for positioning the object. The cursor can be a handle to the object wherein the object can be controlled via the cursor. The term presenting can mean to bring into form or translate one representation to another representation. The term virtual can mean physically tangible though not generally observable by the senses.

The term User Interface UI can be defined as an interface providing access to one or more underlying controls of an application hosted by a computing device. As an example a UI can be a graphical user interface GUI that presents one or more graphical components that can perform a control function in response to an applied action. A user component can be an element in a UI that receives the action and performs a function in response to the action. A Virtual User Interface VUI can be defined as a touchless user interface having one or more virtual components that perform a function in response to a touchless sensory event applied in a sensing field. As an example a touchless sensory event can be a touchless pressing action of a finger on a virtual component in a touchless sensing field. A touchless sensory attribute can be broadly defined as a measure of a touchless sensory event such as a magnitude of the touchless sensory event. A visual layout can be defined as a presentation of user components in a UI that are visible. A virtual layout can be defined as a presentation of virtual components in a VUI that are not directly visible.

In a first embodiment of the present disclosure an Integrated Development Environment IDE executing computer instructions in a computer readable storage medium of a computer system for developing a Virtual User Interface VUI application is provided. The IDE can include a development window for graphically constructing a visual layout of a User Inteface UI to correspond to a virtual layout of a Virtual User Interface VUI and at least one descriptor in the development window that identifies a response of a user interface component in the UI to touchless sensory events applied to a corresponding virtual component in the VUI.

In a second embodiment of the present disclosure a visual toolkit executing computer instructions in a computer readable storage medium of a computer system for creating a VUI application is provided. The visual toolkit includes a development window that presents a visual layout of re locatable user components in a User Interface UI a touchless sensing unit operatively coupled to the computer system that generates a touchless sensing field and at least one descriptor in the development window that describes how a user interface component of the UI responds to touchless sensory events applied to a corresponding virtual component in the VUI. An arranging of the re locatable user components in the UI creates a virtual layout of virtual components in the touchless sensing field for producing the Virtual User Interface VUI application.

In a third embodiment of the present disclosure a computer readable storage medium having stored thereon a computer program having a plurality of code sections executable by a portable computing device is provided. The computer readable storage medium comprises computer instructions for causing the portable computing device to perform the steps of configuring a virtual layout of virtual components modifying touchless sensory attributes of the virtual components and creating a touchless Virtual User Interface VUI application in accordance with the configuring of the virtual components and the touchless sensory attributes.

Referring to an Integrated Development Environment IDE for developing Virtual User Interfaces VUI is shown. Broadly stated a VUI is a touchless user interface application that can be deployed across various platforms to provide touchless user interfacing with an underlying application. The IDE can include a development window a toolbar a component palette and a Graphical User Interface GUI . The GUI can present a visual layout of user interface UI components selected from the component palette . Notably more than the number of user components can be used in creating the VUI with more functions than those shown. The user components selected from the palette can be arranged in the GUI using drag and drop. The user components can be push buttons text entries knobs or sliders but are not herein limited to these. The user interface components can be arranged in the GUI to create a VUI having a virtual layout of virtual components corresponding to a visual layout of the user components . During development a developer of a VUI application can create a VUI by dragging and dropping user components in the GUI . Accordingly the IDE receives a directive to arrange the virtual components in the touchless sensing field and updates a configuration of the virtual layout. For example a developer can create a virtual button component in the touchless sensing field by dragging a UI button component from the palette and positioning it within the GUI . The IDE can generate a VUI application that includes the virtual button component at a location in the virtual layout of the sensing field that corresponds to an approximate location of the UI button component in the GUI . In such regard the IDE allows visual development and programming of virtual user interfaces.

In practice the IDE is a computer programming application that operates from computer instructions executed on a computer readable storage medium. The IDE can include touchless sensing unit a processor a controller and a display . The touchless sensing unit can produce a touchless sensing field which will serve as the virtual layout for virtual components during development of a VUI application. Briefly a developer can arrange user components in a visual layout of the GUI to create a corresponding arrangement of virtual components in a virtual layout of the touchless sensing field . The IDE allows the developer to adjust the sensitivity of virtual components activated via touchless finger actions. The touchless sensing unit can detect movement of an object such as a finger moving within the touchless sensing field generated by the touchless sensing unit .

As an example the touchless sensing unit can be an array of motion detection elements that generate the touchless sensing field . The processor can be communicatively coupled to the touchless sensing unit to identify and track movements of the object within the touchless sensing field . The controller can be communicatively coupled to the processor for generating coordinate information from the movements. The controller and the processor can be integrated with the touchless sensing unit or within a computer connected to the display . The controller can generate touchless sensory events in response to finger movements in the touchless sensing field . In one aspect the controller can generate a coordinate object that identifies a touchless finger push or release motion in the sensing field . The controller can pass the coordinate object to the integrated development environment IDE .

In one aspect the touchless sensing field can be considered a two dimensional layout wherein an object can move in a horizontal X and a vertical Y direction. Movement of an object within the X Y plane can correspond to movement in the vertical and horizontal direction on the display . For example the controller can navigate the cursor on the screen in accordance with object movement in an XY plane of the touchless sensing field . When the cursor location is positioned at a location corresponding to a virtual component in the development window herein called the visual layout the virtual component can be activated. For example the controller can detect a location of an object and position the cursor at a location corresponding to the visual layout. In another aspect movement of the object in a forward and backward direction Z can correspond to activation of a behavior on a Virtual components that is causing a touchless sensory event. The X Y and Z principal axes are interchangeable for providing navigation movement and action behaviors. The IDE allows interchangeability between the principal axes the scaling the resolution and the dimensionality.

The display may support three dimensional D cursory movement such as a Z dimension which allows a cursor to move into and out of the screen. This type of functionality is advantageous within systems such as 3D games wherein a character or object can move if three dimensions or within medical systems such as 3D surgery environments. Referring back to movement within the virtual layout can be identified and processed by the processor and controller . For example a physical location within the touchless sensing field corresponds to a location in the virtual layout which in turn corresponds to a location in the visual layout . Understandably the virtual layout is a virtual presenting or a form or projection of the visual layout in a touchless sensing field .

Referring to a correspondence between a visual layout of the GUI and a virtual layout of a VUI is shown. A visual layout is a layout of user components within the GUI . A virtual layout is a layout of virtual components within the VUI . The virtual layout is a representation of the visual layout within the touchless sensing field . A virtual layout of the VUI in the touchless sensing field corresponds to a visual layout of the GUI within the development window of the IDE . The visual layout of the GUI is the layout of user components that can be seen in the development window of the IDE . The visual layout also corresponds to the visual layout of the VUI application that is created using the IDE . The VUI is a realization of the virtual components in the touchless sensing field .

Again the IDE is a development window for graphically constructing a visual layout of a Graphical User Interface GUI to produce a virtual layout of a Virtual User Interface VUI . The IDE also provides at least one descriptor to identify a response of a user interface component in the GUI to touchless sensory events applied to a corresponding virtual component in the VUI . A touchless sensory event corresponds to a finger action applied to a virtual component such as a pushing action on the virtual push button . A touchless sensory event comprises at least one of a touchless finger positioning a touchless finger push a touchless finger release a touchless finger pause a touchless finger sign a touchless scroll and a touchless select but is not limited to these. The descriptor can include a dialog window that presents at least one modifiable touchless sensory attribute of a touchless sensory event associated with the virtual component . A touchless sensory attribute comprises at least one of an absolute location a duration of time at a location a relative displacement a velocity and an acceleration of a finger in the VUI .

As an example a developer through the dialog window can adjust the sensitivity of a finger press on a virtual push button component for triggering an action on a corresponding user component . For instance a developer during development may increase a depth of a required finger push on the virtual component from cm to cm to activate the user component . That is the developer can adjust a touchless sensory attribute of the virtual component using the descriptor of the UI component during the creating of a VUI. For example a developer can adjust the touchless sensory attributes of a VUI to customize a VUI application for different applications. Moreover a touchless sensory event on a virtual component can produce one of audio or visual feedback on a corresponding UI component in the display window to provide an indication that a finger action has been recognized.

As example a developer can create a graphical layout of user components in the visual layout . For instance referring to a developer can position Virtual components Button and Slider within the visual layout of the GUI . The developer can see the components as they are arranged in the GUI . The sensing unit can project the visual layout as a virtual layout in the touchless sensing field for creating the VUI . In practice the developer may not see how the buttons are arranged in the virtual layout as it is virtual. Accordingly the developer can position a finger within the touchless sensing field to identify the boundaries of the virtual layout and the Virtual components within the layout. The IDE can graphically identify where the finger is in the virtual layout and present the corresponding location in the GUI . The controller identifies a location and movement of the finger within the touchless sensing field and relays the coordinate information to the IDE . The IDE can identify the location of the finger in the virtual layout and the corresponding visual layout on the display such that the developer can see where the finger is relative to the visual layout.

Notably the virtual layout of the GUI and the visual layout of the VUI may have translation differences that is the mapping between the two layouts may not be one to one. Understandably the projection of a virtual layout may encompass a broader space or a narrowed space depending on the sensing field the layouts may also be slightly distorted. The IDE can provide auditory or visual feedback for revealing the location and behavior of the finger within the virtual layout. For example referring back to when the finger is positioned at a virtual location corresponding to a Virtual components such as Button the IDE can change a visual or auditory attribute of the virtual component in the visual layout of the GUI . The visual attribute can be a change of button color a flashing of the button an beep or any other feedback mechanism. The IDE can provide feedback to the user for showing the bounds of the Virtual components. For example the IDE can trace out virtual component boundaries on the visual layout. The IDE can show the position of the finger in the virtual layout of the VUI on the visual layout of the GUI .

The IDE is a software environment for building virtual user interface VUI applications. The IDE provides programming through C C Java .NET or Visual Basic. The IDE can receive and process the coordinate object from the controller for informing a software application such as the Graphical User Interface GUI of touchless sensory events in the touchless sensing field . A coordinate object can be one of an HTML object an XML object a Java Object a C class structure a .NET object or a Java Servlet. The IDE can incorporate the coordinate object into programs and modules for providing functionality to the GUI . The IDE can compile the program code for the GUI and generate the VUI application. The VUI can be deployed with the touchless sensing unit to provide touchless user interface applications. As an example the sensing unit can replace or supplement operation a touchscreen or a tablet and generate a VUI with which a user can interface via touchless finger actions. A VUI application running on a computer system can allow a user to interface a menu system of a communication device or a computer through touchless finger gestures with the touchless sensing unit . As another example a VUI application can allow a user to adjust one or more media controls of a media device via touchless finger actions.

One benefit of the IDE is that the developer can evaluate the resolution and precision of the touchless actions involved in generating a touchless sensory event on a virtual component. A sensory event can be a touchless finger action such as touchless button press a touchless button release a touchless button hold a touchless scroll a touchless single click and a touchless double click. The descriptor allows for a modifying of a touchless sensory attribute of a virtual component wherein the touchless sensory attribute describes how a virtual component responds to a touchless touchless sensory event. The descriptor can be a dialog window comprising at least one modifiable touchless sensory attribute of a touchless sensory event associated for a virtual component.

As an example the descriptor can expose sensory attributes of the touchless sensory event. For example the sensory event may be a finger action applied to a virtual button . The location of the detected finger action can be presented in a coordinate object identifying the touchless sensory event. A sensory attribute can be the coordinate values the size of the virtual component boundary and the type of action. For example descriptor reveals a pushAction was detected whereas descriptor reveals that a slideAction was detected. An exception handler routine in the IDE can provide coordinate information relevant to the sensory event such as how far the slider moved or how far the button was pushed. A developer can integrate the functions and methods provided by the descriptors in conjunction with the exception handlers to identify when sensory events are detected. A VUI application can implement methods of a user component sby extending the exception handling methods through inheritance. A virtual component can extend a user component to inherent touchless sensory event methods. Accordingly sensory event information such as displacement velocity acceleration and length of time can be processed to process the sensory event within the context of the VUI.

Briefly the touchless sensing unit together with the processor and controller communicate touchless sensory events to the IDE for invoking at least one user component response. The touchless sensory event is at least one of a finger positioning a finger movement a finger action and a finger hold within the touchless sensing field. In one aspect a touchless sensory attribute includes at least one of an absolute location a duration of time a relative displacement a velocity and an acceleration of the finger. This information can be provided through the descriptor which can be edited or left intact by the developer within the IDE . Understandably the developer can adjust the sensitivity of Virtual components with the VUI. In one example the Virtual components can be Java Beans. The Java Beans specify parameters and attributes available for the touchless sensory event.

During execution the computer system presenting the VUI application receives touchless sensory events from the touchless sensing unit . The touchless sensing unit generates the touchless sensory events in response to touchless finger actions on virtual components in the VUI and the computer system applies the touchless sensory events to the user components in the GUI based on the touchless sensory attributes in the descriptors . In such regard the IDE allows a developer to compose a virtual layout of virtual components and tune the VUI application for touchless user interface sensitivities. The virtual layout can be considered a projection of the visual layout in a two dimensional or three dimensional space. Understandably the visual layout is represented in two dimensions on the display as limited by the graphics of the display. In one example the touchless sensing unit can project the visual layout onto a two dimensional plane in a three dimensional space. Alternatively the touchless sensing unit can project the visual layout onto a surface such as a desk.

In one embodiment the IDE can further include a form sheet. A touchless sensing unit can project the visual layout containing the Virtual components onto the form sheet. This allows the form sheet to display visual components in a visible layout corresponding to the Virtual components in the virtual layout. The virtual layout may become visible as a result of a touchless sensory projection displaying or printing on a form sheet. The virtual components within the virtual layout can correspond to visual components in the visual layout. In this arrangement a user can touch the form sheet at a location in the visible layout corresponding to elements within the virtual layout for interacting with Virtual components. The IDE can further include a controller for correlating an object action such as a finger action on the form sheet with a virtual component to allow a user to rearrange or access functionality of the Virtual components in the development window.

Referring to the IDE can further include a compiler for compiling a source code of the GUI into at least one code object a linker for converting the code object into relocatable code and a code builder for building the relocatable code into a VUI executable object. The source code can contain variables and functions each having associated memory requirements. For example the variables can be stored in X or Y RAM and the functions which collectively constitute the program can be stored in P RAM.

The compiler may leave the locations of the memory ambiguous as relocatable code objects as the target platform may have certain memory requirements. For example different chips have different regions of memory dedicated to data memory and program memory. Accordingly the linker identifies the regions of memory targeted for the particular processor or device using the VUI application. The linker links the relocatable section of memory produced by compiler into fixed locations in the target device memory. The Code Builder can convert the source code and targeted memory into a VUI executable object. A computer system can run the VUI executable object to generate the VUI application. The sensory system also includes a Flashing Unit for placing the VUI executable object generated by the Code Builder in the touchless sensing unit or any other computer system. The VUI application depends on the reception of sensory events which are provided by the touchless sensing unit . The flashing unit downloads the VUI executable object into the touchless sensing unit such that the sensing unit can communicate sensory events to an underlying application or other computer system hosting the application. The flashing unit can download the executable through a serial port a USB a Bluetooth or a ZigBee connection through are not limited to these.

The IDE can serve as a visual toolkit executing computer instructions in a computer readable storage medium of a computer system for creating a coordinate object for use in a VUI application. The visual toolkit includes the development window that presents a visual layout of re locatable user components in a User Interface UI the touchless sensing unit that generates a touchless sensing field and provides coordinate information to the development window and at least one descriptor in the development window that describes how a user interface component of the UI responds to touchless sensory events applied to a corresponding virtual component in the VUI. An arranging of the re locatable user components in the UI creates a virtual layout of virtual components in the touchless sensing field for producing the Virtual User Interface VUI application.

The touchless sensing unit communicates touchless sensory events in the virtual layout to the user components in the GUI . The visual toolkit is a software program that can be written in C C Java .NET Visual Basic or CGI. The visual toolkit can include touchless sensory API help documentation and it can selectively obfuscate code. The visual toolkit includes the compiler for compiling a source code into at least one code object the linker for converting the code object into relocatable code and the code builder for building the relocatable code into an executable. In practice the visual toolkit identifies a finger action within a touchless sensing field of the touchless sensing unit that corresponds to a touchless sensory event action on a user component such that a user navigates and controls the VUI application through touchless finger control in the touchless sensing field .

The visual toolkit can implement a sensory Application Programming Interface API for providing portability across computing platforms wherein the sensory API exposes methods fields event listeners and event handlers for processing touchless finger actions in the VUI . In such regard the VUI can be communicatively coupled to the IDE through the Sensory API. The Sensory API is portable across applications and describes how touchless sensory events are handled at the service level or application layer. A developer of a VUI application need not know how resources are allocated to the event driven model when a touchless finger action is performed. A developer need only implement the sensory API to receive events and process the events accordingly.

Referring to a Virtual user interface VUI application is shown. The VUI application can include a GUI a sensory Applications Programming Interface API a low level driver and a sensing unit . The VUI is not limited to these components and may include fewer or more than the components shown. The sensing unit can detect sensory events and relay the sensory event through the low level driver to the sensory API . The low level driver can package the object movement events into a coordinate object which describes attributes of the object such as position displacement velocity and acceleration. The low level driver can be written in machine language assembly C C or Java and define base classes that include methods and fields to expose the coordinate and control data produced by the sensing unit . The low level driver converts a touches sensory event on a virtual component in the VUI to an action on a user component in the UI. The low level driver can be one of a tablet driver a touchpad driver a touchscreen driver and a mouse driver. A sensory event can be any movement of an object within a sensory field of the sensing unit . In one particular example an object movement can be a finger press a finger release a finger hold or any other touchless action for interacting with the GUI . The object movement can be intentional or accidental. The API can interpret the sensory events and translate the sensory events into a response action within the GUI . In practice the GUI implements the sensory detection API for receiving sensory events. The API can include an event handler for processing a sensory event and an event listener for receiving a sensory event. The sensory event listener can identify a sensory event and report the event to the sensory event handler. The API can also include an exception handler to indicate and handle an unexpected sensory event.

In this document the terms computer program program computer program medium computer usable medium machine readable medium machine readable storage medium and computer readable medium are used to generally refer to media such as memory and non volatile program memory removable storage drive a hard disk installed in hard disk drive and signals. These computer program products are means for providing software to processing devices. The computer readable medium allows devices to read data instructions messages or message packets and other computer readable information from the computer readable medium. The computer readable medium for example may include non volatile memory such as Floppy ROM Flash memory Disk drive memory CD ROM and other permanent storage. It is useful for example for transporting information such as data and computer instructions between computer systems. Furthermore the computer readable medium may comprise computer readable information in a transitory state medium such as a network link and or a network interface including a wired network or a wireless network that allow a computer to read such computer readable information.

The present invention may be realized in hardware software or a combination of hardware and software. The present invention may be realized in a centralized fashion in one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is VUIted. A typical combination of hardware and software may be a general purpose computer system with a computer program that when being loaded and executed controls the computer system such that it carries out the methods described herein.

The present invention also may be embedded in a computer program product which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

This invention may be embodied in other forms without departing from the spirit or essential attributes thereof. Accordingly reference should be made to the following claims rather than to the foregoing specification as indicating the scope of the invention.

