---

title: Method and apparatus for enhancing digital video effects (DVE)
abstract: A method and apparatus for enhancing digital video effects (DVE) operates to embed DVE functionality within a graphics modeling system and provides the user with an interface configured to present model elements to a user as controllable parameters. In order to embed DVE functionality, a dynamic data structure is introduced as a scene to allow the addition of user defined model elements. The user interface enables the identification of, and access to the newly introduced model elements.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08914725&OS=08914725&RS=08914725
owner: GVBB Holdings S.A.R.L.
number: 08914725
owner_city: Luxembourg
owner_country: LU
publication_date: 20071108
---
This application is a National Stage Application and claims the benefit under 35 U.S.C. 365 of International Application PCT US2007 023582 filed Nov. 8 2007 which was published in accordance with PCT Article 21 2 on Oct. 23 2008 in English and which claims the benefit of U.S. Provisional Patent Application No. 60 922 849 filed on Apr. 11 2007.

The present principles relate to digital video effects DVE systems. More particularly they relate to a method and apparatus for providing user enhanceable DVE.

In general video effects are used to produce transformations of still or moving pictures or rendered visual objects. Typical examples of video effects include but are not limited to video image 3 space transforms such as scaling locating rotating etc. pixel based video image processing such as defocus chromatic shift etc. and other manipulations or combinations of transformations such as bending slicing or warping of the video image surface s into different forms.

Live Broadcast Video Effects allow the technician to transform live video while meeting video broadcast real time demands. Products that produce Live Broadcast Video Effects have various names and trademark names and will herein be referred to by the commonly used NEC trademarked acronym DVE which stands for Digital Video Effects. Currently in DVE products Live Broadcast Video Effects are created by controlling a set of parameters within a DVE system. Most of these parameters act upon ingested video images. The effect creating live broadcast technician is able to manipulate this set of factory created parameters which are pre determined and limited by the product design.

Pre built Video Effects allow a graphics artist to produce an effect in non real time and record resultant individual image frames into a video clip that is appropriate for real time playback. Pre built Video Effects utilize images created by graphics artists and related technicians. These images are often generated by creating and rendering 3D virtual objects as is typical to many 3D or Computer Aided Design CAD modeling systems. The virtual objects and related elements are created by making a 3D model or by other constructive means and then manipulated by varying the constructive parameters of that model. The Pre built Video Effect is made by recording the rendered images into a 2D video format one frame at a time.

According to one aspect of the present invention there is provided a method for providing digital video effects that commences by mapping video onto a graphics model having at least two dimensions. Elements of the graphics model are assigned as user controllable parameter. At least one of a video output signal and a key are produced in response to user adjustment of a controllable parameter.

According to another aspect the present principles provides a method for providing digital video effects including the steps of embedding DVE functionality within a graphics modeling system providing a user interface configured to present model elements to a user as controllable parameters and outputting a video and or a key in response to a user input.

The embedding aspect can include introducing a dynamic data structure as a scene to allow the addition of user defined model elements and providing a user interface to identify and access the newly introduced model elements using typed information.

The introducing aspect can further includes creating a specialized set of customized objects nodes in the graphics modeling system said customized objects including DVE objects as new parts of the scene. During the introducing aspect values of parameters are set as a key frame in a timeline

According to another implementation the dynamic data structure is defined through a description language. The definition of the description language is such that it can be converted to and from a 3D authoring tool.

Other aspects and features of the present principles will become apparent from the following detailed description considered in conjunction with the accompanying drawings. It is to be understood however that the drawings are designed solely for purposes of illustration and not as a definition of the limits of the present principles for which reference should be made to the appended claims. It should be further understood that the drawings are not necessarily drawn to scale and that unless otherwise indicated they are merely intended to conceptually illustrate the structures and procedures described herein.

Early DVE products were designed to help mix video channels by resizing and positioning the image over a background. In order to provide smooth transitions video effects were created utilizing the concepts of key framing and interpolation to generate controlled changes. Desired positions or states are captured as time based key frames or control points for interpolation. The resultant effect is produced by an interpolation of intermediate values while moving in time from key frame to key frame usually with the intent to produce a smooth or continuous change in the image or some other desirable characteristic. Those traditional DVEs directly process the video pixels.

Traditional Digital Video Effects DVE systems in the broadcast domain allow live broadcast technicians to control a fixed set of parameters for the purpose of creating video effects. The User Enhanceable DVE UE DVE is a more advanced type of DVE which allows live broadcast technicians to introduce new elements into the effects building domain and control those elements to produce real time broadcast quality video effects having ranges of motion video representations and forms that are beyond those initially provided by the physical product.

As DVEs have evolved over the years more sophisticated features such as planar warps lighting defocus etc. have been added so that the typical DVE today may have a wide variety of features. However there are certain key features which are necessary for a DVE to be viable for usage in Live Broadcast Production. In addition to being able to produce at least full 10 bit resolution video input and output in real time a viable DVE must have a short pipeline delay less than 3 frames or 0.05 second latency between the time a particular video frame enters and exits the DVE be able to output both a video and key support insertion and editing of key frames and support key framing interpolation run controls for effects. Primary Video Effects building functionality must include support for the ability to transform an image in 3 D space including locate resize and rotate in 3 dimensions and image skew cropping and aspect ratio change . To be a DVE suitable for mid range to high end production quality some additional features such as interpolation path controls shadows splits mirrors slits defocus lights and various warps such as page turn ripple or position modulations are generally needed. shows what it would look like when splits are applied to a re sized and rotated image.

However for live broadcast DVEs current systems continue to use the image processing centric paradigm of processing pixels from the input image using 3 D transform mathematics and projecting the result back into 2D for output as a single video or video and key image This type of system is sometimes called 2.5D . Multiple images from different DVEs e.g. other sources can then be composed or keyed onto a background for the final result.

Since these are planar images the ability to model these images as a 3D scene is often limited to layering or combining the resultant keyed 2D images. One enhancement that is sometimes used is to use a combiner following the DVEs. Some DVEs can produce a depth key which can be used to correctly combine more than one DVE output so that they can be correctly shown combined as intersecting 3D planes. However even in this case the result is limited to combining planar images. For example referring to and if two DVEs are sizing and locating an un warped video image of 3D letters A and H so that the images intersect the combiner will show a straight line intersection of the two video image planes rather than the 3D virtual object intersection that would be produced by a 3D modeling system.

Although there are many interesting features provided by current DVEs the features are limited to built in functionality that is applied to the video controllable by a fixed set of parameters. True fairly sophisticated effects can be achieved with this technique but the results must be carefully managed to maintain the full 3D illusion. An example of this would be a page turn effect see e.g. in which the video image is removed from the screen by simulating the roll off of a page. However if the page turn image is subsequently rotated to an on edge position it is obvious that it is a 2.5D system using a 2D projection of a 3D turned page .

Present day DVEs do not provide a means to construct a new model. For example if a user wanted to create an effect by wrapping the video image onto the surface of a cone this would not be possible unless the product designers had implemented this particular feature and provided parameters for controlling it . Thus in order to achieve this result the user would need to rely upon a graphics artist or non real time content creator solution where the appropriate model a cone has been rendered. However to map live video onto this surface may not be possible as the image would be pre recorded. Note The pre rendered reverse address PRA DVE device does allow a live image to be mapped but again requires special preparation. See the discussion relating to PRA DVE below .

There has been a trend to integrate DVEs into Video Broadcast Switchers aka Video Production Centers Video Mixers . Recent switchers such as the Grass Valley s Kalypso have internal DVEs also known as Transform Engines . With these internal DVEs video sources can be locally processed without sending the signal to an external stand alone system providing benefits in terms of integrated control effects and reduced pipeline delay. The functionality of internal or integrated DVEs is almost identical to stand alone DVEs.

The traditional DVE operates in the following manner. The work buffer defines the current state of the system including its position in 3D space and its warp controls. The transforms are combined into a final result transform matrix and the warp controls. The final result matrix is converted into a reverse address matrix. These results are passed to a set of hardware Reverse Address Generator RAG transform registers and warp control registers. The reverse address generator has a fixed set of registers which include the reverse address matrix and the warp controls. The output control clock steps through the output pixels starting at pixel one line one and stepping to the next pixel in the output buffer. The clock also triggers the RAG to use the values the software placed into the fixed registers reverse transform matrix and the warp controls to calculate the addresses of the source pixels used to create the next output pixel. Steps and are repeated until the entire output image is complete.

This design relies on a fixed set of controls for the live broadcast technician fixed registers for the RAG control and fixed formulas in the RAG hardware. An advantage of this design is that the operator of the DVE can change any factory defined key framable parameter of the effect on the DVE itself and immediately play back this effect that can position warp and process live video. The disadvantage is that the DVE can only perform the limited set of warps and effects that are designed into it by the DVE manufacturer and the fact that no true 3D models can be created by the DVE limits the types of effects that can be performed.

Another approach to a DVE is the Pre rendered Reverse Address DVE PRA DVE . A custom 3D software package is used to create the effect desired. This includes all aspects of the effect including duration position in 3D space and warps. The custom 3D package then performs the reverse address generation RAG calculation in non real time. These addresses are then saved to storage media for later playback. This process can take several minutes. When the operator wants to play back the effect the output control clock steps through the output pixels starting at pixel one line one and stepping through the output image. Where a traditional DVE would use RAG hardware to calculate the reverse addresses on the fly the PRA DVE reads the reverse addresses back from the storage media and uses those addresses to grab the appropriate source image pixels to use to create output image.

Although this approach allows for a wide variety of effects the traditional DVE operator has no access to any of the key frameable parameters on the DVE itself. In order to make any changes to the effects an operator must go back to the specialized software change the effect recalculate the reverse addresses transfer the addresses to the storage media load the reverse addresses from the storage media into the DVE and then play back the effect. This process must be followed for the smallest of changes to the effect including duration of the effect position of objects lighting etc and can take a great deal of time.

Another disadvantage is that the skills required for using a 3D animation package and the skills required to be a live broadcast technician are different and usually require two different people.

The PRA DVE relies completely on the custom offline animation software to perform the reverse address generation. An advantage of the PRA DVE is that the user can create effects with almost any shape with live video. A disadvantage of the PRA DVE is that creating and changing effects is very time consuming.

For all practical purposes 3D effects cannot be changed at normal DVE effect editing speeds and will require the graphics artist in addition to the live broadcast technician. This greatly limits the usefulness of the PRA DVE architecture in a live production situation.

The user enhanceable UE DVE of the present principles is a new type of DVE which is created with and embedded within a 2D and or 3D graphics modeling system. A collection of specific graphics objects have been created i.e. a model which replicates major portions of the traditional DVE system functionality by using graphics modeling system elements while still achieving the real time performance needed to meet video broadcast requirements. By embedding the DVE functionality within a graphics modeling system rather than ingesting 2D video recordings into a video mixing domain effects having live video can be created interactively within a graphics environment. For replication of traditional DVE effects the Live Video is mapped to a virtual surface s within the model to produce a result equivalent to a traditional video presentation and behaviors. However the constraint of factory created parameters is removed. New graphics model elements can be introduced by the user to create new features.

For enabling this capability the present principles utilizes a dynamic data structure rather than a static data structure as known from the prior art . This allows the definition of objects and parameters for support of legacy DVE features but also enables the creation of new objects and parameters by either a content creator who provides new effects for the UE DVE platform or the platform operator. According to the present principles this dynamic data structure is called a scene See element in . In one implementation this scene is represented through the form of a scene graph. This data structure provides an Application Programming Interface API to identify and access new objects and parameters using typed information e.g. floating color or percentage value along with higher level semantics.

Thus through this API elements can be identified and bound to effects creating systems allowing read and write access and thereby achieving effects creating functionality. For example the user can import a particular new model identify its elements fields read the current values in selected fields and then having saved those values into key frames write those values or interpolated values back into their respective field i.e. build and run effects.

This dynamic structure also offers programmable components in order to add new user defined DVEs and image processing video pixels manipulation through the use of scripting language and or common graphics shader code which can be dynamically compiled or interpreted into the rendered See element in .

In general the present principles provide the user with expanded creative opportunities by allowing the introduction of new features with which to build effects. The dynamic data structure is defined through a description language which is preferably one 1 or more of the support syntaxes from 3D authoring tools or through the use of converters for converting the description language to and from a 3D authoring tool. Those of skill in the art will recognize that different description languages and 3D authoring tools may be used without departing from the scope of the present principles. Some examples of description languages that could be use are VRML X3D COLLADA etc. while some examples of 3D authoring tools can be Autodesk Maya Autodesk 3D Studio Blender Softimage etc.

A user can then simply follow the given syntax to create new object and or parameters or externally create them by the use of third party authoring tools. To help with importing of new externally defined objects and parameters a set of User Interfaces can be provided along with the UA DVE as shown by element in . Those User Interfaces will control the UA DVE in a friendly manner to the user in order to perform the imports and necessary conversions some of those User Interfaces can also use the UA DVE Scene APIs or other user interfaces to dynamically create object and parameters directly within the UA DVE platform. These features include the ability to represent live video elements fully within a 3D modeling environment with all the inherent capacity of any graphics object in such an environment including but not limited to reproducing the appearance of real graphics objects. These features are realized by introduction of models or model elements into the graphics modeling system or changing model elements and then by virtue of the fact that the model elements are presented to the user as controllable parameters by setting the values of these parameters as key frames in a timeline to form Live Broadcast Video Effects which can be recalled and run.

The present principles also promises to reduce production costs associated with the traditional graphics artist content creation workflow. For example when a live broadcast technician wants to utilize a traditional graphics department effect modifications can only be made by the graphics artist who must re render the effect into a video clip. However with UE DVE of the present principles the ability to modify some or all of the object parameters directly can be provided to the live broadcast technician to make modifications in the studio and immediately render the effect live in real time. This can easily be performed for any object s parameters imported or not since the dynamic data syntax furnishes typed parameters information for any object of the Scene those can then be controlled through timelines as usually performed in legacy DVEs by defining their values for a user defined number of key frames and types of interpolations.

The UE DVE system of the present principles reproduces the behavior of a DVE and more by realizing a fully functional graphical model of a DVE within a Virtual 3D Objects scene.

Referring to there is shown an exemplary implementation of the UE DVE system according to the present principles. The graphics objects synchronized or Gen locked live video inputs are supplied to the system by the graphics modeling system along with still pictures and other data from other media sources . The computed resultant images are output from the UE DVE frame unit as live video. Controlling parameters for this DVE model are accessible through the UE DVE User Interfaces UI . Additional common control methods may also be supported via various protocols e.g. over Ethernet or serial connections e.g. Sony BVW 75 PBus2 etc .

The key functionality of the UE DVE system is contained within the UE DVE Frame Unit which houses the UE DVE Software Application a Rendering System an Input Image Processing system and an Output Image Processing system see . The Software performs the DVE activities and supports video input and output channels as well as various connections for communications and alternative media or data transport e.g. network or hard drive access. The UE DVE Frame Unit is designed to work in harmony with other video production units such as a Live Production Switcher as shown in . Physical connections for video input and output attach to the Unit and network connections and various serial device connections are provided to the Unit as well.

The UE DVE of the present principles provides a default model which supports most traditional DVE features. For example a page turn effect can be created by selecting the correct warp mode and setting the page turn offset radius and orientation. The position of the video image as a whole can be set by controlling the location and rotation parameters. One distinguishing difference as shown in below is that within the traditional DVE system the page turn is not produced in a true 3D modeling system i.e. 2.5D so when it is rotated it appears incorrect See while the UE DVE page turn shown in and looks correct from any angle because it is truly rendered as a 3D object within a 3D scene.

Like traditional DVEs the UE DVE provides functionality for building Live Video Effects. Effects which capture one or more key frames of the desired scene can be built in the traditional way by setting the system to the desired state and then saving the state as a key frame within an effect.

Since Graphics objects can be composed into the same frame buffer they can intersect with each other Note that the UE DVE of the present principles also offers the possibility to intentionally avoid those intersections by adding Compositing Layers which provide separate frame buffers and can be combined all together as overlays upon output .

Most significantly the UE DVE of the present principles has the ability to add new DVE features. These can be created by introducing changes or additions to the default model. For example the changes or additions can be model elements presented to the user as controllable parameters. Some useful elements are provided internally for easy addition such as Lights Cameras Layers and Transform nodes. Other models or model elements can be imported as files using common descriptive languages such as VRML X3D or COLLADA.

For example a full 3D model of a human head could be imported into the UE DVE and the skin of the human head model can then be dynamically wrapped with a live video image.

In all cases correctly identified elements within the scene are automatically bound to the UE DVE parameter system providing a means to control those elements.

The UE DVE is a system which introduces the creative capacities of a Graphics Modeling system into a DVE making it possible for users such as graphics artists to introduce new model elements which provide new DVE features while satisfying the strict requirements of the live broadcast environment.

The introduced elements can be controlled by the UE DVE user. Thus a live broadcast technician can make rapid changes to the scene as needed without having to return to an offline animation system and real time Live Broadcast Video Effects can be built utilizing this capability. This will result in a saving of production time.

The UE DVE of the present principles exceeds the capability of the Pre rendered Reverse Address PRA DVE and yet provides the speed and ease of editing effects of a traditional DVE. New levels of creativity will be possible for live broadcast technicians using the UE DVE.

At the application the general system functionality is managed including but not limited to configuration the processing of commands from the User Interface s loading management editing and running of Video Effects establishing and maintaining the access to elements within Scene update of Scene state and high level control of the real time rendering processes.

A video effects storage maintains stores values for Video Effects in the form of Key frames and other effect data.

The scene contains the state of the image producing system which upon rendering and related processing produces the resultant output video images. The scene block can use scene graphs and other representations of the dynamic structure.

The Update and Rendering Pipeline Control manages the real time demands of the system. The running of effects i.e. interpolation through Application is synchronized so that Scene is updated at the correct time and the Rendering is done at the correct time to synchronize with the video genlock.

The Input Image Processing receives input images video sources in the form of a Serial Digital Interface SDI and or other video and still picture inputs and processes the input images into internal representations. This processing may include de interlacing application of keys format conversions filtering and other image processing activities. The Input Image Video Sources provides physical Connections such as High Definition Display HDD or co axial cables transport Images e.g. video and or key signals and genlock signals using SDI format and or other video formats into the system. 

A Renderer uses the constructs defined by Scene to render an image for output as controlled by the Rendering Pipeline Control utilizing Input Video or other sources or means . This Renderer can be either software or hardware components or mix of those two. Its main purpose is to be able to perform the rasterization in real time e.g. within video field rate constraints. The Rasterization process includes mapping of any video or still image on objects by a mapping means. In one implementation the renderer constitutes a Graphics Processing Unit GPU used through an OpenGL interface. Other embodiments could support software or hardware that implement accelerated Ray Tracing Algorithms.

The Output Image Processing processes internal representations of video images and outputs as SDI and or other video formats. The processing may include but is not limited to interlacing filtering mixing cropping and format conversions. The Output Image video provides physical connections such as HDD or co axial cables transport images e.g. video and or key signals and genlock signals out of the system. 

The Image Stream transfers picture data from the Input Image Processing to the Renderer . The Image Stream Output means transfers picture data from the Renderer to the Output Image Processing . The Image Bypass enables the Image to be sent directly from the Input Image Processing to the Output Image Processing .

Features and aspects of described implementations may be applied to various applications. Applications include for example individuals using host devices in their homes to communicate with the Internet using an Ethernet over cable communication framework as described above. However the features and aspects herein described may be adapted for other application areas and accordingly other applications are possible and envisioned. For example users may be located outside of their homes such as for example in public spaces or at their jobs. Additionally protocols and communication media other than Ethernet and cable may be used. For example data may be sent and received over and using protocols associated with fiber optic cables universal serial bus USB cables small computer system interface SCSI cables telephone lines digital subscriber line loop DSL lines satellite connections line of sight connections and cellular connections.

The implementations described herein may be implemented in for example a method or process an apparatus or a software program. Even if only discussed in the context of a single form of implementation for example discussed only as a method the implementation of features discussed may also be implemented in other forms for example an apparatus or program . An apparatus may be implemented in for example appropriate hardware software and firmware. The methods may be implemented in for example an apparatus such as for example a processor which refers to processing devices in general including for example a computer a microprocessor an integrated circuit or a programmable logic device. Processing devices also include communication devices such as for example computers cell phones portable personal digital assistants PDAs and other devices that facilitate communication of information between end users.

Implementations of the various processes and features described herein may be embodied in a variety of different equipment or applications particularly for example equipment or applications associated with data transmission and reception. Examples of equipment include video coders video decoders video codecs web servers set top boxes laptops personal computers and other communication devices. As should be clear the equipment may be mobile and even installed in a mobile vehicle.

Additionally the methods may be implemented by instructions being performed by a processor and such instructions may be stored on a processor readable medium such as for example an integrated circuit a software carrier or other storage device such as for example a hard disk a compact diskette a random access memory RAM or a read only memory ROM . The instructions may form an application program tangibly embodied on a processor readable medium. As should be clear a processor may include a processor readable medium having for example instructions for carrying out a process.

As should be evident to one of skill in the art implementations may also produce a signal formatted to carry information that may be for example stored or transmitted. The information may include for example instructions for performing a method or data produced by one of the described implementations. Such a signal may be formatted for example as an electromagnetic wave for example using a radio frequency portion of spectrum or as a baseband signal. The formatting may include for example encoding a data stream packetizing the encoded stream and modulating a carrier with the packetized stream. The information that the signal carries may be for example analog or digital information. The signal may be transmitted over a variety of different wired or wireless links as is known.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made. For example elements of different implementations may be combined supplemented modified or removed to produce other implementations. Additionally one of ordinary skill will understand that other structures and processes may be substituted for those disclosed and the resulting implementations will perform at least substantially the same function s in at least substantially the same way s to achieve at least substantially the same result s as the implementations disclosed.

