---

title: Unit status reporting protocol
abstract: A unit status reporting protocol may also be used for context switching, debugging, and removing deadlock conditions in a processing unit. A processing unit is in one of five states: empty, active, stalled, quiescent, and halted. The state that a processing unit is in is reported to a front end monitoring unit to enable the front end monitoring unit to determine when a context switch may be performed or when a deadlock condition exists. The front end monitoring unit can issue a halt command to perform a context switch or take action to remove a deadlock condition and allow processing to resume.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08019978&OS=08019978&RS=08019978
owner: NVIDIA Corporation
number: 08019978
owner_city: Santa Clara
owner_country: US
publication_date: 20070813
---
Embodiments of the present invention relate generally to context switching and more particularly to context switching that employs a unit status reporting protocol.

Current graphics processing systems maintain state information for a single processing context. A processing context consists of a complete set of rendering state through the entire graphics pipeline. Many programs running on a CPU will need only one processing context for all their graphics although some programs might use multiple processing contexts. For example a program may use one context for each window or one context for graphics and another context to run computing applications on a graphics processor. As more application programs use the graphics processing system to perform graphics processing it is necessary to switch between the different contexts that correspond to each application program. As the capabilities of the graphics processing systems have increased the complexity and execution time for the graphics processing programs has also increased. Therefore the amount of active state information that is maintained within the graphics processing system has also increased.

In order to complete a context switch the active state information is saved in memory and new state for a different context is loaded into the graphics processing system and processing is resumed using the new state. The time needed to unload and load the active state information reduces the processing throughput since the graphics processing system is unavailable during the unloading and loading operations. In some cases the unloading and loading operation may take as long as 500 microseconds. When each context requires a small amount of graphics processing the context switching time may exceed the time spent processing data. Alternatively the graphics processing pipeline may be drained to idle the pipeline so that the size of the active state information is minimized. However some graphic pipelines may be very deep i.e. have a high latency and require too long to drain necessitating a costly active context switch to change to a different context without draining the pipeline.

Accordingly what is needed in the art is a system and method for improving processing throughput when multiple graphics contexts are processed by reducing the context switching time.

Processing units are configured to report status to a front end monitoring unit in order to reduce the time needed to perform context switches. The front end monitoring unit issues a halt request in order to change contexts and each of the processing units enters a halted state as soon as possible after receiving the halt request. Each processing unit enters the halted state when the current state within the processing unit can be saved for processing at a later time. Once a processing unit is in the halted state a context switch may be performed for that processing unit by storing the current state information and loading state information for a different context. The processing units can be configured to enter the halted state quickly and when the current state is small to reduce the time needed to perform the context switch.

The unit status reporting protocol may also be used for debugging and performance monitoring. In particular any processing unit is in one of five states empty active stalled quiescent and halted. The state that a processing unit is in is reported to enable the front end monitoring unit to determine when a context switch may be performed or when a deadlock condition exists that prevents the processing units from processing data.

Various embodiments of a method of the invention for reporting status of a processing unit to a front end monitoring unit include asserting an active status signal from the processing unit to the front end monitoring unit asserting an empty status signal from the processing unit to the front end monitoring unit and asserting a stalled status signal from the processing unit to the front end monitoring unit. The active status signal is asserted when inputs are active outputs are produced a memory access is pending or the processing unit is processing data. The empty status signal is asserted when inputs are not active outputs are not produced a memory access is not pending and the processing unit is not processing data. The stalled status signal is asserted from the processing unit to the front end monitoring unit when inputs are active and the processing unit can process the inputs.

Various embodiments of the invention for a computing device configured to process multiple contexts include a front end monitoring unit that is coupled to a processing unit. The processing unit is configured to assert an active status signal from the processing unit to the front end monitoring unit assert an empty status signal from the processing unit to the front end monitoring unit and assert a stalled status signal from the processing unit to the front end monitoring unit. The active status signal is asserted when inputs are active outputs are produced a memory access is pending or the processing unit is processing data. The empty status signal is asserted when inputs are not active outputs are not produced a memory access is not pending and the processing unit is not processing data. The stalled status signal is asserted from the processing unit to the front end monitoring unit when inputs are active and the processing unit can process the inputs.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

Memory bridge which may be e.g. a Northbridge chip is connected via a bus or other communication path e.g. a HyperTransport link to an I O input output bridge . I O bridge which may be e.g. a Southbridge chip receives user input from one or more user input devices e.g. keyboard mouse and forwards the input to CPU via path and memory bridge . A parallel processing subsystem is coupled to memory bridge via a bus or other communication path e.g. a PCI Express Accelerated Graphics Port or HyperTransport link in one embodiment parallel processing subsystem is a graphics subsystem that delivers pixels to a display device e.g. a conventional CRT or LCD based monitor . A system disk is also connected to I O bridge . A switch provides connections between I O bridge and other components such as a network adapter and various add in cards and . Other components not explicitly shown including USB or other port connections CD drives DVD drives film recording devices and the like may also be connected to I O bridge . Communication paths interconnecting the various components in may be implemented using any suitable protocols such as PCI Peripheral Component Interconnect PCI Express PCI E AGP Accelerated Graphics Port HyperTransport or any other bus or point to point communication protocol s and connections between different devices may use different protocols as is known in the art.

An embodiment of parallel processing subsystem is shown in . Parallel processing subsystem includes one or more parallel processing units PPUs each of which is coupled to a local parallel processing PP memory . An instruction stream buffer that specifies the location of data and program instructions for execution by each PPU may be stored in each PP memory . In general a parallel processing subsystem includes a number U of PPUs where U 1. Herein multiple instances of like objects are denoted with reference numbers identifying the object and parenthetical numbers identifying the instance where needed. PPUs and PP memories may be implemented e.g. using one or more integrated circuit devices such as programmable processors application specific integrated circuits ASICs and memory devices.

As shown in detail for PPU each PPU includes a host interface that communicates with the rest of system via communication path which connects to memory bridge or in one alternative embodiment directly to CPU . In one embodiment communication path is a PCI E link in which dedicated lanes are allocated to each PPU as is known in the art. Other communication paths may also be used. Host interface generates packets or other signals for transmission on communication path and also receives all incoming packets or other signals from communication path and directs them to appropriate components of PPU . For example commands related to processing tasks may be directed to a front end unit while commands related to memory operations e.g. reading from or writing to PP memory may be directed to a memory interface . Host interface front end unit and memory interface may be of generally conventional design and a detailed description is omitted as not being critical to the present invention.

Each PPU advantageously implements a highly parallel processor. As shown in detail for PPU a PPU includes a number C of cores where C 1. Each processing core is capable of executing a large number e.g. tens or hundreds of threads concurrently where each thread is an instance of a program one embodiment of a multithreaded processing core is described below. A processing context encompasses a complete set of state through PPU while a thread may encompass only the state required to shade a single pixel. Threads run inside processing contexts one processing context might contain thousands of running threads. Cores receive processing tasks to be executed via a work distribution unit which receives commands defining processing tasks from a front end unit . Work distribution unit can implement a variety of algorithms for distributing work. For instance in one embodiment work distribution unit receives a ready signal from each core indicating whether that core has sufficient resources to accept a new processing task. When a new processing task arrives work distribution unit assigns the task to a core that is asserting the ready signal if no core is asserting the ready signal work distribution unit holds the new processing task until a ready signal is asserted by a core .

Cores communicate with memory interface to read from or write to various external memory devices. In one embodiment memory interface includes an interface adapted to communicate with local PP memory as well as a connection to host interface thereby enabling the cores to communicate with system memory or other memory that is not local to PPU . Memory interface can be of generally conventional design and a detailed description is omitted.

Cores can be programmed to execute processing tasks relating to a wide variety of applications including but not limited to linear and nonlinear data transforms filtering of video and or audio data modeling operations e.g. applying laws of physics to determine position velocity and other attributes of objects image rendering operations e.g. vertex shader geometry shader and or pixel shader programs and so on. PPUs may transfer data from system memory and or local PP memories into internal on chip memory process the data and write result data back to system memory and or local PP memories where such data can be accessed by other system components including e.g. CPU or another parallel processing subsystem .

Referring again to in some embodiments some or all of PPUs in parallel processing subsystem are graphics processors with rendering pipelines that can be configured to perform various tasks related to generating pixel data from graphics data supplied by instruction stream buffer via memory bridge and bus interacting with local PP memory which can be used as graphics memory including e.g. a conventional frame buffer instruction stream buffer texture maps and the like to store and update pixel data delivering pixel data to display device and the like. In some embodiments PP subsystem may include one or more PPUs that operate as graphics processors and one or more other PPUs that are used for general purpose computations. The PPUs may be identical or different and each PPU may have its own dedicated PP memory device s or no dedicated PP memory device s .

In operation CPU is the master processor of system controlling and coordinating operations of other system components. In particular CPU issues commands that control the operation of PPUs . In some embodiments CPU writes a stream of commands for each PPU to a pushbuffer not explicitly shown in that is specified by instruction stream buffer or and which may be located in system memory PP memory or another storage location accessible to both CPU and PPU . PPU reads the command stream from the pushbuffer and executes commands asynchronously with operation of CPU .

It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology including the number and arrangement of bridges may be modified as desired. For instance in some embodiments system memory is connected to CPU directly rather than through a bridge and other devices communicate with system memory via memory bridge and CPU . In other alternative topologies parallel processing subsystem is connected to I O bridge or directly to CPU rather than to memory bridge . In still other embodiments I O bridge and memory bridge might be integrated into a single chip. The particular components shown herein are optional for instance any number of add in cards or peripheral devices might be supported. In some embodiments switch is eliminated and network adapter and add in cards connect directly to I O bridge .

The connection of PPU to the rest of system may also be varied. In some embodiments PP system is implemented as an add in card that can be inserted into an expansion slot of system . In other embodiments a PPU can be integrated on a single chip with a bus bridge such as memory bridge or I O bridge . In still other embodiments some or all elements of PPU may be integrated on a single chip with CPU .

A PPU may be provided with any amount of local PP memory including no local memory and may use local memory and system memory in any combination. For instance a PPU can be a graphics processor in a unified memory architecture UMA embodiment in such embodiments little or no dedicated graphics PP memory is provided and PPU would use system memory exclusively or almost exclusively. In UMA embodiments a PPU may be integrated into a bridge chip or processor chip or provided as a discrete chip with a high speed link e.g. PCI E connecting the PPU to system memory e.g. via a bridge chip.

As noted above any number of PPUs can be included in a parallel processing subsystem. For instance multiple PPUs can be provided on a single add in card or multiple add in cards can be connected to communication path or one or more of the PPUs could be integrated into a bridge chip. The PPUs in a multi PPU system may be identical to or different from each other for instance different PPUs might have different numbers of cores different amounts of local PP memory and so on. Where multiple PPUs are present they may be operated in parallel to process data at higher throughput than is possible with a single PPU .

Systems incorporating one or more PPUs may be implemented in a variety of configurations and form factors including desktop laptop or handheld personal computers servers workstations game consoles embedded systems and so on.

Data assembler is a fixed function unit that collects vertex data for high order surfaces primitives and the like and outputs the vertex data to vertex processing unit . Vertex processing unit is a programmable execution unit that is configured to execute vertex shader programs transforming vertex data as specified by the vertex shader programs. For example vertex processing unit may be programmed to transform the vertex data from an object based coordinate representation object space to an alternatively based coordinate system such as world space or normalized device coordinates NDC space. Vertex processing unit may read data that is stored in PP memory through memory interface for use in processing the vertex data.

Primitive assembler receives processed vertex data from vertex processing unit and constructs graphics primitives e.g. points lines triangles or the like for processing by geometry processing unit . Geometry processing unit is a programmable execution unit that is configured to execute geometry shader programs transforming graphics primitives received from primitive assembler as specified by the geometry shader programs. For example geometry processing unit may be programmed to subdivide the graphics primitives into one or more new graphics primitives and calculate parameters such as plane equation coefficients that are used to rasterize the new graphics primitives. Geometry processing unit outputs the parameters and new graphics primitives to rasterizer . Geometry processing unit may read data that is stored in PP memory through memory interface for use in processing the geometry data.

Rasterizer scan converts the new graphics primitives and outputs fragments and coverage data to fragment processing unit . Fragment processing unit is a programmable execution unit that is configured to execute fragment shader programs transforming fragments received from rasterizer as specified by the fragment shader programs. For example fragment processing unit may be programmed to perform operations such as perspective correction texture mapping shading blending and the like to produce shaded fragments that are output to raster operations unit . Fragment processing unit may read data that is stored in PP memory through memory interface for use in processing the fragment data. Memory interface produces read requests for data stored in graphics memory decompresses any compressed data and performs texture filtering operations e.g. bilinear trilinear anisotropic and the like. Raster operations unit is a fixed function unit that optionally performs near and far plane clipping and raster operations such as stencil z test and the like and outputs pixel data as processed graphics data for storage in graphics memory. The processed graphics data may be stored in graphics memory for display on display device .

In a conventional system in order to switch contexts each processing unit a graphics processing pipeline is idled after any pending instructions and data are processed and a new context is loaded. Idling all of the processing units in the graphics pipeline take hundreds or thousands of clock cycles. Alternatively if the current context would take too long to execute and reach an idle point an active context switch may be performed by unloading the active context and loading the new context. Since the active context state is larger than the idle context state time needed to switch contexts is longer but it isn t necessary to reach an idle point. In either case it is desirable to minimize the number of clock cycles needed to perform the context switch since the latency negatively impacts the interactivity of an application program that uses the graphics processing capability.

The unit status reporting protocol allows each processing unit to transition to a halted state as quickly as possible in order to perform a context switch. The front end monitoring unit issues a halt command and performs the context switch based on the state reported by each processing unit. Reported status may also be used to detect and remove deadlock conditions. For example when a first processing unit in a pipeline is stalled and unable to output data to a second processing unit the front end monitoring unit may determine that a deadlock condition exists and reset the processing units. In another example the first processing unit may be in a quiescent state waiting to receive more data before initiating processing of data that has been received. When the first processing unit s waiting negatively impacts throughput of the pipeline the front end monitoring unit may determine that a deadlock condition exists and issue a command to force the first processing unit to resume processing of the data. Therefore the unit status reporting protocol may be used for various purposes to improve overall processing performance.

Each processing unit reports its status to front end through a second connection status reporting signals . A status signal connection included in status reporting signals encodes the state information for a processing unit indicating the status as empty active quiescent stalled or halted. The status reporting signals are generated by each processing unit at every clock cycle. Front end monitors the status signals and outputs commands to the processing units to perform context switches or to remove a deadlock condition.

Front end receives a context switch command and initiates the halt sequence in accordance with a halt sequencing protocol. Initially front end broadcasts a request to halt signal to all of the units in the core through a commands connection to issue a halt request. A halt request can be broadcast by generating an active signal and propagating it through commands . Other commands that may be issued via commands include a reset command and a resume processing command.

After a halt request has been issued and front end determines that all of the processing units are halted it continues to issue the halt request command to the processing units through commands to cause each processing unit to remain halted. Thereafter front end performs the context switch during which the current state information for each processing unit is stored in memory via memory interface and the state information of another process is retrieved from memory and the retrieved state information is restored to the processing units. In general the storing and restoring of the state information of the processing units are performed through front end but it is possible for these steps to be carried out directly by the processing units themselves. In some embodiments of the present invention each processing unit is configured to maintain state information for the current context and store state information for one or more additional contexts.

The connections for status reporting signals and context switch commands preferably have a pipelined distribution tree configuration that includes a number of amplifying stages between front end and each of the processing units. The number of amplifying stages is typically 3 or 4. Larger silicon die areas would require a larger number of amplifying stages.

Status reporting signals and commands may each comprise a multiple number of separate physical connections between front end and the processing units. Each separate physical connection may be implemented as one or more wires in sufficient number to encode the status of the processing unit as one of the five statuses and to encode the halt request reset and resume processing commands. Also in the preferred embodiment some processing units roll up or merge its status with the status of one or more other processing units and some do not. This can be done by performing an AND operation on the incoming status signal with its status signal and transmitting the signal resulting from the AND operation. As a result the number of separate connections between front end and the processing units is typically although it does not have to be greater than 1 and less than N where N is the number of processing units.

In some embodiments of the present invention commands is implemented with a separate connection between front end and each processing unit in order to provide a command to one or more processing units rather than all of the processing units. Separate connections allow for each processing unit to perform a context switch once that processing unit is halted rather than waiting for all of the processing units to be halted. Additionally separate connections allow for front end to command a quiescent processing unit to resume processing or to reset one or more of the processing units to eliminate a deadlock condition.

A deadlock condition exists when one or more of the processing units are unable to continue or resume processing inputs causing the other processing units in the pipeline to also become unable to continue processing inputs. For example a deadlock condition may exist when a downstream processing unit stalls an upstream processing unit preventing the upstream processing unit from outputting data. If the stall condition is not removed a deadlock condition exists. The stall condition may be removed under normal conditions when a memory request is completed when another downstream unit begins accepting inputs when processing of inputs completes when additional inputs are received from another upstream unit or the like.

A deadlock condition may also exist when a processing unit waits for one or more inputs before initiating processing. A processing unit may be configured to gather several inputs to process the inputs more efficiently in particular to generate a single memory access rather than generating several smaller memory accesses. When the processing unit holds off processing and remains in a quiescent state waiting for another input the processing unit can cause a deadlock condition. Such a deadlock condition can be identified by examining the unit status reporting signals and it can be removed by issuing a resume processing command to the quiescent processing unit.

When front end receives a context switch command from the host interface it initiates the halt sequence in accordance with the halt sequencing protocol by broadcasting a halt request command to all of the processing units through commands . A halt request command is broadcast to the processing units through commands when halt request command is asserted changes from a low to a high level . In response to the halt request command each processing unit proceeds to transition to the halted state as quickly as possible as described in conjunction with . Unit status indicates the state of one processing unit as a pre halted status . Pre halted status can be any of the five states e.g. empty active stalled quiescent and halted. When the processing unit has transition from pre halted status to halted status the processing unit is in the halted state and reports that status to front end as shown by halt status .

When front end determines that all of the processing units are halted it maintains the halt request command through commands in order to freeze the current state information of the processing units. Then front end performs the context switch as shown by context switch waveform. First the current state information of the processing units is stored in the memory as shown by context store . The current state information may be stored locally within each processing unit or in PP memory or system memory .

After the current state information is saved front end issues a reset command shown as reset of reset command . When the reset command is received by each processing unit the processing unit resets clearing any storage resources e.g. registers first in first out FIFO memories and the like within the processing units. Once a processing unit is reset processing cannot resume for the current state since the complete current state information is no longer available.

After the processing unit has been reset the state information of another context is retrieved from the memory and the processing unit is restored to the retrieved state information as shown by context restore . Once the halt request command is removed the processing unit enters the state that it was in when the restored context was stored and reports that state to front end as the unit status. When halt request command is negated the processing unit resumes processing of the restored state and reports post halted status to front end .

When a reset command is received a processing unit enters empty . Transitions between states are represented by arcs. Arcs and are followed when a reset command is received to transition from another state into empty . The processing unit remains in empty by following arc when the inputs to the processing unit are idle and either a halt request is not received or the processing unit cannot halt. The processing unit transitions to halted by following arc when a halt request is received. The processing unit transitions from empty to active by following arc when one or more of the inputs to the processing unit are active not idle and either a halt request or reset command is not received.

The processing unit remains in active by following arc when one or more of the inputs are active the processing unit is processing data the processing unit is outputting processed data or a memory access is pending. The memory access may be either a read or write request that has not been completed by either receiving the data or being acknowledged by memory interface . The processing unit transitions from active to empty by following arc when the inputs are idle the processing unit is no longer processing data no memory access is pending and no output is produced.

The processing unit transitions from active to stalled by following arc when a unit that receives an output from the processing unit i.e. a downstream unit is stalling the processing unit and the processing unit has an output for the unit that is blocking the processing of data. In some embodiments of the present invention processing units are configured to buffer outputs and several portions of processed data may be queued for output when a receiving unit cannot accept the processed data. In that case the processing unit enters stalled when the buffer is full the processing unit has more processed data for output and the processing unit cannot accept inputs or process any more data until the receiving unit accepts an output.

The processing unit remains in stalled by following arc when the processing unit cannot process data has processed data for output and the output of the processed data is blocked. The processing unit transitions from stalled to active by following arc when output of the processed data is no longer blocked or the processing unit is able to resume processing data. The processing unit transitions from stalled to halted by following arc when a halt request command is received and the processing unit can halt. A processing unit can halt when no memory accesses are pending and the state information can be stored without outputting any processed data to another unit and restored at a later time to resume processing the context. The processing unit transitions from active to halted by following arc when a halt request command is received and the processing unit can halt.

The processing unit transitions from active to quiescent by following arc when the inputs are not active and the processing unit has not completed processing of data. The processing unit may be waiting to receive additional inputs in order to continue processing data or may be waiting for a pending memory access to complete. The processing unit also transitions from active to quiescent when memory interface provides a negative acknowledgement NACK to the processing unit in response to a memory access request. A NACK may be produced when the processing unit requests an address that is not within the address space allocated to parallel processing subsystem .

The processing unit remains in quiescent by following arc when all of the inputs are idle. The processing unit transitions from quiescent to active by following arc when one or more inputs become active. The processing unit transitions from quiescent to halted by following arc when a halt request command is received and the processing unit can halt. The processing unit remains in halted by following arc as long as the halt request command is issued and the reset command is not issued. The processing unit transitions from halted to quiescent by following arc when the halt request command is removed the processing unit has pending data to process the inputs are idle and no outputs are produced. The processing unit transitions from halted to active by following arc when the halt request command is removed and one or more of the following occurs an input is active an output is produced or the processing unit has data that can be processed. The processing unit transitions from halted to empty by following arc when the halt request command is removed the processing unit has no data to process and no outputs are produced. In preferred embodiments of the present invention the processing units are configured to transition into halted state as soon as possible i.e. within 200 clock cycles.

If in step the processing unit determines that at least one input is not active then the processing unit proceeds to step and transitions to state halted . If in step the processing unit determines that it is not in state empty then in step the processing unit determines if it is in state active and if so the processing unit proceeds to step . Otherwise in step the processing unit determines if it is in state quiescent and if not then in step the processing unit determines if it is in state stalled . If in step the processing unit determines that it is in state stalled then the processing unit proceeds to step . Otherwise the processing unit proceeds to step . If in step the processing unit determines that it is in state quiescent then it also proceeds to step . In step the processing unit determines if it can halt and if not the processing unit remains in step . Otherwise the processing unit proceeds to step and transitions to state halted .

Once a processing unit is in halted the current context being executed by the processing unit and represented by the current state information can be stored in memory. State information pertaining to a different context to be executed after the context switch are read from memory and the processing unit is restored with the state information read from the memory. Therefore the halt state halted is useful to allow the processing units to context switch efficiently i.e. quickly and when the amount of memory needed to store the active state information for a context is minimized. Furthermore inclusion of a quiescent state quiescent ensures that a processing unit that is not active while waiting for additional inputs or a pending memory accesses is not mistakenly assumed to be empty by front end . The stalled state stalled allows for front end and software via device driver to determine if one or more of the processing units are deadlocked. Front end may be configured to removed the deadlock conditions and allow processing to resume.

The invention has been described above with reference to specific embodiments. Persons skilled in the art however will understand that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. One embodiment of the invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored. The foregoing description and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense.

