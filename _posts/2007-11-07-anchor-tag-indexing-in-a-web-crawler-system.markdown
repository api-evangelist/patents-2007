---

title: Anchor tag indexing in a web crawler system
abstract: Provided is a method and system for indexing documents in a collection of linked documents. A link log, including one or more pairings of source documents and target documents is accessed. A sorted anchor map, containing one or more target document to source document pairings, is generated. The pairings in the sorted anchor map are ordered based on target document identifiers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08484548&OS=08484548&RS=08484548
owner: Google Inc.
number: 08484548
owner_city: Mountain View
owner_country: US
publication_date: 20071107
---
This application is a divisional of U.S. application Ser. No. 10 614 113 filed Jul. 3 2003 now U.S. Pat. No. 7 308 643 entitled Anchor Tag Indexing in a Web Crawler System which is incorporated herein by reference in its entirety.

The present invention relates to the field of crawler systems for crawling a collection of linked documents such as hyperlinked documents stored on servers coupled to the Internet or in an intranet and in particular the present invention relates to a method and apparatus for indexing anchor tags and other information from documents other than the indexed document that contains a link to the indexed document.

Search engines provide a powerful source of indexed documents from the Internet that can be rapidly scanned. However as the number of documents in the Internet grows it takes ever longer time periods between the time when a page is crawled by a robot and the time that it can be indexed and made available to a search engine. Furthermore it takes ever longer time periods to replace or update a page once it has been indexed. Therefore what is needed in the art are systems and methods for crawling and indexing web pages to reduce the latency between the time when a web page is either posted or updated on the Internet and the time when a representation of the new or updated web page is indexed and made available to a search engine.

In addition to problems associated with the latency between the time the content of a web page changes and the time that content can be indexed the growth of the number of documents on the Internet poses additional challenges to the development of an effective search engine system. When a user submits a query to a search engine system he expects a short list of highly relevant web pages to be returned. Previous search engine systems when indexing a web page associate only the contents of the web page itself with the web page. However in a collection of linked documents such as resides on the Internet valuable information about a particular web page may be found outside the contents of the web page itself. For example so called hyperlinks that point to a web page often contain valuable information about a web page. The information in or neighboring a hyperlink pointing to a web page can be especially useful when the web page contains little or no textual information itself. Thus what is needed in the art are methods and systems of indexing information about a document the information residing on other documents in a collection of linked documents so as to produce an index that can return a list of the most highly relevant documents in response to a user submitted query.

The present invention addresses the problems alluded to above by enabling information from other documents in a collection of linked documents to be included in the part of an index that corresponds to a particular document. Some embodiments associate a subset of the text on a first web page containing a link to a second web page with an index entry corresponding to the second web page. This is advantageous where the first web page contains a more accurate description of the second web page than the text of the second web page itself. Additionally some types of web pages e.g. image files video files programs and so on contain little or no textual information that can be indexed by a text based index. For these types of pages textual information on pages linking to the page may be the only source of textual information about the page. Further advantages of this approach include the ability to index a web page before the web page has been crawled. Currently the collection of web pages residing on the Internet include pages whose content changes rapidly pages that are unpublished and republished frequently and multimedia pages that may have little or no textual content. The present invention which facilitates indexing information about a document that is not contained in the document itself enables more effective and efficient text based indexing systems for web search engines.

Some embodiments provide a method of processing information related to documents in a collection of linked documents. First a link log is accessed. The link log includes a plurality of link records. Each link record in turn includes a respective source document identifier corresponding to a respective source document address and a respective list of target document identifiers corresponding to respective target document addresses. A sorted anchor map is output. The sorted anchor map includes a plurality of anchor records. Each anchor record includes a respective target document identifier corresponding to a respective target document address and a respective list of source document identifiers corresponding to a respective list of source document addresses. The anchor records are ordered in the sorted anchor map based on their respective target document identifiers. In the collection of documents a document located at the source document address corresponding to an anchor record s source document identifier in the record s list of source document identifiers contains at least one outbound link the at least one outbound link pointing to a corresponding target document address. Additionally the target document address corresponds to the respective target document identifier for the anchor record. In some embodiments each anchor record in the sorted anchor map further comprises a respective list of annotations.

Some embodiments provide methods that include repeating the accessing and outputting so as to produce a layered set of sorted anchor maps. When a merge condition has been satisfied a subset of the layered set of sorted anchor maps is merged producing a merged anchor map. The merged anchor map includes a plurality of merged anchor map records each merged anchor record corresponding to at least one anchor record from the subset of the layered set of sorted anchor maps wherein the merged anchor records are ordered in the merged anchor map based on their respective target document identifiers.

Some embodiments further include outputting a sorted link map. The sorted link map includes a plurality of link map records. Each link map record includes a respective source document identifier and a respective list of target document identifiers. Some embodiments provide methods that further include repeating the accessing outputting a sorted anchor map and outputting a sorted link map so as to produce a layered set of sorted anchor maps and a layered set of sorted link maps. In some of these embodiments when a merge condition has been satisfied a subset of the layered set of sorted link maps is merged producing a merged link map. The merged link map includes a plurality of merged link map records. Each merged link record corresponds to at least one link record from the subset of the layered set of sorted link maps. The merged link records are ordered in the merged link map based on their respective source document identifiers.

Some embodiments provide a system for processing information about documents in a collection of linked documents. The system includes a link log and a global state manager configured to access the link log. The link log includes a plurality of link records. Each link record includes a respective source document identifier corresponding to a respective source document address and a respective list of target document identifiers corresponding to respective target document addresses. The global state manager is configured to output a sorted anchor map. The sorted anchor map includes a plurality of anchor records each anchor record comprising a respective target document identifier and a respective list of source document identifiers. The plurality of anchor records are ordered in the sorted anchor map based at least in part on their respective target document identifiers. Furthermore for at least one anchor record a document located at the source document address corresponding to a source document identifier in the list of source document identifiers contains at least one outbound link. The at least one outbound link points to a corresponding target document address. The target document address corresponds to the respective target document identifier for the at least one anchor record.

Some embodiments further provide a page ranker. The page ranker determines a PageRank or some other query independent relevance metric for a particular document based on the output of the global state manager.

Another aspect of the present invention provides a computer program product for use in conjunction with a computer system. The computer program product includes a computer readable storage medium and a computer program mechanism therein. The computer program mechanism includes 

a link log data structure the link log comprising a plurality of link records wherein each link record comprises a respective source document identifier corresponding to a respective source document address and a respective list of target document identifiers corresponding to respective target document addresses 

The global state manager module contains instructions for writing to the sorted anchor map data structure. The plurality of anchor records are ordered in the sorted anchor map data structure based at least in part on their respective target document identifiers. Furthermore the collection of linked documents is arranged such that for at least one anchor record a document located at the source document address corresponds to a source document identifier in the list of source document identifiers contains at least one outbound link. The outbound link points to a corresponding target document address. The target document address corresponds to the respective target document identifier for the at least one anchor record. Some embodiments further include an indexer module. The indexer module includes instructions for building an index of the collection of documents based at least in part on the contents of the sorted anchor map data structure.

Data structure for storing URLs. Referring to a three layer data structure is illustrated. Base layer of data structure comprises a sequence of segments . In one embodiment each segment comprises more than two hundred million uniform resource locations URLs . Together segments represent a substantial percentage of the addressable URLs in the entire Internet.

Periodically e.g. daily one of the segments is deployed for crawling purposes as described in more detail below. In addition to segments there exists a daily crawl layer . In one embodiment daily crawl layer comprises more than fifty million URLs. Daily crawl layer comprises the URLs that are to be crawled more frequently than the URLs in segments . In addition daily crawl layer comprises high priority URLs that are discovered by system during a current epoch.

In some embodiments data structure further comprises an optional real time layer . In some embodiments optional real time layer comprises more than five million URLs. The URLs in real time layer are those URLs that are to be crawled multiple times during a given epoch e.g. multiple times per day . For example in some embodiments the URLs in optional real time layer are crawled every few minutes. Real time layer also comprises newly discovered URLs that have not been crawled but should be crawled as soon as possible.

The URLs in layers and are all crawled by the same robots . However the results of the crawl are placed in indexes that correspond to layers and as illustrated in and described in more detail below. Layers and are populated by a URL scheduler based on the historical or expected frequency of change of the content of the web pages at the URLs and a measure of URL importance as described in more detail below.

URL Discovery. There are a number of different sources for the URLs used to populate data structure . One source of URLs is the direct submission of URLs by users to the search engine system. Another source of URLs is through discovery of outgoing links on crawled pages. A third source of URLs is through submissions e.g. time based submissions from third parties who have agreed to provide content. For example such third parties can give links as they are published updated or changed.

Automated time based submissions are made possible using technologies such as RDF Site Summary RSS and Resource Description Framework RDF . RSS is a protocol an application of XML that provides an open method of syndicating and aggregating Web content. Using RSS files a data feed can be created that supplies headlines links and article summaries from a Web site. RDF is a syntax for specifying metadata.

Before storage in data structure a URL and the content of the corresponding page is processed by a series of modules that are designed to ensure content uniformity and to prevent the indexing of duplicate pages. For example one such process is a URL rewrite module. The URL rewrite module strips or rewrites commands in URL statements that would produce undesirable output. For example if a URL includes a statement that inhibits output of a certain column the statement is stripped from the URL. In addition to examining the syntax of specific URLs there is a host duplicate detection module. The host duplicate detection module attempts to determine which hosts are complete duplicates of each other by examining incoming URLs.

Exemplary methods. An exemplary system and method by which the URLs in data structure are crawled is respectively illustrated in . The exemplary method illustrated in describes events that take place during a predetermined time period termed an epoch. Each time an epoch elapses a new epoch is initiated by re executing steps and of the process illustrated in . An epoch can be set to any length of time. In one embodiment an epoch is one day.

Step . In step URL scheduler determines which URLs will be crawled in each epoch and stores that information in data structure . Controller selects a segment from base layer for crawling. The selected segment is referred to herein as the active segment. Typically at the start of each epoch controller selects a different segment from base layer as the active segment so that over the course of several epochs all the segments are selected for crawling in a round robin style.

URL scheduler revises daily layer and optional real time layer by moving URLs to layers and from base layer or vice versa. The decision as to whether to add or remove URLs from daily layer and real time layer is based on information in history logs that indicates how frequently the content associated with the URLs is changing as well as individual URL page ranks that are set by page rankers . In some embodiments the determination as to what URLs are placed in layers and as opposed to layer is made by computing a daily score of the form daily score page rank URL change frequency

The mechanism by which URL scheduler obtains URL change frequency data is best understood by reviewing . When a URL is accessed by a robot the information is passed through content filters . Content filters among other things determine whether a URL has changed and when a URL was last accessed by a robot . This information is placed in history logs which are passed back to URL scheduler . By reviewing the log records for a particular URL each of which indicates whether the content of a URL changed since the immediately previous time the URL was crawled the URL schedule or other module can compute a URL change frequency. This technique is particularly useful for identifying URL s whose content i.e. the content of the page at the URL changes very infrequently or perhaps not at all. Furthermore the computation of a URL change frequency can include using supplemental information about the URL. For instance the URL scheduler may maintain or access information about web sites i.e. URL s whose content is known to change quickly.

A query independent score also called a document score is computed for each URL by URL page rankers . Page rankers compute a page rank for a given URL by considering not only the number of URLs that reference a given URL but also the page rank of such referencing URLs. Page rank data can be obtained from URL managers . A more complete explanation of the computation of page rank is found in U.S. Pat. No. 6 285 999 which is hereby incorporated by reference as background information.

URL history log can contain URLs that are not found in data structure . For instance the URL history log may contain log records for URL s that no longer exist. The URL history log may also contain log records for URL s that exist but that which the URL scheduler will no longer schedule for crawling e.g. due to a request by the website owner that the URL not be crawled due to objectionable content or for any other reasons .

In cases where URL scheduler determines that a URL should be placed in a segment of base layer an effort is made to ensure that the placement of the URL into a given segment of base layer is random or pseudo random so that the URLs to be crawled are evenly distributed or approximately evenly distributed over the segments. In some embodiments the fingerprint of the URL is used to achieve the random selection of a segment to place the URL. A fingerprint is for example a 64 bit number or a value of some other predetermined bit length that is generated from the corresponding URL by first normalizing the URL text for example converting host names to lower case and then passing the normalized URL through a fingerprinting function that is similar to a hash function with the exception that the fingerprint function guarantees that the fingerprints are well distributed across the entire space of possible numbers. In some embodiments the fingerprint modulus N where N is the number of segments in base layer e.g. fingerprint modulus in the case where there are 12 segments in base layer is used to select the segment in which to place a given URL. In some embodiments additional rules are used to partition URLs into a segment of base layer daily layer and real time layer .

In some embodiments it is not possible to crawl all the URLs in an active segment daily layer and real time layer during a given epoch. In one embodiment this problem is addressed using two different approaches. In the first approach a crawl score is computed for each URL in active segment daily layer and real time layer . Only those URLs that receive a high crawl score e.g. above a threshold value are passed on to the next stage URL managers . In the second approach URL scheduler refines an optimum crawl frequency for each such URL and passes the crawl frequency information on to URL managers . The crawl frequency information is then ultimately used by URL managers to decide which URLs to crawl. These two approaches are not mutually exclusive and a combined methodology for prioritizing the URLs to crawl based on both the crawl score and the optimum crawl frequency may be used.

In embodiments where a crawl score is computed URL scheduler determines which URLs will be crawled on the Internet during the epoch by computing a crawl score for each URL. Those URLs that receive a high crawl score e.g. above a predefined threshold are passed on to the next stage URL managers whereas those URLs that receive a low crawl score e.g. below the predefined threshold are not passed on to the next stage during the given epoch. There are many different factors that can be used to compute a crawl score including the current location of the URL active segment daily segment or real time segment URL page rank and URL crawl history. URL crawl history is obtained from URL history logs . Although many possible crawl scores are possible in one embodiment the crawl score is computed as crawl score page rank change frequency time since last crawl .

Additionally many modifications to the crawl score including cutoffs and weights are possible. For example the crawl score of URLs that have not been crawled in a relatively long period of time can be upweighted so that the minimum refresh time for a URL is a predetermined period of time such as two months.

In embodiments where crawl frequency is used URL scheduler sets and refines a URL crawl frequency for each URL in data structure . URL crawl frequency for a given URL represents the optimum crawl frequency or more generally a selected or computed crawl frequency for a URL. The crawl frequency for URLs in daily layer and real time layer will tend to be shorter than the crawl frequency of URLs in base layer . Crawl frequency for any given URL can range from a minute or less to a time period that is on the order of months. In one embodiment the optimal crawl frequency for a URL is computed based on the historical change frequency of the URL and the page rank of the URL.

In addition to other responsibilities URL scheduler determines which URLs are deleted from data structure and therefore dropped from system . URLs are removed from data structure to make room for new URLs that are to be added to data structure . In some embodiments a keep score is computed for each URL in data structure . The URLs are then sorted by this keep score and URLs that receive a low keep score are eliminated as newly discovered URLs are added to data structure . In some embodiments the keep score is the page rank of a URL that is determined by page rankers .

Step . In step URL managers receive the active segment as well as layers and from URL scheduler . In typical embodiments because of the computational demands imposed upon URL managers each manager is resident on its own dedicated server. Further in some embodiments real time layer is managed by a separate URL manager that holds all or a substantial portion of layer in high speed random access memory. The active segment and daily layers are partitioned into the remaining URL managers . Typically this partitioning is performed using a modulo function or similar function on the fingerprint values or a portion of a fingerprint value derived from each URL in the active segment and daily layers so as to partition these URLs into a set of approximately equal sets partitions . Each of these sets is assigned to a different URL manager of a plurality of URL managers .

In some embodiments data structure is partitioned and stored in a plurality of servers. In such embodiments this plurality of servers is represented by URL scheduler . URLs that are copied from the URL scheduler servers are distributed to the servers hosting the URL managers on a random basis. Further the number of servers used by URL scheduler and the number of servers that host URL managers are constrained such that they are relatively primed. That is there is no common divider between i the number of servers used by URL scheduler and ii the number of servers hosting URL managers . One example of a relatively primed topology is the case in which URL scheduler is partitioned across 11 servers and there are 13 servers for the URL managers .

When the number of servers between two stages of system is relatively primed a modulo function can be used to randomly distribute URLs to servers in the next stage. For example in one embodiment the fingerprint of each URL that is be copied from URL scheduler to URL manager is obtained. Next a modulus of the fingerprint or the modulus of a portion of the fingerprint or of a function of the fingerprint is taken. Specifically the modulus that is taken of the fingerprint is the number of possible destination servers. Thus in the example where URL scheduler is partitioned across 11 servers and there are 13 servers for URL managers then modulus of the URL fingerprint of a respective URL is used to determine which of the 13 servers the respective URL will be sent.

As discussed above in some embodiments real time layer is managed by a separate URL manager that holds all or a substantial portion of the real time layer in high speed random access memory. Thus in some embodiments the i number of servers that host URLs other than those from real time layer and ii number of servers that host portions of data structure other than real time layer are relatively primed e.g. 11 and 13 .

The use of constrained numbers of servers is used in various stages of system . For example in some embodiments the number of DupServers global state managers indexers and and page rankers is constrained so that they are hosted by relatively primed numbers of servers. Advantages of such a randomization scheme are at least two fold. First this scheme reduces the complexity of the computer code required to ensure random distribution of URLs across a plurality of servers at a given stage of system . Rather than relying on complex randomization schemes all that is required is computation of the URL fingerprint or a portion of the URL fingerprint modulus the number of servers in the next stage. Second the randomization itself ensures that if a server fails at any stage of system the data that is not processed due to the failure represents a randomized sample of the URLs in data structure rather than a complete category of URLs. Such randomization therefore reduces the affect that individual server failure has on system . When a server fails isolated URLs from across data structure may not get indexed or updated during a given epoch. The impact of a server failure on users of the search engine is less noticeable when the impacted URLs are randomized than when whole categories of URLs are impacted e.g. not indexed by a failure of a server in system . Thus the process illustrated in can progress even when a server at any given stage is working slower than other servers at the same stage or is in fact down altogether.

In state information for URLs is stored in a hash table . Each URL manager stores information about the URLs that have been assigned to the URL manager in the hash table which is stored in random access memory. The normalized hash of the URL fingerprint serves as the index to the table .

The storage of URLs in hash tables on each server hosted by a URL manager is advantageous because it provides a way of quickly accessing URL state information. For example to obtain state information for a particular URL all that is required is to look up the record having the hash value that corresponds to the hash of the URL. Such a lookup process is more efficient than searching through records of all the URLs held by all the URL managers for a desired URL.

Representative URL state information stored in hash tables includes the URL s fingerprint called the URL fingerprint URL page rank and the layer or to which the URL belongs. In some embodiments URL page rank is not stored in hash table but is stored in a data structure that is available to each URL manager . A wide range of additional URL state information can be stored in hash table including information that is obtained from or derived from status logs history logs and page rankers. Representative state information that can be stored in hash tables is described below.

Each of the URL managers other than the URL manager that hosts real time layer perform a variety of functions. For instance they scan link logs to discover new URLs. Link logs comprise all the links that are found on scanned web pages during the current epoch. URLs that have been seen by system before but have not been scheduled for crawl during the current epoch are ignored. Newly discovered URLs are added to a hash table of a URL manager . The hash table data structure provides an advantageous mechanism for quickly determining whether a table contains a URL discovered in a link log. Rather than scanning large lists of URLs to determine whether a URL in a link log is new the URL from the link log is simply hashed and a search is made for the resultant hash value in each hash table . If a URL is found in a link log and is not in any hash table it is added to the hash table of one of the URL managers .

Referring to URL managers scan status logs in order to update the state of URLs that have been delivered to URL server to be crawled. The URL server distributes the URLs to be crawled among the robot crawlers . Status logs are sequential logs that are generated by content filters . Content filters receive content i.e. retrieved documents from the robot crawlers . Status logs include the status of URLs that have been handed to URL server by URL manager during the current epoch. Status logs indicate whether a URL was successfully crawled by a robot. If the status for a URL says crawled then a URL manager knows that the URL has been crawled and work with the URL is finished until the crawl period for that URL has elapsed. Crawl status is stored in field in the exemplary hash table illustrated in . Likewise if status log returns a HTTP 404 value indicating that the URL does not exist then the URL manager knows that work with the URL is complete at least until the next crawl period. Alternatively if status log does not include a record of the URL or indicates that that the URL was busy when the web crawler robot attempted to access the URL than URL manager reschedules the URL for crawling. Still further if a status log indicates that content filter has deleted the URL the URL manager removes the URL from the respective hash table and the URL is no longer crawled.

In some embodiments the number of URLs hosted by URL managers exceeds the number of URLs than can be crawled during a given epoch and or there is a risk that URLs hosted by URL managers will be crawled on an infrequent basis during a given epoch. In such embodiments the URL status information maintained for URLs by URL managers can be used to ensure that to the extent possible the URLs matching select criteria are given high priority for crawling. In other words URL state information can be used to prioritize which URLs will be sent to URL server . Several different factors can be used to accomplish this prioritization such as URL crawl interval and URL page rank to name a few. URL managers obtain the page rank of URLs from page rankers . Page rankers maintain a persistent record of the page rank of URLs and actively update the page rank of URLs using link maps as described in more detail below. Crawl interval represents a target frequency that a URL should be crawled. For example if a URL has a crawl interval of two hours the URL manager will attempt to crawl the URL every two hours. Any number of criteria to be used to prioritize which URLs will be delivered upon request to URL server including URL characteristics such as the category of the URL. Representative URL categories include but are not limited to news URLs international URLs language categories e.g. French German Japanese etc. and file type categories e.g. postscript powerpoint pdf html . The URL characteristics for a URL may identify a plurality of URL categories to which the URL belongs.

Step . Periodically URL server makes requests from URL managers for URLs. In response URL managers provide URL server with URLs. In some embodiments URL server requests specific types of URLs from URL managers based on a policy e.g. eighty percent foreign URLs twenty percent news URLs that URL server is enforcing. URL managers are able to service such requests because of the URL state information they store for each URL in hash tables . Additionally URL server attempts to ensure that each URL manager contributes URL requests.

URL server distributes URLs from URL managers to robots to be crawled. Conceptually a robot is a program that automatically traverses the Web s hypertext structure by retrieving a document at a URL and recursively retrieving all documents that are referenced by the retrieved document. The term recursive as used here is not limited to any specific traversal algorithm. However in a system that retrieves and indexes billions of documents this simple recursive methodology is not workable. Instead each robot crawls the documents assigned to it by the URL server . The robot passes retrieved documents to the content filters which process the links in the downloaded pages from which the URL scheduler determines which pages are to be crawled. Robots are unlike normal web browsers such as Internet Explorer Microsoft Redmond Wash. . For instance when a robot retrieves the document at a URL it does not automatically retrieve content e.g. images embedded in the document through the use of object or other tags. Also in one embodiment the robots are configured to not follow permanent redirects . Thus when a robot encounters a URL that is permanently redirected to another URL the robot does not automatically retrieve the document at the target address of the permanent redirect.

In some instances URL server avoids overloading any particular target server not shown that is accessed by the robots . The URL server determines the maximum number of URL requests to be sent to any particular host. It does this by making a procedure call to a server called the host load server not shown . The host load server stores information for each known host server i.e. a server storing documents known to the search engine indicating the maximum request load to be imposed by entire search engine on the host server number and the portion of that load which is currently in use or reserved by robots. The URL server sends a load reservation request to the host load server requesting the right to send download requests to a specified host server and receives back an indication of the number of download requests that the URL server can allocate to a robot. In other words the URL server will be told how many URLs the URL server can send to a robot for downloading. Then URL server parcels out the proper number of URLs to robots in accordance with the load reservation granted to the URL server by the host load server. The robots take these URLs and download or at least attempts to download the documents at those URLs. When URL server runs out of URLs to process it requests more URLs from URL managers . Furthermore when a robot completes the process of downloading the set of URLs it received from the URL server the host load reservations made by the URL server are released. Alternately host load reservations are made for a fixed period of time and are automatically released upon the expiration of that fixed period of time.

Step . In step a plurality of robots crawl URLs that are provided to the robots by URL server . In some embodiments robots use a calling process that requires domain name system DNS resolution. DNS resolution is the process by which host names URLs are resolved into their Internet Protocol IP addresses using a database that provides a mapping between host names URLs and IP addresses. In some embodiments enhancements to known DNS resolution schemes are provided in order to prevent DNS resolution from becoming a bottleneck to the web crawling process in which hundreds of millions of URLs must be resolved in a matter of hours. One of these enhancements is the use of a dedicated local database that stores the IP addresses for URLs that have been crawled by system in the past which reduces the system s reliance on DNS servers on the Internet. This allows URLs that have been previously crawled by system to be pre resolved with respect to DNS resolution. The use of a local DNS resolution database enables a high percentage of the system s DNS resolution operations to be handled locally at very high speed. Only those URLs that are not represented on local DNS database e.g. because they have not been previously crawled are resolved using conventional DNS resources of the Internet. As a result the IP addresses of URLs are readily accessible when they are needed by a robot . Also the system presents a much lower load on the DNS servers that would otherwise be needed to perform DNS resolution on every URL to be crawled.

Robots use various protocols to download pages associated with URLs e.g. HTTP HTTPS gopher File Transfer Protocol etc. . Robots do not follow permanent redirects that are found at URLs that they have been requested to crawl. Rather they send the source and target i.e. redirect URLs of the redirect to the content filters . Referring to content filters take the redirect URLs and place them in link logs where they are passed back to URL managers . The URL managers in turn determine when and if such redirect URLs will be assigned to a robot for crawling. Robots do follow temporary redirects and obtain page information from the temporary redirects.

Step . Pages obtained from URLs that have been crawled by robots are delivered to the content filters . In typical embodiments there is more than one content filter in system because of the computational demands of the content filter . In step content filter sends information about each retrieved page to DupServer to determine if the document is a duplicate of other pages. In one embodiment the information sent to the DupServer about each page includes the URL fingerprint of the page the content fingerprint of the page the page s page rank and an indicator as to whether the page is source for a temporary or permanent redirect. When a duplicate is found the page rankings of the duplicate pages at other URLs are compared and the canonical page for the set of duplicate pages is identified. If the page presented to the DupServer is not the canonical page of the set of duplicate pages the content filter does not forward the page to the respective RTlog for indexing. Rather the content filter makes an entry for the page in the history log creates or updates an entry for the URL in the status log and then ceases work on the page. In effect a non canonical page is deleted from the search engine except for the entries in the history log and status log. In addition to identifying duplicate web pages DupServer assists in the handling of both temporary and permanent redirects encountered by the robots .

Examples of stages where the number of servers used to host the stage is constrained have been described. For example the number of servers used to host data structure is constrained relative to the number of servers used to host URL managers such that they are relatively primed. However there are examples in system in which the number of servers used to host a stage is not constrained such that it is relatively primed with respect to the number of servers used to host a prior or subsequent stage. The number of servers used to host content filters represents one such example. In other words the number of servers used to host content filters is not constrained such that it is relatively primed with respect to the number of robots . In fact in some embodiments the number of servers used to host content filters is a multiple of the number of servers used to host robots .

Step . In the embodiment illustrated in FIGS. and A B the content filters write out four types of log files link logs RTlogs or history logs and status logs . With the exception of those URLs that have been flagged as not being canonical pages i.e. not suitable for indexing by the DupServer URLs that have been crawled by robots are processed as described below. For those URLs that have been flagged as not suitable for indexing content filter will insert corresponding records in all RTlogs the appropriate link log and the history logs .

Referring to a link log contains one link record per URL document. A URL document is a document obtained from a URL by a robot and passed to content filter . Each record lists the URL fingerprints of all the links URLs that are found in the URL document associated with a record as well as the text that surrounds the link. For example text can state to see a picture of Mount Everest click here where the page identified by the link represents an image of Mount Everest. In one embodiment link log is partitioned or segmented across multiple servers typically using a modulo function or similar function on a fingerprint value or a portion of a fingerprint value associated with the URL so as to partition records across a plurality of servers so that content filter can append to the link logs at a very high bandwidth.

Referring to an RTlog stores the documents obtained by robots . Each document is coupled with the page rank that was assigned to the source URL of the document to form a pair . In other words if a document is obtained from URL XYZ then the document is paired with the page rank assigned to the URL XYZ and this pair is stored in an RTlog. As illustrated in there are three RTlogs one for each layer in data structure . That is there is an RTlog base for the active segment of base layer an RTlog daily for daily layer and an RTlog real time for real time layer . As in the case of link log each RTlog is partitioned or segmented typically using a modulo function or similar function on a fingerprint value or a portion of a fingerprint value associated with the source URL of each document so as to partition pairs across a plurality of servers so that data can be written to and read from the RTlogs at a very high bandwidth. Although not shown in some embodiments the RTlog includes the document URL or URL fingerprint.

Referring to a history log comprises a record for each URL that has been crawled by a robot . As illustrated in there are a wide range of possible fields that can be included in each record . One field is crawl status . Crawl status indicates whether the corresponding URL has been successfully crawled. Other field is the content checksum also known as the content fingerprint. When pages have identical content they will also have the same content fingerprint . URL scheduler can compare these content fingerprint with a previous content fingerprint obtained for the corresponding URL identified by URL fingerprint in the history log record on a previous crawl to ascertain whether the web page has changed since the last crawl. Similarly URL scheduler can use link checksum to determine whether any of the outbound links on the web page associated with the corresponding URL have changed since the last crawl. Source provides an indication of whether robot accessed the URL using the Internet or an internal repository of URLs. Time taken to download provides an indication of how long it took a robot to download the web page associated with the corresponding URL in the last crawl. Error condition records any errors that were encountered by a robot during the crawl. An example of an error condition is HTTP 404 which indicates that the web page does not exist.

Referring to the structure of a status log in accordance with one embodiment is described. There is a record for each URL that has been crawled by a robot . The record records the full URL associated with the record as well as a fingerprint of the corresponding URL . In the embodiment illustrated in status log further comprises crawl status and content checksum as described above in conjunction with . Further status log comprises the outgoing links that were identified in the web page associated with the URL during the crawl. The outgoing links comprise a list of the URL fingerprints of the URLs that are found in the web page. Further still status log has a duplicate status field that stores information about whether DupServer has identified the corresponding URL as a duplicate i.e. non canonical URL or not.

Step . In step indexers and obtain documents from the RTlogs on a high throughput basis and generate indices for those documents. When the indices are provided to the servers of the front end querying system not shown these documents become searchable by the users of the front end querying system.

Step . In step global state manager reads link logs and uses the information in the log files to create link maps and anchor maps . Link maps are keyed by the fingerprints of the source URLs in the link logs i.e. the URLs that respectively correspond to each record . The records in link map are similar to records in link log with the exception that text is stripped and the records are keyed by the fingerprint of the normalized value of the source URL. Link maps are used by page rankers to adjust the page rank of URLs within data structure . Such page rankings persists between epochs.

In addition to creating link maps global state manager creates anchor maps . In contrast to records in a link map records in an anchor map are keyed by the fingerprints of outbound URLs present in link log . Thus each record in an anchor map comprises the fingerprint of an outbound URL and the text that corresponds to the URL in link log . Anchor maps are used by indexer and to facilitate the indexing of anchor text as well as to facilitate the indexing of URLs that do not contain words. For example consider the case in which the target document at an outbound URL is a picture of Mount Everest and there are no words in the target document. However text associated with the URL states that To see a picture of Mount Everest view this link. Text although not in the target document at outbound URL indicates that the outbound URL has to do with Mount Everest . Thus indexers and use anchor maps to make associations between outbound URLs and text . These associations are used to index outbound URLs for searching by users in a front end search system not shown .

Referring to a collection of documents is depicted. The documents and may be one of any number of types of information that can be transmitted over a network including text files word processing files audio clips video clips and any other type of electronic data. The collection of documents made available to computers over the Internet in this way is commonly referred to as the World Wide Web the Web . Each document and in the collection is locatable via a respective document address. In embodiments where collection is all or part of the Web the respective document addresses are typically uniform resource locators URLs . In other embodiments the respective document addresses include other forms of network addresses. In still other embodiments the entire collection may reside on one computer system the respective document addresses including file system directory information.

In one embodiment the documents and in collection are available at URLs from one of the segments in base layer . In other embodiments the documents and in collection are available at URLs the URLs belonging to a subset of the segment in base layer . Such a subset is referred to as a partition of segment . In still other embodiments the documents and in collection are available at URLs the URLs belonging to daily crawl layer . In other embodiments the documents and in collection are available at URLs the URLs belonging to real time crawl layer . In a preferred embodiment collection includes a large subset of the documents available on the World Wide Web currently comprising approximately 3 billion documents.

Referring again to document contains a plurality of outbound links . Each outbound link points to a target document address typically the URL of a target document. For example link points to the URL of target document . Links are typically contained within a region of document known as an anchor tag. The structure and function of anchor tags are well known to those of skill in the art of hypertext markup language HTML composition and interpretation. Amongst other features anchor tag may include anchor text. Anchor text is contained in document near the URL associated with link . Typically the anchor text in anchor tag is delimited by the opening and closing markup tags and respectively.

The anchor text in anchor tag may contain useful information about document . For example the anchor text may include the statement this is an interesting website about cats. If document is unavailable for retrieval at the time crawling of collection is performed this anchor text provides textual information that can be searched by keyword. Document may be unavailable for crawling because the server on which it is hosted is not operational at the time of crawling the server on which it is hosted challenges the robot for a password or any number of other reasons. Additionally document may be an image file a video file or an audio file in which case there is no textual information readily available from the contents of document . So if the text from anchor tag is indexed as part of the indexing of document a user who submits a query containing the term cat may receive a list of documents including document . Another advantage of indexing the anchor text from anchor tag together with document occurs in cases where document contains more accurate information about document than the textual contents of document itself. For example document may be a relatively authoritative web page that contains text near or in an anchor tag associated with link stating that the server that hosts web page is frequently unavailable. Page may contain no text indicating that it is frequently unavailable. If page is successfully crawled and indexed a user of a search engine employing the index will have no way to learn of the potential unavailability of page unless information from page is returned in response to a query.

In addition to outbound links associated with document are inbound links . Relative to inbound link for example document is a target document. Thus source document includes link link pointing to the URL at which document resides. Document which is a source document relative to link may also contain an annotation in region of link . The annotation may also be an anchor tag. Although as depicted in source documents containing links pointing to the URL of target document contain only one link documents may contain any number of links

Processing link logs. depicts part of a web crawling system in accordance with some embodiments. As described previously in conjunction with the discussions of A and B above URL scheduler URL managers robots and content filters interact to produce link log . Link log includes one or more link log records . Each record includes a respective source document identifier a respective list of target document identifiers and preferably a respective list of annotations. For example record includes the source document identifier URL. In a preferred embodiment the source document identifier is a URL fingerprint also referred to as URL FP or simply as FP associated with the URL. A URL fingerprint is preferably a 64 bit integer determined by applying a hash function or other one way function to a URL. Record also contains a list of target document identifiers the list including URL URL and URL. Each of the target document identifiers are preferably represented by URL fingerprints in link log .

In a preferred embodiment each record in link log further includes a respective list of annotations . For example in record includes a list of annotations the list including annotations and . An annotation can include text from an anchor tag in the document at the source document address corresponding to the source document identifier URL. The text included in an annotation can be a continuous block of text from the source document in which case it is referred to as a text passage Annotations also in some embodiments include text outside the anchor tag in the document referred to by URL. For example a text passage for inclusion in annotation may be determined from text within a predetermined distance of an anchor tag in a source document. The predetermined distance could be based on a number of characters in the HTML code of the source document the placement of other anchor tags in the source document or any one of a number of other predefined criteria hereinafter called anchor text identification criteria.

In some embodiments annotations also include a list of attributes of the text they include. The list may contain one two or any number of entries. When the text in annotation is included in a source document that is composed in HTML examples of attributes include but are not limited to 

Other examples of attributes include text position number of characters in the text passage number of words in the text passage and so on.

Referring again to the relationship amongst the source document identifier URL the list of target document identifiers in record and the list of annotations in record will now be explained. URL may be the fingerprint of the URL at which document resides. In this case each entry in the list of target document addresses in record correspond to a link in document . Thus target document identifier URL may be the URL fingerprint of the URL to which link points. Similarly target document identifier URL may be the URL fingerprint of the document to which link points. Each entry in the list of annotations preferably also refers to a link in document . Thus annotation may contain text from the anchor tag associated with link . As a final example if source document identifier URL in record L from link log includes the URL fingerprint of the URL at which document resides URL will be the URL fingerprint of the URL at which document resides and annotation may include text from anchor tag in document .

Production of sorted link maps and sorted anchor maps. Referring to global state manager accesses a portion of link log . Global state manager then produces sorted link map M and sorted anchor map N. In some embodiments production of link map M and anchor map N do not occur simultaneously as explained in more detail in conjunction with the discussion of below. In a preferred embodiment the portion of link log that is accessed by global state manager is chosen by global state manager so that all records in portion can reside in the random access memory RAM of global state manager . For example portion may correspond the largest subset of records that require no more than 1 GB gigabyte of RAM to store. In this way the processing necessary to produce sorted anchor map N and sorted link map M can be performed rapidly as minimal access to secondary storage such as a hard disk drive in required by global state manage .

Referring to the structure of a sorted anchor map is depicted. Anchor map may contain annotations that include information from the anchor tags in source documents. The advantages of providing this information to indexer have been described above. Indexer in some embodiments is configured to build an index of the collection of documents based at least in part on the sorted anchor map. When the search engine receives a query not only can it search the contents of a document itself for one or more of the query terms it can also search any annotations associated with a document for one or more of the query terms.

Only one set of sorted anchor maps is maintained even in embodiments where the base layer daily layer and real time layer are crawled simultaneously. Sorted anchor map includes one or more anchor map records . Each record includes a respective target document identifier a respective list of source document identifiers and a respective list of annotations. For example record includes source document identifier . In a preferred embodiment target document identifier is a URL fingerprint of a corresponding URL. Identifier contains the URL fingerprint URL . Records are ordered in map in accordance with their respective target document identifiers. In some embodiments where the target document identifiers are URL fingerprints records are ordered so the target document identifier monotonically increases with the position of the record in the map. For example in these embodiments URL is greater than URL so record appears after record in map . In other embodiments where the target document identifiers are URL fingerprints records are ordered so the target document identifier monotonically decreases with the position of the record in the map. When indexers or not shown access map for information about a particular target document identifier not all records need to be searched in embodiments where records are ordered based on the target document identifier. For example binary search techniques can be used to quickly locate the record corresponding to the particular target document identifier.

Referring again to record further includes a list of source document identifiers including the entries URL URL . . . URL N. Each entry in the list of source document identifiers corresponds to a source document address. Additionally the source document residing at each of the corresponding source document addresses contains a link pointing to the target document residing at the URL corresponding to the target document identifier . Thus for example the document at the URL corresponding to URL contains a link pointing to the document corresponding to URL . The source document identifiers are preferably the URL fingerprints of their corresponding source document addresses.

In addition to the list of source document identifiers record includes a list of annotations . Each annotation is associated with an entry in the list of source document identifiers. For example annotation is associated with URL annotation with URL and so on. An annotation may contain text near or in an anchor tag in the source document corresponding to the associated source document identifier. For example when annotation contains the text what URL says about URL this text is found in the source document corresponding to URL fingerprint URL .

Sometimes annotation is a delete entry. For example annotation is a delete entry. A delete entry is generated by global state manager when it determines that a link no longer exists. For example global state manager may have written a record in sorted anchor map with target document identifier URL and a respective entry for source document identifier URL . Later manager by examining link log layered set of sorted link maps or both may determine that the document corresponding to URL no longer contains a link to the document corresponding to URL . To address this situation the global state manager is configured to generate a delete entry in the current sorted anchor map .

Referring to details of a sorted link map are depicted. Map includes one or more link map records . Each record includes a respective source document identifier. For example record contains source document identifier . The source document identifiers are preferably URL fingerprints. Thus source document identifier contains URL fingerprint URL . URL is the fingerprint of a URL of an associated source document. For example URL is the fingerprint of document . Each record further includes a list of target document identifiers. For example record contains a list of target document identifiers . Target document identifiers are preferably URL fingerprints. Continuing the example target identifiers each correspond to a link contained in source document corresponding to and identified by source document identifier URL . Thus document contains a link to document and there is a corresponding target document identifier that corresponds to the URL in link . In this case the URL in link has a URL fingerprint URL and document resides at this URL.

In addition to containing information about one or more source documents map is organized so as to make the link information readily accessible. Referring again to records are ordered in map based on their respective source document identifiers. In some embodiments where the source document identifiers are URL fingerprints records are ordered so the source document identifier monotonically increases with the position of the record in the map. For example in these embodiments URL is greater than URL so record appears after record in map . In other embodiments where the target document identifiers are URL fingerprints records are ordered so the source document identifier monotonically decreases with the position of the record in the map. When page rankers access map for information about a particular source document identifier not all records need to be searched in embodiments where records are ordered based on the source document identifier. For example binary search techniques can be used to quickly locate the record corresponding to the particular source document identifier.

Merging layered sorted maps. Referring to sorted link maps and are produced by global state manager . Although as depicted in link record sorter link map merger anchor sorter and anchor map merger are separate modules in state manager in other embodiments they may all be contained in the same module. In still other embodiments any number of modules may perform the functions of state manager with the duties of the various modules in divided in any number of ways thereupon.

Specifically sorted link maps and are produced by link record sorter module in global state manager whenever a link log flush condition is satisfied. The flush condition may be a function of the time since a last link map was produced the amount of new data present in the link log the amount of memory available to the global state manager i.e. memory available in the server that executes the global state manager or any combination or subset thereof. Link record sorter outputs each sorted link map at a specific time referred to as an associated production time. Each sorted link map is thus associated with a production time. The production time of a sorted link map may be stored explicitly in the map. In some embodiments the production time is implicitly stored by the position of a sorted link map in the layered set of sorted link maps. Thus sorted link map can be determined to have an associated production time that is earlier than that of sorted link map but later than that of sorted link map .

In addition to outputting sorted link maps link record sorter writes to anchor log . Anchor log includes a plurality of records. The records in anchor log have a format similar to that of the records in link log . Some records in anchor log includes a source document identifier a list of target document identifiers and a list of annotations. Records in anchor log can also contain a delete link entry or a delete node entry. A delete link entry includes a source document identifier a target document identifier and a special marker in the annotation field of the record indicating that all links between the source document identifier and the target document identifier are to be removed. Link record sorter generates a delete link entry when it encounters two records for a particular source document a portion of link log shown in and the two records contain contradictory information about the target documents of the particular source document. For example when a first and a second record both contain URL as the source document identifier the first record contains URL in the list of target document identifiers and the second record does not contain URL in the list of target document identifiers sorter may generate a delete link entry. If the first record is older appearing earlier in link log than the second record sorter generates a delete link entry in anchor log . The delete link entry contains URL URL and a special marker as the source document identifier the target document identifier and the annotation respectively. If on the other hand the first record is newer appearing later in log than the second record the link from URL to URL was published after the generation of the second record. Thus in this case sorter does not generate a delete link entry.

A delete node entry is generated by link record sorter when sorter determines based on the records in portion of link log that a target or source document has been removed altogether. For example if by comparing the records in link log it is determined that two links no longer exist both of which point to URL sorter determines in some embodiments that the document corresponding to URL has disappeared altogether and generates an appropriate delete node entry. Alternately the web crawler may receive information when attempting to download URL that the document no longer exists and this information may be inserted by the content filters into the link log . That information in the link log is then used by the global state manager to generate a delete node entry in a sorted link map. A delete node entry includes the document identifier of the document to be deleted and a special marker identifying the record as a delete node entry.

Referring back to when page rankers require information from the layered set of sorted link maps about a document with a particular source document identifier page rankers sometimes search each sorted link map in layered set . If more than one record containing the particular source document is found each record possibly from a different map page rankers must determine how to merge the information in the more than one record. In some embodiments page rankers simply take the most recent information available in set i.e. the respective list of target documents from the record containing the particular source document identifier in the sorted link map with the most recent production time. Other records that contain the particular source document identifier are disregarded. In one embodiment page rankers need not traverse all the maps in layered set to determine the information required about a particular source document. Rather page rankers traverse maps in order of descending recency. Thus map N is searched first followed by map N 1 map N 2 and so on. However in other embodiments all the maps are searched in parallel using parallel threads or servers because this is the fastest and most efficient way to search the sorted link maps. While this methodology may appear to use more resources it completes the search task faster which is of primary importance in a web crawler that crawls very large numbers of pages e.g. hundreds of billions of pages .

Once produced sorted link maps are not written to again. To prevent the amount of storage required for layered set from increasing indefinitely as new sorted link maps are added to set and to keep the access time for lookup of a particular source document identifier for example by page rankers from becoming too long a mechanism is needed to consolidate the information contained in older maps in layered set of link maps . Thus global state manager when a merge condition has been satisfied performs a merge operation on a subset of the maps in layered set . In some embodiments state manager may have a predetermined time schedule for determining when the merge condition has been satisfied and consequently when to perform merge operations. For example state manager may periodically merge subsets of maps . In other embodiments other criteria are used as part of the merge condition. For example state manager may find that the merge condition is satisfied any time the number of maps in set exceeds a predetermined number. As another example the merge condition may be satisfied any time state manager has been idle for a predetermined amount of time or predetermined number of processor cycles. The merge condition may also be a function of the amount of link data in the unmerged sorted link maps the amount of link data may vary from one set of sort link maps to another .

Referring to link map merger module part of global state manager performs a merge operation on a subset of the set of sorted link maps . Although in the subset contains the three maps and in other embodiments and under other conditions the subset could contain fewer or more maps. The set of sorted link maps that are merged are generally contiguous or neighboring maps within the layered set of sorted link maps . Furthermore the sorted link maps that are merged are preferably similar in size for example having sizes that are within a factor of 2 of each other. Merging large files with much smaller files is less efficient in terms of computational resources used than merging similarly sized files. Thus small link maps are merged with other small link maps and similarly sized larger merged link maps are merged with each other.

Merger outputs merged link map M 1 . Merged link map M 1 includes one or more records each record containing a source document identifier and list of target document identifiers. Each record in map M 1 contains the same document identifier as one or more records in the subset of sorted link maps. When more than one record in the subset exists for a particular source document identifier the most recent record is transferred to merged map M 1 . Additionally the merge operation may generate delete link and delete node entries in anchor log upon detecting contradictory information in two or more records in the subset for a particular source document identifier. Finally after generation of map M 1 is complete the link map merger outputs the merged link map to layered set and schedules the link maps in the merged subset in maps and for destruction at a later time. The merged link maps are retained temporarily to enable rollbacks and other recovery operations.

Still referring to sorted anchor maps and are produced by global state manager at various production times. The production times of each sorted anchor map may be stored explicitly or implicitly for example by ordering in the layered set of sorted anchor maps . Anchor sorter part of state manager outputs sorted anchor maps at their respective production times. When generating a sorted anchor map sorter reads all or a portion of anchor log and consolidates all of the information about a particular target document therein generating an anchor map record for the particular target document. Sorter repeats this process for all target document identifiers in anchor log generating a plurality of anchor map records. If there are any delete link or delete node entries for the particular target document they are processed if possible or left in the record. It is not possible to process a delete link entry for example when the portion of the anchor log selected for processing by sorter contains no other information about the referenced link. Finally sorter sorts all consolidated records based on the value of their respective target document identifiers and outputs a sorted anchor map .

Indexers access information in the set of sorted anchor maps and must access all maps containing a particular target document identifier. Referring back to when indexers require information from the layered set of sorted anchor maps about a document with a particular target document identifier indexers sometimes search each sorted anchor map in layered set . If more than one record containing the particular target document is found each record possibly from a different map indexers must determine how to merge the information in the more than one record. In a preferred embodiment indexers simply take all the information available in set i.e. the lists of target documents from all records containing the particular target document identifier in the sorted anchor map . In these embodiments indexers must traverse all the maps in layered set to determine the information required about a particular target document.

In some embodiments the indexers also access information in the set of sorted anchor maps corresponding to links to one or more duplicates of a page that is being indexed. In these embodiments the RTlog entry for a page contains a list of the URL fingerprints of a set of such duplicate pages for pages having duplicate pages . The list is preferably limited in size to have no more than K entries where K is a predetermined integer preferably having a value between 2 and 10. The indexers access the anchor text for the links pointing to each of the identified duplicate pages and index that anchor text as part of the process of indexing the page. As a result a wider range of anchor text is included in the text that is treated as being part of or associated with a page for purposes of indexing the content of the page. This is particularly useful for instance when one or more of the links to one or more of the non canonical pages has anchor text in a different language than the anchor text of the links to the canonical page.

Global state manager when an anchor merge condition has been satisfied performs a merge operation on a subset of the maps in layered set . In some embodiments state manager may have a predetermined time schedule for determining when the anchor merge condition has been satisfied and consequently when to perform anchor merge operations. For example state manager may periodically merge subsets of maps . In other embodiments other criteria are used as part of the anchor merge condition. For example state manager may find that the anchor merge condition is satisfied any time the number of maps in set exceeds a predetermined number or the amount of data in the maps exceeds a predefined threshold. As another example the anchor merge condition may be satisfied any time state manager has been idle for a predetermined amount of time or predetermined number of processor cycles. 

Referring to when global state manager detects that an anchor merge condition has been satisfied anchor map merger performs a merge operation on a subset and of layered set of sorted anchor maps . As a result anchor map merger produces merged anchor map N 1 . The anchor merge condition is not necessarily satisfied at the same time that the link merge condition related to set is satisfied. Anchor sorter outputs sorted anchor maps at specific times referred to hereafter as associated production times. Each sorted anchor map is thus associated with a production time. The production time of a sorted link map may be stored explicitly in the map. In preferred embodiments the production time is implicitly stored by the position of a sorted anchor map in the layered set of sorted anchor maps. Thus sorted anchor map can be determined to have an associated production time that is earlier than that of sorted anchor map but later than that of sorted anchor map .

Referring now to the details of an anchor merge operation are described. First a subset of sorted anchor maps are collected for merging. Although K such maps are shown in . . . K any number of two or more maps may be gathered for this purpose. In some embodiments all records containing a particular target document identifier in the subset of maps are accessed read and flagged. All such records are flagged so that information contained therein is not processed twice. Flagging can comprise actual deletion of the record in its corresponding map maintenance of a list for example in the anchor map merger in extrinsic to the maps or by any one of a number of other means as one skilled in the art of data structure design would readily appreciate. In the particular target document identifier is URL . The set of all records containing source document identifier URL include records from map record from map and record from map K. In record URL appears in the list of source document identifiers and some anchor text appears in a corresponding entry in the list of annotations. In record URL appears in the list of source document identifiers and a delete entry appears in a corresponding entry in the list of annotations. Because map is more recent than map the conflicting information about the link from URL to URL is resolved by not including URL in the list of source document identifiers in merged anchor map N 1 . Were map less recent than map the conflict would be resolved by inserting URL in the list of source document identifiers . Record in merged anchor map N 1 also contains for a given target source document pairing the most recent annotation found from amongst the subset of sorted anchor maps being merged.

Page Ranking Query independent relevance determination. Once the layered set of sorted link maps contains at least one sorted link map a page rank can be computed for one or more documents. Page rankers compute a page rank for a given document by considering not only the number of documents that contain links pointing to the URL at which the document resides but also the page rank of such documents. For example referring to documents . . . X all contain links pointing to the URL at which document resides. Thus the page rank of document depends on the number of links as well as the page ranks of documents . Each of the documents may contain other links not depicted that do not point to the URL of document . The page rank of document is given in some embodiments by the expression PR 1002 1 PR 1004 1 1004 1 PR 1004 2 1004 2 . . . PR 1004 1004 where PR n denotes the page rank of document n C n is the number of outgoing links in document n and d is a number in the range between 0 and 1. In some embodiments d is 0.85.

To compute the page rank of one or more documents based on the information contained in layered set of link maps it is not easy to employ the above expression directly. Thus in preferred embodiments the computation of page rank proceeds by starting with an initial page rank for each document computing for one or more records in one or more maps a partial page rank contribution from the source document to each of the target documents in the record and continuously updating the estimates of the page ranks of documents as new information becomes available from set . For example in the partial contribution of document to the document is PR 1012 1 PR 1002 1002 . The current estimate of the page rank of document at any time is simply PR 1012 1 1 PR 1012 1 where the sum is taken over all documents that are known to link to document .

Page rank data can also be obtained from URL managers . A more complete explanation of the computation of page rank is found in U.S. Pat. No. 6 285 999 which is hereby incorporated by reference in its entirety.

A computer system for anchor tag processing. In a preferred embodiment web page indexing system is implemented using one or more computer systems as schematically shown in . As will be appreciated by those of skill in the art search engine systems designed to process large volumes of queries may use more complicated computer architectures than the one shown in . For instance a front end set of servers may be used to receive and distribute queries among a set of back end servers that actually process the queries. In such a system the system shown in would be one of the back end servers.

The computer system will typically have one or more central processing units CPU s a network or other communications interface primary and secondary storage and one or more communication busses for interconnecting these components. Primary and secondary storage can include high speed random access memory and can also include non volatile memory such as one or more magnetic disk storage devices not shown . Primary and secondary storage can include mass storage that is remotely located from the central processing unit s . Primary and secondary storage or alternatively one or more storage devices e.g. one or more nonvolatile storage devices within storage includes a non transitory computer readable storage medium. The primary and secondary storage or the non transitory computer readable storage medium of storage preferably stores 

an operating system that includes procedures for handling various basic system services and for performing hardware dependent tasks 

a network interface module that is used for connecting the system to various other computers e.g. the page rankers and content filters in and for accessing and receiving associated log files e.g. link log in via one or more communication networks such as the Internet other wide area networks local area networks metropolitan area networks and so on and

a global state manager module configured to access a link log data structure and preferably including instructions for writing to a sorted link map and a sorted anchor map .

The primary and secondary storage or the non transitory computer readable storage medium of storage of the computer system may also store one or more of the following additional modules and data structures 

an indexer module for generating a real time index daily index base index or any subset or combination thereof 

However in some embodiments the index modules and these data structures or a subset thereof are stored on different servers than the server that executes the global state manager module . These servers are interconnected by a high speed communication network enabling the global state manager to efficiently perform its tasks despite the fact that the link log data it reads and or the sets of maps it generates are stored elsewhere.

Preferably link log data structure further includes one or more link log records . Each record preferably includes

Global state manager can include executable procedures sub modules tables and other data structures. In some embodiments global state manager includes instructions for detecting the satisfaction of a merge condition and executing a merge operation. The merge condition may depend on any number of temporal or storage considerations. In some embodiments global state manager further includes instructions for detecting the satisfaction of an anchor merge condition and executing an anchor merge operation. The anchor merge condition may depend on any number of temporal or storage considerations.

All references cited herein are incorporated herein by reference in their entirety and for all purposes to the same extent as if each individual publication or patent or patent application was specifically and individually indicated to be incorporated by reference in its entirety for all purposes.

The foregoing description for purposes of explanation used specific nomenclature to provide a thorough understanding of the invention. However it will be apparent to one skilled in the art that the specific details are not required in order to practice the invention. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated. Thus the foregoing disclosure is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings.

It is intended that the scope of the invention be defined by the following claims and their equivalents

